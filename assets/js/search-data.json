{"0": {
    "doc": "Configure ADF with private link",
    "title": "Configure ADF with private link",
    "content": "In this article, you will learn how to configure a private link service between Azure Data Factory (ADF) and CluedIn. Prerequisites . | CluedIn endpoint is configured to be private. For detailed instruction, see Internal load balancer. | Host name resolution is configured with private IP. For detailed instruction, see Host name resolution. | . If you have any questions, you can request CluedIn support by sending an email to support@cluedin.com (or reach out to your delivery manager if you have a committed deal). To configure private link between ADF and CluedIn . | Create a private link service to CluedIn private endpoint as described in Private link service with private endpoint. You may skip creating a private endpoint as we will use managed private endpoint from ADF. | In Azure Data Factory Studio, go to the Manage tab. In the Security section, select Managed private endpoints. | Select New. | Find and select the private link service. | Enter the following information: . | Name – enter cluedin-private-endpoint. | Account selection method – select From Azure subscriptions. | Azure subscriptions – enter the subscription of private link service created in step 1. | Private link service – enter the name of private link service created in step 1. | FQDN names – enter your company CluedIn host name. The following names are provided as an example: . | app.company.com . | clean.company.com . | cluedin.company.com . | . | . | Select Create. Once the managed private endpoint is created, wait for provisioning state to be Succeeded. | Go to the private link service created in step 1 and approve the managed private endpoint: . | Open the private link service created in step 1. | Go to Settings &gt; Private endpoint connections. | Check and approve the managed private endpoint. Make sure the private endpoint name matches [adf name].[managed private endpoint name]. | . Once approved, your ADF should gain connectivity towards CluedIn private endpoint. Next, create ADF pipeline using the Copy data or Data flow activity to send data to CluedIn. | . ",
    "url": "/microsoft-integration/adf-integration/private-link",
    
    "relUrl": "/microsoft-integration/adf-integration/private-link"
  },"1": {
    "doc": "Architecture overview",
    "title": "Architecture overview",
    "content": "Below is a high-level design of what gets deployed from the Azure Marketplace Application (AMA) wizard and what you can expect after installing the product. For a detailed design surrounding the networking, please visit Advanced network configuration . ",
    "url": "/deployment/architecture",
    
    "relUrl": "/deployment/architecture"
  },"2": {
    "doc": "Before you start",
    "title": "On this page",
    "content": ". | Guiding principles | Our take on MDM | Benefits of upgrading | . | Audience | Time to read | . | Business User, Data Project Lead | 3 min | . Before you start implementing your CluedIn project, take a moment to read this page and get to know us better. We believe that understanding our guiding principles and our approach to modern MDM will help you achieve outstanding results. ",
    "url": "/playbooks/before-you-start#on-this-page",
    
    "relUrl": "/playbooks/before-you-start#on-this-page"
  },"3": {
    "doc": "Before you start",
    "title": "Guiding principles",
    "content": ". Aim for better data, not perfect data. At CluedIn, our mission is to enhance the quality of your data. We promise to support you throughout your journey, and we ask that you trust our expertise in handling Master Data Management (MDM) projects that involve multiple sources and millions of records. Commit to collaboration. We understand that you have an in-depth knowledge of your data, but running CluedIn projects is our specialty, it is something we do all day, every day. This expertise allows us to provide you with the best possible support and guidance. Embrace the CluedIn approach. We recommend resetting your preconceptions about MDM and embracing the CluedIn methodology. While there may be a learning curve initially, once you get the hang of it, we are confident you will find the experience rewarding and beneficial. Trust in our experience. Trust our experience and let us guide you through the process. Our platform is designed to make your data management more efficient, agile, and scalable. Let’s work together to achieve your data management goals, ensuring better data quality and more effective use of your resources. ",
    "url": "/playbooks/before-you-start#guiding-principles",
    
    "relUrl": "/playbooks/before-you-start#guiding-principles"
  },"4": {
    "doc": "Before you start",
    "title": "Our take on MDM",
    "content": "We recommend a three-step approach to MDM. 1. Acknowledge challenges. MDM is complex. Ingesting data from 20 sources and handling 20 million records is no easy task. If it were, your data would already be in perfect shape. 2. Ensure organizational readiness. MDM is not just about having the right tools; it is also about how prepared your organization is to embrace change and prioritize data quality in decision-making. By reading these lines, it’s clear you already value data quality. However, we must also acknowledge that resistance is inevitable. Challenging the status quo is always hard, but you made the right choice by picking CluedIn, and we have strategies to overcome this resistance. 3. Overcome resistance with value. Resistance can come from various quarters—departments unwilling or unable to share data, technical challenges, firewalls, security, and permissions issues. These obstacles are normal, and we are experienced in dealing with them. The key to turning detractors into supporters lies in demonstrating value. ",
    "url": "/playbooks/before-you-start#our-take-on-mdm",
    
    "relUrl": "/playbooks/before-you-start#our-take-on-mdm"
  },"5": {
    "doc": "Before you start",
    "title": "Benefits of upgrading",
    "content": "We always recommend using the latest version of CluedIn. By upgrading to the latest version, you get maximum value at minimal risk. Here are the benefits of upgrading: . | New features – access the latest tools and functionalities designed to make your data management more effective. | Improved performance – experience faster processing, better efficiency, and enhanced stability. | Enhanced user experience – enjoy a more intuitive and user-friendly interface. | . We understand that upgrades can be daunting, and we want to reassure you that we’re here to help if anything goes wrong. In that case, we offer: . | Rollback options – we can always roll back to a previous version. | Backups – we maintain comprehensive backups to ensure your data is safe and can be restored when needed. | . ",
    "url": "/playbooks/before-you-start#benefits-of-upgrading",
    
    "relUrl": "/playbooks/before-you-start#benefits-of-upgrading"
  },"6": {
    "doc": "Before you start",
    "title": "Before you start",
    "content": " ",
    "url": "/playbooks/before-you-start",
    
    "relUrl": "/playbooks/before-you-start"
  },"7": {
    "doc": "Centralised Master Data Management",
    "title": "Centralised Master Data Management (MDM) with CluedIn — End-to-End Implementation Guide",
    "content": "Goal: Operate a single, governed “system of record” for master data (Customers, Suppliers, Products, etc.) with CluedIn at the core. Use Microsoft Power Apps to capture/maintain master data via forms and Power Automate to orchestrate validation, approvals, deduplication, and write-back — with full auditability, stewardship, and rollback in CluedIn. ",
    "url": "/centralised-master-data-management#centralised-master-data-management-mdm-with-cluedin--end-to-end-implementation-guide",
    
    "relUrl": "/centralised-master-data-management#centralised-master-data-management-mdm-with-cluedin--end-to-end-implementation-guide"
  },"8": {
    "doc": "Centralised Master Data Management",
    "title": "What You’ll Build (at a glance)",
    "content": ". | CluedIn Core . | Canonical entities &amp; relationships | Reference data (RDM), quality rules, matching, survivorship | Stewardship work queues, audit history, rollback | Exports/APIs to downstream apps | . | Power Platform Layer . | Power Apps: secure forms for create/update requests (new customer, supplier update, product introduction, etc.) | Power Automate: early validation, duplicate check, approvals (business + data stewardship), and commit to CluedIn | Optional Dataverse: staging + business workflow metadata | . | Operational Guardrails . | Least-privilege access, API tokens, environment separation | Monitoring &amp; alerts; runbooks for rollback and incident response | . | . ",
    "url": "/centralised-master-data-management#what-youll-build-at-a-glance",
    
    "relUrl": "/centralised-master-data-management#what-youll-build-at-a-glance"
  },"9": {
    "doc": "Centralised Master Data Management",
    "title": "Outcomes",
    "content": ". | A centralised MDM hub in CluedIn with golden records for in-scope domains. | Standardised, validated, deduplicated master data pushed to consuming systems. | Self-service create/update via Power Apps with Power Automate approvals. | Early validation and dedupe preview before anything hits production data. | Full lineage, audit, and rollback for every change. | . ",
    "url": "/centralised-master-data-management#outcomes",
    
    "relUrl": "/centralised-master-data-management#outcomes"
  },"10": {
    "doc": "Centralised Master Data Management",
    "title": "Prerequisites",
    "content": ". | CluedIn environment (non-prod + prod) with admin access. | Defined MDM scope &amp; ownership (data domains, data owners, stewards). | API access to CluedIn (see Administration → API Tokens). | Power Platform environment with permissions to create Power Apps, Power Automate flows, and (optionally) Dataverse tables. | A dedicated VM/server for heavy CluedIn jobs (ingestion, matching, toolkit), not a laptop (prevents timeouts). | . ",
    "url": "/centralised-master-data-management#prerequisites",
    
    "relUrl": "/centralised-master-data-management#prerequisites"
  },"11": {
    "doc": "Centralised Master Data Management",
    "title": "PART A — Establish the CluedIn MDM Foundation",
    "content": " ",
    "url": "/centralised-master-data-management#part-a--establish-the-cluedin-mdm-foundation",
    
    "relUrl": "/centralised-master-data-management#part-a--establish-the-cluedin-mdm-foundation"
  },"12": {
    "doc": "Centralised Master Data Management",
    "title": "Step A1 — Define Scope, Ownership, and SLAs",
    "content": ". | Pick domains for the first wave (e.g., Customer, Supplier, Product). | Assign RACI per domain (Data Owner, Steward, Approver). | Define SLAs for requests (e.g., new supplier in ≤ 1 business day). | Document mandatory fields, validation policies, and matching evidence (e.g., TaxID/DUNS for suppliers; GTIN/Brand+MPN for products). | . Deliverables: Scope doc, RACI matrix, SLA table, mandatory-field matrix. ",
    "url": "/centralised-master-data-management#step-a1--define-scope-ownership-and-slas",
    
    "relUrl": "/centralised-master-data-management#step-a1--define-scope-ownership-and-slas"
  },"13": {
    "doc": "Centralised Master Data Management",
    "title": "Step A2 — Model Canonical Entities &amp; Relationships",
    "content": ". | In Entity Explorer, create canonical types (e.g., Customer, Supplier, Product, Address, Contact, etc.). | Define attributes, data types, and constraints; keep variant vs base split when relevant (e.g., Product vs Variant). | Model relationships (edges) you’ll need for survivorship and governance (e.g., Supplier -&gt; Product, Customer -&gt; Address). | . Tip: Keep entity design stable; evolve via versioning, not breaking changes. ",
    "url": "/centralised-master-data-management#step-a2--model-canonical-entities--relationships",
    
    "relUrl": "/centralised-master-data-management#step-a2--model-canonical-entities--relationships"
  },"14": {
    "doc": "Centralised Master Data Management",
    "title": "Step A3 — Reference Data Management (RDM)",
    "content": ". | Load canonical sets (ISO country, currency, UoM, tax codes, product categories). | Build mappings from source codes → canonical codes. | Version and Publish approved sets. | Add rules for Find Closest Match where semantics matter (e.g., category alignment). | . Goal: 100% code mapping coverage to avoid downstream fragmentation. ",
    "url": "/centralised-master-data-management#step-a3--reference-data-management-rdm",
    
    "relUrl": "/centralised-master-data-management#step-a3--reference-data-management-rdm"
  },"15": {
    "doc": "Centralised Master Data Management",
    "title": "Step A4 — Data Quality Rules (Normalize + Validate)",
    "content": "Implement Data Part Rules to standardize before any matching: . | Names/text: trim, case, punctuation, diacritics | Emails: regex; Phones: E.164; Addresses: parsing + country ISO-2 | Identifiers: checksums/structure (VAT/DUNS/GTIN/IBAN) | RDM canonicalization (currency, UoM, tax code) | Tags: Standardized, InvalidEmail, InvalidPhone, UnknownCountry, etc. | . Outcome: Reliable inputs for deterministic matching. ",
    "url": "/centralised-master-data-management#step-a4--data-quality-rules-normalize--validate",
    
    "relUrl": "/centralised-master-data-management#step-a4--data-quality-rules-normalize--validate"
  },"16": {
    "doc": "Centralised Master Data Management",
    "title": "Step A5 — Matching &amp; Survivorship (Golden Records)",
    "content": ". | Configure blocking &amp; match keys per domain (e.g., Supplier uses TaxID/DUNS → high confidence). | Set thresholds: auto-merge (≥0.97), steward review (0.90–0.97). | Define Golden Record Rules (attribute precedence + tie-breakers). | Add actions: completeness scoring, verification flags, tags. | . Safety: Prove Unmerge and Undo on a sandbox dataset before live. ",
    "url": "/centralised-master-data-management#step-a5--matching--survivorship-golden-records",
    
    "relUrl": "/centralised-master-data-management#step-a5--matching--survivorship-golden-records"
  },"17": {
    "doc": "Centralised Master Data Management",
    "title": "Step A6 — Stewardship &amp; Governance",
    "content": ". | Configure work queues (invalids, duplicates, missing mandatory fields). | Enable History and train stewards on Undo and Unmerge. | Set Permissions (roles, least privilege; sensitive fields restricted). | . ",
    "url": "/centralised-master-data-management#step-a6--stewardship--governance",
    
    "relUrl": "/centralised-master-data-management#step-a6--stewardship--governance"
  },"18": {
    "doc": "Centralised Master Data Management",
    "title": "Step A7 — Exports &amp; Downstream Interfaces",
    "content": ". | Define export views for consuming systems (CRM, ERP, PIM, DW). | Start read-only, validate mappings, then move to authoritative sync. | Keep lineage columns (source IDs, timestamps, quality flags). | . ",
    "url": "/centralised-master-data-management#step-a7--exports--downstream-interfaces",
    
    "relUrl": "/centralised-master-data-management#step-a7--exports--downstream-interfaces"
  },"19": {
    "doc": "Centralised Master Data Management",
    "title": "PART B — Power Platform Intake: Forms, Early Validation, Workflow",
    "content": "Two patterns are common. Pick the one that fits your org (or mix them): . Pattern 1 — Direct-to-CluedIn: Power Apps → Flow → CluedIn API (no Dataverse). Pattern 2 — Dataverse Staging: Power Apps writes to a Dataverse table; Flow orchestrates validation + CluedIn upsert. ",
    "url": "/centralised-master-data-management#part-b--power-platform-intake-forms-early-validation-workflow",
    
    "relUrl": "/centralised-master-data-management#part-b--power-platform-intake-forms-early-validation-workflow"
  },"20": {
    "doc": "Centralised Master Data Management",
    "title": "Step B1 — Security &amp; Connectors",
    "content": "In CluedIn . | Generate a least-privilege API token (Administration → API Tokens). | Scope it to create/update the specific domain(s) and read for dedupe preview. | Store the token securely (Key Vault/Power Platform environment variable). | . In Power Platform . | Create a Custom Connector (or use HTTP actions): . | Base URL: your CluedIn API endpoint. | Auth: Bearer token header (or OAuth if your environment supports it). | . | Define operations you’ll call often (examples): . | POST /entities/{type}/validate (preflight validation) | POST /entities/{type}/staging (stage request) | POST /matching/preview (dedupe preview) | POST /workflows/{name}/trigger (optional) | GET /entities/{type}/{id} (read back) (Endpoint names vary by tenant/version; map to your API catalogue.) | . | . Tip: Store the token in a Power Platform connection reference or environment variable; never hardcode in flows. ",
    "url": "/centralised-master-data-management#step-b1--security--connectors",
    
    "relUrl": "/centralised-master-data-management#step-b1--security--connectors"
  },"21": {
    "doc": "Centralised Master Data Management",
    "title": "Step B2 — Power Apps: Build the Request Forms",
    "content": ". | App Type: Canvas app (start with “New Customer/Supplier/Product” form). | Data Model: Either write directly to CluedIn via custom connector or to a Dataverse staging table (recommended for audit + drafts). | Form Sections: . | Identity: legal name/trading name, TaxID/DUNS (B2B), GTIN/Brand/MPN (Product), key personal info (B2C policy). | Contact/Address: email/phone, parsed address fields (Line1/City/Region/Postcode/Country). | Reference Data: country/currency/UoM/category (driven by RDM). | Attachments (optional): documents for KYC/Onboarding (ID proof, contracts). | Notes &amp; Justification for business approval. | . | Client-side early checks (Power Apps) . | Regex for email/phone; country code in a picklist bound to canonical RDM. | Simple duplicate hint: as the user types TaxID/Domain/GTIN, call a search endpoint; surface “Possible match found” links. | Visualize completeness with a progress bar. | . | . UX tip: Use normalisers (uppercasing codes, trimming whitespace) in the form before submit. ",
    "url": "/centralised-master-data-management#step-b2--power-apps-build-the-request-forms",
    
    "relUrl": "/centralised-master-data-management#step-b2--power-apps-build-the-request-forms"
  },"22": {
    "doc": "Centralised Master Data Management",
    "title": "Step B3 — Power Automate: Early Validation Flow",
    "content": "Trigger: Power Apps (OnSelect) or Dataverse (OnCreate). Flow steps (detailed): . | Initialize context (request id, domain, requester, environment). | Compose payload (map form fields to CluedIn schema). | HTTP — Validation: call CluedIn preflight validation (or rules evaluate endpoint). | On fail → return errors to the app; highlight fields; set status = ValidationFailed. | . | HTTP — Dedupe Preview: call CluedIn matching preview with high-confidence keys. | If candidate matches exist (score ≥ review threshold), return list to the app for user review. | Optionally, halt and require steward review before continuing. | . | HTTP — Reference Matching (optional): use Find Closest Match for categories/payment terms etc. Keep original value for lineage. | Branch: . | If No risk (passes threshold + no candidates) → Auto-route to Approval. | If Medium risk (candidates exist) → Create a Steward Task in CluedIn; wait for decision. | If High risk (strong dupes or policy flags) → Reject with guidance. | . | Approval(s): . | Use Start and wait for an approval (Power Automate). | Approvers: Data Owner + Domain Steward (parallel or sequential). | Reminder/timeout policy aligned to SLA. | Store decision &amp; comments (Dataverse or in CluedIn notes). | . | Commit: . | HTTP — Upsert to Staging in CluedIn with RequestId and RequestedBy. | HTTP — Trigger Rules to normalize &amp; compute completeness. | HTTP — Run Matching (if not already) and Promote to Golden per survivorship. | . | Notify &amp; Return: . | Update Power Apps with result id, completeness score, and any steward actions. | Send Teams/Email confirmation (optional). | . | Error handling: . | Catch 4xx/5xx → log to Dataverse “IntegrationLog”, notify support channel, and surface user-friendly message. | . | . Best practices: . | Keep idempotency: pass a requestId; replays don’t create duplicates. | Use parallel branches for enrichment that doesn’t block approval. | . ",
    "url": "/centralised-master-data-management#step-b3--power-automate-early-validation-flow",
    
    "relUrl": "/centralised-master-data-management#step-b3--power-automate-early-validation-flow"
  },"23": {
    "doc": "Centralised Master Data Management",
    "title": "Step B4 — Power Automate: Update Requests (Change Management)",
    "content": "Repeat B3 for update scenarios: . | Lock sensitive fields (e.g., TaxID, DUNS) behind extra approval. | Pre-check: compare proposed changes vs current Golden; highlight breaking changes (e.g., category re-map). | Route to Data Owner for approval and Steward for quality checks. | Commit via CluedIn upsert with change reason and source evidence. | . ",
    "url": "/centralised-master-data-management#step-b4--power-automate-update-requests-change-management",
    
    "relUrl": "/centralised-master-data-management#step-b4--power-automate-update-requests-change-management"
  },"24": {
    "doc": "Centralised Master Data Management",
    "title": "Step B5 — Steward Collaboration &amp; Exception Handling",
    "content": ". | If dedupe preview returns ambiguous matches, the flow: . | Creates a CluedIn Steward Task referencing the request data. | Waits for the steward to Approve/Reject/Request changes. | Continues or exits based on steward outcome. | . | For policy exceptions (e.g., missing documents), branch to a conditional approval that requires additional evidence uploaded in the app. | . ",
    "url": "/centralised-master-data-management#step-b5--steward-collaboration--exception-handling",
    
    "relUrl": "/centralised-master-data-management#step-b5--steward-collaboration--exception-handling"
  },"25": {
    "doc": "Centralised Master Data Management",
    "title": "PART C — CluedIn Workflows Post-Commit",
    "content": " ",
    "url": "/centralised-master-data-management#part-c--cluedin-workflows-post-commit",
    
    "relUrl": "/centralised-master-data-management#part-c--cluedin-workflows-post-commit"
  },"26": {
    "doc": "Centralised Master Data Management",
    "title": "Step C1 — Rules &amp; Matching Execution",
    "content": ". | On upsert: run Data Part Rules, Find Closest Match (for lookups), then Matching with thresholds. | If auto-merge: promote to Golden and tag Verified. | If steward review required: record sits in Match Review queue. | . ",
    "url": "/centralised-master-data-management#step-c1--rules--matching-execution",
    
    "relUrl": "/centralised-master-data-management#step-c1--rules--matching-execution"
  },"27": {
    "doc": "Centralised Master Data Management",
    "title": "Step C2 — Golden Record Survivorship &amp; Publishing",
    "content": ". | Apply attribute precedence (ERP &gt; Registry &gt; CRM, etc.). | Compute CompletenessScore, VerificationFlags, and channel-specific Reachability. | Include lineage (SourceSystem, SourceRecordID) and request metadata for traceability. | Publish to downstream exports (CRM/ERP/PIM/DW) on schedule or ev | . ",
    "url": "/centralised-master-data-management#step-c2--golden-record-survivorship--publishing",
    
    "relUrl": "/centralised-master-data-management#step-c2--golden-record-survivorship--publishing"
  },"28": {
    "doc": "Centralised Master Data Management",
    "title": "Centralised Master Data Management",
    "content": " ",
    "url": "/centralised-master-data-management",
    
    "relUrl": "/centralised-master-data-management"
  },"29": {
    "doc": "Clean",
    "title": "Clean",
    "content": "Data cleaning helps you identify and correct any issues in your data: normalize value representations, transform capitalization, modify values, and more. After you’ve cleaned your data, you can use it to gain valuable insights that help you make more informed decisions. In this section, you will learn how to fix data quality issues through the use of clean projects. In addition, you will learn how to proactively prevent the same data quality issues by generating rules based on your actions in the clean project. The following diagram shows the basic steps of cleaning the data in CluedIn. This section covers the following areas: . | Creating a clean project – discover different options that you can use to create a clean project. | Managing a clean project – learn how to work with a clean project, fix data issues, and prevent the same issues from happening again. | Reference information about clean projects – find information about clean project statuses and get an understanding of the clean project workflow. | . ",
    "url": "/preparation/clean",
    
    "relUrl": "/preparation/clean"
  },"30": {
    "doc": "CluedIn in your data architecture",
    "title": "On this page",
    "content": ". | High-level data architecture | Fabric use case | Databricks use case | Azure Data Factory use case | Synapse use case | . In this article, we’ll explain the global way CluedIn fits into your organization’s data architecture. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#on-this-page"
  },"31": {
    "doc": "CluedIn in your data architecture",
    "title": "High-level data architecture",
    "content": "To begin with, CluedIn can connect with a variety of different systems and services, both to receive source data and to export golden records. The following diagram illustrates the most common source systems that can send data to CluedIn’s ingestion endpoints and the most common export systems that can receive golden records from CluedIn. The most common source systems include: . | Microsoft Fabric – an end-to-end analytics and data platform designed for enterprises that require a unified solution. It encompasses data movement, processing, ingestion, transformation, real-time event routing, and report building. | Azure Data Factory – Azure’s cloud ETL service for scale-out serverless data integration and data transformation. It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management. | Azure Synapse Analytics – a limitless analytics service that brings together enterprise data warehousing and Big Data analytics. It supports both SQL-based and Spark-based analytics, enabling a wide range of data processing and analysis tasks. | Databricks – a unified analytics platform that integrates with cloud data storage, allowing for efficient data processing and management. It provides a web interface where you can create a notebook that connects to Spark, allowing you to perform data transformation tasks. From the data architecture standpoint, the purpose of Databricks is similar to Microsoft Fabric since both platforms provide robust tools for managing and processing large volumes of data. | . These source systems usually retrieve data from data storages—data lake, delta lake, OneLake, or SQL Server. Then, they send the source data to CluedIn using an ingestion endpoint. Regardless of the source system, the pipelines for sending the source data to the ingestion endpoint in CluedIn are similar. Your data architecture might also include additional elements such as: . | Data storage services: . | Data Lake – a storage repository that holds a large amount of data in its native, raw format. | OneLake – a single, unified, logical data lake for the whole organization. OneLake comes automatically with every Microsoft Fabric tenant with no infrastructure to manage. Essentially, the purpose of Data Lake and OneLake is similar as they both serve as centralized repositories for storing large volumes of data. However, OneLake is specifically designed for integration within the Microsoft Fabric ecosystem. | . | Data processing services: . | Apache Spark – a distributed processing system designed for big data workloads. It provides a multi-language analytics engine for large-scale data engineering and data processing. Spark is integrated into Microsoft Fabric, which allows for fast data processing using notebooks. Spark can read data both from Data Lake and OneLake. | SQL Pool – provides scalable and high-performance SQL-based analytics capabilities. It integrates seamlessly Fabric Warehouse to enable comprehensive data processing and analytics workflows. | . | Subsystems in Fabric: . | Lakehouse – a data architecture platform for storing, managing, and analyzing structured and unstructured data in a single location. You can read the data stored in the Lakehouse through Spark. | Warehouse – a next-generation data warehousing solution within Microsoft Fabric designed to provide high performance and scalability while simplifying configuration and management.​ A Warehouse in Fabric is essentially a SQL Pool. | . | . These elements will be illustrated further down when we look at the use cases for each source system. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#high-level-data-architecture",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#high-level-data-architecture"
  },"32": {
    "doc": "CluedIn in your data architecture",
    "title": "Fabric use case",
    "content": "Microsoft Fabric can read data from data storages with the help of notebooks and send that data into CluedIn. Also, Fabric can read data from CluedIn and send it to data storages. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#fabric-use-case",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#fabric-use-case"
  },"33": {
    "doc": "CluedIn in your data architecture",
    "title": "Databricks use case",
    "content": "Databricks can read data from data storages with the help of notebooks and send that data into CluedIn. Also, Databricks can read data from CluedIn and send it to data storages. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#databricks-use-case",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#databricks-use-case"
  },"34": {
    "doc": "CluedIn in your data architecture",
    "title": "Azure Data Factory use case",
    "content": "Azure Data Factory can operate as a standalone product, being a source system for CluedIn. It also can be a part of Microsoft Fabric, providing data ingestion pipelines. Azure Data Factory can read data from data storages and send that data into CluedIn. Also, Azure Data Factory can read data from CluedIn and send it to data storages. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#azure-data-factory-use-case",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#azure-data-factory-use-case"
  },"35": {
    "doc": "CluedIn in your data architecture",
    "title": "Synapse use case",
    "content": "Azure Synapse Analytics can read data from data storages with the help of Spark and SQL Pool and send that data to CluedIn. ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#synapse-use-case",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture#synapse-use-case"
  },"36": {
    "doc": "CluedIn in your data architecture",
    "title": "CluedIn in your data architecture",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture",
    
    "relUrl": "/playbooks/data-engineering-playbook/cluedin-in-your-data-architecture"
  },"37": {
    "doc": "Householding Use Case",
    "title": "Householding MDM in CluedIn — Step-by-Step Implementation Guide",
    "content": "Householding MDM creates a reliable, privacy-aware view of the people who live together and their shared attributes (address, contact channels, preferences, relationships). This guide walks you through the end-to-end build in CluedIn: ingest → standardize → match/link (person &amp; address) → household formation → golden record → governance → publish. ",
    "url": "/householding#householding-mdm-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/householding#householding-mdm-in-cluedin--step-by-step-implementation-guide"
  },"38": {
    "doc": "Householding Use Case",
    "title": "Outcomes",
    "content": ". | One golden Household per dwelling (with effective dating for moves). | Linked entities: Household ↔ Person(s) ↔ Address(es) ↔ Accounts/Orders/Policies ↔ Preferences/Consents. | Accurate roles (e.g., Head, Partner, Dependent, Roommate) and membership periods. | Better targeting, deduplication, and compliance (opt-outs and legal notices aggregated). | . ",
    "url": "/householding#outcomes",
    
    "relUrl": "/householding#outcomes"
  },"39": {
    "doc": "Householding Use Case",
    "title": "Prerequisites",
    "content": ". | Source access: CRM, billing/subscriptions, e-commerce/loyalty, service/support, marketing lists, (optional) property/geo sources. | RACI: Ensure stewards have Accountable access to governed objects. | For heavy matching runs, use a dedicated server/VM (not a laptop) to avoid timeouts. | Agreed business rules for: . | How to define a “household” (shared address + time overlap ± surname/relationship evidence). | How to aggregate preferences/consents (“most restrictive wins” recommended). | . | . ",
    "url": "/householding#prerequisites",
    
    "relUrl": "/householding#prerequisites"
  },"40": {
    "doc": "Householding Use Case",
    "title": "Reference Data Model",
    "content": "Core entities . | Person (individual) | Address (standardized, geocoded; includes unit/apartment) | Household (grouping) | HouseholdMembership (Person ↔ Household with role &amp; date range) | Preference / Consent (per Person; optionally aggregated to Household) | Account / Order / Policy (linked for analytics) | . Key attributes (examples) . | Address: Line1, Line2/Unit, City, Region, Postcode, Country, GeoPoint, GeoID/PropertyID, DPV/AV status. | Household: HouseholdId (stable key), PrimaryAddressId, Type (Family, Shared, Multi-Family, Dorm), Size, Start/End dates. | Membership: Role, StartDate, EndDate, Evidence (AddressOverlap, Relationship, FinancialLink). | . ",
    "url": "/householding#reference-data-model",
    
    "relUrl": "/householding#reference-data-model"
  },"41": {
    "doc": "Householding Use Case",
    "title": "Step 1 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | CRM persons/contacts, service/install addresses, billing/ship addresses. | Marketing lists (email/phone), loyalty/e-com profiles. | Optional: property datasets (parcel/building IDs), geocoding service outputs. | Load ad-hoc spreadsheets for legacy segments. | . Capture provenance on ingest: SourceSystem, SourceRecordID, LoadTimestamp. ",
    "url": "/householding#step-1--connect--ingest-sources",
    
    "relUrl": "/householding#step-1--connect--ingest-sources"
  },"42": {
    "doc": "Householding Use Case",
    "title": "Step 2 — Define Entities &amp; Mappings",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create Person, Address, Household, HouseholdMembership, Preference/Consent. | Map source fields to canonical attributes (normalize naming; keep Unit/Apt explicit). | Persist Address.Raw alongside parsed components for lineage. | . ",
    "url": "/householding#step-2--define-entities--mappings",
    
    "relUrl": "/householding#step-2--define-entities--mappings"
  },"43": {
    "doc": "Householding Use Case",
    "title": "Step 3 — Address Standardization &amp; Geocoding",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to normalize addresses before matching: . | Parse Line1/Line2, extract Unit/Apt; normalize street types and directional suffixes. | Normalize Postcode/Region (ISO where applicable). | Geocode to GeoPoint; attach GeoID/PropertyID when available. | Validate deliverability (e.g., DPV/AV flags) and tag issues. | . Recommended tags: StandardizedAddress, InvalidAddress, MissingUnit, POBox. ",
    "url": "/householding#step-3--address-standardization--geocoding",
    
    "relUrl": "/householding#step-3--address-standardization--geocoding"
  },"44": {
    "doc": "Householding Use Case",
    "title": "Step 4 — Person Identity Resolution",
    "content": "Portal: Entity Matching → Person . | Blocking keys: (Email), (Phone), (LastName + DOB), (CustomerId). | High-confidence: exact email OR exact phone OR government/customer ID. | Medium: fuzzy name + same birthdate OR same email domain + strong address similarity. | Review in Data Stewardship; approve merges; Unmerge when needed. | . Avoid over-merging roommates with same last name but different emails/phones. ",
    "url": "/householding#step-4--person-identity-resolution",
    
    "relUrl": "/householding#step-4--person-identity-resolution"
  },"45": {
    "doc": "Householding Use Case",
    "title": "Step 5 — Address Deduplication",
    "content": "Portal: Entity Matching → Address . | Blocking: (Postcode + HouseNumber), (GeoID), (GeoPoint rounded). | High-confidence: exact normalized address (including Unit/Apt). | Candidate: same building (GeoID) + similar line components; send to stewardship. | Keep P.O. Boxes separate; never merge PO Box with street delivery points. | . ",
    "url": "/householding#step-5--address-deduplication",
    
    "relUrl": "/householding#step-5--address-deduplication"
  },"46": {
    "doc": "Householding Use Case",
    "title": "Step 6 — Household Formation Rules",
    "content": "Portal: Governance → Rules (Golden Record &amp; Data Part Rules) Create Household keys and membership logic: . | Household Key (example) HHKey = hash(GeoID or (Address.Line1+Line2+Postcode+Country) + CohabitationWindow) | Formation conditions (any of): . | ≥2 Person entities share the same normalized Address (including Unit) with overlapping residency periods. | A Person moves in (address change) to an existing household and shares a relationship (spouse/child) or financial link (shared account). | . | Membership periods: set StartDate when first evidence appears; set EndDate when address changes or account closes. | . Tags: CandidateHousehold, ConfirmedHousehold, SharedResidence, Dormitory, MultiUnit. ",
    "url": "/householding#step-6--household-formation-rules",
    
    "relUrl": "/householding#step-6--household-formation-rules"
  },"47": {
    "doc": "Householding Use Case",
    "title": "Step 7 — Roles &amp; Evidence",
    "content": "Portal: Data Stewardship → Household Review . | Assign roles per member: Head, Partner, Dependent, Roommate. | Evidence priority: Shared last name &amp; age gap (family), shared contract/account (financial), tenancy record, emergency contact links. | Store EvidenceType on HouseholdMembership. | . ",
    "url": "/householding#step-7--roles--evidence",
    
    "relUrl": "/householding#step-7--roles--evidence"
  },"48": {
    "doc": "Householding Use Case",
    "title": "Step 8 — Household Golden Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define how household-level attributes are chosen: . | Attribute | Precedence (high → low) | Tie-breaker | . | PrimaryAddress | Verified service/billing &gt; CRM mailing &gt; marketing | Most recent verified | . | PrimaryPhone/Email | Landline at address &gt; Head’s preferred &gt; shared contact | Consent state = granted | . | Language/Locale | Majority of members &gt; Head’s preference | Most recent update | . | HouseholdType | Evidence-based (Family &gt; Shared &gt; Dorm &gt; Multi-Family) | Steward override | . | Preferences/Consents | Most restrictive wins (opt-out propagates) | N/A | . | Compute HouseholdSize, HasChildren (derived), and CompletenessScore. | Tag DoNotContact at household if any member has DoNotContact=true (policy). | . ",
    "url": "/householding#step-8--household-golden-survivorship",
    
    "relUrl": "/householding#step-8--household-golden-survivorship"
  },"49": {
    "doc": "Householding Use Case",
    "title": "Step 9 — Consent &amp; Preference Aggregation",
    "content": "Portal: Rules (Data Part &amp; Golden) . | Household suppression: if any member opt-outs, set household DoNotContact=true. | Channel logic: e.g., suppress SMS if no members consent to SMS; allow email if any consenting adult (confirm legal stance). | Keep person-level consent for precision; use household-level only for targeting/ops. | . ",
    "url": "/householding#step-9--consent--preference-aggregation",
    
    "relUrl": "/householding#step-9--consent--preference-aggregation"
  },"50": {
    "doc": "Householding Use Case",
    "title": "Step 10 — Quality Scoring &amp; Dashboards",
    "content": "Portal: Data Quality → Profiling / Dashboards Track KPIs: . | Address deliverability %, unit capture rate, geocoding success. | Duplicate person rate, duplicate address rate. | Household coverage (people with a HouseholdId). | False-merge &amp; unmerge counts; membership with missing dates. | . ",
    "url": "/householding#step-10--quality-scoring--dashboards",
    
    "relUrl": "/householding#step-10--quality-scoring--dashboards"
  },"51": {
    "doc": "Householding Use Case",
    "title": "Step 11 — Governance, Security &amp; Lifecycle",
    "content": "Portal: Governance → Permissions / Retention / Policies . | Restrict access to sensitive person attributes (DOB, IDs). | Retention: keep ended memberships and prior addresses for N years (audit &amp; analytics). | Enable rollback: merges, household formation, and role changes captured in History (undo/restore supported). | . ",
    "url": "/householding#step-11--governance-security--lifecycle",
    
    "relUrl": "/householding#step-11--governance-security--lifecycle"
  },"52": {
    "doc": "Householding Use Case",
    "title": "Step 12 — Publish / Integrate",
    "content": "Portal: Exports . | CRM/Marketing: export Household table (id, primary address/contact, size, type) + HouseholdMembership for segmentation. | Service/Billing: publish golden addresses to reduce delivery failures. | Analytics/DW: wide conformed model (Person ↔ Household ↔ Address). | APIs/Eventing: emit household-changed events for downstream sync. | . Start read-only, validate mapping &amp; suppression behavior; then make CluedIn authoritative where appropriate. ",
    "url": "/householding#step-12--publish--integrate",
    
    "relUrl": "/householding#step-12--publish--integrate"
  },"53": {
    "doc": "Householding Use Case",
    "title": "Step 13 — Scheduling &amp; Operations",
    "content": ". | Schedule person/address matching, household refresh, consent aggregation, and exports. | Run heavy matching and rebuilds off-peak on a dedicated VM/server to avoid timeouts. | Alert on spikes: sudden household churn can indicate address parsing issues upstream. | . ",
    "url": "/householding#step-13--scheduling--operations",
    
    "relUrl": "/householding#step-13--scheduling--operations"
  },"54": {
    "doc": "Householding Use Case",
    "title": "Step 14 — Validate &amp; UAT",
    "content": ". | Sample 100–200 households across multi-unit buildings, single-family homes, dorms/aged-care. | Verify roles, membership dates, and suppression logic. | Business sign-off from CRM/Marketing/Service &amp; Privacy teams. | . ",
    "url": "/householding#step-14--validate--uat",
    
    "relUrl": "/householding#step-14--validate--uat"
  },"55": {
    "doc": "Householding Use Case",
    "title": "Go-Live Checklist",
    "content": ". | Address standardization + geocoding &gt; 95% success; unit capture validated. | Person &amp; Address matching thresholds tuned; unmerge tested. | Household formation rules approved; role assignment guidelines documented. | Consent aggregation (“most restrictive wins”) validated end-to-end. | Exports/APIs tested with downstream systems. | Permissions &amp; retention configured; audit/rollback verified. | Product Toolkit package built &amp; promoted from staging to prod (run on dedicated box with Accountable access). | . ",
    "url": "/householding#go-live-checklist",
    
    "relUrl": "/householding#go-live-checklist"
  },"56": {
    "doc": "Householding Use Case",
    "title": "Example Rules (Snippets)",
    "content": "Address Normalize (Data Part Rule) . | Condition: Address.Raw present | Actions: parse components; standardize abbreviations; set Address.Normalized; geocode; tag StandardizedAddress or InvalidAddress. | . Create Household Key . | Condition: Address.Valid=true | Action: HouseholdKey = hash(Address.Normalized+Postcode+Country); (optionally add Unit and GeoID). | . Membership Inference . | Condition: two Persons share HouseholdKey with residence overlap ≥ 60 days | Action: create HouseholdMembership for both with role defaults (Head for account holder; else Unknown); tag CandidateHousehold. | . Consent Aggregation . | Condition: any member DoNotContact=true | Action: set Household.DoNotContact=true; store AggregatedFrom=PersonIds. | . Golden Survivorship . | PrimaryAddress: verified service address &gt; billing address &gt; CRM; tie-break newest verified. | PrimaryPhone: shared landline &gt; Head’s preferred &gt; most recent; require consent. | . ",
    "url": "/householding#example-rules-snippets",
    
    "relUrl": "/householding#example-rules-snippets"
  },"57": {
    "doc": "Householding Use Case",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Merging different apartments: always include Unit/Apt or GeoID in address matching. | Dorms &amp; aged-care: many unrelated people at one address — require extra evidence (relationship/financial link) to form a household. | Surname-only logic: causes roommate conflation — use multi-evidence (address + email/phone + relationship). | PO Boxes: do not use for household formation; treat separately from street delivery points. | Churn from upstream formatting changes: lock down address normalization and monitor drift. | Running big rebuilds on laptops: use dedicated compute to prevent timeouts. | . ",
    "url": "/householding#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/householding#common-pitfalls--how-to-avoid-them"
  },"58": {
    "doc": "Householding Use Case",
    "title": "Success Metrics",
    "content": ". | Household precision ≥ 98% (low false merges); recall ≥ 95% for eligible addresses. | Mail bounce rate ↓; delivery success ↑ after golden address adoption. | Suppression accuracy ≥ 99% for household-level opt-outs. | Duplicate Person rate ↓; average time to household new mover ↓. | Unmerge rate trending down month-over-month after tuning. | . ",
    "url": "/householding#success-metrics",
    
    "relUrl": "/householding#success-metrics"
  },"59": {
    "doc": "Householding Use Case",
    "title": "Summary",
    "content": "Householding MDM in CluedIn depends on clean addresses, robust person/address matching, and clear household formation rules with dates and roles. Keep golden household attributes canonical, aggregate preferences conservatively, and validate across tricky dwelling types (multi-unit, dorms, aged-care) before go-live. ",
    "url": "/householding#summary",
    
    "relUrl": "/householding#summary"
  },"60": {
    "doc": "Householding Use Case",
    "title": "Householding Use Case",
    "content": " ",
    "url": "/householding",
    
    "relUrl": "/householding"
  },"61": {
    "doc": "Regulatory Compliance",
    "title": "Regulatory Compliance in CluedIn — Step-by-Step Implementation Guide",
    "content": "This guide shows how to implement a Regulatory Compliance program in CluedIn covering data discovery, classification, subject rights (DSAR/SAR), minimization, retention, lineage, and auditing. It’s designed to help you meet frameworks like GDPR, CCPA/CPRA, LGPD, and sector policies. ",
    "url": "/regulatory-compliance#regulatory-compliance-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/regulatory-compliance#regulatory-compliance-in-cluedin--step-by-step-implementation-guide"
  },"62": {
    "doc": "Regulatory Compliance",
    "title": "Outcomes",
    "content": ". | End-to-end visibility of where personal data lives, how it’s processed, and who can access it. | Automated classification, tagging, and policy enforcement (minimization, masking, retention). | Subject rights fulfillment (access/export/delete/restrict) with auditability. | Dashboards &amp; alerts for continuous compliance posture monitoring. | . ",
    "url": "/regulatory-compliance#outcomes",
    
    "relUrl": "/regulatory-compliance#outcomes"
  },"63": {
    "doc": "Regulatory Compliance",
    "title": "Prerequisites",
    "content": ". | Source access to systems that store personal data (CRM, ERP, Support, Product, Marketing, Data Warehouse, File Stores). | RACI set so the implementation team has Accountable access to governed objects. | A dedicated VM/server for heavy jobs (discovery scans, toolkit exports/imports). | . ",
    "url": "/regulatory-compliance#prerequisites",
    
    "relUrl": "/regulatory-compliance#prerequisites"
  },"64": {
    "doc": "Regulatory Compliance",
    "title": "Reference Compliance Model",
    "content": "Core entities . | Person (data subject), Customer, Employee, Prospect | Consent (purpose, lawful basis, status, timestamp) | ProcessingActivity (system/process, purpose, legal basis) | DataAsset (tables/files/collections with PII) | DSAR (request, identity proof, scope, due date, status) | . Tags &amp; Taxonomy (examples) . | Sensitivity: PII, SensitivePII, Financial, Health, Minor | Purpose: Marketing, Support, Billing, Analytics | Residency/Transfer: EU, US, RestrictedTransfer | Policy state: RetentionDue, DoNotSell, DoNotContact, MaskInUI | . ",
    "url": "/regulatory-compliance#reference-compliance-model",
    
    "relUrl": "/regulatory-compliance#reference-compliance-model"
  },"65": {
    "doc": "Regulatory Compliance",
    "title": "Step 1 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source Connect systems that likely contain personal data (CRM, support/ticketing, auth/IdP, billing, web/app events, marketing automation, data lake/warehouse, shared file stores). | Start with read-only access where possible. | Capture source metadata (owner, region, DPO contact) as attributes. | . ",
    "url": "/regulatory-compliance#step-1--connect--ingest-sources",
    
    "relUrl": "/regulatory-compliance#step-1--connect--ingest-sources"
  },"66": {
    "doc": "Regulatory Compliance",
    "title": "Step 2 — Data Discovery Scan",
    "content": "Portal: Compliance → Discovery Run automated scans to detect PII patterns (email, phone, national IDs, names, addresses). | Include unstructured stores (notes, PDFs, CSV drops) if available. | Tag assets with PII/SensitivePII and map them to ProcessingActivities. | . ",
    "url": "/regulatory-compliance#step-2--data-discovery-scan",
    
    "relUrl": "/regulatory-compliance#step-2--data-discovery-scan"
  },"67": {
    "doc": "Regulatory Compliance",
    "title": "Step 3 — Classification &amp; Tagging Rules",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to consistently classify records/fields: . | Pattern rules: detect emails, phones, tax IDs → tag PII. | Domain-specific rules: payroll/benefits → tag SensitivePII. | Source-based rules: marketing lists → tag Marketing. | Confidence scoring + tags: ClassifiedHigh/Medium/Low. | . Tip: Keep names short and consistent (e.g., ContainsPII, DoNotSell). ",
    "url": "/regulatory-compliance#step-3--classification--tagging-rules",
    
    "relUrl": "/regulatory-compliance#step-3--classification--tagging-rules"
  },"68": {
    "doc": "Regulatory Compliance",
    "title": "Step 4 — Build the Person Graph",
    "content": "Portal: Entity Matching → Person/Customer/Employee Link identities across systems to a single Person: . | Blocking keys: (email), (phone), (name+dob), (customerId). | High-confidence rules: exact email/phone; Medium: fuzzy name + same domain/country. | Review candidates in Data Stewardship; enable Unmerge for errors. | . This enables subject-centric compliance actions later. ",
    "url": "/regulatory-compliance#step-4--build-the-person-graph",
    
    "relUrl": "/regulatory-compliance#step-4--build-the-person-graph"
  },"69": {
    "doc": "Regulatory Compliance",
    "title": "Step 5 — Consent &amp; Purpose Management",
    "content": "Portal: Entity Explorer / Rules Model Consent with attributes (purpose, lawful basis, source, timestamp, expiry). | Rules to enforce consent: . | If Consent(purpose=Marketing).status != granted → tag DoNotContact. | If DoNotSell (CPRA) → exclude from ad/export segments; tag DoNotSell. | . | . ",
    "url": "/regulatory-compliance#step-5--consent--purpose-management",
    
    "relUrl": "/regulatory-compliance#step-5--consent--purpose-management"
  },"70": {
    "doc": "Regulatory Compliance",
    "title": "Step 6 — Data Minimization &amp; Masking",
    "content": "Portal: Governance → Rules (Golden Record &amp; Data Part) . | Masking rule examples: . | Mask PAN/IBAN except last 4 for UI: set MaskInUI=true + store hashed surrogate. | Drop free-text PII from analytics exports: remove or redact fields. | . | Minimization: . | For Analytics purpose, persist only hashed IDs; remove names/emails. | . | . ",
    "url": "/regulatory-compliance#step-6--data-minimization--masking",
    
    "relUrl": "/regulatory-compliance#step-6--data-minimization--masking"
  },"71": {
    "doc": "Regulatory Compliance",
    "title": "Step 7 — Retention Policies",
    "content": "Portal: Governance → Retention Policies . | Define policies by entity/purpose (e.g., Billing data = 7 years, Marketing leads = 24 months inactivity). | Action: Archive first; promote to Delete after validation. | Add tags on approaching expiry: RetentionDue. | . ",
    "url": "/regulatory-compliance#step-7--retention-policies",
    
    "relUrl": "/regulatory-compliance#step-7--retention-policies"
  },"72": {
    "doc": "Regulatory Compliance",
    "title": "Step 8 — DSAR / SAR Workflow (Subject Rights)",
    "content": "Portal: Compliance → Requests (DSAR) . | Intake: Create a DSAR entity with requester identity proof and scope (access/export/delete/restrict). | Locate: Use the Person graph to collect linked records across systems. | Assemble: Generate export package (JSON/CSV/PDF) excluding masked internal notes. | Delete/Restrict: Apply rules to remove or lock records (respect legal holds). | Audit: Log timestamps, handler, and evidence. | SLAs: Track due dates (e.g., GDPR 30 days); dashboard alerts for SLA breaches. | . ",
    "url": "/regulatory-compliance#step-8--dsar--sar-workflow-subject-rights",
    
    "relUrl": "/regulatory-compliance#step-8--dsar--sar-workflow-subject-rights"
  },"73": {
    "doc": "Regulatory Compliance",
    "title": "Step 9 — Data Lineage &amp; Auditability",
    "content": "Portal: Entity Explorer → History and Dashboards . | Ensure merges, overrides, masking, exports, and deletions are captured in History. | Maintain lineage from DataAsset → ProcessingActivity → Person. | . ",
    "url": "/regulatory-compliance#step-9--data-lineage--auditability",
    
    "relUrl": "/regulatory-compliance#step-9--data-lineage--auditability"
  },"74": {
    "doc": "Regulatory Compliance",
    "title": "Step 10 — Access Control &amp; RACI",
    "content": "Portal: Governance → Permissions . | Restrict SensitivePII fields; grant Accountable to stewards. | Create data-domain roles (DPO, Marketing, Support). | Enforce MaskInUI in views and exports. | . ",
    "url": "/regulatory-compliance#step-10--access-control--raci",
    
    "relUrl": "/regulatory-compliance#step-10--access-control--raci"
  },"75": {
    "doc": "Regulatory Compliance",
    "title": "Step 11 — Compliance Dashboards",
    "content": "Portal: Data Quality / Compliance Dashboards Track: . | % entities with PII classified | DSAR backlog &amp; SLA compliance | Records by purpose &amp; consent status | Retention due/overdue | Access/permission change logs | Cross-border transfer inventory | . ",
    "url": "/regulatory-compliance#step-11--compliance-dashboards",
    
    "relUrl": "/regulatory-compliance#step-11--compliance-dashboards"
  },"76": {
    "doc": "Regulatory Compliance",
    "title": "Step 12 — Publish/Integrate Controls",
    "content": "Portal: Exports . | Marketing suppression list: export DoNotContact/DoNotSell. | Analytics feed: export minimized Person view (no direct identifiers). | Security tooling: send AccessAudit events to SIEM. | ERP/CRM: sync consent flags and masked fields. | . Start read-only; validate mapping; then enable authoritative syncs. ",
    "url": "/regulatory-compliance#step-12--publishintegrate-controls",
    
    "relUrl": "/regulatory-compliance#step-12--publishintegrate-controls"
  },"77": {
    "doc": "Regulatory Compliance",
    "title": "Step 13 — Scheduling &amp; Operations",
    "content": ". | Schedule discovery rescans, consent refresh, retention checks, DSAR jobs. | Run heavy scans off-peak on a dedicated server/VM to avoid timeouts. | Alert on anomalies (sudden surge in PII detection, SLA breach risk). | . ",
    "url": "/regulatory-compliance#step-13--scheduling--operations",
    
    "relUrl": "/regulatory-compliance#step-13--scheduling--operations"
  },"78": {
    "doc": "Regulatory Compliance",
    "title": "Step 14 — Validate, UAT &amp; Promote",
    "content": ". | Test on 50–100 real subjects; verify consent enforcement, masking, and DSAR outputs. | Legal/DPO sign-off on survivorship, masking, and retention behaviors. | Package with Product Toolkit; ensure Accountable permissions; promote to staging → production. | . ",
    "url": "/regulatory-compliance#step-14--validate-uat--promote",
    
    "relUrl": "/regulatory-compliance#step-14--validate-uat--promote"
  },"79": {
    "doc": "Regulatory Compliance",
    "title": "Example Rules (Snippets)",
    "content": "Classify Email as PII . | Condition: Email matches pattern | Action: add tag PII; set MaskInUI=true. | . Enforce Do Not Contact . | Condition: Consent(Marketing).status != granted OR tag DoNotSell | Action: add tag DoNotContact; exclude from Marketing exports. | . Retention — Marketing Lead Inactivity 24m . | Condition: LastActivity &gt; 24 months ago AND Customer=false | Action: tag RetentionDue; after approval → Archive (then Delete). | . DSAR — Restrict Processing . | Condition: DSAR(type=Restrict).status = approved | Action: set ProcessingStatus=Restricted; mask identifiers in exports. | . ",
    "url": "/regulatory-compliance#example-rules-snippets",
    
    "relUrl": "/regulatory-compliance#example-rules-snippets"
  },"80": {
    "doc": "Regulatory Compliance",
    "title": "Go-Live Checklist",
    "content": ". | All high-risk DataAssets scanned and classified (PII, SensitivePII). | Person graph validated; false merges &lt; 2%. | Consent model enforced; suppression feeds tested. | Masking/minimization rules verified in UI and exports. | Retention jobs scheduled; archive → delete flow validated. | DSAR workflow tested (access/export/delete/restrict) with audit artifacts. | Permissions/RACI applied; sensitive fields restricted. | Dashboards and alerts configured; SIEM integration (if applicable). | Promotion package built with Product Toolkit on a dedicated box. | . ",
    "url": "/regulatory-compliance#go-live-checklist",
    
    "relUrl": "/regulatory-compliance#go-live-checklist"
  },"81": {
    "doc": "Regulatory Compliance",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Over-matching people by name only → require email/phone or multi-evidence. | Unscoped masking → accidentally remove needed analytics fields; prefer minimize over blanket delete. | Retention deletes re-ingested data → align ingestion filters with retention policies. | Consent not enforced downstream → wire suppression tags into every export and segment. | Running scans on laptops → use dedicated compute to avoid timeouts and throttling. | . ",
    "url": "/regulatory-compliance#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/regulatory-compliance#common-pitfalls--how-to-avoid-them"
  },"82": {
    "doc": "Regulatory Compliance",
    "title": "Success Metrics",
    "content": ". | ≥ 95% PII assets classified; 100% of sensitive stores inventoried. | DSAR SLA compliance ≥ 99%; end-to-end time ↓ month over month. | % of entities with valid consent for each purpose; suppression accuracy ≥ 99.5%. | Retention coverage (% entities under active policy) and overdue items = 0. | Reduction in access to sensitive fields (least-privilege trend). | . ",
    "url": "/regulatory-compliance#success-metrics",
    
    "relUrl": "/regulatory-compliance#success-metrics"
  },"83": {
    "doc": "Regulatory Compliance",
    "title": "Summary",
    "content": "By combining discovery, classification, consent enforcement, minimization/masking, retention, subject-rights workflows, lineage, and auditing, CluedIn gives you a repeatable, auditable framework for regulatory compliance. Start small, validate with real samples, and iterate rules until legal and operational stakeholders sign off. ",
    "url": "/regulatory-compliance#summary",
    
    "relUrl": "/regulatory-compliance#summary"
  },"84": {
    "doc": "Regulatory Compliance",
    "title": "Regulatory Compliance",
    "content": " ",
    "url": "/regulatory-compliance",
    
    "relUrl": "/regulatory-compliance"
  },"85": {
    "doc": "Data Analytics Use Case",
    "title": "MDM for Data Analytics in CluedIn — Step-by-Step Implementation Guide",
    "content": "This guide shows how to use CluedIn as the master data backbone for analytics: unify entities, standardize attributes, resolve identities, manage reference data, and publish analytics-ready conformed dimensions and facts to your data warehouse/BI stack. ",
    "url": "/data-analytics-use-case#mdm-for-data-analytics-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/data-analytics-use-case#mdm-for-data-analytics-in-cluedin--step-by-step-implementation-guide"
  },"86": {
    "doc": "Data Analytics Use Case",
    "title": "Outcomes",
    "content": ". | Trusted Golden Records for core dimensions (Customer, Product, Supplier, Location, Employee). | Standardized reference data (currencies, UoM, tax codes, categories) with cross-system mappings. | Analytics-ready conformed dimensions (with surrogate keys and optional SCD Type 2 history). | Clean facts linked to conformed dimensions (optional), with quality flags and lineage. | Automated exports to the warehouse (e.g., Snowflake/BigQuery/Databricks) and semantic layer. | . ",
    "url": "/data-analytics-use-case#outcomes",
    
    "relUrl": "/data-analytics-use-case#outcomes"
  },"87": {
    "doc": "Data Analytics Use Case",
    "title": "Prerequisites",
    "content": ". | Access to in-scope sources (ERP/CRM/PIM/e-commerce/support/finance + spreadsheets). | Target analytics design agreed (star/schema, mandatory attributes, SCD policy, surrogate key strategy). | RACI: stewards/owners have Accountable access to governed objects. | A dedicated VM/server for heavy ingestion, matching, and toolkit operations (avoid laptops). | . ",
    "url": "/data-analytics-use-case#prerequisites",
    
    "relUrl": "/data-analytics-use-case#prerequisites"
  },"88": {
    "doc": "Data Analytics Use Case",
    "title": "Reference Model",
    "content": "Master Entities (dimensions) . | Customer, Product, Supplier, Location, Employee (extend as needed) | . Reference Data (RDM) . | Currency, UoM, Country/Region, Category/Taxonomy, PaymentTerms, Channel | . Analytics Views (publish) . | DimCustomer, DimProduct, DimSupplier, DimLocation, DimEmployee | Optional facts: FactOrder, FactInvoice, FactSubscription (linked using surrogate keys) | . History Policy . | Type 1 (overwrite) for non-analytical changes; Type 2 (row versioning) for attributes you want to trend over time. | . ",
    "url": "/data-analytics-use-case#reference-model",
    
    "relUrl": "/data-analytics-use-case#reference-model"
  },"89": {
    "doc": "Data Analytics Use Case",
    "title": "Step 1 — Define Analytics Scope &amp; KPIs",
    "content": "Portal: Governance → Policies . | List target dashboards/models (e.g., Revenue, Retention, Supply Chain, Merchandising). | For each, identify conformed dimensions required and mandatory attributes. | Decide SCD policy per dimension (Type 1 vs Type 2 fields). | Capture business definitions (data contracts) for key metrics. | . ",
    "url": "/data-analytics-use-case#step-1--define-analytics-scope--kpis",
    
    "relUrl": "/data-analytics-use-case#step-1--define-analytics-scope--kpis"
  },"90": {
    "doc": "Data Analytics Use Case",
    "title": "Step 2 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | Connect all contributing systems (ERP/CRM/PIM/etc.). | Capture provenance (SourceSystem, SourceRecordID, IngestedAt). | Start with samples; validate shapes; then schedule full loads. | . ",
    "url": "/data-analytics-use-case#step-2--connect--ingest-sources",
    
    "relUrl": "/data-analytics-use-case#step-2--connect--ingest-sources"
  },"91": {
    "doc": "Data Analytics Use Case",
    "title": "Step 3 — Standardize &amp; Normalize Inputs",
    "content": "Portal: Governance → Rules (Data Part Rules) Create normalization rules so downstream matching and BI are reliable: . | Text: trim, case, punctuation, HTML strip, diacritics. | Addresses: parse components; ISO-2 for Country; optional geocoding. | Identifiers: validate VAT/ABN/DUNS/GTIN/IBAN; normalize MPN/SKU. | Phones/Emails: E.164 phone; email regex + canonicalization (policy-dependent). | Units &amp; Currency: convert to canonical (RDM) and store both raw + canonical. | . Tags: Standardized, InvalidFormat, LegacySource, NeedsStewardReview. ",
    "url": "/data-analytics-use-case#step-3--standardize--normalize-inputs",
    
    "relUrl": "/data-analytics-use-case#step-3--standardize--normalize-inputs"
  },"92": {
    "doc": "Data Analytics Use Case",
    "title": "Step 4 — Reference Data Management (RDM)",
    "content": "Portal: RDM (or Entity Explorer + Rules) . | Load canonical code sets (ISO country, ISO 4217, UoM, tax codes, product categories). | Build mappings from source codes → canonical values (CodeMapping). | Version and publish approved sets; validate 100% mapping coverage. | . ",
    "url": "/data-analytics-use-case#step-4--reference-data-management-rdm",
    
    "relUrl": "/data-analytics-use-case#step-4--reference-data-management-rdm"
  },"93": {
    "doc": "Data Analytics Use Case",
    "title": "Step 5 — Identity Resolution (Deduplicate &amp; Link)",
    "content": "Portal: Entity Matching Configure per domain: . | Customer: TaxID/Email/Phone; Name+Address fuzzy with country guard. | Product: GTIN or Brand+MPN; FuzzyTitle + Category as candidate only. | Supplier: TaxID/DUNS; Name+Country; Email domain as supplemental. | Location: Address normalization + GeoID/GeoPoint. | . Run matching; review in Data Stewardship; approve merges; Unmerge errors. ",
    "url": "/data-analytics-use-case#step-5--identity-resolution-deduplicate--link",
    
    "relUrl": "/data-analytics-use-case#step-5--identity-resolution-deduplicate--link"
  },"94": {
    "doc": "Data Analytics Use Case",
    "title": "Step 6 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define attribute precedence and tie-breakers per entity (example): . | Attribute | Precedence (high → low) | Tie-breaker | . | Legal/Primary | ERP &gt; Registry &gt; CRM | Most recent verified | . | Contact fields | CRM &gt; ERP &gt; Support | Consent state | . | Category/Taxon | RDM canonical &gt; source | Steward override | . | Identifiers | Verified source only | N/A | . | Descriptions | PIM/Tech &gt; ERP &gt; e-com | Richest (length + spec coverage) | . Compute CompletenessScore; tag AnalyticsReady when mandatory fields are present. ",
    "url": "/data-analytics-use-case#step-6--golden-record-survivorship",
    
    "relUrl": "/data-analytics-use-case#step-6--golden-record-survivorship"
  },"95": {
    "doc": "Data Analytics Use Case",
    "title": "Step 7 — Conformed Dimensions (Analytics Views)",
    "content": "Portal: Exports (Modeling) Create analytics-friendly dimension views from Golden Records: . | Add surrogate keys (CustomerSK, ProductSK, etc.). | Include business keys (source IDs) for lineage. | Flatten nested attributes; include quality flags and CompletenessScore. | Map reference data to canonical values (never raw codes in dims). | . SCD Type 2 (Optional) . | Add ValidFrom, ValidTo, IsCurrent. | Version on material changes (e.g., Customer Segment, Product Category, Region). | . ",
    "url": "/data-analytics-use-case#step-7--conformed-dimensions-analytics-views",
    
    "relUrl": "/data-analytics-use-case#step-7--conformed-dimensions-analytics-views"
  },"96": {
    "doc": "Data Analytics Use Case",
    "title": "Step 8 — Facts Cleanup &amp; Conformance (Optional)",
    "content": "Portal: Entity Explorer / Rules . | Clean transactional facts (orders/invoices) — dates, currency, UoM. | Link to Golden dimensions using natural keys → then replace with surrogate keys. | Add quality columns (e.g., IsLinked=1/0, DQ_FailCount). | . ",
    "url": "/data-analytics-use-case#step-8--facts-cleanup--conformance-optional",
    
    "relUrl": "/data-analytics-use-case#step-8--facts-cleanup--conformance-optional"
  },"97": {
    "doc": "Data Analytics Use Case",
    "title": "Step 9 — Data Quality Gates for Analytics",
    "content": "Portal: Data Quality → Profiling / Scorecards . | Define pass/fail gates (e.g., DimCustomer.ValidEmail ≥ 98%). | Track: completeness, validity, uniqueness, timeliness per dimension and source. | Create alerts on regressions and failed gates; route to stewards. | . ",
    "url": "/data-analytics-use-case#step-9--data-quality-gates-for-analytics",
    
    "relUrl": "/data-analytics-use-case#step-9--data-quality-gates-for-analytics"
  },"98": {
    "doc": "Data Analytics Use Case",
    "title": "Step 10 — Publish to Warehouse &amp; Semantic Layer",
    "content": "Portal: Exports . | Export Dim* and Fact* views to the warehouse (Snowflake/BigQuery/etc.). | Include CDC (changed rows only) where possible. | Optionally emit events/APIs for near-real-time consumption. | In the semantic layer/BI: map measures and join keys to Dim* SKs. | . Start read-only to validate joins and metrics, then operationalize. ",
    "url": "/data-analytics-use-case#step-10--publish-to-warehouse--semantic-layer",
    
    "relUrl": "/data-analytics-use-case#step-10--publish-to-warehouse--semantic-layer"
  },"99": {
    "doc": "Data Analytics Use Case",
    "title": "Step 11 — Governance, Security &amp; Lineage",
    "content": "Portal: Governance → Permissions / Policies / History . | Restrict sensitive fields (PII, cost). | Keep History for merges/overrides; enable rollback/undo. | Expose lineage: raw → standardized → golden → Dim*/Fact*. | . ",
    "url": "/data-analytics-use-case#step-11--governance-security--lineage",
    
    "relUrl": "/data-analytics-use-case#step-11--governance-security--lineage"
  },"100": {
    "doc": "Data Analytics Use Case",
    "title": "Step 12 — Scheduling &amp; Operations",
    "content": ". | Schedule ingestion, matching, golden updates, quality gates, and exports. | Run heavy jobs off-peak on a dedicated server/VM. | Monitor runtimes and queue depth; investigate anomalies (usually upstream drift). | . ",
    "url": "/data-analytics-use-case#step-12--scheduling--operations",
    
    "relUrl": "/data-analytics-use-case#step-12--scheduling--operations"
  },"101": {
    "doc": "Data Analytics Use Case",
    "title": "Step 13 — UAT &amp; Go-Live",
    "content": ". | Validate joins: Fact* rows match exactly with Dim* surrogate keys. | Reconcile key metrics vs legacy BI (tolerance agreed). | Stakeholder sign-off (Analytics, Finance, Ops). | Package with Product Toolkit; promote to staging → prod (ensure Accountable access). | . ",
    "url": "/data-analytics-use-case#step-13--uat--go-live",
    
    "relUrl": "/data-analytics-use-case#step-13--uat--go-live"
  },"102": {
    "doc": "Data Analytics Use Case",
    "title": "Go-Live Checklist",
    "content": ". | RDM mappings complete; 100% of source codes mapped. | Golden survivorship rules approved; AnalyticsReady tagging accurate. | Dim* tables include SKs, lineage columns, SCD policy implemented. | Facts conformed and linked to dimensions; sample reconciliation passed. | DQ gates &amp; alerts configured; dashboards show baselines. | Exports scheduled; CDC tested; warehouse/semantic models wired. | Rollback plan documented (unmerge, undo rules/cleaning projects). | . ",
    "url": "/data-analytics-use-case#go-live-checklist",
    
    "relUrl": "/data-analytics-use-case#go-live-checklist"
  },"103": {
    "doc": "Data Analytics Use Case",
    "title": "Example Rules (Snippets)",
    "content": "Country Canonicalization (Data Part) . | Condition: Country not in ISO2 | Actions: map alias (UK→GB); on fail tag UnknownCountry. | . Customer Identity (Matching) . | Auto-merge: exact TaxID OR exact Email OR exact Phone. | Candidate: fuzzy Name ≥ 0.92 AND same Country AND address similarity ≥ 0.9 (steward review). | . Golden Survivorship (Customer Segment) . | Precedence: CRM &gt; ERP; | If missing, infer from rules (e.g., lifetime value) and tag Inferred. | . SCD2 Versioning Trigger (DimCustomer) . | Condition: Segment or Region changes | Actions: close current row (ValidTo=now, IsCurrent=false); open new row (ValidFrom=now, IsCurrent=true). | . AnalyticsReady Tag . | If required attrs present AND no critical DQ failures → set AnalyticsReady=true. | . ",
    "url": "/data-analytics-use-case#example-rules-snippets",
    
    "relUrl": "/data-analytics-use-case#example-rules-snippets"
  },"104": {
    "doc": "Data Analytics Use Case",
    "title": "KPIs &amp; Targets (Examples)",
    "content": ". | Duplicate rate in Golden dimensions &lt; 1–2%. | DimCustomer.ValidEmail ≥ 98%, DimProduct.InvalidGTIN ≤ 0.5%. | Mapping coverage for RDM = 100%. | % Facts linked to all required dims ≥ 99.5%. | BI reconciliation error ≤ 0.5% on core metrics. | . ",
    "url": "/data-analytics-use-case#kpis--targets-examples",
    
    "relUrl": "/data-analytics-use-case#kpis--targets-examples"
  },"105": {
    "doc": "Data Analytics Use Case",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Skipping RDM → unmapped codes create exploding cardinality in BI; enforce 100% mapping. | Over-fuzzy matching → require high-confidence evidence for auto-merge; steward the rest. | Mixing channel overrides into Golden → keep Golden canonical; handle channel transforms in exports/semantic layer. | Not versioning analytical attributes → adopt SCD2 for attributes used in cohorting/segmentation. | Running heavy jobs on laptops → use dedicated compute to avoid timeouts and throttling. | . ",
    "url": "/data-analytics-use-case#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/data-analytics-use-case#common-pitfalls--how-to-avoid-them"
  },"106": {
    "doc": "Data Analytics Use Case",
    "title": "Summary",
    "content": "With CluedIn, you can turn disparate operational data into clean, governed, analytics-ready dimensions and facts. Standardize inputs, resolve identities, manage reference data, define survivorship, and publish conformed tables with history and lineage so downstream analytics are trusted and repeatable. ",
    "url": "/data-analytics-use-case#summary",
    
    "relUrl": "/data-analytics-use-case#summary"
  },"107": {
    "doc": "Data Analytics Use Case",
    "title": "Data Analytics Use Case",
    "content": " ",
    "url": "/data-analytics-use-case",
    
    "relUrl": "/data-analytics-use-case"
  },"108": {
    "doc": "Reference Data Management",
    "title": "Reference Data Management (RDM) in CluedIn — Step-by-Step Implementation Guide",
    "content": "Reference data (codes, lists, and hierarchies) underpins every analytics and operational workflow: countries, currencies, units of measure, cost centers, product categories, payment terms, GL accounts, etc. This guide shows how to build a governed, versioned, and syndicated RDM capability in CluedIn. ",
    "url": "/reference-data-management#reference-data-management-rdm-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/reference-data-management#reference-data-management-rdm-in-cluedin--step-by-step-implementation-guide"
  },"109": {
    "doc": "Reference Data Management",
    "title": "Outcomes",
    "content": ". | Authoritative code sets (canonical values) for each domain (e.g., Country, Currency, UoM, ProductCategory, CostCenter). | Mappings/crosswalks from source codes → canonical codes, with full lineage and audit. | Versioning &amp; effective dating for controlled change (draft → approved → published). | Hierarchies (single or alternate rollups) with validation and impact awareness. | Syndication of approved sets to ERP/CRM/data warehouse/BI via exports/APIs. | . ",
    "url": "/reference-data-management#outcomes",
    
    "relUrl": "/reference-data-management#outcomes"
  },"110": {
    "doc": "Reference Data Management",
    "title": "Prerequisites",
    "content": ". | Read access to systems that currently host code lists (ERP, CRM, e-commerce, finance, spreadsheets). | Agreement on canonical owners per domain (who approves changes). | RACI: Implementers and data stewards need Accountable access to governed objects. | A dedicated VM/server for heavy initial loads, matching, and toolkit operations. | . ",
    "url": "/reference-data-management#prerequisites",
    
    "relUrl": "/reference-data-management#prerequisites"
  },"111": {
    "doc": "Reference Data Management",
    "title": "Reference Model",
    "content": "Entities (suggested) . | CodeSet — e.g., Country, Currency, UoM, ProductCategory, CostCenter. | CodeValue — one row per canonical value (attributes: Code, Label, Description, EffectiveFrom, EffectiveTo, Status). | SourceCodeValue — raw values from each source system (attributes: SourceSystem, SourceCode, SourceLabel). | CodeMapping — crosswalk from SourceCodeValue → CodeValue (attributes: MatchConfidence, ApprovedBy, ApprovedAt). | HierarchyNode — parent/child structure for hierarchical sets (attributes: ParentId, ChildId, AltHierarchyName, Level, EffectiveFrom/To). | . Statuses . | Draft → Proposed → Approved → Published → Deprecated → Retired | . ",
    "url": "/reference-data-management#reference-model",
    
    "relUrl": "/reference-data-management#reference-model"
  },"112": {
    "doc": "Reference Data Management",
    "title": "Step 1 — Define Scope &amp; Ownership",
    "content": "Portal: Governance → Policies / Permissions . | List domains in scope (start with 2–3 high-impact sets, e.g., Country, Currency, ProductCategory). | Assign Domain Owner(s) and Stewards per set. | Document approval SLAs and release frequency (e.g., monthly). | . ",
    "url": "/reference-data-management#step-1--define-scope--ownership",
    
    "relUrl": "/reference-data-management#step-1--define-scope--ownership"
  },"113": {
    "doc": "Reference Data Management",
    "title": "Step 2 — Connect &amp; Ingest Source Lists",
    "content": "Portal: Data Sources → Add Source . | Connect ERP/CRM/Finance lists and relevant spreadsheets. | For each list, ingest into SourceCodeValue with SourceSystem and load timestamp. | Profile data (duplicates, nulls, length/format distribution). | . Tip: Use sampling to inspect variants early (e.g., US vs USA vs 840). ",
    "url": "/reference-data-management#step-2--connect--ingest-source-lists",
    
    "relUrl": "/reference-data-management#step-2--connect--ingest-source-lists"
  },"114": {
    "doc": "Reference Data Management",
    "title": "Step 3 — Create Code Sets &amp; Canonical Schemas",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create CodeSet, CodeValue, SourceCodeValue, CodeMapping, HierarchyNode. | Seed canonical CodeValue tables for stable domains (e.g., ISO 3166 country, ISO 4217 currency). | Add validation attributes: Status, EffectiveFrom/To, Owner, Version. | . ",
    "url": "/reference-data-management#step-3--create-code-sets--canonical-schemas",
    
    "relUrl": "/reference-data-management#step-3--create-code-sets--canonical-schemas"
  },"115": {
    "doc": "Reference Data Management",
    "title": "Step 4 — Standardize &amp; Validate Source Codes",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to clean and normalize incoming source codes: . | Trim/uppercase codes; collapse whitespace and punctuation. | Normalize diacritics in labels; map known synonyms (U.S. → US). | Validate against canonical patterns (length, charset). | Tag issues: InvalidFormat, UnknownCode, NeedsStewardReview. | . Recommended tags: Standardized, Alias, DeprecatedSourceCode. ",
    "url": "/reference-data-management#step-4--standardize--validate-source-codes",
    
    "relUrl": "/reference-data-management#step-4--standardize--validate-source-codes"
  },"116": {
    "doc": "Reference Data Management",
    "title": "Step 5 — Build the Mapping (Crosswalk)",
    "content": "Portal: Entity Matching / Data Stewardship . | Auto-match exact code or exact label to canonical CodeValue. | Fuzzy match for labels (e.g., similarity ≥ 0.95) as candidates only. | Create/maintain CodeMapping for each (SourceSystem, SourceCode) → CodeValue. | Route low-confidence matches to Steward Review; capture ApprovedBy/At. | . Pitfall: Do not auto-create new canonical values from fuzzy matches; require steward approval. ",
    "url": "/reference-data-management#step-5--build-the-mapping-crosswalk",
    
    "relUrl": "/reference-data-management#step-5--build-the-mapping-crosswalk"
  },"117": {
    "doc": "Reference Data Management",
    "title": "Step 6 — Golden Rules for Canonical Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) . | For shared domains (e.g., product categories coming from multiple apps), set precedence: MasterTaxonomy &gt; ERP &gt; eCommerce &gt; CSV. | Tie-breakers: most recent change, highest steward confidence, or owner’s source. | Prevent conflicting edits by locking Approved/Published values to owner role only. | . Action examples . | On approval → set Status=Published, add tag Authoritative. | On deprecation → set EffectiveTo, tag Deprecated, update mappings to successor value. | . ",
    "url": "/reference-data-management#step-6--golden-rules-for-canonical-survivorship",
    
    "relUrl": "/reference-data-management#step-6--golden-rules-for-canonical-survivorship"
  },"118": {
    "doc": "Reference Data Management",
    "title": "Step 7 — Versioning &amp; Effective Dating",
    "content": "Portal: Entity Explorer / Rules . | Add a Version to CodeSet (e.g., ProductCategory v2025.08). | When changing codes/labels: . | Create new CodeValue with EffectiveFrom=YYYY-MM-DD. | Set predecessor EffectiveTo. | Migrate CodeMapping entries (track change reason). | . | Keep Draft vs Published states; only publish after UAT. | . ",
    "url": "/reference-data-management#step-7--versioning--effective-dating",
    
    "relUrl": "/reference-data-management#step-7--versioning--effective-dating"
  },"119": {
    "doc": "Reference Data Management",
    "title": "Step 8 — Hierarchy Management",
    "content": "Portal: Entity Explorer → Hierarchies (via HierarchyNode) . | Load parent/child edges for the set (e.g., Category tree, Cost Center rollup). | Validate acyclic structure; enforce single parent (or allow alternate hierarchies using AltHierarchyName). | Add effective dates for time-variant hierarchies. | Create validation rules: no orphans, root count = 1 (unless multi-root by design), max depth. | . ",
    "url": "/reference-data-management#step-8--hierarchy-management",
    
    "relUrl": "/reference-data-management#step-8--hierarchy-management"
  },"120": {
    "doc": "Reference Data Management",
    "title": "Step 9 — Stewardship Workflow",
    "content": "Portal: Data Stewardship . | Work queue for: new code proposals, unmapped source codes, hierarchy edits, deprecations. | Approve/Reject with justification; changes recorded in History. | Use tags to drive workflow: NeedsApproval, NeedsMapping, BreakingChange. | . ",
    "url": "/reference-data-management#step-9--stewardship-workflow",
    
    "relUrl": "/reference-data-management#step-9--stewardship-workflow"
  },"121": {
    "doc": "Reference Data Management",
    "title": "Step 10 — Quality Checks &amp; Policies",
    "content": "Portal: Data Quality → Profiling / Rules . | Uniqueness: canonical Code unique per CodeSet. | Completeness: required attributes populated for Published. | Integrity: every SourceCodeValue maps to a CodeValue. | No deprecated usage in exports (flag if EffectiveTo &lt; today). | Policy: new values must have owner + description; hierarchy edits require two-person approval. | . ",
    "url": "/reference-data-management#step-10--quality-checks--policies",
    
    "relUrl": "/reference-data-management#step-10--quality-checks--policies"
  },"122": {
    "doc": "Reference Data Management",
    "title": "Step 11 — Publish &amp; Integrate",
    "content": "Portal: Exports . | Publish lookup tables and mappings to ERP/CRM/DW/ML features. | Provide both wide (CodeValue) and narrow (SourceCode → CanonicalCode) feeds. | Offer read API for applications that need live lookups. | Start read-only, validate with consumers, then make CluedIn authoritative. | . ",
    "url": "/reference-data-management#step-11--publish--integrate",
    
    "relUrl": "/reference-data-management#step-11--publish--integrate"
  },"123": {
    "doc": "Reference Data Management",
    "title": "Step 12 — Scheduling &amp; Release Management",
    "content": ". | Schedule source refresh, rematching, validation, and export jobs. | Run large batch reconciliations off-peak on a dedicated server/VM. | Use Product Toolkit to package CodeSet, CodeValue, rules, and exports; ensure Accountable permissions before promoting to staging → prod. | . ",
    "url": "/reference-data-management#step-12--scheduling--release-management",
    
    "relUrl": "/reference-data-management#step-12--scheduling--release-management"
  },"124": {
    "doc": "Reference Data Management",
    "title": "Step 13 — UAT &amp; Go-Live Checklist",
    "content": ". | All in-scope source codes ingested and standardized. | 100% of SourceCodeValue entries mapped or triaged. | Golden survivorship/precedence approved by domain owners. | Hierarchies validated (no cycles/orphans; effective dates correct). | Version labeled and Published; change log prepared. | Downstream consumers validated against new lookups and mappings. | Runbooks for deprecations, renames, and rollback documented. | . ",
    "url": "/reference-data-management#step-13--uat--go-live-checklist",
    
    "relUrl": "/reference-data-management#step-13--uat--go-live-checklist"
  },"125": {
    "doc": "Reference Data Management",
    "title": "Example Rules (Snippets)",
    "content": "Normalize Code (Data Part Rule) . | Condition: SourceCode present | Action: uppercase; strip punctuation; set Code_Normalized; tag Standardized. | . Auto-map Exact Codes . | Condition: Code_Normalized equals canonical Code in same CodeSet | Action: create CodeMapping with MatchConfidence=1.0; tag AutoMapped. | . Candidate Fuzzy Label Match . | Condition: label similarity ≥ 0.95 | Action: create candidate CodeMapping with MatchConfidence=0.95; tag NeedsStewardReview. | . Deprecate &amp; Redirect . | Condition: CodeValue.Status = Deprecated | Action: enforce SuccessorCode; update downstream export to map deprecated → successor. | . ",
    "url": "/reference-data-management#example-rules-snippets",
    
    "relUrl": "/reference-data-management#example-rules-snippets"
  },"126": {
    "doc": "Reference Data Management",
    "title": "Monitoring &amp; KPIs",
    "content": ". | Mapping coverage: % SourceCodeValue mapped (target ≥ 99.5%). | Data quality: duplicates = 0; invalid/unknown codes trend ↓. | Time to approve new code: median hours/days vs SLA. | Consumer adoption: % downstream systems using CluedIn lookups. | Incident rate from reference data changes (should trend to zero). | . ",
    "url": "/reference-data-management#monitoring--kpis",
    
    "relUrl": "/reference-data-management#monitoring--kpis"
  },"127": {
    "doc": "Reference Data Management",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Letting sources drift (new codes appear unmapped) → schedule nightly ingest + alert on UnknownCode. | Over-fuzzy mapping → require steward approval for &lt; 1.0 confidence. | Breaking changes mid-cycle → use Draft versions; publish on release cadence; communicate impact. | Hierarchy loops → enforce acyclic validation + tests before publish. | Running heavy reconciliations on laptops → use dedicated compute to avoid timeouts. | . ",
    "url": "/reference-data-management#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/reference-data-management#common-pitfalls--how-to-avoid-them"
  },"128": {
    "doc": "Reference Data Management",
    "title": "Rollback &amp; Audit",
    "content": ". | Every change (new code, rename, mapping edit, hierarchy move) appears in History with who/when/why. | Use Undo/Restore Previous Value for attribute fixes; Unmerge not typically needed for RDM but available for accidental consolidations. | For erroneous releases: republish the prior Version (set its EffectiveFrom to now, mark failed version Deprecated). | . ",
    "url": "/reference-data-management#rollback--audit",
    
    "relUrl": "/reference-data-management#rollback--audit"
  },"129": {
    "doc": "Reference Data Management",
    "title": "Summary",
    "content": "With CluedIn you can centralize reference data, create reliable crosswalks, govern changes with versioning and approvals, and distribute authoritative values to every consumer. Start with high-impact sets, lock down ownership, automate the mundane, and publish on a predictable cadence to eliminate code chaos. ",
    "url": "/reference-data-management#summary",
    
    "relUrl": "/reference-data-management#summary"
  },"130": {
    "doc": "Reference Data Management",
    "title": "Reference Data Management",
    "content": " ",
    "url": "/reference-data-management",
    
    "relUrl": "/reference-data-management"
  },"131": {
    "doc": "Customer Acceleration Case",
    "title": "Customer Onboarding Acceleration in CluedIn — Step-by-Step Implementation Guide",
    "content": "Accelerate onboarding by using CluedIn to ingest, validate, enrich, deduplicate, and publish complete, compliant customer records to CRM and workflow systems. This guide shows you how to go from raw application data to a ready-to-work Golden Customer with SLAs, dashboards, and audit-ready governance. ",
    "url": "/customer-acceleration-case#customer-onboarding-acceleration-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/customer-acceleration-case#customer-onboarding-acceleration-in-cluedin--step-by-step-implementation-guide"
  },"132": {
    "doc": "Customer Acceleration Case",
    "title": "Outcomes",
    "content": ". | Golden Customer (Person/B2C or Account+B2B Contacts) created automatically when requirements are met. | Readiness score and checklist (KYC/consent/completeness) driving downstream task creation. | Fewer manual touches via auto-standardization, enrichment, and duplicate prevention. | Faster time-to-first-action (e.g., welcome email, provisioning, contract activation). | Full audit &amp; rollback for every transformation and decision. | . ",
    "url": "/customer-acceleration-case#outcomes",
    
    "relUrl": "/customer-acceleration-case#outcomes"
  },"133": {
    "doc": "Customer Acceleration Case",
    "title": "Prerequisites",
    "content": ". | Source access: web forms, CRM leads, identity/KYC providers, e-signature, payments/billing, support/ITSM, legacy sheets. | Clarify your onboarding policy: . | Required documents/fields (per segment/region). | KYC/sanctions/credit checks needed by product or jurisdiction. | SLA targets (e.g., 95% within 24h). | . | RACI: Stewards/owners must have Accountable access to governed objects. | Use a dedicated server/VM for heavy runs (ingestion/matching/toolkit)—avoid laptops to prevent timeouts. | . ",
    "url": "/customer-acceleration-case#prerequisites",
    
    "relUrl": "/customer-acceleration-case#prerequisites"
  },"134": {
    "doc": "Customer Acceleration Case",
    "title": "Reference Data Model",
    "content": "Core entities . | Lead / Application | Person (B2C) and/or Account (B2B) + Contact | Address, ContactPoint (Email/Phone) | Document (ID proof, contracts), Consent | KYCCheck / SanctionsCheck / CreditCheck | Subscription / Contract (optional) | Task / Case (stewardship or operational follow-ups) | . Key attributes (examples) . | Required fields by policy: Name, Email, Phone, Country, Address, DOB (B2C), TaxID/DUNS + Domain (B2B). | Evidence: KYCStatus, SanctionsStatus, CreditStatus, ConsentStatus, DocsReceived. | Derived: ReadinessScore, OnboardingStage, CompletenessScore. | . Tags (suggested) . | Standardized, InvalidEmail, InvalidPhone, InvalidAddress, MissingDocs, KYCRequired, KYCFailed, SanctionsHit, NeedsStewardReview, ReadyToCreate, ReadyToActivate. | . ",
    "url": "/customer-acceleration-case#reference-data-model",
    
    "relUrl": "/customer-acceleration-case#reference-data-model"
  },"135": {
    "doc": "Customer Acceleration Case",
    "title": "Step 1 — Define Onboarding Policy &amp; SLAs",
    "content": "Portal: Governance → Policies . | List mandatory fields &amp; documents per segment/region/product. | Define gating checks (KYC, sanctions, credit, consent). | Set SLA thresholds and owners (e.g., “All retail signups KYC within 4 hours”). | Translate policy into rule specs (see Steps 4–7). | . ",
    "url": "/customer-acceleration-case#step-1--define-onboarding-policy--slas",
    
    "relUrl": "/customer-acceleration-case#step-1--define-onboarding-policy--slas"
  },"136": {
    "doc": "Customer Acceleration Case",
    "title": "Step 2 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | Web form/app feed (applications), CRM leads, e-sign, KYC providers, payments/billing, ticketing/ITSM. | Capture provenance: SourceSystem, SourceRecordID, IngestedAt. | Start with samples; then enable scheduled or event-driven ingestion. | . ",
    "url": "/customer-acceleration-case#step-2--connect--ingest-sources",
    
    "relUrl": "/customer-acceleration-case#step-2--connect--ingest-sources"
  },"137": {
    "doc": "Customer Acceleration Case",
    "title": "Step 3 — Standardize &amp; Validate Inputs (Pre-Checks)",
    "content": "Portal: Governance → Rules (Data Part Rules) Create normalization rules before matching: . | Names/Text: trim, case, punctuation, diacritics. | Email: lower-case; regex; tag InvalidEmail. | Phone: format to E.164; tag InvalidPhone on parse fail. | Address: parse components; ISO-2 Country; optional geocoding; tag InvalidAddress. | IDs: normalize TaxID/ABN/DUNS (B2B), DOB (B2C) validation. | Consent: standardize purposes/channels; store source &amp; timestamp. | . ",
    "url": "/customer-acceleration-case#step-3--standardize--validate-inputs-pre-checks",
    
    "relUrl": "/customer-acceleration-case#step-3--standardize--validate-inputs-pre-checks"
  },"138": {
    "doc": "Customer Acceleration Case",
    "title": "Step 4 — Identity Resolution (Duplicate Prevention)",
    "content": "Portal: Entity Matching . Person (B2C / Contacts) . | Blocking: (Email), (Phone), (LastName + DOB). | Auto-merge: exact Email or exact Phone. | Candidate: Fuzzy Name ≥ 0.92 AND Address similarity ≥ 0.9 (same Country) → steward. | . Account (B2B) . | Blocking: (Domain), (TaxID), (Country + CleanNameSoundex). | Auto-merge: exact Domain or exact TaxID. | Candidate: Fuzzy LegalName ≥ 0.92 + Country guard. | . Linking . | Link new Lead/Application to existing Golden Customer when a high-confidence match exists—reduces re-keying and speeds onboarding. | . ",
    "url": "/customer-acceleration-case#step-4--identity-resolution-duplicate-prevention",
    
    "relUrl": "/customer-acceleration-case#step-4--identity-resolution-duplicate-prevention"
  },"139": {
    "doc": "Customer Acceleration Case",
    "title": "Step 5 — Evidence &amp; Risk Checks (KYC/Sanctions/Credit)",
    "content": "Portal: Data Sources (KYC/Risk) + Rules . | Ingest results from KYC/AML/Sanctions/Credit providers. | Normalize statuses to a common schema (Passed, Review, Failed, Expired). | Rules: . | If SanctionsHit=true → tag SanctionsHit, block readiness. | If KYCStatus=Expired OR DocsMissing=true → tag KYCRequired; create Task. | If CreditStatus &lt; threshold → tag NeedsStewardReview or route to manual approval. | . | . ",
    "url": "/customer-acceleration-case#step-5--evidence--risk-checks-kycsanctionscredit",
    
    "relUrl": "/customer-acceleration-case#step-5--evidence--risk-checks-kycsanctionscredit"
  },"140": {
    "doc": "Customer Acceleration Case",
    "title": "Step 6 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define attribute precedence and tie-breakers: . | Attribute | Precedence (high → low) | Tie-breaker | . | Primary Email | CRM &gt; Application &gt; Support &gt; Marketing | Most recently verified | . | Primary Phone | CRM &gt; Application &gt; Support | Valid E.164 + last confirmed | . | Address | Verified delivery &gt; Billing &gt; CRM | Most recent verified | . | Account Domain | CRM &gt; Verified from Email Domain | Steward override | . | Segment/Plan | Application &gt; CRM | Most recent | . | Consents | Most restrictive wins per channel | N/A | . Derived . | CompletenessScore and ReadinessScore (see Step 7). | OnboardingStage (Application → KYC → Contract → Activated). | . ",
    "url": "/customer-acceleration-case#step-6--golden-record-survivorship",
    
    "relUrl": "/customer-acceleration-case#step-6--golden-record-survivorship"
  },"141": {
    "doc": "Customer Acceleration Case",
    "title": "Step 7 — Readiness Score &amp; Checklist",
    "content": "Portal: Data Quality → Scorecards or Rules Compute a ReadinessScore and a binary ReadyToActivate flag: . ",
    "url": "/customer-acceleration-case#step-7--readiness-score--checklist",
    
    "relUrl": "/customer-acceleration-case#step-7--readiness-score--checklist"
  },"142": {
    "doc": "Customer Acceleration Case",
    "title": "Customer Acceleration Case",
    "content": " ",
    "url": "/customer-acceleration-case",
    
    "relUrl": "/customer-acceleration-case"
  },"143": {
    "doc": "ERP Migration",
    "title": "ERP Migration in CluedIn — Step-by-Step Implementation Guide",
    "content": "ERP migrations (e.g., SAP, Oracle, Dynamics, NetSuite) demand clean, reconciled, and governed master and transactional data. CluedIn helps you discover, cleanse, standardize, deduplicate, enrich, and publish trusted data into the target ERP, reducing risk and accelerating migration timelines. ",
    "url": "/erp-migration#erp-migration-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/erp-migration#erp-migration-in-cluedin--step-by-step-implementation-guide"
  },"144": {
    "doc": "ERP Migration",
    "title": "Outcomes",
    "content": ". | Trusted Golden Records for master data domains (Customers, Suppliers, Products, Chart of Accounts, Cost Centers, etc.). | Standardized and reconciled reference data (currencies, units of measure, payment terms, tax codes). | Cleansed and deduplicated transactions (open POs, invoices, orders) linked to correct masters. | Audit trail and rollback capabilities for all transformations. | Exports packaged in migration-ready format for ERP cutover. | . ",
    "url": "/erp-migration#outcomes",
    
    "relUrl": "/erp-migration#outcomes"
  },"145": {
    "doc": "ERP Migration",
    "title": "Prerequisites",
    "content": ". | Source system access: legacy ERP(s), CRM, PIM, Finance/HR systems, spreadsheets. | Agreement on scope: which entities are in migration wave (Customers, Vendors, Products, GL, etc.). | Target ERP design: field mappings, canonical formats, mandatory attributes, coding standards. | RACI: Ensure stewards/owners have Accountable access to governed objects. | Dedicated environment/VM for heavy ingestion and matching runs. | . ",
    "url": "/erp-migration#prerequisites",
    
    "relUrl": "/erp-migration#prerequisites"
  },"146": {
    "doc": "ERP Migration",
    "title": "Reference Data Model",
    "content": "Core Master Data Entities . | Customer | Supplier | Product / Material | Employee (for HR modules) | GLAccount, CostCenter | . Reference Data Entities . | Currency | UnitOfMeasure | TaxCode | PaymentTerms | . Transactional Entities (optional, for cutover) . | PurchaseOrder | SalesOrder | Invoice | . ",
    "url": "/erp-migration#reference-data-model",
    
    "relUrl": "/erp-migration#reference-data-model"
  },"147": {
    "doc": "ERP Migration",
    "title": "Step 1 — Connect &amp; Ingest Legacy Sources",
    "content": "Portal: Data Sources → Add Source . | Add legacy ERP extracts (Customers, Vendors, Products, GL, etc.). | Add supporting systems (CRM, PIM, Finance). | Add spreadsheets/flat files for reference data overrides. | Profile volumes, completeness, duplicates, and data health. | . Tip: Load into staging entities with clear provenance (SourceSystem, SourceRecordID). ",
    "url": "/erp-migration#step-1--connect--ingest-legacy-sources",
    
    "relUrl": "/erp-migration#step-1--connect--ingest-legacy-sources"
  },"148": {
    "doc": "ERP Migration",
    "title": "Step 2 — Define Target Canonical Entities",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create canonical entity types aligned with target ERP design. | Map legacy attributes → canonical attributes. | Example: VendorName → Supplier.LegalName | Example: MatDesc → Product.Description | . | Preserve lineage: store original values + mapping metadata. | . ",
    "url": "/erp-migration#step-2--define-target-canonical-entities",
    
    "relUrl": "/erp-migration#step-2--define-target-canonical-entities"
  },"149": {
    "doc": "ERP Migration",
    "title": "Step 3 — Standardize &amp; Cleanse Data",
    "content": "Portal: Governance → Rules (Data Part Rules) . | Names: trim, case, remove stopwords. | Addresses: parse, normalize, geocode, standardize country codes (ISO). | Identifiers: validate Tax IDs, DUNS, IBANs, GTINs. | Reference Data: map currencies, UoM, tax codes to canonical sets. | Contacts: normalize phone numbers to E.164; validate emails. | . Tags: Standardized, InvalidTaxID, InvalidUoM, LegacyCode. ",
    "url": "/erp-migration#step-3--standardize--cleanse-data",
    
    "relUrl": "/erp-migration#step-3--standardize--cleanse-data"
  },"150": {
    "doc": "ERP Migration",
    "title": "Step 4 — Deduplicate &amp; Match",
    "content": "Portal: Entity Matching . | Configure blocking &amp; matching keys per domain: . | Customer: Tax ID, Email, Fuzzy Name + Address. | Supplier: Tax ID, DUNS, Fuzzy Name + Country. | Product: Brand + MPN, GTIN, Fuzzy Title + Category. | . | Run matching jobs; steward ambiguous cases in Data Stewardship. | Approve merges, unmerge mistakes, tag ambiguous records. | . ",
    "url": "/erp-migration#step-4--deduplicate--match",
    
    "relUrl": "/erp-migration#step-4--deduplicate--match"
  },"151": {
    "doc": "ERP Migration",
    "title": "Step 5 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) . | Define attribute precedence: ERP &gt; CRM &gt; Spreadsheet &gt; Other. | Apply tie-breakers (most recent, highest quality, verified source). | Compute CompletenessScore. | Tag records: ReadyForERP, Incomplete, NeedsStewardship. | . ",
    "url": "/erp-migration#step-5--golden-record-survivorship",
    
    "relUrl": "/erp-migration#step-5--golden-record-survivorship"
  },"152": {
    "doc": "ERP Migration",
    "title": "Step 6 — Reference Data Management",
    "content": "Portal: Governance → Reference Data (RDM) . | Create canonical code sets for Currency, UoM, Tax Codes, Payment Terms. | Map all legacy codes to canonical values. | Validate completeness: 100% of source codes must map. | Version &amp; publish approved sets for ERP cutover. | . ",
    "url": "/erp-migration#step-6--reference-data-management",
    
    "relUrl": "/erp-migration#step-6--reference-data-management"
  },"153": {
    "doc": "ERP Migration",
    "title": "Step 7 — Transactional Data Preparation (Optional)",
    "content": "Portal: Entity Explorer / Rules . | Cleanse open transactions (POs, invoices, orders). | Link to new Golden Records (CustomerId, SupplierId, ProductId). | Validate mandatory fields required by target ERP. | Drop obsolete/closed transactions per business rules. | . ",
    "url": "/erp-migration#step-7--transactional-data-preparation-optional",
    
    "relUrl": "/erp-migration#step-7--transactional-data-preparation-optional"
  },"154": {
    "doc": "ERP Migration",
    "title": "Step 8 — Stewardship &amp; Governance",
    "content": "Portal: Data Stewardship / Governance . | Queue unresolved duplicates, invalid addresses, or unmapped codes for review. | Enforce RACI roles (Accountable = stewards, Informed = migration team). | Track approvals in History; enable rollback for changes. | . ",
    "url": "/erp-migration#step-8--stewardship--governance",
    
    "relUrl": "/erp-migration#step-8--stewardship--governance"
  },"155": {
    "doc": "ERP Migration",
    "title": "Step 9 — Export Migration Packages",
    "content": "Portal: Exports . | Create export pipelines aligned with ERP migration load templates. | Example: Customer master load (CSV/XML/JSON). | Example: Supplier master load. | Example: Product master load. | . | Include mapping keys (old → new IDs). | Validate against ERP staging load; adjust formats. | . ",
    "url": "/erp-migration#step-9--export-migration-packages",
    
    "relUrl": "/erp-migration#step-9--export-migration-packages"
  },"156": {
    "doc": "ERP Migration",
    "title": "Step 10 — Validate &amp; UAT",
    "content": ". | Sample 100+ customers/suppliers/products across sources. | Confirm Golden attributes align with ERP requirements. | Validate reference data usage (currencies, UoM, tax codes). | Test full end-to-end load into ERP test environment. | Steward sign-off + ERP functional team approval. | . ",
    "url": "/erp-migration#step-10--validate--uat",
    
    "relUrl": "/erp-migration#step-10--validate--uat"
  },"157": {
    "doc": "ERP Migration",
    "title": "Step 11 — Cutover &amp; Go-Live",
    "content": ". | Freeze legacy systems for final extract. | Re-run ingestion, cleansing, matching, golden record, and exports. | Deliver final migration package to ERP team. | Run post-load reconciliation dashboards (counts, sums, duplicates). | . ",
    "url": "/erp-migration#step-11--cutover--go-live",
    
    "relUrl": "/erp-migration#step-11--cutover--go-live"
  },"158": {
    "doc": "ERP Migration",
    "title": "Step 12 — Post-Migration Governance",
    "content": ". | Use CluedIn as ongoing MDM hub feeding ERP &amp; other systems. | Continue to steward Golden Records; enforce retention &amp; policies. | Monitor KPIs: duplicate rate, completeness, ERP load exceptions. | . ",
    "url": "/erp-migration#step-12--post-migration-governance",
    
    "relUrl": "/erp-migration#step-12--post-migration-governance"
  },"159": {
    "doc": "ERP Migration",
    "title": "Example Rules (Snippets)",
    "content": "Normalize Address . | Condition: Address.Raw present | Action: parse into Line1, City, Postcode, Country (ISO); geocode; tag StandardizedAddress. | . Currency Mapping . | Condition: CurrencyCode not in ISO4217 | Action: map legacy code (e.g., USDOLLARS) → USD; tag LegacyCode. | . Golden Survivorship (Customer Name) . | Source precedence: ERP &gt; CRM &gt; Marketing list. | Tie-breaker: longest non-abbreviated version. | . Tag Ready for ERP . | If completeness ≥ 90% AND mandatory fields populated → ReadyForERP=true. | . ",
    "url": "/erp-migration#example-rules-snippets",
    
    "relUrl": "/erp-migration#example-rules-snippets"
  },"160": {
    "doc": "ERP Migration",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Ignoring reference data: unmapped codes cause ERP load failures → enforce 100% mapping. | Over-merging customers/suppliers: require Tax ID or multi-evidence for auto-merge. | Late data freeze: always plan final extract &amp; delta runs. | Running heavy jobs on laptops: use a dedicated VM/server to avoid timeouts. | Skipping audit trail: regulators may require proof → keep History enabled. | . ",
    "url": "/erp-migration#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/erp-migration#common-pitfalls--how-to-avoid-them"
  },"161": {
    "doc": "ERP Migration",
    "title": "Success Metrics",
    "content": ". | Duplicate rate &lt; 2% across Customers/Suppliers/Products. | 100% reference code mapping coverage. | Completeness ≥ 95% for mandatory ERP attributes. | ERP load exception rate &lt; 0.5%. | Stewardship backlog cleared before cutover. | . ",
    "url": "/erp-migration#success-metrics",
    
    "relUrl": "/erp-migration#success-metrics"
  },"162": {
    "doc": "ERP Migration",
    "title": "Summary",
    "content": "ERP Migration with CluedIn ensures that data entering the new system is clean, standardized, governed, and auditable. By creating Golden Records, reconciling reference data, preparing transactions, and exporting in ERP-ready formats, you minimize cutover risk and establish CluedIn as an ongoing MDM hub post-migration. ",
    "url": "/erp-migration#summary",
    
    "relUrl": "/erp-migration#summary"
  },"163": {
    "doc": "ERP Migration",
    "title": "ERP Migration",
    "content": " ",
    "url": "/erp-migration",
    
    "relUrl": "/erp-migration"
  },"164": {
    "doc": "Customer 360 Case",
    "title": "Customer 360 in CluedIn — Step-by-Step Implementation Guide",
    "content": "Customer 360 creates a single, trusted view of each customer across CRM, e-commerce, marketing, support, billing, and product systems. This guide walks you end-to-end: ingest → standardize → match/link → golden record → enrich → govern → publish. ",
    "url": "/customer360#customer-360-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/customer360#customer-360-in-cluedin--step-by-step-implementation-guide"
  },"165": {
    "doc": "Customer 360 Case",
    "title": "Outcomes",
    "content": ". | A golden customer record per real-world customer (person or organization). | Linked entities: Customer/Person/Account ↔ Contacts/Addresses ↔ Orders/Tickets/Subscriptions/Interactions ↔ Consents/Preferences. | Quality, lineage, and rollback for stewardship and compliance. | Publish/Sync to CRM, marketing, CS tooling, and analytics. | . ",
    "url": "/customer360#outcomes",
    
    "relUrl": "/customer360#outcomes"
  },"166": {
    "doc": "Customer 360 Case",
    "title": "Prerequisites",
    "content": ". | Source access: CRM, e-commerce, marketing automation, support/ITSM, billing/subscriptions, product telemetry (optional), spreadsheets. | Clarify the customer model: . | B2C: primary entity = Person with optional Household. | B2B: Account/Organization with linked Contact(Person) and roles. | . | RACI: Stewards/owners need Accountable access. | Use a dedicated server/VM for heavy ingestion/matching/toolkit operations (avoid laptops). | . ",
    "url": "/customer360#prerequisites",
    
    "relUrl": "/customer360#prerequisites"
  },"167": {
    "doc": "Customer 360 Case",
    "title": "Reference Data Model",
    "content": "Core entities . | Person (B2C or contact in B2B) | Account / Organization (B2B) | Customer (canonical wrapper pointing to Person or Account) | Address, ContactPoint (Email/Phone), Consent, Preference | Order, Subscription, Ticket/Case, Interaction (events) | Optional: Household (for Householding), Segment, LifecycleStage | . Key attributes (examples) . | Person: FirstName, LastName, DOB, Email(s), Phone(s), PrimaryAddress, MarketingOptIn, Country. | Account: LegalName, TaxID/DUNS, Domain, Industry, Size, BillingAddress. | Customer (derived): Status, Segment, LTV, LastActivity, CompletenessScore. | . Relationships . | Account ↔ Contacts(Person) | Customer ↔ Orders/Subscriptions/Tickets/Interactions | Customer ↔ Consents/Preferences | Person ↔ Household (optional) | . ",
    "url": "/customer360#reference-data-model",
    
    "relUrl": "/customer360#reference-data-model"
  },"168": {
    "doc": "Customer 360 Case",
    "title": "Step 1 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | CRM masters (leads/contacts/accounts). | E-commerce platform (customers, orders). | Marketing automation (lists, events, consents). | Support/ITSM (tickets/cases). | Billing/subscriptions (plans, invoices). | Spreadsheets/CSV (legacy imports). | . Capture provenance on ingest: SourceSystem, SourceRecordID, IngestedAt. ",
    "url": "/customer360#step-1--connect--ingest-sources",
    
    "relUrl": "/customer360#step-1--connect--ingest-sources"
  },"169": {
    "doc": "Customer 360 Case",
    "title": "Step 2 — Define Entities &amp; Mappings",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create Person, Account, Customer, Address, ContactPoint, Consent, Order, Subscription, Ticket, Interaction. | Map source fields → canonical attributes (e.g., crm.email → ContactPoint.Email). | Persist lineage (original values + mapping metadata). | . B2B note: Model Customer as the Account golden, with linked Contacts for communications. ",
    "url": "/customer360#step-2--define-entities--mappings",
    
    "relUrl": "/customer360#step-2--define-entities--mappings"
  },"170": {
    "doc": "Customer 360 Case",
    "title": "Step 3 — Standardization &amp; Normalization (Data Part Rules)",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to clean inputs before matching: . | Names/Text: trim, case, punctuation; strip HTML/emoji where policy requires. | Emails: lower-case; basic regex; (optional) provider-specific canonicalization. | Phones: convert to E.164; tag InvalidPhone if parse fails. | Addresses: parse components; normalize ISO-2 Country; optional geocoding. | IDs: normalize CustomerID, AccountID; validate TaxID/DUNS for B2B. | Domains: extract from email for B2B domain inference. | . Recommended tags: Standardized, InvalidEmail, InvalidPhone, InvalidAddress, LegacySource. ",
    "url": "/customer360#step-3--standardization--normalization-data-part-rules",
    
    "relUrl": "/customer360#step-3--standardization--normalization-data-part-rules"
  },"171": {
    "doc": "Customer 360 Case",
    "title": "Step 4 — Identity Resolution (Matching &amp; Linking)",
    "content": "Portal: Entity Matching . Person (B2C / Contacts) . | Blocking keys: (Email), (Phone), (LastName + DOB), (CustomerId). | High-confidence: exact Email OR exact Phone OR exact CustomerId. | Candidate: fuzzy Name ≥ 0.92 AND Address similarity ≥ 0.9 (same Country). | Review in Data Stewardship; approve merges; Unmerge errors. | . Account (B2B) . | Blocking: (Domain), (TaxID), (Country + CleanNameSoundex). | High-confidence: exact TaxID OR exact Domain. | Candidate: fuzzy LegalName ≥ 0.92 AND same Country; supplement with address/phone. | . Person↔Account Linking (B2B) . | Link Contacts to Accounts by explicit AccountId; else infer by email domain + stewardship review. | . Pitfalls to avoid . | Over-merging by name alone (married names, common surnames). Require multi-evidence. | Cross-country merges; guard with Country. | . ",
    "url": "/customer360#step-4--identity-resolution-matching--linking",
    
    "relUrl": "/customer360#step-4--identity-resolution-matching--linking"
  },"172": {
    "doc": "Customer 360 Case",
    "title": "Step 5 — Stewardship Workflow",
    "content": "Portal: Data Stewardship → Match Review . | Triage by confidence and impact (e.g., number of linked orders). | Approve/Reject; tag AmbiguousMatch for edge cases. | Use History for traceability; Unmerge if needed (rollback captured). | . ",
    "url": "/customer360#step-5--stewardship-workflow",
    
    "relUrl": "/customer360#step-5--stewardship-workflow"
  },"173": {
    "doc": "Customer 360 Case",
    "title": "Step 6 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define attribute precedence and tie-breakers: . | Attribute | Precedence (high → low) | Tie-breaker | . | Primary Email | CRM &gt; E-com &gt; Support &gt; Marketing | Most recently verified/active | . | Primary Phone | CRM &gt; Support &gt; E-com | Valid E.164 + last confirmed | . | Name | CRM &gt; E-com | Longest non-marketing version | . | Address | Verified Delivery &gt; Billing &gt; CRM | Most recent verified | . | Account Domain | CRM &gt; Verified from Email Domain | Steward override | . | Segment/Stage | CRM &gt; Derived (rules) | Most recent | . | Consents | Most restrictive wins (by channel) | N/A | . Actions . | Compute CompletenessScore (weight required attributes). | Tag MarketingReachable when Email valid + opt-in and Phone valid (if SMS). | Tag CustomerLifecycleStage (Lead/Prospect/Active/ChurnRisk) via rules. | . ",
    "url": "/customer360#step-6--golden-record-survivorship",
    
    "relUrl": "/customer360#step-6--golden-record-survivorship"
  },"174": {
    "doc": "Customer 360 Case",
    "title": "Step 7 — Consent &amp; Preference Management",
    "content": "Portal: Entity Explorer / Rules . | Model Consent with Purpose, Channel, Status, Timestamp, Source. | Enforce via rules: . | If Consent(Marketing.Email) != granted → tag DoNotContactEmail. | If DoNotSell/CPRA → tag and propagate to exports. | . | Aggregate consents from multiple systems; preserve source and evidence. | . ",
    "url": "/customer360#step-7--consent--preference-management",
    
    "relUrl": "/customer360#step-7--consent--preference-management"
  },"175": {
    "doc": "Customer 360 Case",
    "title": "Step 8 — Enrichment (Optional)",
    "content": "Portal: Data Sources / Rules . | Append firmographics (B2B: industry, size), demographics (B2C policy-dependent), geocoding, or CLV/propensity scores. | Track LastEnrichedAt, provider, and confidence; re-check on schedule. | . ",
    "url": "/customer360#step-8--enrichment-optional",
    
    "relUrl": "/customer360#step-8--enrichment-optional"
  },"176": {
    "doc": "Customer 360 Case",
    "title": "Step 9 — Data Quality Scoring &amp; Dashboards",
    "content": "Portal: Data Quality → Profiling / Scorecards / Dashboards . | KPIs: valid email/phone %, address standardization %, duplicate rate, consent coverage, completeness by segment/source. | Create alerts on regressions (e.g., Valid Email &lt; 98%). | . ",
    "url": "/customer360#step-9--data-quality-scoring--dashboards",
    
    "relUrl": "/customer360#step-9--data-quality-scoring--dashboards"
  },"177": {
    "doc": "Customer 360 Case",
    "title": "Step 10 — Governance, Security &amp; Lifecycle",
    "content": "Portal: Governance → Permissions / Policies / Retention . | Restrict PII fields; apply least-privilege access. | Retention (e.g., inactive leads &gt; 24 months → archive/delete). | Audit: merges/overrides/exports are captured in History; Undo/Restore supported for data changes and cleaning projects. | . ",
    "url": "/customer360#step-10--governance-security--lifecycle",
    
    "relUrl": "/customer360#step-10--governance-security--lifecycle"
  },"178": {
    "doc": "Customer 360 Case",
    "title": "Step 11 — Publish / Integrate",
    "content": "Portal: Exports . | CRM: push Golden Customer + key attributes; keep GUID mapping (old→new). | Marketing: export reachable audiences with consent flags and suppression lists. | Support/CS: sync authoritative contact details and lifecycle stage. | Analytics/DW: publish wide DimCustomer + relationship tables (Orders, Tickets, Interactions). | APIs/Eventing: emit customer-changed events. | . Start read-only to validate mappings, then make CluedIn authoritative where appropriate. ",
    "url": "/customer360#step-11--publish--integrate",
    
    "relUrl": "/customer360#step-11--publish--integrate"
  },"179": {
    "doc": "Customer 360 Case",
    "title": "Step 12 — Scheduling &amp; Operations",
    "content": ". | Schedule ingestion, matching, golden updates, enrichment, and exports (run heavy jobs off-peak on a dedicated VM/server to avoid timeouts). | Monitor job runtimes and queue depth; investigate spikes (often source drift). | . ",
    "url": "/customer360#step-12--scheduling--operations",
    
    "relUrl": "/customer360#step-12--scheduling--operations"
  },"180": {
    "doc": "Customer 360 Case",
    "title": "Step 13 — Validate &amp; UAT",
    "content": ". | Sample 100–200 customers across sources/segments. | Verify no cross-person conflation; confirm B2B Person↔Account links. | Validate survivorship outcomes and consent enforcement end-to-end. | Business sign-off from CRM/Marketing/Support/Compliance. | . ",
    "url": "/customer360#step-13--validate--uat",
    
    "relUrl": "/customer360#step-13--validate--uat"
  },"181": {
    "doc": "Customer 360 Case",
    "title": "Go-Live Checklist",
    "content": ". | All in-scope sources ingested with provenance. | Matching thresholds tuned; Unmerge tested; false-positive rate low. | Golden survivorship rules approved; CompletenessScore thresholds set. | Consent model enforced; suppression feeds validated. | Exports mapped and tested in CRM/MA/CS and DW. | Permissions &amp; retention configured; audit/rollback verified. | Product Toolkit package built and promoted (run on dedicated box with Accountable access). | . ",
    "url": "/customer360#go-live-checklist",
    
    "relUrl": "/customer360#go-live-checklist"
  },"182": {
    "doc": "Customer 360 Case",
    "title": "Example Rules (Snippets)",
    "content": "Normalize Email (Data Part Rule) . | Condition: Email present | Actions: lower-case; regex validation; tag InvalidEmail on fail. | . Phone to E.164 . | Condition: Phone present | Actions: parse/format; tag InvalidPhone on failure. | . Customer Identity (Person Auto-merge) . | If Email exact OR Phone exact OR CustomerId exact → Confidence = 1.0 (auto). | Else if FuzzyName ≥ 0.92 AND AddressSim ≥ 0.9 AND Country equal → Confidence = 0.95 (steward). | . Golden Survivorship (Primary Email) . | Precedence: CRM &gt; E-com &gt; Support &gt; Marketing; tie-break = most recently verified + reachable consent. | . MarketingReachable Flag . | If ValidEmail AND Consent(Email)=granted → MarketingReachable=true; else false. | . Lifecycle Stage (Derived) . | If Orders&gt;0 AND LastOrder &lt; 365d → Active; | If Orders&gt;0 AND LastOrder ≥ 365d → ChurnRisk; | If Orders=0 AND LastInteraction &lt; 90d → Prospect. | . ",
    "url": "/customer360#example-rules-snippets",
    
    "relUrl": "/customer360#example-rules-snippets"
  },"183": {
    "doc": "Customer 360 Case",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Over-fuzzy merges → require high-confidence evidence (email/phone/ID); steward candidates. | Channel overrides pollute golden → keep Golden canonical; apply channel transforms in exports. | Ignoring consent in exports → wire suppression flags into all downstream feeds. | Address issues cause duplicates → standardize and geocode before matching. | Running heavy jobs on laptops → use dedicated compute to prevent timeouts. | . ",
    "url": "/customer360#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/customer360#common-pitfalls--how-to-avoid-them"
  },"184": {
    "doc": "Customer 360 Case",
    "title": "Success Metrics",
    "content": ". | Duplicate rate (Person/Account) &lt; 1–2% with high precision auto-merge. | Valid Email ≥ 98%, Valid Phone ≥ 95%; Address standardized ≥ 97%. | Consent coverage for marketable customers ≥ 99% accurate. | Time-to-resolve stewardship queue ↓; MarketingReachable growth ↑. | Downstream CRM/MA/CS exception rate ↓ after go-live. | . ",
    "url": "/customer360#success-metrics",
    
    "relUrl": "/customer360#success-metrics"
  },"185": {
    "doc": "Customer 360 Case",
    "title": "Summary",
    "content": "Customer 360 in CluedIn unifies identities, standardizes attributes, enforces consent, and publishes a governed, audit-ready customer truth to every system. Start with clean identifiers and solid survivorship, validate merges with real samples, and keep Golden canonical while handling channel-specific needs in your exports. ",
    "url": "/customer360#summary",
    
    "relUrl": "/customer360#summary"
  },"186": {
    "doc": "Customer 360 Case",
    "title": "Customer 360 Case",
    "content": " ",
    "url": "/customer360",
    
    "relUrl": "/customer360"
  },"187": {
    "doc": "Data impact workshop",
    "title": "On this page",
    "content": ". | Data selection principles | Data impact quadrant | Data impact workshop scenario . | Step 1: Identify critical data sources | Step 2: List data sources | Step 3: List business domains | Step 4: Put sources with domains on the quadrant | Step 5: Create a list with your priorities | . | . | Audience | Time to read | . | Business User, Data Project Lead, Data Steward, Data Analyst, Data Architect, Data Engineer | 8 min | . You are here in the data journey . This article outlines the importance of the data impact workshop as the first step of your data ingestion process. The data impact workshop is a collaborative session designed to help you identify your business domains, determine the data sources to ingest, and assess their criticality to your operations. If you have signed the statement of work (SOW) with CluedIn, the data impact workshop will be conducted by a CluedIn expert. If you haven’t signed an SOW with CluedIn, use this playbook to conduct the data impact workshop on your own. ",
    "url": "/playbooks/data-ingestion-playbook/data-impact-workshop#on-this-page",
    
    "relUrl": "/playbooks/data-ingestion-playbook/data-impact-workshop#on-this-page"
  },"188": {
    "doc": "Data impact workshop",
    "title": "Data selection principles",
    "content": "Data for an MDM project is typically characterized by ease of access, criticality, and completeness. In this section, we explore how these characteristics influence the data you select for your use cases and provide a recommended approach to demonstrate the value of your MDM project. Ease of access vs. Criticality . We always advise you to strike a balance between the ease of access and the criticality of your data. If you want your MDM project to be successful, you will need it to operate on the data that is the most valuable to you. However, from time to time, the access to this data can be restricted or time-consuming as it requires multiple approval steps and buy-in from the business department before starting. Criticality vs. Completeness . The main data ingestion principle for your first use case is: “Focus on the critical data you have access to and start with a simple implementation”. That is why we believe that is probably more important for you to demonstrate value on a sample of critical data rather than having a full solution on the sources that are deemed “nice to have”. So, always favor criticality over completeness. Recommended approach . At the start of your MDM project, use data that will showcase the full value of your MDM solution. Avoid focusing on “nice-to-have” use cases where you may have full access but struggle to gain business buy-in. For a businessperson, seeing a full solution for 10,000 records is a strong proof of value. You can demonstrate the solution, gather feedback, and start preparing your next use cases. This approach aligns with agile methodology, allowing your team to work on new use cases with business stakeholders based on the feedback received, while simultaneously growing the number of records in the test and production environments. ",
    "url": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-selection-principles",
    
    "relUrl": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-selection-principles"
  },"189": {
    "doc": "Data impact workshop",
    "title": "Data impact quadrant",
    "content": "The data impact quadrant is a tool that helps you visualize and assess data based on two key criteria: impact and access to data. The purpose of the data impact quadrant is to help you map your sources of data and your business domains in order to evaluate where you should spend your time first. What are the goals of your data impact quadrant? . | Understand where you can gain value with minimal effort within your organization. While it can be challenging to access critical data, this approach will at least align your team with expectations and keep project stakeholders in the line of business (LOB) informed. | Understand what can and cannot be delivered at the start of the project. We’ve often seen data teams face pressure due to misaligned expectations from the start. It’s not uncommon for people in the line of business (LOB) to have unrealistic timelines, especially when the data team hasn’t even received the necessary data to begin with. Therefore, it’s crucial to follow the workshop and present workshop results to your stakeholders. By doing so, you can communicate the criticality of your demands and the impact of any delays to your stakeholders. | Understand what blockers may arise for each source of data. Create the risk assessment matrix that you can update over time and share with your business stakeholders. Sometimes, a good critical escalation call can unlock the majority of your needs that you were waiting to resolve for months. So, make sure to track the blockers on the source of data. An MDM project requires access to critical data, and you need to be clear that to deliver the value, you will need access to it. | . How can critical escalation lead to better project alignment? . Early in the process, you may need to escalate critical issues. While this may not be a call you like to have, it is often necessary. We’ve seen data teams work on use cases that provided minimal value from the stakeholders’ perspective, leading to bigger problems down the line and even project cancellations. To avoid this, we encourage you to raise your voice internally and maintain transparency with your stakeholders. If they’re not on board, you may face negativity and high pressure later, having to change use cases or sources extremely fast due to misalignment between your work and stakeholder expectations. We recommend providing at least a monthly demo and status update to ensure stakeholders in the line of business (LOB) are aware of the current situation, as well as the sources and tasks you’re working on. ",
    "url": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-impact-quadrant",
    
    "relUrl": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-impact-quadrant"
  },"190": {
    "doc": "Data impact workshop",
    "title": "Data impact workshop scenario",
    "content": "Now that you understand the importance of the data impact quadrant, let’s explore how it works. Follow this link to find a ready-to-use Miro board for conducting the data impact quadrant exercise with your team. Alternatively, you can use other tools to conduct exercise. The following image illustrates the data impact quadrant created during a workshop. Generally, the data impact workshop consists of 5 steps. Step 1: Identify critical data sources . The first step is to identify your data sources. What is critical data? . To find out what your critical data is, answer the following questions: . | If I remove this source, can my company operate without interruption? . | If the answer is yes, this is not critical data. | If the answer is no, it may be critical data. | . | If I remove this source, what monetary impacts would it have? . | If I remove this source, the entire operation line is down. This would indicate that the source is critical. | If I remove this source, our sales team cannot sell any more. This would indicate that the source is critical. | If I remove this source, our support team cannot operate. This would indicate that the source might be critical. | . | . We cannot really do this exercise for you, all we can do is to help you ask the right questions. However, if your MDM project does not involve critical data, be aware that you may need to put in extra effort to demonstrate its value to business stakeholders and management. While we cannot determine what your company deems critical, we can assist you in that process. Step 2: List data sources . The second step is to list each source with a brief description and assign a priority to each. | Priority | Source | Description | Value for MDM | . | 1. | CRM | Hold all the information we have around our customers | Required normalization work as lots of manual work on reporting | . | 2. | ERP | Hold all the business domain important for our production | Disconnected from the CRM detail that should be hold the truth | . | 3. | … | … |   | . Step 3: List business domains . The third step is to add business domains for each source. Keep in mind that there may be overlap. | Priority | Domain | Source | Value for MDM | . | 1. | Customer | CRM | Represent who buys our services/products | . | 2. | Prospect | CRM | Represent a potential customer | . | 3. | Deals | CRM | Represent a deal that is signed or sent | . | 4. | Customer | ERP | Represent who will get our products | . | 5. | Product | ERP | Represent the product we are selling | . | 6. | Shipping Info | ERP | Represent the shipping contact for a given customer | . Step 4: Put sources with domains on the quadrant . The fourth step is to place the sources along with domains on the quadrant. Each of your team members should place the sources along with business domains on the quadrant. It’s important for your team to discuss and align on what they consider “critical” and what they perceive as hard or easy to access. One team member might have access you weren’t aware of, or you might have different perspectives on what defines critical data. Now is a good time to have those conversations and reach alignment. Step 5: Create a list with your priorities . Once you’ve reached an agreement, create a list of data sources, starting from the top right and moving to the bottom left, to establish your priorities. The following image shows the direction in which you should list the data sources. The following table is an example of what the result of the exercise might look like. | Priority | Domain | Source | Value for MDM | . | 1. | Customer | CRM | Represent who buys our services/products | . | 2. | Customer | ERP | Represent who will get our products | . | 3. | Shipping Info | ERP | Represent the shipping contact for a given customer | . | 4. | Product | ERP | Represent the product we are selling | . | 4. | Deals | CRM | Represent a deal that is signed or sent | . | 5. | Prospect | CRM | Represent a potential customer | . With this result, we know we need to work with Customers and with the ERP and the CRM source first. We already have the start of a plan. ",
    "url": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-impact-workshop-scenario",
    
    "relUrl": "/playbooks/data-ingestion-playbook/data-impact-workshop#data-impact-workshop-scenario"
  },"191": {
    "doc": "Data impact workshop",
    "title": "Data impact workshop",
    "content": " ",
    "url": "/playbooks/data-ingestion-playbook/data-impact-workshop",
    
    "relUrl": "/playbooks/data-ingestion-playbook/data-impact-workshop"
  },"192": {
    "doc": "1. CluedIn Fundamentals",
    "title": "1. CluedIn Fundamentals",
    "content": "Level: Beginner . CluedIn Fundamentals is the starting point of a progressive 4-level journey designed to help you master the platform. In this first stage, you’ll learn how to ingest, clean, and explore data using CluedIn’s flexible graph-based model. Presented by: Pierre Derval, Chief Product Officer at CluedIn . Presentation: Download PPT . ",
    "url": "/training/fundamentals",
    
    "relUrl": "/training/fundamentals"
  },"193": {
    "doc": "1.1. Ingestion",
    "title": "1.1. Ingestion",
    "content": "In the course of our comprehensive CluedIn Fundamentals training, we’ll explore how to build a single customer view by transforming disconnected data sources from CRM and ERP systems into unified and trusted golden records. This training is divided into several sessions, where you’ll learn how to perform key operations in CluedIn. Presented by: Pierre Derval, Chief Product Officer at CluedIn . Presentation: Download PPT . Training plan: . | Session | Objective | Watch time | . | 1.1.1. Ingesting a CSV file | - Ingest a CSV file containing customer records from the CRM system. | 5 mins | . | 1.1.2. Ingesting a database table | - Ingest a database table with customer data from an ERP system. | 6 mins | . | 1.1.3. Building a single customer view | - Build a single customer view by identifying and merging duplicate records from the CRM and the ERP. | 36 mins | . | 1.1.4. Identifying and labelling incorrect data | - Identify and label invalid records using tags. | 12 mins | . ",
    "url": "/training/fundamentals/ingestion",
    
    "relUrl": "/training/fundamentals/ingestion"
  },"194": {
    "doc": "Product 360",
    "title": "Product 360 in CluedIn — Step-by-Step Implementation Guide",
    "content": "Product 360 delivers a single, trusted view of each product across ERP, PIM, e‑commerce, supplier feeds, and catalog systems. This guide walks you through the end-to-end build in CluedIn: ingest → standardize → match/link → golden record → enrich → govern → publish. ",
    "url": "/product360#product-360-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/product360#product-360-in-cluedin--step-by-step-implementation-guide"
  },"195": {
    "doc": "Product 360",
    "title": "Outcomes",
    "content": ". | One golden product record per product (and per variant) with complete, standardized attributes. | Linked entities: Product ↔ Variant ↔ Category/Taxonomy ↔ Supplier ↔ Price ↔ Inventory ↔ Assets (images/specs). | Quality, lineage, and auditability for compliance and omnichannel consistency. | Publish/sync to PIM, e‑commerce, ERP, search, and analytics. | . ",
    "url": "/product360#outcomes",
    
    "relUrl": "/product360#outcomes"
  },"196": {
    "doc": "Product 360",
    "title": "Prerequisites",
    "content": ". | Source access: ERP (item master), PIM, e‑commerce platform, supplier feeds (CSV/Excel/API), DAM (images/specs), pricing/inventory. | RACI: Ensure Accountable access to objects you will import/export. | Use a dedicated server/VM for heavy runs (ingestion, matching, toolkit exports/imports). | . ",
    "url": "/product360#prerequisites",
    
    "relUrl": "/product360#prerequisites"
  },"197": {
    "doc": "Product 360",
    "title": "Reference Data Model",
    "content": "Core Entities . | Product (base/SPU) | Variant (SKU level) | Category / Taxonomy (canonical categories, possibly multiple hierarchies) | Supplier (links to Supplier 360) | Price (list/promo, channel, currency, effective dates) | Inventory (location, on‑hand, safety stock) | Asset (images, datasheets, manuals, specs PDF) | . Key Attributes (examples) . | Product: Brand, Manufacturer, MPN, Title, Description, Category, Attributes (JSON or normalized columns), LifecycleStatus. | Variant: SKU, GTIN/EAN/UPC, Size, Color, Material, Dimensions, Weight, Pack/Case/Unit relationships. | Asset: URL/Path, Type, Format, Resolution, Primary/Secondary, Hash. | . Relationships . | Product has many Variants, Assets, Prices, Inventory records. | Product belongs to one or more Categories/Taxonomies. | Product supplied by Supplier(s); kit/bundle links to components (BOM). | . ",
    "url": "/product360#reference-data-model",
    
    "relUrl": "/product360#reference-data-model"
  },"198": {
    "doc": "Product 360",
    "title": "Step 1 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | Add ERP item master and attributes. | Add PIM/e‑commerce catalogs (including channel overrides). | Add supplier feeds (cost, specs) and DAM (assets). | Add pricing and inventory sources (if modeled in CluedIn). | . Tips . | Start with samples to inspect field coverage and formats. | Capture source metadata: SourceSystem, LoadTimestamp, primary keys. | . ",
    "url": "/product360#step-1--connect--ingest-sources",
    
    "relUrl": "/product360#step-1--connect--ingest-sources"
  },"199": {
    "doc": "Product 360",
    "title": "Step 2 — Define Entities &amp; Mappings",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create Product, Variant, Category, Asset, Price, Inventory. | Map source fields to target attributes (e.g., ShortDesc → Title). | Persist provenance (SourceSystem, SourceRecordID, SourceField). | . Tip: Keep variant‑level attributes (SKU, GTIN, Color/Size) separate from product‑level attributes (Brand, Title). ",
    "url": "/product360#step-2--define-entities--mappings",
    
    "relUrl": "/product360#step-2--define-entities--mappings"
  },"200": {
    "doc": "Product 360",
    "title": "Step 3 — Standardization &amp; Normalization",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to normalize inputs before matching: . | Text: trim, case, punctuation; remove HTML; normalize diacritics. | Units: convert dimensions/weight to canonical units (e.g., cm/kg). | Identifiers: validate/checksum for GTIN/EAN/UPC; normalize MPN (strip spaces). | Taxonomy: map source categories → canonical taxonomy (use RDM if available). | Assets: compute file hash; flag duplicates; classify by type. | . Tags (recommended): Standardized, InvalidGTIN, MissingKeyAttrs, LegacySource. ",
    "url": "/product360#step-3--standardization--normalization",
    
    "relUrl": "/product360#step-3--standardization--normalization"
  },"201": {
    "doc": "Product 360",
    "title": "Step 4 — Product Identity Resolution (Matching)",
    "content": "Portal: Entity Matching → Product / Variant . | Blocking keys to reduce comparisons: . | (GTIN) when present | (Brand + MPN) | (NormalizedTitle + Category) | . | . | Match rules (examples): . | Variant (SKU) auto‑merge: exact GTIN OR exact SKU within same Brand. | Product (SPU) auto‑merge: exact Brand + MPN. | Candidate (steward): FuzzyTitle ≥ 0.92 AND Brand equal AND similar key attributes (e.g., size/pack). | . | Set thresholds (e.g., Auto‑merge ≥ 0.97; Steward review 0.90–0.97). | Run matching; review in Data Stewardship (approve/reject/Unmerge). | . Pitfall: Don’t merge variants across different packs (e.g., single vs 12‑pack). Include PackSize / UoM in rules. ",
    "url": "/product360#step-4--product-identity-resolution-matching",
    
    "relUrl": "/product360#step-4--product-identity-resolution-matching"
  },"202": {
    "doc": "Product 360",
    "title": "Step 5 — Stewardship: Review &amp; Merge",
    "content": "Portal: Data Stewardship → Match Review . | Validate candidates using GTIN, Brand+MPN, and key attributes. | Approve merges, reject false positives, and tag AmbiguousMatch when needed. | Use Unmerge if a mistake is found (audit preserved in History). | . ",
    "url": "/product360#step-5--stewardship-review--merge",
    
    "relUrl": "/product360#step-5--stewardship-review--merge"
  },"203": {
    "doc": "Product 360",
    "title": "Step 6 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define precedence per attribute and tie‑breakers: . | Attribute | Precedence (high → low) | Tie‑breaker | . | Brand, MPN | ERP &gt; PIM &gt; Supplier | Most recent verified | . | Title | PIM &gt; ERP &gt; e‑commerce | Longest non‑marketing title | . | Description | PIM (tech) &gt; ERP &gt; e‑commerce | Richest (char count + spec fields) | . | Category | Canonical Taxonomy (RDM) &gt; Source | Steward override | . | GTIN | PIM/ERP only | N/A | . | Attributes | PIM &gt; ERP &gt; Supplier | Highest completeness | . | Images/Assets | DAM &gt; PIM &gt; e‑commerce | Resolution &amp; aspect ratio | . | LifecycleStatus | ERP &gt; PIM | Most restrictive status | . Actions . | Tag Sellable when required attributes complete and status active. | Compute CompletenessScore and tag LowCompleteness &lt; 0.85. | . ",
    "url": "/product360#step-6--golden-record-survivorship",
    
    "relUrl": "/product360#step-6--golden-record-survivorship"
  },"204": {
    "doc": "Product 360",
    "title": "Step 7 — Asset Management (Images/Docs)",
    "content": "Portal: Data Sources (DAM) / Rules . | Deduplicate by hash; pick PrimaryImage per variant (min resolution, white background if policy). | Validate asset links; move broken links to stewardship queue. | Channel‑specific renditions (size/aspect) can be generated downstream; store metadata. | . ",
    "url": "/product360#step-7--asset-management-imagesdocs",
    
    "relUrl": "/product360#step-7--asset-management-imagesdocs"
  },"205": {
    "doc": "Product 360",
    "title": "Step 8 — Pricing &amp; Inventory (Optional Modeling)",
    "content": "Portal: Entity Explorer / Rules . | Price: store List, Sale, Currency, Channel, EffectiveFrom/To. | Inventory: store Location, OnHand, AvailableToSell. | Compute InStock flag for channel exports. | . ",
    "url": "/product360#step-8--pricing--inventory-optional-modeling",
    
    "relUrl": "/product360#step-8--pricing--inventory-optional-modeling"
  },"206": {
    "doc": "Product 360",
    "title": "Step 9 — Quality Scoring &amp; Dashboards",
    "content": "Portal: Data Quality → Profiling / Dashboards KPIs: completeness %, invalid identifiers, duplicate rate, image coverage, attribute fill by category, enrichment freshness. ",
    "url": "/product360#step-9--quality-scoring--dashboards",
    
    "relUrl": "/product360#step-9--quality-scoring--dashboards"
  },"207": {
    "doc": "Product 360",
    "title": "Step 10 — Governance, Security &amp; Lifecycle",
    "content": "Portal: Governance → Permissions / Policies . | Restrict sensitive attributes (e.g., cost). | Enforce RACI for stewards and domain owners. | Apply Retention (e.g., archive discontinued after N years; keep specs/lineage). | . ",
    "url": "/product360#step-10--governance-security--lifecycle",
    
    "relUrl": "/product360#step-10--governance-security--lifecycle"
  },"208": {
    "doc": "Product 360",
    "title": "Step 11 — Enrichment (Optional)",
    "content": "Portal: Data Sources / Rules . | Pull manufacturer specs, regulatory attributes (RoHS/REACH), and alternate identifiers. | Track LastEnrichedAt and provider; re‑check on schedule. | . ",
    "url": "/product360#step-11--enrichment-optional",
    
    "relUrl": "/product360#step-11--enrichment-optional"
  },"209": {
    "doc": "Product 360",
    "title": "Step 12 — Publish / Integrate",
    "content": "Portal: Exports . | PIM/e‑commerce: golden product + variant attributes, primary assets, channel flags. | ERP: authoritative updates where CluedIn is source of truth. | Analytics/DW: wide denormalized table for BI/search. | APIs/Eventing: expose read endpoints and change events. | . Start read‑only to validate mappings, then switch to authoritative syncs. ",
    "url": "/product360#step-12--publish--integrate",
    
    "relUrl": "/product360#step-12--publish--integrate"
  },"210": {
    "doc": "Product 360",
    "title": "Step 13 — Scheduling &amp; Operations",
    "content": ". | Schedule ingestion, matching, golden updates, enrichment, and exports (off‑peak for heavy jobs). | Monitor runtimes; alert on spikes or queue backlogs. | Use Product Toolkit for packaging to staging/prod; run on a dedicated box with Accountable access. | . ",
    "url": "/product360#step-13--scheduling--operations",
    
    "relUrl": "/product360#step-13--scheduling--operations"
  },"211": {
    "doc": "Product 360",
    "title": "Step 14 — Validate &amp; UAT",
    "content": ". | Sample 50–100 high‑volume products across categories. | Verify no cross‑product conflation; confirm variant separation and image correctness. | Business sign‑off from PIM/e‑commerce/ERP owners. | . ",
    "url": "/product360#step-14--validate--uat",
    
    "relUrl": "/product360#step-14--validate--uat"
  },"212": {
    "doc": "Product 360",
    "title": "Go‑Live Checklist",
    "content": ". | All in‑scope sources ingested; mappings validated. | Matching thresholds tuned; false‑positive rate low; unmerge tested. | Golden survivorship approved; completeness thresholds set. | Canonical taxonomy mapping verified. | Asset primary/alt images validated. | Exports tested end‑to‑end in target systems. | Permissions, retention, and audit verified. | Promotion package built with Product Toolkit on dedicated VM. | . ",
    "url": "/product360#golive-checklist",
    
    "relUrl": "/product360#golive-checklist"
  },"213": {
    "doc": "Product 360",
    "title": "Example Rules (Snippets)",
    "content": "Normalize GTIN / MPN . | Condition: GTIN present | Action: strip non‑digits; validate checksum; tag InvalidGTIN on fail. | Condition: MPN present → uppercase, remove spaces/hyphens → MPN_Normalized. | . Auto‑merge Variant . | If GTIN exact OR (SKU exact AND Brand equal) → Confidence 1.0 (auto). | . Candidate Product Merge . | If Brand equal AND MPN_Normalized equal → Confidence 0.98 (auto). | Else if FuzzyTitle ≥ 0.92 AND Category equal AND KeyAttrSim ≥ 0.9 → Confidence 0.95 (steward). | . Golden Survivorship . | Title: PIM &gt; ERP &gt; e‑commerce; prefer longest non‑marketing title. | PrimaryImage: choose highest resolution; white background preferred; tag Primary. | . Sellable Flag . | If required attrs complete (Title, Brand, Category, GTIN, PrimaryImage) AND LifecycleStatus=Active AND InStock=true → set Sellable=true. | . ",
    "url": "/product360#example-rules-snippets",
    
    "relUrl": "/product360#example-rules-snippets"
  },"214": {
    "doc": "Product 360",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Merging different pack sizes → include Pack/UoM in match rules. | Over‑reliance on fuzzy title → require GTIN or Brand+MPN for auto‑merge. | Channel overrides pollute golden → keep Golden canonical; apply channel transforms in exports. | Dirty units/attributes → standardize units before matching; centralize UoM with RDM. | Running heavy jobs on laptops → use dedicated compute to avoid timeouts. | . ",
    "url": "/product360#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/product360#common-pitfalls--how-to-avoid-them"
  },"215": {
    "doc": "Product 360",
    "title": "Success Metrics",
    "content": ". | Duplicate rate ↓; auto‑merge precision ≥ 98%. | Golden completeness ≥ 90% per category; image coverage ≥ 95%. | Time‑to‑publish new SKU ↓ (hours, not days). | Downstream catalog exceptions ↓; price/inventory sync accuracy ↑. | . ",
    "url": "/product360#success-metrics",
    
    "relUrl": "/product360#success-metrics"
  },"216": {
    "doc": "Product 360",
    "title": "Summary",
    "content": "Product 360 in CluedIn unifies product and variant data, establishes clear survivorship, governs taxonomy and assets, and reliably syndicates to every channel. Start with clean identifiers and taxonomy, validate merges with high‑confidence evidence, and keep golden canonical while handling channel‑specific needs in exports. ",
    "url": "/product360#summary",
    
    "relUrl": "/product360#summary"
  },"217": {
    "doc": "Product 360",
    "title": "Product 360",
    "content": " ",
    "url": "/product360",
    
    "relUrl": "/product360"
  },"218": {
    "doc": "Release 2024.12",
    "title": "On this page",
    "content": ". | New home dashboard | Access control | Role access status | Exporting golden records | Workflows | Deleting data parts and golden records | History of golden record relations | Data set filters and operations | New export targets | Knowledge hub | Help panel | Duplicate actions | . This article outlines new features and improvements in CluedIn 2024.12. The following sections contain brief description of new features and links to related articles. ",
    "url": "/release-notes/2024-12#on-this-page",
    
    "relUrl": "/release-notes/2024-12#on-this-page"
  },"219": {
    "doc": "Release 2024.12",
    "title": "New home dashboard",
    "content": "New, interactive home dashboard provides quick and simple access to the most important elements in your master data management project. It consists of 4 parts: . | Cards with the information about golden records, sources, clean projects, deduplication projects, and streams. Each card contains the number of corresponding items, link to the relevant area of the application, and link to documentation. | Charts that provide a comprehensive view over all of your business domains used for organizing golden records. Just hover over an element on the chart, and you’ll see the number of golden records belonging to a specific business domain. | Statistics on billable records, users, business domains, vocabularies, and vocabulary keys. | Useful links to documentation and resources, where you can find help and guidance on various activities in CluedIn. | . To access new home dashboard, turn on the corresponding feature flag. ",
    "url": "/release-notes/2024-12#new-home-dashboard",
    
    "relUrl": "/release-notes/2024-12#new-home-dashboard"
  },"220": {
    "doc": "Release 2024.12",
    "title": "Access control",
    "content": "Access control gives you fine-grained control over who can view specific golden records and vocabulary keys. Together with source control (previously, global security filter), access control helps you configure reliable and secure access to data in CluedIn. For more information, see: . | Data access – here, you’ll find details about the combination of source control and access control. | Access control – here, you’ll find instructions on how to create and configure an access control policy. | . ",
    "url": "/release-notes/2024-12#access-control",
    
    "relUrl": "/release-notes/2024-12#access-control"
  },"221": {
    "doc": "Release 2024.12",
    "title": "Role access status",
    "content": "Role access status provides better visibility and transparency of the feature access that each role has. Now, you can see which actions are governed by each specific claim, and whether a role has access to a particular action. For more information, see: . | Roles – here, you’ll find a list of all CluedIn roles and an explanation of claims and access levels. | Feature access – here, you’ll learn about the main settings that define access to CluedIn features: roles and ownership. | Claims – here, you’ll find detailed explanation of each claim used in CluedIn. | . To view role access status, turn on the corresponding feature flag. ",
    "url": "/release-notes/2024-12#role-access-status",
    
    "relUrl": "/release-notes/2024-12#role-access-status"
  },"222": {
    "doc": "Release 2024.12",
    "title": "Exporting golden records",
    "content": "After performing a search, you can now export your results in one of the following formats: JSON, CSV, XLSX, or Parquet. For more information, see Export search results. To access the golden records export feature, turn on the corresponding feature flag. ",
    "url": "/release-notes/2024-12#exporting-golden-records",
    
    "relUrl": "/release-notes/2024-12#exporting-golden-records"
  },"223": {
    "doc": "Release 2024.12",
    "title": "Workflows",
    "content": "New Workflow module is designed to help you streamline and track approvals and notifications for specific activities in CluedIn. When a change requires approval, an approval request is automatically sent to the responsible users via Outlook or the Approvals app in Teams, where they can approve or reject a change. However, keep in mind that the approvals and notifications that are sent outside of CluedIn are intended only for SSO users. To access workflows, turn on the corresponding feature flag. For more information on how to configure and use workflows, see Workflows. ",
    "url": "/release-notes/2024-12#workflows",
    
    "relUrl": "/release-notes/2024-12#workflows"
  },"224": {
    "doc": "Release 2024.12",
    "title": "Deleting data parts and golden records",
    "content": "Previously, you could delete data parts by data source, which resulted in removing those data parts from every golden record where they were used. Now, you can delete individual data parts from a specific golden record. For more information, see Delete individual data parts. In addition, now you can also delete a golden record. However, keep in mind that deleting golden records is permanent and irreversible, so carefully consider which golden records you want to delete. For more information, see Delete golden records. ",
    "url": "/release-notes/2024-12#deleting-data-parts-and-golden-records",
    
    "relUrl": "/release-notes/2024-12#deleting-data-parts-and-golden-records"
  },"225": {
    "doc": "Release 2024.12",
    "title": "History of golden record relations",
    "content": "Now, you can view all outgoing relations for a golden record on the History tab. Here, you can view edge properties, filter edges by type or property, easily find edge details, or delete an edge if you no longer need it. For more information, see History . ",
    "url": "/release-notes/2024-12#history-of-golden-record-relations",
    
    "relUrl": "/release-notes/2024-12#history-of-golden-record-relations"
  },"226": {
    "doc": "Release 2024.12",
    "title": "Data set filters and operations",
    "content": "Data set filters and operations help you analyze uploaded data and make changes to it on the Preview tab of the data set. This is useful when you want to prepare your data for processing by fixing some data quality issues or editing the contents of the cell. To access data set filters and operations, turn on the corresponding feature flag. For more information, see Data set filters and operations. ",
    "url": "/release-notes/2024-12#data-set-filters-and-operations",
    
    "relUrl": "/release-notes/2024-12#data-set-filters-and-operations"
  },"227": {
    "doc": "Release 2024.12",
    "title": "New export targets",
    "content": "Azure Data Lake, OneLake, and Dataverse have been added to the list of predefined export targets. In addition, now you can add a user-friendly name to the export target. This way it will be easier to identify it in the list of export targets. For more information, see: . | Azure Data Lake connector – here, you’ll find the prerequisites and instructions for configuring the Azure Data Lake connector. | OneLake connector – here, you’ll find the prerequisites and instructions for configuring the OneLake connector. | . ",
    "url": "/release-notes/2024-12#new-export-targets",
    
    "relUrl": "/release-notes/2024-12#new-export-targets"
  },"228": {
    "doc": "Release 2024.12",
    "title": "Knowledge hub",
    "content": "Knowledge hub is a place where you can quickly find helpful resources and documentation. It consists of 3 parts: . | Documentation – here, you can find links to the most important articles for each CluedIn module. To get access to documentation, turn on the Knowledge Hub feature flag. | Articles – here, you’ll find our news and articles with insights and industry best practices. | Videos – here, you’ll find webinars, events, and discussions on various topics. To get access to articles and videos, turn on the CluedIn News feature flag. | . ",
    "url": "/release-notes/2024-12#knowledge-hub",
    
    "relUrl": "/release-notes/2024-12#knowledge-hub"
  },"229": {
    "doc": "Release 2024.12",
    "title": "Help panel",
    "content": "Help panel is a place where you can quickly report any issues you encounter while using CluedIn, as well as let us know about your ideas for improvements and new features. ",
    "url": "/release-notes/2024-12#help-panel",
    
    "relUrl": "/release-notes/2024-12#help-panel"
  },"230": {
    "doc": "Release 2024.12",
    "title": "Duplicate actions",
    "content": "Now you can easily duplicate clean projects, rules, deduplication projects, streams, glossary terms, and more. This action allows you to create new elements in CluedIn using the configurations of the existing elements. To access the duplication feature, turn on the corresponding feature flag. ",
    "url": "/release-notes/2024-12#duplicate-actions",
    
    "relUrl": "/release-notes/2024-12#duplicate-actions"
  },"231": {
    "doc": "Release 2024.12",
    "title": "Release 2024.12",
    "content": " ",
    "url": "/release-notes/2024-12",
    
    "relUrl": "/release-notes/2024-12"
  },"232": {
    "doc": "CluedIn SaaS",
    "title": "CluedIn SaaS",
    "content": "CluedIn SaaS is the easiest and quickest way to start with CluedIn. You do not need an IT team to install and start using CluedIn. Check out the following video for an overview of CluedIn SaaS installation process. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. Useful resources: . | Installation – learn about the difference between CluedIn SaaS and PaaS. | Pricing – learn about our pricing options (pay-as-you-go and committed deal). | . ",
    "url": "/deployment/saas",
    
    "relUrl": "/deployment/saas"
  },"233": {
    "doc": "Supplier 360",
    "title": "Supplier 360 in CluedIn — Step-by-Step Implementation Guide",
    "content": "Supplier 360 delivers a single, trusted, and actionable view of each supplier across ERP, procurement, finance/AP, risk, and contract systems. This guide walks you through the end-to-end build in CluedIn: ingest → standardize → match/link → golden record → enrich → govern → publish. ",
    "url": "/supplier360#supplier-360-in-cluedin--step-by-step-implementation-guide",
    
    "relUrl": "/supplier360#supplier-360-in-cluedin--step-by-step-implementation-guide"
  },"234": {
    "doc": "Supplier 360",
    "title": "Outcomes",
    "content": ". | A golden supplier record per supplier (and per site) with complete, standardized attributes. | Linked entities: supplier ↔ sites ↔ contacts ↔ contracts ↔ POs ↔ invoices ↔ risk events. | Quality, lineage, and auditability for compliance and operations. | Publish/Sync to downstream systems (ERP, procurement, analytics). | . ",
    "url": "/supplier360#outcomes",
    
    "relUrl": "/supplier360#outcomes"
  },"235": {
    "doc": "Supplier 360",
    "title": "Prerequisites",
    "content": ". | Access to source systems (e.g., SAP/Oracle ERP, Ariba/Coupa, AP platform, spreadsheets). | RACI: Ensure you have Accountable access for the objects you’ll import/export. | A dedicated environment (or VM) for heavy jobs (ingestion, matching, toolkit operations). | . ",
    "url": "/supplier360#prerequisites",
    
    "relUrl": "/supplier360#prerequisites"
  },"236": {
    "doc": "Supplier 360",
    "title": "Reference Data Model",
    "content": "Core entities . | Supplier (legal entity level) | SupplierSite (site/plant/branch) | Contact (people) | Contract | PurchaseOrder | Invoice | RiskEvent (e.g., sanctions hit, delivery failure, ESG breach) | . Common attributes (examples) . | Supplier: LegalName, TradingName, TaxID (VAT/ABN/EIN), DUNS, RegistrationNo, Country, Address, BankAccount (IBAN/BIC), EmailDomain, Category, Status. | SupplierSite: SiteName, Address, Country, GLN, ParentSupplierId. | . Relationships . | Supplier has many SupplierSites, Contacts, Contracts, POs, Invoices, RiskEvents. | . ",
    "url": "/supplier360#reference-data-model",
    
    "relUrl": "/supplier360#reference-data-model"
  },"237": {
    "doc": "Supplier 360",
    "title": "Step 1 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | Add ERP master data (Vendors/Suppliers + bank data). | Add procurement networks (catalog, onboarding). | Add AP/Finance (invoices, payments). | Add risk/enrichment sources (sanctions, credit, ESG), optional. | Add ad-hoc files (CSV/XLS) for legacy/one-off data. | . Tips . | Use sampling first to inspect field sparsity and formats. | Define source metadata (system name, extraction time, primary keys). | . ",
    "url": "/supplier360#step-1--connect--ingest-sources",
    
    "relUrl": "/supplier360#step-1--connect--ingest-sources"
  },"238": {
    "doc": "Supplier 360",
    "title": "Step 2 — Define Entities &amp; Mappings",
    "content": "Portal: Entity Explorer → + Create Entity Type . | Create Supplier and SupplierSite entity types. | Map source fields → target attributes (normalize names like VendorName → LegalName). | Track provenance (SourceSystem, SourceRecordID). | . Tip: Keep site-level data separate from the parent supplier; many duplicates stem from conflating these. ",
    "url": "/supplier360#step-2--define-entities--mappings",
    
    "relUrl": "/supplier360#step-2--define-entities--mappings"
  },"239": {
    "doc": "Supplier 360",
    "title": "Step 3 — Standardization &amp; Normalization Rules",
    "content": "Portal: Governance → Rules (Data Part Rules) Create rules to standardize inputs before matching: . | Names: strip punctuation/stopwords; uppercase/lowercase consistency. | Addresses: split house/street/city/postcode; normalize country codes (ISO-2). | Phones: E.164 standardization. | Tax IDs: remove separators; validate checksum per country. | Bank: IBAN/BIC validation. | Email domains: extract domain from contact emails. | . Recommended tags . | Standardized, NeedsValidation, InvalidTaxID, InvalidBank, LegacySource. | . ",
    "url": "/supplier360#step-3--standardization--normalization-rules",
    
    "relUrl": "/supplier360#step-3--standardization--normalization-rules"
  },"240": {
    "doc": "Supplier 360",
    "title": "Step 4 — Configure Identity Resolution (Matching)",
    "content": "Portal: Entity Matching → Supplier . | Blocking keys (to reduce comparisons): . | (Country + CleanedLegalNameSoundex) | (TaxID) when present | (EmailDomain) (fallback) | . | Match rules (examples): . | High-confidence: exact TaxID OR exact DUNS. | Medium: CleanedLegalName fuzzy ≥ 0.92 AND same Country. | Supplemental: phone OR email domain OR address similarity ≥ 0.9. | . | Set thresholds (e.g., Auto-merge ≥ 0.97; Steward review 0.90–0.97). | Run a matching job and review candidates in Data Stewardship. | . Pitfall to avoid: Over-aggressive fuzzy name matching across different countries. Always include Country as a condition unless you have multi-national evidence. ",
    "url": "/supplier360#step-4--configure-identity-resolution-matching",
    
    "relUrl": "/supplier360#step-4--configure-identity-resolution-matching"
  },"241": {
    "doc": "Supplier 360",
    "title": "Step 5 — Stewardship: Review &amp; Merge",
    "content": "Portal: Data Stewardship → Match Review . | Inspect candidate pairs; verify with TaxID, DUNS, or contracts. | Approve merges; reject false positives; tag edge cases (AmbiguousMatch). | Use Unmerge if a mistake is found (audit captured in History). | . Tip: Triage by confidence and impact (number of downstream links). ",
    "url": "/supplier360#step-5--stewardship-review--merge",
    
    "relUrl": "/supplier360#step-5--stewardship-review--merge"
  },"242": {
    "doc": "Supplier 360",
    "title": "Step 6 — Golden Record Survivorship",
    "content": "Portal: Governance → Rules (Golden Record Rules) Define attribute precedence and tie-breakers: . | Attribute | Precedence (highest → lowest) | Tie-breaker | . | LegalName | ERP &gt; Procurement &gt; AP &gt; CSV | Most recent update | . | TaxID/DUNS | ERP only | N/A | . | Address | Site-specific registry &gt; ERP &gt; Procurement | Highest quality score | . | BankAccount | ERP (verified) &gt; AP | Verified flag = true | . | Category | Procurement taxonomy &gt; ERP | Most frequent value | . | Status | Risk compliant &gt; Pending &gt; On hold | Most recent risk check | . Add Actions: . | Tag VerifiedSupplier when TaxID valid &amp; sanctions clear. | Compute completeness score and tag LowCompleteness &lt; 0.8. | . ",
    "url": "/supplier360#step-6--golden-record-survivorship",
    
    "relUrl": "/supplier360#step-6--golden-record-survivorship"
  },"243": {
    "doc": "Supplier 360",
    "title": "Step 7 — Enrichment &amp; Risk",
    "content": "Portal: Data Sources → Add Source (enrichment), Governance → Rules . | Pull official registry details (legal address, directors where available). | Run sanctions/PEP checks; tag SanctionsHit with evidence fields. | Append credit/ESG risk ratings; store dates and providers. | . Tip: Separate enrichment freshness (LastCheckedAt) and set a rule to re-check periodically. ",
    "url": "/supplier360#step-7--enrichment--risk",
    
    "relUrl": "/supplier360#step-7--enrichment--risk"
  },"244": {
    "doc": "Supplier 360",
    "title": "Step 8 — Quality Scoring &amp; Dashboards",
    "content": "Portal: Data Quality → Profiling / Dashboards . | Define KPIs: completeness %, duplicates, invalid TaxID, invalid bank, address parse rate. | Track Golden Record creation rate and match precision (steward overrides vs auto-merge). | . ",
    "url": "/supplier360#step-8--quality-scoring--dashboards",
    
    "relUrl": "/supplier360#step-8--quality-scoring--dashboards"
  },"245": {
    "doc": "Supplier 360",
    "title": "Step 9 — Governance, Security &amp; Lifecycle",
    "content": "Portal: Governance → Policies / Retention / Permissions . | Apply RACI: Accountable access for stewards; Restricted for sensitive fields (bank). | Retention: e.g., archive inactive suppliers after N years; delete after audit closure. | Audit: Ensure changes (merges, overrides) are captured in History; enable rollback. | . ",
    "url": "/supplier360#step-9--governance-security--lifecycle",
    
    "relUrl": "/supplier360#step-9--governance-security--lifecycle"
  },"246": {
    "doc": "Supplier 360",
    "title": "Step 10 — Publish / Integrate",
    "content": "Portal: Exports . | ERP sync: push Supplier + Site golden attributes. | Procurement: push status/category/compliance flags. | Analytics: export wide table to warehouse/BI. | Eventing/APIs: expose CRUD/read APIs for master data consumers. | . Tip: Start with a read-only feed to validate mapping, then switch to authoritative sync. ",
    "url": "/supplier360#step-10--publish--integrate",
    
    "relUrl": "/supplier360#step-10--publish--integrate"
  },"247": {
    "doc": "Supplier 360",
    "title": "Step 11 — Scheduling &amp; Operations",
    "content": ". | Schedule ingestion, matching, and golden updates (off-peak where possible). | Monitor job runtimes; alert on spikes (likely due to source anomalies). | For large batches, run heavy jobs on a dedicated server/VM to avoid timeouts. | . ",
    "url": "/supplier360#step-11--scheduling--operations",
    
    "relUrl": "/supplier360#step-11--scheduling--operations"
  },"248": {
    "doc": "Supplier 360",
    "title": "Step 12 — Validate &amp; UAT",
    "content": ". | Sample checks: 50–100 suppliers across regions and sources. | Verify tax/bank/address survivorship; confirm no cross-supplier conflation. | Have AP/Procurement/Compliance sign off on a go-live checklist (below). | . ",
    "url": "/supplier360#step-12--validate--uat",
    
    "relUrl": "/supplier360#step-12--validate--uat"
  },"249": {
    "doc": "Supplier 360",
    "title": "Step 13 — Package &amp; Promote",
    "content": "Portal/CLI: Product Toolkit . | Export entities, rules, policies, dashboards. | Promote to staging → production. | Ensure you have Accountable RACI and run large toolkit jobs on a dedicated box. | . ",
    "url": "/supplier360#step-13--package--promote",
    
    "relUrl": "/supplier360#step-13--package--promote"
  },"250": {
    "doc": "Supplier 360",
    "title": "Go-Live Checklist",
    "content": ". | Ingestion schedules configured and ran successfully. | Matching thresholds tuned; low false-positive rate verified. | Golden survivorship rules approved by data owners. | Sanctions/credit enrichment configured and refreshed. | Exports mapped and validated in target systems. | Retention &amp; access policies applied. | Runbooks for rollback/unmerge documented. | . ",
    "url": "/supplier360#go-live-checklist",
    
    "relUrl": "/supplier360#go-live-checklist"
  },"251": {
    "doc": "Supplier 360",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Conflating sites with legal entities → Model SupplierSite separately; match on site identifiers. | Over-reliance on name similarity → Require TaxID/DUNS or multi-evidence for auto-merge. | Dirty TaxID formats → Normalize and validate before matching. | Bank detail risk → Use verified source precedence; never crowdsource survivorship. | Timeouts on big runs → Stagger jobs; use dedicated compute; batch by country or source. | Re-polluting golden records → Disable or adjust problematic Rules before rollback/re-runs. | . ",
    "url": "/supplier360#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/supplier360#common-pitfalls--how-to-avoid-them"
  },"252": {
    "doc": "Supplier 360",
    "title": "Example Rules (Patterns)",
    "content": "Data Part Rule — Normalize TaxID . | Condition: TaxID present | Action: strip non-alphanumerics; upper; set TaxID_Normalized; validate checksum; tag InvalidTaxID if fail. | . Matching — Auto-merge . | If TaxID_Normalized exact OR DUNS exact → Confidence = 1.0 (auto). | Else if CleanedLegalName fuzzy ≥ 0.92 AND Country equal AND (EmailDomain OR AddressSim ≥ 0.9) → Confidence = 0.96 (steward). | . Golden Record — Survivorship . | LegalName: ERP &gt; Registry &gt; Procurement (most recent tie-break). | Status: if SanctionsHit then OnHold else Active. | Tag VerifiedSupplier when (TaxIDValid AND SanctionsClear AND completeness ≥ 0.85). | . ",
    "url": "/supplier360#example-rules-patterns",
    
    "relUrl": "/supplier360#example-rules-patterns"
  },"253": {
    "doc": "Supplier 360",
    "title": "Troubleshooting",
    "content": ". | Too many duplicates: Lower auto-merge threshold; add blocking by Country; require TaxID/DUNS. | Missed matches: Add alternative keys (RegistrationNo, GLN, EmailDomain); tune fuzzy threshold upward/downward. | Wrong golden values: Review precedence order; check source quality; re-run with corrected rules. | Slow processing: See performance article; run heavy jobs on dedicated compute; schedule off-peak. | . ",
    "url": "/supplier360#troubleshooting",
    
    "relUrl": "/supplier360#troubleshooting"
  },"254": {
    "doc": "Supplier 360",
    "title": "Success Metrics",
    "content": ". | Duplicate rate ↓, auto-merge precision ≥ 98%. | Golden record completeness ≥ 85–95% (target by region). | Time to onboard new supplier ↓ (days → hours). | Sanctions/risk check freshness ≤ N days. | Downstream exception rate (ERP/procurement) ↓. | . ",
    "url": "/supplier360#success-metrics",
    
    "relUrl": "/supplier360#success-metrics"
  },"255": {
    "doc": "Supplier 360",
    "title": "Summary",
    "content": "Supplier 360 in CluedIn combines standardization, robust matching, clear survivorship, and governed publishing to deliver one trusted view of every supplier. Follow the steps above, validate with real samples, and iterate your rules until precision and completeness meet your business targets. ",
    "url": "/supplier360#summary",
    
    "relUrl": "/supplier360#summary"
  },"256": {
    "doc": "Supplier 360",
    "title": "Supplier 360",
    "content": " ",
    "url": "/supplier360",
    
    "relUrl": "/supplier360"
  },"257": {
    "doc": "Implement Data Quality Management",
    "title": "Data Quality in CluedIn — Step‑by‑Step Implementation Guide",
    "content": "A Data Quality (DQ) program in CluedIn gives you measurable, automated, and auditable control over your data. This guide walks you through: define scope → profile → standardize → validate → score → steward → monitor → publish. ",
    "url": "/data-quality-use-case#data-quality-in-cluedin--stepbystep-implementation-guide",
    
    "relUrl": "/data-quality-use-case#data-quality-in-cluedin--stepbystep-implementation-guide"
  },"258": {
    "doc": "Implement Data Quality Management",
    "title": "Outcomes",
    "content": ". | Clear DQ policy (dimensions, rules, thresholds, SLAs). | Automated profiling, validation, and standardization with rules. | Scorecards &amp; dashboards per entity/attribute and per source. | Stewardship workflows for remediation with full audit &amp; rollback. | Continuous monitoring and alerts on regressions. | . ",
    "url": "/data-quality-use-case#outcomes",
    
    "relUrl": "/data-quality-use-case#outcomes"
  },"259": {
    "doc": "Implement Data Quality Management",
    "title": "Prerequisites",
    "content": ". | Source access to data domains in scope (e.g., Customers, Products, Suppliers, Orders). | Business‑agreed definitions for DQ dimensions and pass/fail thresholds. | RACI: Stewards/Owners must have Accountable permissions. | A dedicated server/VM for heavy profiling, matching, and toolkit operations. | . ",
    "url": "/data-quality-use-case#prerequisites",
    
    "relUrl": "/data-quality-use-case#prerequisites"
  },"260": {
    "doc": "Implement Data Quality Management",
    "title": "Step 1 — Define Scope &amp; Quality Policy",
    "content": "Portal: Governance → Policies . | Pick entities/attributes in scope (e.g., Customer.Email, Product.GTIN, Address.Country). | Choose dimensions &amp; thresholds (examples): . | Completeness: Email ≥ 98%, Phone ≥ 90% | Validity: GTIN checksum pass ≥ 99.5% | Uniqueness: unique CustomerID = 100% | Consistency: Country uses ISO‑2 = 100% | Timeliness: LastUpdated &lt; 30 days ≥ 95% | . | Document SLAs and owners per rule (who fixes what, by when). | . ",
    "url": "/data-quality-use-case#step-1--define-scope--quality-policy",
    
    "relUrl": "/data-quality-use-case#step-1--define-scope--quality-policy"
  },"261": {
    "doc": "Implement Data Quality Management",
    "title": "Step 2 — Connect &amp; Ingest Sources",
    "content": "Portal: Data Sources → Add Source . | Add CRM/ERP/PIM/e‑commerce/finance/file feeds as required. | Capture provenance: SourceSystem, SourceRecordID, IngestedAt. | Start with sample loads, then scale. | . ",
    "url": "/data-quality-use-case#step-2--connect--ingest-sources",
    
    "relUrl": "/data-quality-use-case#step-2--connect--ingest-sources"
  },"262": {
    "doc": "Implement Data Quality Management",
    "title": "Step 3 — Baseline Profiling",
    "content": "Portal: Data Quality → Profiling . | Run profiling per entity to see: null counts, distinct counts, pattern distributions, min/max/avg lengths, outliers. | Export the baseline as your before snapshot. | Tag anomalies: DataDrift, OutlierPattern, UnexpectedNulls. | . ",
    "url": "/data-quality-use-case#step-3--baseline-profiling",
    
    "relUrl": "/data-quality-use-case#step-3--baseline-profiling"
  },"263": {
    "doc": "Implement Data Quality Management",
    "title": "Step 4 — Standardization (Normalize Inputs)",
    "content": "Portal: Governance → Rules (Data Part Rules) Create normalization rules before validation so checks are fair: . | Names &amp; Text: trim, case normalization, strip HTML. | Addresses: parse line components; normalize country codes (ISO‑2); optional geocoding. | Phones: convert to E.164; tag InvalidPhone on failure. | Emails: lower‑case, canonicalize dots for known providers (policy‑dependent). | Identifiers: remove spaces/dashes; compute checks (e.g., VAT/ABN/GTIN). | Units: convert to canonical (e.g., cm/kg) and tag UnitNormalized. | . Recommended tags: Standardized, NeedsValidation, InvalidFormat, LegacySource. ",
    "url": "/data-quality-use-case#step-4--standardization-normalize-inputs",
    
    "relUrl": "/data-quality-use-case#step-4--standardization-normalize-inputs"
  },"264": {
    "doc": "Implement Data Quality Management",
    "title": "Step 5 — Validation Rules (Attribute‑Level)",
    "content": "Portal: Governance → Rules (Data Part Rules) Express the pass/fail logic per attribute and tag failures: . | Email Validity: regex + MX (if available) → tag InvalidEmail. | Phone Validity: E.164 parse → tag InvalidPhone. | Country Consistency: enforce ISO‑2 set; map aliases; tag UnknownCountry. | Date Validity: not in future/past-policy; tag InvalidDate. | Identifier Checks: checksum (VAT/ABN/GTIN/IBAN); tag InvalidTaxID, InvalidGTIN. | . Keep rules simple &amp; modular; prefer many small, focused rules over one complex rule. ",
    "url": "/data-quality-use-case#step-5--validation-rules-attributelevel",
    
    "relUrl": "/data-quality-use-case#step-5--validation-rules-attributelevel"
  },"265": {
    "doc": "Implement Data Quality Management",
    "title": "Step 6 — Entity‑Level Constraints",
    "content": "Portal: Entity Matching / Rules . | Uniqueness: assert unique keys (CustomerID, SKU). On conflict, tag DuplicateKey and route to stewardship. | Referential Integrity: child → parent exists (e.g., Order.CustomerId must exist). Tag OrphanReference. | Cross‑field Consistency: e.g., State valid for Country, EndDate &gt;= StartDate. | . ",
    "url": "/data-quality-use-case#step-6--entitylevel-constraints",
    
    "relUrl": "/data-quality-use-case#step-6--entitylevel-constraints"
  },"266": {
    "doc": "Implement Data Quality Management",
    "title": "Step 7 — Compute Scores &amp; Scorecards",
    "content": "Portal: Data Quality → Scorecards (or implement via Rules) Define per‑attribute and per‑entity scores. Example weighting: . | Store Score_AttributeX, Score_Entity, Score_Source. | Tag entities by band: DQ_Gold (≥0.95), DQ_Silver (0.90–0.95), DQ_Bronze (&lt;0.90). | . ",
    "url": "/data-quality-use-case#step-7--compute-scores--scorecards",
    
    "relUrl": "/data-quality-use-case#step-7--compute-scores--scorecards"
  },"267": {
    "doc": "Implement Data Quality Management",
    "title": "Step 8 — Stewardship &amp; Cleaning Projects",
    "content": "Portal: Data Stewardship &amp; Data Quality → Cleaning Projects . | Auto‑create work queues from rule failures (e.g., all InvalidEmail for Customer). | Use Cleaning Projects to bulk fix: standardize values, remap codes, correct formats. | Everything is logged in History; you can Undo Project or Restore Previous Value if needed. | . ",
    "url": "/data-quality-use-case#step-8--stewardship--cleaning-projects",
    
    "relUrl": "/data-quality-use-case#step-8--stewardship--cleaning-projects"
  },"268": {
    "doc": "Implement Data Quality Management",
    "title": "Step 9 — Dashboards &amp; Alerts",
    "content": "Portal: Data Quality → Dashboards Track by entity, attribute, and source: . | Completeness %, validity %, uniqueness violations, orphan counts. | Score trends (7/30/90‑day). | Top failing rules and top offending sources. | . Set alerts (email/webhook) when: . | A KPI breaches threshold (e.g., Valid Email &lt; 98%). | Regression from previous week by &gt; X%. | New pattern emerges (data drift). | . ",
    "url": "/data-quality-use-case#step-9--dashboards--alerts",
    
    "relUrl": "/data-quality-use-case#step-9--dashboards--alerts"
  },"269": {
    "doc": "Implement Data Quality Management",
    "title": "Step 10 — Automated Remediation (Where Safe)",
    "content": "Portal: Rules (Data Part &amp; Golden) . | Safe auto‑fixes (e.g., country alias mapping, phone formatting) can run continuously. | Risky fixes route to stewards; tag NeedsStewardReview. | For downstream suppression (e.g., invalid emails), tag DoNotContact. | . ",
    "url": "/data-quality-use-case#step-10--automated-remediation-where-safe",
    
    "relUrl": "/data-quality-use-case#step-10--automated-remediation-where-safe"
  },"270": {
    "doc": "Implement Data Quality Management",
    "title": "Step 11 — Governance, Access &amp; Lifecycle",
    "content": "Portal: Governance → Permissions / Policies / Retention . | Restrict sensitive fields (PII) and steward-only attributes. | Apply Retention Policies where appropriate (e.g., delete stale logs after N days). | Maintain audit: every change is in History with who/when/why. | . ",
    "url": "/data-quality-use-case#step-11--governance-access--lifecycle",
    
    "relUrl": "/data-quality-use-case#step-11--governance-access--lifecycle"
  },"271": {
    "doc": "Implement Data Quality Management",
    "title": "Step 12 — Publish Quality‑Assured Data",
    "content": "Portal: Exports . | Publish conformed entity views plus quality flags/scores. | Provide suppression lists (e.g., invalid emails). | Offer wide tables for BI with Score_Entity and failure counts. | . Start read‑only to validate, then wire quality gates into downstream loaders. ",
    "url": "/data-quality-use-case#step-12--publish-qualityassured-data",
    
    "relUrl": "/data-quality-use-case#step-12--publish-qualityassured-data"
  },"272": {
    "doc": "Implement Data Quality Management",
    "title": "Step 13 — Scheduling &amp; Operations",
    "content": ". | Schedule profiling, validation, scoring, and exports (off‑peak for heavy runs). | For big jobs, use a dedicated VM/server to avoid timeouts and throttling. | Monitor job durations and queue depth; investigate spikes. | . ",
    "url": "/data-quality-use-case#step-13--scheduling--operations",
    
    "relUrl": "/data-quality-use-case#step-13--scheduling--operations"
  },"273": {
    "doc": "Implement Data Quality Management",
    "title": "Step 14 — UAT &amp; Go‑Live",
    "content": ". | Validate on a sample set across sources. | Confirm rule accuracy (precision/recall) with business owners. | Dry‑run alerts and dashboards; tune thresholds. | Package with Product Toolkit and promote (ensure Accountable access). | . ",
    "url": "/data-quality-use-case#step-14--uat--golive",
    
    "relUrl": "/data-quality-use-case#step-14--uat--golive"
  },"274": {
    "doc": "Implement Data Quality Management",
    "title": "Go‑Live Checklist",
    "content": ". | DQ policy documented (dimensions, thresholds, SLAs, owners). | Normalization &amp; validation rules active; tests passing. | Scorecards &amp; dashboards show baseline and targets. | Steward queues configured; Cleaning Projects tested; Undo verified. | Exports include quality flags/scores; consumers validated. | Schedules &amp; alerts configured; on‑call/triage process in place. | . ",
    "url": "/data-quality-use-case#golive-checklist",
    
    "relUrl": "/data-quality-use-case#golive-checklist"
  },"275": {
    "doc": "Implement Data Quality Management",
    "title": "Example Rules (Snippets)",
    "content": "Email Validity . | Condition: Email present | Actions: lower‑case; regex check; tag InvalidEmail on fail. | . Phone Normalize . | Condition: Phone present | Actions: parse to E.164; tag InvalidPhone on parse failure. | . Country ISO‑2 Enforcement . | Condition: Country not in ISO2 set | Actions: map alias (e.g., UK→GB, U.S.→US); if unknown, tag UnknownCountry. | . Date Consistency . | Condition: EndDate &lt; StartDate | Actions: tag InvalidDateRange; send to stewardship. | . Uniqueness (CustomerID) . | Condition: duplicate CustomerID detected | Actions: tag DuplicateKey; route to steward queue. | . GTIN Check . | Condition: GTIN present | Actions: strip non‑digits; checksum; tag InvalidGTIN on fail. | . Timeliness . | Condition: Now - LastUpdated &gt; 30 days | Actions: tag StaleRecord. | . ",
    "url": "/data-quality-use-case#example-rules-snippets",
    
    "relUrl": "/data-quality-use-case#example-rules-snippets"
  },"276": {
    "doc": "Implement Data Quality Management",
    "title": "KPIs &amp; Targets (Examples)",
    "content": ". | Valid Email ≥ 98%, Valid Phone ≥ 95%. | Address Standardized ≥ 97%; Unknown Country &lt; 0.1%. | Duplicate Key violations = 0. | Orphan References = 0. | Entity Score ≥ 0.95 sustained over 30 days. | Mean time to remediate stewarded issues &lt; 5 days. | . ",
    "url": "/data-quality-use-case#kpis--targets-examples",
    
    "relUrl": "/data-quality-use-case#kpis--targets-examples"
  },"277": {
    "doc": "Implement Data Quality Management",
    "title": "Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Skipping standardization before validation → false failures. Normalize first. | Over‑aggressive regex → false negatives; keep patterns realistic. | One giant rule → hard to debug; split into small rules. | No owner for each KPI → assign accountable roles. | Running profiling on laptops → use dedicated compute to avoid timeouts. | Fixes re‑applied by other rules → adjust/disable the problematic rule before reprocessing. | . ",
    "url": "/data-quality-use-case#common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/data-quality-use-case#common-pitfalls--how-to-avoid-them"
  },"278": {
    "doc": "Implement Data Quality Management",
    "title": "Summary",
    "content": "A successful Data Quality program in CluedIn combines clear policy, modular rules, transparent scoring, and active stewardship. Start small, measure relentlessly, automate safe fixes, and keep governance tight so improvements stick. ",
    "url": "/data-quality-use-case#summary",
    
    "relUrl": "/data-quality-use-case#summary"
  },"279": {
    "doc": "Implement Data Quality Management",
    "title": "Implement Data Quality Management",
    "content": " ",
    "url": "/data-quality-use-case",
    
    "relUrl": "/data-quality-use-case"
  },"280": {
    "doc": "Use Cases Summary",
    "title": "Getting the Best Out of the Use Cases",
    "content": "The Use Case walkthroughs in CluedIn are designed to show you how the platform solves real-world data challenges step by step. To get the most value out of these guides, it’s important to approach them not just as tutorials, but as learning opportunities that connect directly to your own data landscape. ",
    "url": "/usecases-summary#getting-the-best-out-of-the-use-cases",
    
    "relUrl": "/usecases-summary#getting-the-best-out-of-the-use-cases"
  },"281": {
    "doc": "Use Cases Summary",
    "title": "How to Use These Walkthroughs Effectively",
    "content": "1. Follow Along with Real Data . Where possible, connect a test data source or a subset of your real data. This makes the examples more relevant and helps you see how CluedIn responds to the specific quirks of your data. 2. Treat Each Use Case as a Building Block . Each walkthrough highlights one capability — data quality, entity linking, compliance, analytics, etc. As you progress, you’ll see how they combine into a larger data strategy. 3. Explore Beyond the Steps . The walkthroughs provide a structured path, but CluedIn is flexible. After completing a use case, try adjusting rules, tags, or exports to experiment with different outcomes. 4. Use Tags, Notes, and Comments . As you work through examples, make use of CluedIn’s tagging and annotation features. This builds your own catalog of learnings that you can reuse later. 5. Compare to Your Business Goals . At the end of each walkthrough, ask: . | How does this align with my current data challenges? | What changes would I make to apply this in production? | Which teams would benefit from this outcome? | . ",
    "url": "/usecases-summary#how-to-use-these-walkthroughs-effectively",
    
    "relUrl": "/usecases-summary#how-to-use-these-walkthroughs-effectively"
  },"282": {
    "doc": "Use Cases Summary",
    "title": "Best Practices for Success",
    "content": ". | Start Small, Scale Gradually: Run walkthroughs on a small dataset before expanding to production. | Collaborate Across Teams: Involve colleagues from compliance, analytics, and operations so everyone sees the value. | Document as You Go: Capture your findings and decisions. They’ll become the blueprint for your production setup. | Measure Improvements: Use CluedIn dashboards to track data quality, entity resolution rates, and processing times as you apply the use cases. | Iterate Often: The best results come from refining rules, retention policies, and enrichment as your understanding deepens. | . ",
    "url": "/usecases-summary#best-practices-for-success",
    
    "relUrl": "/usecases-summary#best-practices-for-success"
  },"283": {
    "doc": "Use Cases Summary",
    "title": "Next Steps",
    "content": ". | Complete each walkthrough in sequence to build a strong foundation. | Apply the lessons learned to your own business data. | Use the CluedIn Documentation Portal to dive deeper into advanced topics like Rules, Matching, and Governance. | Share feedback with your CluedIn team or community — your insights will help others get more value from these use cases. | . ",
    "url": "/usecases-summary#next-steps",
    
    "relUrl": "/usecases-summary#next-steps"
  },"284": {
    "doc": "Use Cases Summary",
    "title": "Summary",
    "content": "The use cases are not just step-by-step guides — they are launch pads for success with CluedIn. By following along with real data, connecting the walkthroughs to your business goals, and iterating as you learn, you’ll build both confidence in the platform and a cleaner, more reliable data foundation for your organization. ",
    "url": "/usecases-summary#summary",
    
    "relUrl": "/usecases-summary#summary"
  },"285": {
    "doc": "Use Cases Summary",
    "title": "Use Cases Summary",
    "content": " ",
    "url": "/usecases-summary",
    
    "relUrl": "/usecases-summary"
  },"286": {
    "doc": "Backup and restore",
    "title": "On this page",
    "content": ". | Set up a backup solution | Restore an environment | Runbooks | . ",
    "url": "/deployment/infra-how-tos/ama-backup#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/ama-backup#on-this-page"
  },"287": {
    "doc": "Backup and restore",
    "title": "Set up a backup solution",
    "content": "To prevent data loss and to ensure the safety and availability of your containerized applications, set up a backup solution for your Azure Kubernetes Service (AKS) clusters. The backup solution is deployed post-installation and can be done by CluedIn on your behalf. If you do not have the backup solution in place, reach out to CluedIn support for assistance. The backup configuration focuses on two core elements for the CluedIn product. These are the helm values and the persistent databases that run inside of your cluster on disk. The process works by leveraging the automation account deployed to your managed resource group to run two main runbooks. One runbook will back up helm, and the other will back up the databases. During the backup run, it will temporarily shut down the instance by scaling down all the pods running in Kubernetes to prevent any potential data loss. It will then take snapshots of each persistent disk and place these inside the managed resource group by default. Once a backup has happened, it will reduce total snapshots to the supplied retention count (default is 7), starting with the oldest existing snapshot before scaling CluedIn up. This process takes roughly 20 minutes to complete and is highly recommended to run out of hours. To set up the schedules . | Navigate to the automation account located in the managed resource group of the environment that you want to back up. | On the left side, select Schedules . | Select Add a schedule, and then specify the appropriate time, time zone, and days to run the backup. Then, select Create. | On the left side, select Runbooks . | Select backup-helm-values. Then, on the left side, select Schedules. | Select Add a schedule, and then select the schedule that you’ve just created. Fill in the parameters. | Repeat the process for backup-cluedin. | . Once the schedules are set up, the automation account should proceed to run backup as per your configuration. For any further information about backup, reach out to CluedIn support. ",
    "url": "/deployment/infra-how-tos/ama-backup#set-up-a-backup-solution",
    
    "relUrl": "/deployment/infra-how-tos/ama-backup#set-up-a-backup-solution"
  },"288": {
    "doc": "Backup and restore",
    "title": "Restore an environment",
    "content": "If you’re ever in a situation where you need to restore an environment, this section will only cover an in-place restore, and not a disaster recovery to another region. It is highly recommended to reach out to CluedIn support in the first instance. To restore an environment . | Scale down the pods on your AKS so no CluedIn pods are running. | Restore the disks in the snapshot location over top of the existing PVCs. The disk name and tags must match during a restore, otherwise you may run into problems during operation. All restored disks must be from the same time. There may be a few minutes between each disk as it runs sequentially. | Once all disks have been restored from the same time, proceed to scale the cluster back up. After 5 minutes, the instance should then be rolled back. | . ",
    "url": "/deployment/infra-how-tos/ama-backup#restore-an-environment",
    
    "relUrl": "/deployment/infra-how-tos/ama-backup#restore-an-environment"
  },"289": {
    "doc": "Backup and restore",
    "title": "Runbooks",
    "content": "To automate backup and restore tasks, use runbooks. These runbooks should be deployed to the required environment using Terraform. | Disaster recovery runbook – responsible for orchestrating the backup &gt; copy &gt; restore process. | Backup runbook – responsible for capturing snapshots of all persistent disks used by CluedIn. | Copy snapshots runbook – responsible for copying snapshots from one Azure location to another. | Restore runbook – responsible for removing all persistent disks and restoring them from snapshots. | . ",
    "url": "/deployment/infra-how-tos/ama-backup#runbooks",
    
    "relUrl": "/deployment/infra-how-tos/ama-backup#runbooks"
  },"290": {
    "doc": "Backup and restore",
    "title": "Backup and restore",
    "content": " ",
    "url": "/deployment/infra-how-tos/ama-backup",
    
    "relUrl": "/deployment/infra-how-tos/ama-backup"
  },"291": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "On this page",
    "content": ". | Create a new pipeline | Configure source | Configure sink | Debug pipeline | Validate result in CluedIn | . This guide provides step-by-step instructions for configuring an Azure Data Factory (ADF) Copy data activity to send data to CluedIn using a private endpoint. Use the Copy data activity if you do not need to make any data transformations before sending data to CluedIn. When data transformations such as aggregation, filtering, or applying complex business logic are required before sending data to CluedIn, use the Data flow activity instead. Prerequisites  . | Configure a private link service between ADF and CluedIn as described in Configure ADF with private link. | Make sure you have Azure Data Lake with some CSV data.  . | Create an ingestion endpoint and authorization token in CluedIn as described in Add ingestion endpoint.  . | . Configuring an ADF pipeline with the Copy data activity consists of 4 steps: . | Creating a new pipeline . | Configuring the source . | Configuring the sink . | Debugging the pipeline . | Validating the result in CluedIn . | . ",
    "url": "/microsoft-integration/adf-integration/copy-data#on-this-page",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#on-this-page"
  },"292": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Create a new pipeline",
    "content": ". | On the Azure Data Factory home page, select New &gt; Pipeline. | In the Activities pane, expand the Move and transform category, and then drag the Copy data activity to the pipeline canvas. | . ",
    "url": "/microsoft-integration/adf-integration/copy-data#create-a-new-pipeline",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#create-a-new-pipeline"
  },"293": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Configure source",
    "content": ". | Go to the Source tab. Select + New to create a source dataset. | In the New dataset pane, find and select Azure Data Lake Storage Gen2, and then select Continue. | In the Select format pane, select DelimitedText, and then select Continue. | In the Set properties pane, enter the name for the dataset. Then, expand the Linked service dropdown list and select + New. | In the New linked service pane, provide the following details: . | Name – enter the name for your linked service. | Account selection method – select From Azure subscription. | Azure Subscription – select the subscription of your Azure Data Lake. | Storage account name – select the name of your Azure Data Lake storage account. | . | Test connection, and then select Create. After the linked service is created, you’ll be taken back to the Set properties pane. | In the File path section, add the path to the appropriate folder/file of your Azure Data Lake. | Select OK. | . ",
    "url": "/microsoft-integration/adf-integration/copy-data#configure-source",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#configure-source"
  },"294": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Configure sink",
    "content": ". | Go to the Sink tab. Select + New to create a sink dataset. | In the New dataset pane, find and select REST, and then select Continue. | In the Set properties pane, enter the name for the dataset. Then, expand the Linked service dropdown list and select + New. | In the New linked service pane, provide the following details: . | Name – enter the name for your linked service. | Base URL – enter the URL of the ingestion endpoint in CluedIn. For more information, see Send data. | Authentication type – select Anonymous. | Auth headers – add a new header with the following details: . | Name – enter Authorization. | Value – enter Bearer, add a space, and then paste the token from CluedIn. For more information, see Send data. | . | . | Test connection, and then select Create. After the linked service is created, you’ll be taken back to the Set properties pane. | Select OK. | In the Request method field, select POST. | . ",
    "url": "/microsoft-integration/adf-integration/copy-data#configure-sink",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#configure-sink"
  },"295": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Debug pipeline",
    "content": "Once the source and sink are configured, you can debug the pipeline to ensure it is working correctly. To debug the pipeline . | On the toolbar, select Debug. | Monitor the status of the pipeline run on the Output tab at the bottom of the window. | . ",
    "url": "/microsoft-integration/adf-integration/copy-data#debug-pipeline",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#debug-pipeline"
  },"296": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Validate result in CluedIn",
    "content": "Once the ADF pipeline is triggered successfully, you should see the data flowing into CluedIn. You can view the incoming records on the Preview tab of the data set. ",
    "url": "/microsoft-integration/adf-integration/copy-data#validate-result-in-cluedin",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data#validate-result-in-cluedin"
  },"297": {
    "doc": "Configure ADF pipeline with Copy data activity",
    "title": "Configure ADF pipeline with Copy data activity",
    "content": " ",
    "url": "/microsoft-integration/adf-integration/copy-data",
    
    "relUrl": "/microsoft-integration/adf-integration/copy-data"
  },"298": {
    "doc": "Enricher",
    "title": "Enricher",
    "content": "Enrichers allow you to enhance and improve the quality and completeness of your golden records by incorporating additional information from third-party sources. The following diagram shows the basic steps of enriching the data in CluedIn. This section covers the following areas: . | Concept of enricher – explore how an enricher works and where it fits in the data life cycle in CluedIn. | Adding an enricher – learn how to configure and manage an enricher. | Reference information about enrichers – find information about built-in enrichers available in CluedIn. If you need a custom enricher, use the REST API enricher or contact us, and we will build one for you. | . ",
    "url": "/preparation/enricher",
    
    "relUrl": "/preparation/enricher"
  },"299": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "On this page",
    "content": ". | Power Platform integration | Purview integration | Copilot integration | Fabric integration | Event Hubs integration | Azure Data Factory integration | . In this article, you will find an overview of various Microsoft integrations, enabling you to leverage the ones you need to enhance your data management capabilities in CluedIn. ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#on-this-page"
  },"300": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Power Platform integration",
    "content": "Microsoft Power Platform is a suite of low-code/no-code tools designed to empower users to create custom business applications, automate workflows, analyze data, and build websites. Several of its key components—Power Apps, Power Automate, and Dataverse—can be integrated with CluedIn. The most common use case of Power Platform integration involves using a Power Apps form for data input and a Power Automate workflow for approving the data input. Once the input is approved, it is written to the Dataverse table. The Dataverse table is then synchronized with CluedIn and the updated data is sent to the ingestion endpoint. Once the data is in CluedIn, it undergoes processing and becomes a data part of an existing golden record, which is then sent to the Dataverse export target by stream. The above use case is specific for data changes in the source system. In addition to this, CluedIn offers in-app Workflow module for streamlining and tracking approvals and notifications for specific activities happening in CluedIn. For example, you can set up a workflow to notify the owners of a vocabulary key when other users make changes. The owners will receive an approval request in Outlook or the Approvals app in Teams, where they can approve or reject the changes. The changes will be applied only if at least one of the owners approves them. To sum up, if you want to implement a process for approving and syncing data entry changes from Dataverse to CluedIn, consider using Power Platform integration. ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#power-platform-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#power-platform-integration"
  },"301": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Purview integration",
    "content": "Microsoft Purview is a comprehensive set of solutions that can help your organization govern, protect, and manage data, wherever it lives. If you provide Purview with a source of data (for example, a blob storage or a SQL table), it will scan it and build the data map. This map helps you understand what data you have, where it is located, and how it is being used. Additionally, Purview can classify the data, apply labels for sensitivity, and ensure compliance with data governance policies. Purview also captures data lineage, which tracks the lifecycle of data from its origin through its various transformations and movements across the data estate. With Purview integration, CluedIn can take the assets from Purview and turn them into ready-for-insight data, which is then exported to the needed target system. What is more, you can configure synchronization between Purview glossaries and CluedIn vocabularies. Once the data is exported, Purview can scan it to automatically pick up on the mastered data coming out of CluedIn. For more information about Purview integration, see: . | Microsoft Purview integration . | Microsoft training on how build an end-to-end data governance and master data management stack with Microsoft Purview and CluedIn . | . ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#purview-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#purview-integration"
  },"302": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Copilot integration",
    "content": "CluedIn Copilot is an AI assistant that leverages Azure OpenAI to help you interact within the CluedIn platform using natural language commands. For example, you can prompt Copilot to tag golden records based on specific criteria or analyze and describe a specific dataset. While Copilot can simplify data management tasks, keep in mind that the results of Copilot actions should be carefully reviewed. Consider enabling Copilot when you have a clear understanding of your business use case and when you know which actions can be delegated to Copilot. It is important to understand how to perform a specific task in CluedIn and then evaluate if it can be handled by Copilot. For more information about Copilot skills in CluedIn, see Work with Copilot integration. ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#copilot-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#copilot-integration"
  },"303": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Fabric integration",
    "content": "Microsoft Fabric is an end-to-end analytics and data platform designed for enterprises that require a unified solution. It encompasses data movement, processing, ingestion, transformation, real-time event routing, and report building. By integrating Fabric with CluedIn, you can achieve seamless data integration, enhanced data quality, and robust data management capabilities. In addition to this, CluedIn offers OneLake connector to export golden records to the centralized storage in OneLake. If you use Fabric, Databricks, Snowflake, Synapse, or any other Python-based data platform, you can easily integrate with CluedIn using Python SDK as described in our instructions here. ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#fabric-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#fabric-integration"
  },"304": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Event Hubs integration",
    "content": "Azure Event Hubs is a native data-streaming service in the cloud that can stream millions of events per second, with low latency, from any source to any destination. By integrating Azure Event Hubs and CluedIn, you can enable the transmission of important events from CluedIn to your event hub. For example, adding or updating a data source, adding a business domain, creating a deduplication project, and more. You can then connect your event hub to Fabric and build reports or alerts for different events. This way you can get valuable insights about the activities in CluedIn, such as the number of deduplication or clean projects that were created last week. For more information, see Event Hub integration. In addition to this, CluedIn offers the Azure Event Hub connector. You can configure it and use it in a stream to export golden records to a specific event hub. ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#event-hubs-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#event-hubs-integration"
  },"305": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Azure Data Factory integration",
    "content": "Azure Data Factory is Azure’s cloud ETL service for data integration and data transformation. With over 90 built-in connectors, Azure Data Factory enables you to acquire data from a wide range of sources such as relational databases, big data and NoSQL systems, data lakes and storages, and many more. By integrating Azure Data Factory with CluedIn, you can build automated data flows and leverage CluedIn’s core capabilities in data quality and enrichment to ensure that the data is clean, reliable, and ready for insights. For more information, see: . | Azure Data Factory integration . | Configure Azure Data Factory with private link . | . ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#azure-data-factory-integration",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations#azure-data-factory-integration"
  },"306": {
    "doc": "Extending CluedIn with Microsoft integrations",
    "title": "Extending CluedIn with Microsoft integrations",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations",
    
    "relUrl": "/playbooks/data-engineering-playbook/extending-cluedin-with-microsoft-integrations"
  },"307": {
    "doc": "How to approach your CluedIn project",
    "title": "On this page",
    "content": ". | Start small | Embrace Agile | . | Audience | Time to read | . | Business User, Data Project Lead, Data Steward, Data Analyst, Data Architect, Data Engineer | 5 min | . This article outlines two fundamental steps for approaching any data project in CluedIn. Following these steps will help you achieve attainable value right from the start. ",
    "url": "/playbooks/how-to-approach-your-cluedin-project#on-this-page",
    
    "relUrl": "/playbooks/how-to-approach-your-cluedin-project#on-this-page"
  },"308": {
    "doc": "How to approach your CluedIn project",
    "title": "Start small",
    "content": "Regardless of your use case, we always recommend starting your data project in CluedIn with a limited number of records from one or several data sources. By experiencing full CluedIn cycle—ingest, clean, deduplicate, and stream—on a small scale first, you can thoroughly understand the process before scaling up in terms of records and sources. Here are the key benefits of starting small: . | Agility and flexibility – quickly adapt and iterate based on immediate feedback. | Fast learning – understand the CluedIn process on a manageable scale. | Quick adjustments – make necessary changes without the overhead of managing vast amounts of data. | Reduced risk – minimize the risk of large-scale failures by perfecting the process on a smaller data set. | . One of our guiding principles is: “Aim for better data, not perfect data”. Striving for 100% perfection from the start often leads to failure, which is why we advocate for an Agile approach. This allows for continuous improvement and adaptation. While CluedIn simplifies MDM processes, handling a vast amount of data from the start can be overwhelming. This is why starting small allows you to tackle these MDM challenges: . | Complexity in MDM steps – each MDM step can introduce significant complexity. Managing 100 records is far easier than managing 1,000,000 records. | Load on source systems – even though CluedIn can handle it efficiently, ingesting millions of records can impose a considerable load on source systems, slowing down your progress and increasing the risk of errors. | IT obstacles – each data introduces a unique set of IT challenges. You might need to contact multiple data owners, navigate permissions, and deal with network security issues. | . What happens if you don’t start small? . Approaching a data project with the traditional Waterfall methodology can be rigid and slow. Initially, you would ingest data from all your data sources. Next, you would clean the data. Only after that would you perform deduplication activities. At any stage, there’s a significant risk of errors due to the large volume of data. Let’s say you have 10 sources, each with 1,000,000 records, and you ingest data from all of them before any cleaning or deduplication activities. This whole process could take months, annoy your IT team, and you would not have improved the quality of your data at all. Suppose you succeeded to ingest those 10,000,000 records. Next, you will have to apply cleaning and deduplication rules on a whole difference scale. Of course, CluedIn allows you to filter down and clean and deduplicate your records on a smaller scale. However, keep in mind that cleaning 1,000 records out of a set of 10,000,000 means that these records can be merged or exported to other parts of the system, such as rules or streams. This means you won’t have control over your data and will be forced to play catch-up to determine what should have been done earlier. Additionally, you’ll need to stream large amounts of data multiple times whenever changes are made. Although CluedIn’s streams are performant, it is always easier to stream 1,000 records, then 10,0000, and finally 10,000,000, rather than streaming 10,000,000 records three times. ",
    "url": "/playbooks/how-to-approach-your-cluedin-project#start-small",
    
    "relUrl": "/playbooks/how-to-approach-your-cluedin-project#start-small"
  },"309": {
    "doc": "How to approach your CluedIn project",
    "title": "Embrace Agile",
    "content": "In CluedIn, we believe it is essential to create value as soon as possible. Start by improving small data sets from various sources, and then expand along two dimensions: the number of records and the number of sources. Start with the data you have; don’t wait—improved data is better than your current data. Therefore, focus on creating a virtuous cycle before adding more data. You can start with as little as 10 records and achieve the value immediately. With those 10 records, you can learn how to perform the main tasks in CluedIn: . | Map | Enrich | Clean | Apply business rules | Deduplicate | Stream | . Once you have successfully ingested, cleaned, deduplicated, and streamed a small dataset of 10 records, you can scale up and apply the same process to a larger set of 10,000 records. The more exercises you do, the deeper your understanding of data will become. If you make a mistake at any step, you can always revert your work and try again until you achieve the desired outcome. This iterative process ensures that you add value to your data and enhance your knowledge of CluedIn. Our experience with data projects has shown that initial perceptions of data quality often differ from reality. This is a common and manageable challenge, and we are here to help you enhance your data quality and reveal its potential. ",
    "url": "/playbooks/how-to-approach-your-cluedin-project#embrace-agile",
    
    "relUrl": "/playbooks/how-to-approach-your-cluedin-project#embrace-agile"
  },"310": {
    "doc": "How to approach your CluedIn project",
    "title": "How to approach your CluedIn project",
    "content": " ",
    "url": "/playbooks/how-to-approach-your-cluedin-project",
    
    "relUrl": "/playbooks/how-to-approach-your-cluedin-project"
  },"311": {
    "doc": "1.1.1. Ingesting a CSV file",
    "title": "1.1.1. Ingesting a CSV file",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll start implementing the use case presented in the introduction to 1.1. Ingestion—creating a single source of truth for customer data by integrating information from CRM and ERP systems. Our focus here is on ingesting a CSV file containing customer records from the CRM system. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Training file: crm-customer.csv . Useful resources: . | Getting started with data ingestion . | Golden records . | . ",
    "url": "/training/fundamentals/ingesting-csv-file",
    
    "relUrl": "/training/fundamentals/ingesting-csv-file"
  },"312": {
    "doc": "CluedIn PaaS",
    "title": "CluedIn PaaS",
    "content": "CluedIn Master Data Management is an Azure Managed Application (AMA) that is deployed within your company’s Azure infrastructure. As a managed application, CluedIn is easy to deploy and operate. In addition, our support team can help you with the installation processes. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. Installing CluedIn PaaS through the Azure Marketplace allows you to use simple hourly pricing and upgrade to a full license when needed. So you can freely use CluedIn for a few hours of investigation or dig deeper and integrate with a suite of Azure services to develop your master data management solution. ",
    "url": "/deployment/azure-marketplace",
    
    "relUrl": "/deployment/azure-marketplace"
  },"313": {
    "doc": "Pick the right tool",
    "title": "On this page",
    "content": ". | File | Endpoint | Azure Data Factory | Microsoft Fabric | Microsoft Purview | Database | Crawler | Tools reference | Next step | . | Audience | Time to read | . | Data Engineer, Data Analyst | 7 min | . You are here in the data journey . Before you start . | Make sure you have conducted the data impact workshop to understand what sources you want to use first. | . This article outlines all the possible tools that can be used to ingest data into CluedIn. By understanding the specifics of each tool, you can decide which one will be the most suitable for your use case. Now that you have a list of sources that you want to use, you can prepare a data ingestion plan. The first decision to make is how to ingest the data. CluedIn provides multiple out-of-the-box options for data ingestion (as shown in the diagram below). However, the best way to get the data from the source depends on your infrastructure. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#on-this-page",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#on-this-page"
  },"314": {
    "doc": "Pick the right tool",
    "title": "File",
    "content": "CluedIn supports the following file formats to upload your records into the system: CSV, JSON, XLS, XLSX, and Parquet. When to use files for data ingestion? . | When you need to provide a data sample. | When the source system will be decommissioned. | When you need to manually export the data from the source system due to various reasons (for example, an internal policy, a limitation of the source system, a choice from the business not to have live connection with the source). | . The files are quick and easy to ingest. However, opting for files removes the automated pipeline aspect of your MDM project, resulting in more manual operations for a source. It is up to you to decide if having some of your records manually uploaded is sufficient for you. All in all, file upload should be primarily used for testing purposes and data sampling to set up your mapping or test some rules, ideally in a sandbox environment. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#file",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#file"
  },"315": {
    "doc": "Pick the right tool",
    "title": "Endpoint",
    "content": "The endpoint is the most generic way to send data to CluedIn. It is a simple HTTP POST endpoint to send JSON array to CluedIn. This lightweight method allows you to operate with any kind of system, provided you have the ability to make an HTTP request. The endpoint is used as the underlying mechanism for integration with Azure Data Factory, Microsoft Fabric, and Microsoft Purview. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#endpoint",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#endpoint"
  },"316": {
    "doc": "Pick the right tool",
    "title": "Azure Data Factory",
    "content": "Azure Data Factory is a great tool for creating data pipelines and pushing data into CluedIn. With hundreds of connectors available, it’s highly likely that you’re already using Azure Data Factory. This makes it the recommended option for data ingestion. Why is Azure Data Factory a recommended data ingestion option? . | It makes the data pipeline more extensible—you can load the data once and add extra pipeline steps to send data to other systems in addition to CluedIn. | The data flow is less dependent on the MDM system, meaning you can reuse the data pipeline to send data to other systems. | Azure Data Factory is simple to set up and run, and connecting it to CluedIn is easy. Once you’ve configured it for one source, you can easily do the same for all other sources. | . ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#azure-data-factory",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#azure-data-factory"
  },"317": {
    "doc": "Pick the right tool",
    "title": "Microsoft Fabric",
    "content": "If your sources are not supported by Azure Data Factory, check if they are supported by Microsoft Fabric, which also offers many connectors that allow you to connect to different types of data stores. Data engineers may prefer Microsoft Fabric, as they often favor writing Python code over setting up data pipelines through a UI. However, we believe this decision should be made as a team, as it implies that an engineer will be required to upgrade and maintain the data pipeline. We are not aware of the skills and dependencies of each data team, so it will be up to you to decide. In any case, CluedIn can easily connect to Microsoft Fabric as described here. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#microsoft-fabric",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#microsoft-fabric"
  },"318": {
    "doc": "Pick the right tool",
    "title": "Microsoft Purview",
    "content": "If you are a Microsoft Purview user and you want to sync your assets with CluedIn, you can do so using our Purview integration as described here. To configure Purview integration, you need to take into account how the assets are connected. Specifically, it matters whether you need to integrate the assets from an existing data pipeline or architecture, or if you can directly connect the assets with your MDM project. If the latter is your option, then Purview integration will fit your needs. It will help not only govern your assets but also to fix the low-level data (records) from those assets, improving data quality. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#microsoft-purview",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#microsoft-purview"
  },"319": {
    "doc": "Pick the right tool",
    "title": "Database",
    "content": "CluedIn can connect to various types of databases, supporting MS SQL, MySQL, and Postgres dialects. This option allows for full or partial data ingestion; however, we recommend using it as a one-time operation. When to use databases for data ingestion? . | When you want to import data from an old MDM project. | When you want to decommission old systems and keep your MDM project as a sink for data. | When you have small to medium-sized databases (1 million records per table) that do not have many changes over time. | When your databases are not connected to other systems. This way you can avoid putting pressure on your databases. | . What to do if you have large databases? . If you have large databases that need to connect to multiple systems, we recommend using an Azure Data Factory (ADF) pipeline. This approach ensures that the ingestion process within your data ecosystem is done only once, reducing the load on your database. Additionally, it provides greater extensibility if you need to add extra pipeline steps to send data to other systems. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#database",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#database"
  },"320": {
    "doc": "Pick the right tool",
    "title": "Crawler",
    "content": "When all of the above options do not fit your needs, you can use a crawler mechanism that we built to get data into CluedIn. One of the possible uses of a crawler is to migrate from Master Data Services (MDS) to CluedIn. Over the years, we have built many crawlers, some of which are very specific and tailored for customers. While this is not our first choice, be aware that this option exists and can be used when you really need custom solution. Of course, charges may apply if CluedIn needs to write custom code solely for your use case. If your data engineers know C#, they can also build their own crawler with CluedIn if they want to. The template is open source and can be used if required. However, please reach out to us before pursuing this option, as it would need to address a very specific and tailored use case that none of the aforementioned tools can handle. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#crawler",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#crawler"
  },"321": {
    "doc": "Pick the right tool",
    "title": "Tools reference",
    "content": "This section provides brief descriptions of the usage for each data ingestion option. | Data ingestion type | Usage | . | File | Use files for data ingestion as a one-time operation in scenarios such as decommissioning a system, providing a data sample, or when the source owner is reluctant to give direct access to the source. | . | Endpoint | This option represents a generic way to push data into CluedIn by sending an HTTP request with a JSON array. | . | Azure Data Factory | With many connectors maintained by Microsoft, this option is a good choice to create data pipelines and send the data to CluedIn. An ADF pipeline can be set up by a person with no coding experience. | . | Microsoft Fabric/Azure Databricks | This option is suitable for you if you already use Microsoft Fabric (or Azure Databricks). It offers many connectors, but may require writing Python code to set up the data pipeline. | . | Microsoft Purview | Connect to Microsoft Purview if you want to sync and clean some assets from Purview using CluedIn. | . | Database | Use database ingestion as a one-time operation in scenarios such as decommissioning a system or when dealing with small to medium-sized databases that are not connected to other systems. | . | Crawler | Use crawlers for Master Data Services (MDS) migrations or in scenarios where a connection to Azure Data Factory or Microsoft Fabric is not feasible. If needed, you can write a custom crawler for source ingestion using C#. | . ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#tools-reference",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#tools-reference"
  },"322": {
    "doc": "Pick the right tool",
    "title": "Next step",
    "content": "After defining the tools you’ll use for ingesting your sources, you can start the actual ingestion process as described in Ingest data. ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool#next-step",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool#next-step"
  },"323": {
    "doc": "Pick the right tool",
    "title": "Pick the right tool",
    "content": " ",
    "url": "/playbooks/data-ingestion-playbook/pick-the-right-tool",
    
    "relUrl": "/playbooks/data-ingestion-playbook/pick-the-right-tool"
  },"324": {
    "doc": "Release 2025.05",
    "title": "On this page",
    "content": ". | New Ingestion dashboard | Sources page update | Monitoring page update | Source record approval | Data set validations | Power Fx formulas | New access control actions | Search experience update | Audit log actions update | Stream logs | Terminology changes | . This article outlines new features and improvements in CluedIn 2025.05. The following sections contain brief description of new features and links to related articles. ",
    "url": "/release-notes/2025-05#on-this-page",
    
    "relUrl": "/release-notes/2025-05#on-this-page"
  },"325": {
    "doc": "Release 2025.05",
    "title": "New Ingestion dashboard",
    "content": "The new Ingestion dashboard is designed to simplify your work with data sources and make data management more efficient and user-friendly. You’ll find a quick and simple way to start uploading your data into CluedIn from files, ingestion endpoints, databases, manual data entry projects, and crawlers. Additionally, you’ll be able to quickly identify which data sets or manual data entry projects require attention, enabling you to prioritize and address issues efficiently. For more information, see Ingestion. ",
    "url": "/release-notes/2025-05#new-ingestion-dashboard",
    
    "relUrl": "/release-notes/2025-05#new-ingestion-dashboard"
  },"326": {
    "doc": "Release 2025.05",
    "title": "Sources page update",
    "content": "The Sources page (previously, Data Sources) now displays the number of sources per type and allows filtering the sources by type. Now, you can quickly find the needed source without expanding the group. ",
    "url": "/release-notes/2025-05#sources-page-update",
    
    "relUrl": "/release-notes/2025-05#sources-page-update"
  },"327": {
    "doc": "Release 2025.05",
    "title": "Monitoring page update",
    "content": "The Monitoring page for data sets created from ingestion endpoints now displays the ingestion progress over time in the form of hourly and daily ingestion reports. Additionally, we’ve improved the endpoint index to help you track each payload by receipt ID. This way, you can review all records sent to CluedIn in a specific request. You can also view the golden records produced from each payload and delete those golden records if you no longer need them. We’ve also added a list of potential errors that can occur with the data set, along with remediation steps and the status for each error. For more information, see Monitoring for ingestion endpoints. ",
    "url": "/release-notes/2025-05#monitoring-page-update",
    
    "relUrl": "/release-notes/2025-05#monitoring-page-update"
  },"328": {
    "doc": "Release 2025.05",
    "title": "Source record approval",
    "content": "Source record approval is a mechanism that ensures only verified records are sent for processing. This is particularly useful for data sets created via an ingestion endpoint. After completing the initial full data load, you may want to ingest only delta records on a daily basis. The approval mechanism helps ensure that only verified delta records are processed. Data source owners can review these delta records and decide whether they should be processed. Source record approval can also be beneficial for manual data entry projects, as it grants project owners full control over the records created. Project owners can review new records added by non-owner users and decide whether they should be processed and turned into golden records. For more information, see Approval. ",
    "url": "/release-notes/2025-05#source-record-approval",
    
    "relUrl": "/release-notes/2025-05#source-record-approval"
  },"329": {
    "doc": "Release 2025.05",
    "title": "Data set validations",
    "content": "Data set validations allow you to check source records for errors, inconsistencies, and missing values, and to correct invalid values. You can use auto-validation, where CluedIn analyzes the fields and suggest appropriate validation methods, or configure your own validation methods. By using data set validations, you can enhance the quality of source records and prevent incorrect records from becoming golden records. For more information, see Validations. ",
    "url": "/release-notes/2025-05#data-set-validations",
    
    "relUrl": "/release-notes/2025-05#data-set-validations"
  },"330": {
    "doc": "Release 2025.05",
    "title": "Power Fx formulas",
    "content": "You can now use Power Fx formulas in rules to set up filters, conditions, and actions. With the help of Excel-like formulas, you can perform querying, equality testing, decision making, type conversion, and string manipulation based on the supported properties of a data part or a golden record. For more information, see Power Fx formulas in rules. ",
    "url": "/release-notes/2025-05#power-fx-formulas",
    
    "relUrl": "/release-notes/2025-05#power-fx-formulas"
  },"331": {
    "doc": "Release 2025.05",
    "title": "New access control actions",
    "content": "Previously, you could only use the Allow Access action in the policy rule to give access to all or specific vocabulary keys in golden records. Now, you have more granular control over the types of access to golden records and their properties: . | View – to allow view-only access to all or specific vocabulary keys in golden records. | Mask – to restrict access to sensitive data, allowing certain users or roles to know that the value exists but remains hidden. | Add/edit – to grant full control over specific properties in golden records, allowing certain users or roles to add new properties to the golden record or edit existing properties. | . For more information, see Access control. ",
    "url": "/release-notes/2025-05#new-access-control-actions",
    
    "relUrl": "/release-notes/2025-05#new-access-control-actions"
  },"332": {
    "doc": "Release 2025.05",
    "title": "Search experience update",
    "content": "You can quickly find your recent searches, private saved searches, and shared saved searches by clicking anywhere in the search box. We’ve also improved the selection of business domains for search—now, you can view the number of golden records per business domain. Additionally, to simplify the process of adding columns to the search results page, we’ve updated the vocabulary key selector. Now, the vocabulary keys are grouped by vocabularies, allowing you to conveniently add all or specific vocabulary keys. For more information, see Search. ",
    "url": "/release-notes/2025-05#search-experience-update",
    
    "relUrl": "/release-notes/2025-05#search-experience-update"
  },"333": {
    "doc": "Release 2025.05",
    "title": "Audit log actions update",
    "content": "We’ve expanded the list of audit log actions to help you track changes to clean projects, deduplication projects, users, and vocabularies. Additionally, the audit log now displays the activation and deactivation of enrichers, crawlers, and export targets, as well as the addition or removal of permissions for these items. This enhancement provides greater visibility and control, ensuring you can effectively audit and manage all activities. ",
    "url": "/release-notes/2025-05#audit-log-actions-update",
    
    "relUrl": "/release-notes/2025-05#audit-log-actions-update"
  },"334": {
    "doc": "Release 2025.05",
    "title": "Stream logs",
    "content": "We have added stream logs to help you verify the successful export of your golden records and identify issues if something goes wrong. On the golden record page, you can now find a list of streams that exported the golden record, along with the date it was sent to the export target. Additionally, each stream now includes a stream log, where you’ll find all golden records exported by that stream. Stream logs can help you effectively monitor and troubleshoot your streams to ensure everything runs smoothly. For more information, see Stream logs. ",
    "url": "/release-notes/2025-05#stream-logs",
    
    "relUrl": "/release-notes/2025-05#stream-logs"
  },"335": {
    "doc": "Release 2025.05",
    "title": "Terminology changes",
    "content": "To simplify CluedIn interface and make it more intuitive and better aligned with common industry concepts, we have changed some terms used in the platform. There are three major changes: . | Entity type is now referred to as Business domain. | Entity origin code is now referred to as Primary identifier. | Entity codes are now referred to as Identifiers. | . The functionality behind the terms remains the same; only the terms have been changed. Learn more in Terminology changes. ",
    "url": "/release-notes/2025-05#terminology-changes",
    
    "relUrl": "/release-notes/2025-05#terminology-changes"
  },"336": {
    "doc": "Release 2025.05",
    "title": "Release 2025.05",
    "content": " ",
    "url": "/release-notes/2025-05",
    
    "relUrl": "/release-notes/2025-05"
  },"337": {
    "doc": "Streams",
    "title": "Streams",
    "content": "When you have the ready-to-use data in CluedIn, you can send it to any external system where the data can be used for executing various business tasks. The following diagram shows the basic steps of sending the records from CluedIn to an external system. When you start the stream, all records matching the stream’s filters will be sent to the external system (target). If new records appear in CluedIn and they match the stream’s filters, they will be automatically sent to the target. In addition to that, if you make any changes to the records in CluedIn—for example, fix some values by running a clean project—these changes will be automatically made in the corresponding records stored within the target. This section covers the following areas: . | Creating a stream – learn how to create a stream, configure the export target for the stream, and define which golden records and specific properties to send to an external system. | Managing streams – learn how to edit a stream and work with the stream controls, as well how these actions affect the stream. | Reference information about streams – find information about stream statuses and other stream indicators. | . ",
    "url": "/consume/streams",
    
    "relUrl": "/consume/streams"
  },"338": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "/paas-operations/configuration",
    
    "relUrl": "/paas-operations/configuration"
  },"339": {
    "doc": "Customization in CluedIn",
    "title": "On this page",
    "content": ". | Customization of data ingestion | Customization of rules | Customization of enrichers | Customization of export targets for a stream | Customizations using Microsoft Fabric (or Azure Databricks) | . In this article, you will learn about customizations that can be implemented in CluedIn. While this article focuses on the most common customization options, do not hesitate to reach out to CluedIn Customer Success Team if your use case requires other solutions. ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#on-this-page"
  },"340": {
    "doc": "Customization in CluedIn",
    "title": "Customization of data ingestion",
    "content": "CluedIn offers multiple options for getting your data into the platform. However, you can further customize the data ingestion process if your use case requires it. There are three ways to customize data ingestion in CluedIn: . | Customization via advanced mapping – if you want to customize the CluedIn mapping experience, you can write JavaScript code at any time. This can be useful for implementing complex conditional mapping, as the UI mapping may become too complex for such tasks. For more information, see Advanced mapping code. | Customization via advanced source record validation – if you want to validate source records before processing, you can write JavaScript code. This can be useful if you want to improve the quality of source records and prevent incorrect records from being added to golden records. For more information, see Validations. | Customization via crawler written in C# – if the existing options for ingesting data into CluedIn do not meet your needs—for example, if neither Azure Data Factory, Microsoft Fabric, nor CluedIn have a specific connector—you can use a custom crawler. This can be useful if you use a legacy system that works on-premises or a very custom system. You can get acquainted with the example of crawler template here. | . ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-data-ingestion",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-data-ingestion"
  },"341": {
    "doc": "Customization in CluedIn",
    "title": "Customization of rules",
    "content": "In CluedIn, there are three types of rules: data part rules, survivorship rules, and golden record rules. Regardless of the type of rule, the rule structure is the same. Essentially, a rule is an object that allows you to transform the data and modify the data processing logic. CluedIn offers a variety of actions for each rule type to cover the most common data transformation tasks. However, if out-of-the-box actions are not enough, and you need something non-standard, you can write custom rule actions. There are three ways to customize rule actions in CluedIn: . | Custom action in data part rules written in C# – if the list of actions in data part rules does not contain the action you need, you can reach out to CluedIn Customer Success Team to get help with preparing custom rule action for your use case. | Custom action in survivorship rules written in C# – if the list of actions in survivorship rules does not contain the action you need, you can reach out to CluedIn Customer Success Team to get help with preparing custom rule action for your use case. | Custom action in golden record rules written in C# – if the list of actions in golden record rules does not contain the action you need, you can reach out to CluedIn Customer Success Team to get help with preparing custom rule action for your use case. | . After the custom rule action is ready, it will appear in the list of actions for the specific rule type, allowing you to select it while creating a rule. Example of custom data part rule action . One of the examples of custom rule actions if the Enrich action. It provides a flexible way to enrich data in CluedIn with external data sources. The Enrich action requires an API that accepts a list of vocabulary keys and returns a list of properties. The returned properties are saved with a specified vocabulary prefix. The API can be implemented as an Azure Function, a REST API, or any other service that can be accessed via HTTP. The API is also responsible for getting the data from the external source, processing it, and returning the properties to CluedIn. The Enrich action takes the following parameters: . | URL – the URL of the enriching API. | Payload – comma-separated list of vocabulary keys to send to the API. | Vocabulary Prefix – the vocabulary prefix used to save properties returned by the API. | . When the Enrich action is invoked, it sends the payload to the API (HTTP POST) and saves the returned properties with the specified vocabulary prefix. If you would like to implement the Enrich action, reach out to CluedIn Customer Success Team. ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-rules",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-rules"
  },"342": {
    "doc": "Customization in CluedIn",
    "title": "Customization of enrichers",
    "content": "CluedIn offers a variety of enrichers to help you enhance your golden records with information from external sources. However, if the out-of-the-box list of enrichers is not sufficient, we can create a custom enricher in C# for your specific system. For example, an enricher that calls an internal API to enrich your data. ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-enrichers",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-enrichers"
  },"343": {
    "doc": "Customization in CluedIn",
    "title": "Customization of export targets for a stream",
    "content": "CluedIn offers a variety of export targets to cater for different export systems. Among them are OneLake, SQL Server, Azure Data Lake, and many others. If you cannot find the needed connector in the list and require a custom solution, reach out to CluedIn and our Data Engineers will help you implement a custom solution. Example of custom export target . One of the examples of custom export targets is Kafka. It provides an efficient way to export golden records from CluedIn to Kafka. The Kafka connector requires the following parameters: . | Bootstrap Servers – Kafka server address. | SASL Username – the username or the key. | SASL Password – the secret or the password. | . When you start the stream with the Kafka connector, CluedIn first checks if the connection can be established. Then, CluedIn checks for the streaming mode—event mode or synchronized—that is used in the particular export target configuration. Finally, every time a golden record is modified in CluedIn, the corresponding event is sent to Kafka. If you use Kafka in your operations and would like to send golden records from CluedIn to Kafka topic, reach out to CluedIn support. ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-export-targets-for-a-stream",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#customization-of-export-targets-for-a-stream"
  },"344": {
    "doc": "Customization in CluedIn",
    "title": "Customizations using Microsoft Fabric (or Azure Databricks)",
    "content": "If all of the above customization options are not sufficient for you, consider opting for customizations using Microsoft Fabric (or Azure Databricks). You can run your own scripts using CluedIn Python SDK to perform such tasks as: . | Ingestion . | Enrichment . | Custom export . | Custom reporting . | . Regardless of the customization you need, get in touch with our Customer Success Team. We have implemented many customizations, so we might already have what you are looking for. ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin#customizations-using-microsoft-fabric-or-azure-databricks",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin#customizations-using-microsoft-fabric-or-azure-databricks"
  },"345": {
    "doc": "Customization in CluedIn",
    "title": "Customization in CluedIn",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/customization-in-cluedin",
    
    "relUrl": "/playbooks/data-engineering-playbook/customization-in-cluedin"
  },"346": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "On this page",
    "content": ". | Create a new pipeline | Configure source | Configure sink | Debug pipeline in ADF | Validate result in CluedIn | . This guide provides step-by-step instructions for configuring an Azure Data Factory (ADF) Data flow activity to send data to CluedIn. This integration enables seamless data transfer from your Azure Data Lake (or other sources) to CluedIn using a private endpoint. The Data flow activity in ADF is ideal when data transformations such as aggregation, filtering, or applying complex business logic are required before sending data to CluedIn. If no transformations are needed, use the Copy data activity instead. Prerequisites  . | Configure a private link service between ADF and CluedIn as described in Configure ADF with private link. | Ensure your data is available within Azure, commonly stored in Azure Data Lake or Blob Storage. | Create an ingestion endpoint and authorization token in CluedIn as described in Add ingestion endpoint.  . | . Configuring an ADF pipeline with the Data flow activity consists of 4 steps: . | Creating a new pipeline . | Configuring the source . | Configuring the sink . | Debugging the pipeline . | Validating the result in CluedIn . | . ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#on-this-page",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#on-this-page"
  },"347": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Create a new pipeline",
    "content": ". | On the Azure Data Factory home page, select New &gt; Pipeline. | In the Activities pane, expand the Move and transform category, and then drag the Data flow activity to the pipeline canvas. | Select the new Data flow activity on the canvas, and then go to the Settings tab to edit its details. | . ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#create-a-new-pipeline",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#create-a-new-pipeline"
  },"348": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Configure source",
    "content": ". | Next to the Data flow field, select + New. | Select Add Source. | On the Source Settings tab, do the following: . | Enter the Output stream name. | In Source type, select Dataset. | Next to the Dataset dropdown list, select + New. | . | In the New dataset pane, select your data stored within an Azure storage account (for example, Azure Data Lake Storage Gen2). Then, select Continue. | In the Select format pane, select DelimitedText, and then select Continue. | In the Set properties pane, enter the name for the dataset. Then, expand the Linked service dropdown list, and select + New. | Configure the service details: . | Name – enter the name for your linked service. | Account selection method – select From Azure subscription. | Azure subscription – select the subscription of your Azure Data Lake. | Storage account name – select the name of your Azure Data Lake storage account. | . | Test the connection and then create the new linked service. | On the Set properties pane, in the File path section, add the path to the appropriate folder/file within your Azure Data Lake. | Select OK. | . ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#configure-source",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#configure-source"
  },"349": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Configure sink",
    "content": ". | Next to the data source, select the plus icon, and then find and select Sink. | On the Sink tab, do the following: . | Enter the Output stream name. | In Incoming stream, make sure the data source created in the previous step is selected. | In Sink type, select Dataset. | Next to the Dataset dropdown list, select + New. | . | In the New dataset pane, find and select REST, and then select Continue. | In the Set properties pane, enter the name for the dataset. Then, expand the Linked service dropdown list and select + New. | Configure the service details: . | Name – enter the name for your linked service. | Base URL – enter the URL of the ingestion endpoint in CluedIn. You can find this URL in the data set that you created for ingesting data into CluedIn. For more information, see Send data. | Authentication type – select Anonymous. | Auth headers – add a new header with the following details: . | Name – enter Authorization. | Value – enter Bearer, add a space, and then paste the token from CluedIn. You can find the token in CluedIn by going to Administration &gt; API Tokens. For more information, see Send data. | . As a result, the new linked service should be configured similar to the following. | . | Test connection, and then select Create. | After the sink is configured, go to the Settings tab, and then do the following: . | Ensure the Insert method is set to POST. | Change the Delete method, Upsert method, and Update method to None. | Set the Http Compression type to GZip. If you do not set the HTTP Compression type to GZip, you might encounter an error when starting the pipeline. | Set the Batch size to 10,000 to ensure smoother transfer. | . | . ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#configure-sink",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#configure-sink"
  },"350": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Debug pipeline in ADF",
    "content": "Once the source and sink are configured, you can debug the pipeline to ensure it works correctly. To debug the pipeline . | On the toolbar, select Debug. | In the Output tab at the bottom of the window, monitor the pipeline run status. | . ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#debug-pipeline-in-adf",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#debug-pipeline-in-adf"
  },"351": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Validate result in CluedIn",
    "content": "Once the ADF pipeline is triggered successfully, you should see the data flowing into CluedIn. You can view the incoming records on the Preview tab of the data set. ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity#validate-result-in-cluedin",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity#validate-result-in-cluedin"
  },"352": {
    "doc": "Configure ADF pipeline with Data flow activity",
    "title": "Configure ADF pipeline with Data flow activity",
    "content": " ",
    "url": "/microsoft-integration/adf-integration/data-flow-activity",
    
    "relUrl": "/microsoft-integration/adf-integration/data-flow-activity"
  },"353": {
    "doc": "Ingest data",
    "title": "On this page",
    "content": ". | Data ingestion instructions | Structure of ingested data | Data ingestion results | Next step | . | Audience | Time to read | . | Data Engineer, Data Analyst | 3 min | . You are here in the data journey . Before you start . | Make sure you have conducted the data impact workshop to understand what sources you want to use first. | Make sure you are familiar with the available tools for data ingestion and have picked the right tool for your use case. | . Now that you have prepared a list of sources and selected a tool for data ingestion, you can start the actual data ingestion process. This process consists of three steps—ingest, map, and process. In this article, we’ll focus on the first step to get your data into CluedIn. While ingesting your data, remember one of our project principles—start small. The idea is to focus on one end-to-end data flow, and not to load 10 million records at once as this will become a burden in the development phase of your project. ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data#on-this-page",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data#on-this-page"
  },"354": {
    "doc": "Ingest data",
    "title": "Data ingestion instructions",
    "content": "In the following table, you’ll find links to video trainings and documentation for each data ingestion tool. Follow the steps for your tool of choice to get your data into CluedIn. | Tool | Link to documentation | . | File | Link to training video and documentation. | . | Endpoint | Link to training video and documentation. | . | Database | Link to documentation. | . | Azure Data Factory | Link to training video. | . | Microsoft Fabric/Azure Databricks | Link to documentation. | . | Microsoft Purview | Link to documentation. | . | Crawler | Link to documentation. | . Data ingestion limitations . The current public release of CluedIn does not support nested data and will flatten nested objects. Depending on the structure of your nested data, the current flattening approach might be suitable. However, in some cases, you might need to process the nested object in a separate data set. We are currently working on the support of nested objects in the future. ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data#data-ingestion-instructions",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data#data-ingestion-instructions"
  },"355": {
    "doc": "Ingest data",
    "title": "Structure of ingested data",
    "content": "When the data is ingested into CluedIn, it will be represented in the following structure. | Group – this is a folder to organize your sources in a logical form. | Data source – this is an object that contains the necessary information on how to connect to the source (if applicable), as well as the users and roles that have permissions to the source. Think of it like a database in SQL Server. | Data set – this is the actual data obtained from the source. The data set contains unprocessed records, mapping information, quarantine capabilities, and rules applied to your raw records. Think of it like a table in a SQL Server database. | . ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data#structure-of-ingested-data",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data#structure-of-ingested-data"
  },"356": {
    "doc": "Ingest data",
    "title": "Data ingestion results",
    "content": "If you followed our instructions, you should see the result similar to the following. Data source containing data sets . Data set containing raw records on the Preview tab . The main goal of data ingestion is to have some records on the Preview tab. ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data#data-ingestion-results",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data#data-ingestion-results"
  },"357": {
    "doc": "Ingest data",
    "title": "Next step",
    "content": "Ensure that the required records are available on the Preview tab of the data set. Once the necessary data is ingested, you can start the mapping process. ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data#next-step",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data#next-step"
  },"358": {
    "doc": "Ingest data",
    "title": "Ingest data",
    "content": " ",
    "url": "/playbooks/data-ingestion-playbook/ingest-data",
    
    "relUrl": "/playbooks/data-ingestion-playbook/ingest-data"
  },"359": {
    "doc": "1.1.2. Ingesting a database table",
    "title": "1.1.2. Ingesting a database table",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll explore another method of data ingestion—by bringing in a database table with customer data from an ERP system. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Training file: erp-customer.csv (import this file into your database to replicate the process demonstrated during the training) . Useful resources: . | Getting started with data ingestion . | Golden records . | . ",
    "url": "/training/fundamentals/ingesting-database-table",
    
    "relUrl": "/training/fundamentals/ingesting-database-table"
  },"360": {
    "doc": "Prepare for your CluedIn project",
    "title": "On this page",
    "content": ". | Complete self-training | Define ownership | Onboard your team | Prepare use cases | . | Audience | Time to read | . | Business User, Data Project Lead, Data Steward, Data Analyst, Data Architect, Data Engineer | 5 min | . This article outlines four key steps to prepare for the implementation of you project in CluedIn. Following these steps will ensure that you and your team are ready to proceed with confidence. ",
    "url": "/playbooks/prepare-for-your-cluedin-project#on-this-page",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project#on-this-page"
  },"361": {
    "doc": "Prepare for your CluedIn project",
    "title": "Complete self-training",
    "content": "Visit our documentation portal and complete the Getting Started section. This will provide a solid foundation and help you understand how CluedIn differs from traditional MDM platforms. Be open to new methodologies and concepts presented in the training materials. This shift in mindset is crucial for leveraging CluedIn’s full potential. ",
    "url": "/playbooks/prepare-for-your-cluedin-project#complete-self-training",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project#complete-self-training"
  },"362": {
    "doc": "Prepare for your CluedIn project",
    "title": "Define ownership",
    "content": "Establish a clear picture of who owns what. During the course of your CluedIn project, numerous small decisions will need to be made. Knowing who can make decisions at any moment is crucial. This clarity helps CluedIn provide timely support when needed. The key ownership areas include: . | IT Infrastructure – assign someone to be responsible for the IT infrastructure. This person will address challenges and act as the primary contact for all IT-related requests. | Business domain – identify owners or stakeholders for each business domain. These individuals will decide on the rules, needs, and value for their respective domains. | Data ownership – each data source should have a designated owner. This can be someone internal or external to your team. For example, if you are using data from your CRM system, ensure someone owns that data. | . Depending on the size of your organization, there are 2 approaches for defining ownership: . | Small organization – it is enough to define the Accountable person for each key ownership area. | Large organization – build a RACI matrix (Responsible, Accountable, Consulted, Informed) for each key ownership area and clearly communicate the involvement required from these owners. Even if they won’t be using CluedIn directly, effective data management depends on their active participation. | . When you add a new source or a new business domain, go back to the RACI matrix and update it accordingly. The RACI matrix should probably be in the first page of your internal wiki, SharePoint, or Confluence site. ",
    "url": "/playbooks/prepare-for-your-cluedin-project#define-ownership",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project#define-ownership"
  },"363": {
    "doc": "Prepare for your CluedIn project",
    "title": "Onboard your team",
    "content": "This section offers guidelines to onboard your team for the CluedIn implementation journey. By following these guidelines, your team will be well-equipped to overcome challenges and achieve your data management goals. To prepare your team to embark on the CluedIn implementation journey, follow these steps: . | Foster team collaboration – emphasize the importance of teamwork. Project implementation is a journey with ups and downs, and success depends on collective effort and support. | Encourage self-training – allow team members to experiment and self-train. This hands-on experience is vital for learning and overcoming initial discomfort. | Promote experimentation – foster a culture of experimentation where it’s acceptable to make mistakes and learn from them. Think of implementing CluedIn like learning to ride a bike. The first time you try, it’s likely you’ll fall a few times. Just like you needed practice, support, and maybe some help from your parents when learning to ride a bike, your team will need to go through a similar process. At first, it might feel uncomfortable, and you might need guidance from CluedIn experts. However, with persistence, you’ll gradually become more comfortable and eventually, you’ll be able to ride smoothly on your own. This journey of trial, error, and gradual improvement will ultimately lead to mastery. | Seek help when needed – don’t hesitate to reach out for support if the team encounters roadblocks. CluedIn support is available to assist, but the primary focus should be on self-discovery and problem-solving. Additionally, you can book an enablement training with us by reaching out to our sales team. | . ",
    "url": "/playbooks/prepare-for-your-cluedin-project#onboard-your-team",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project#onboard-your-team"
  },"364": {
    "doc": "Prepare for your CluedIn project",
    "title": "Prepare use cases",
    "content": "If you read our article on how to approach a data project in CluedIn, you already know the importance of starting small. Here are additional recommendations for preparing your first use cases: . | Priority 1: Few Records – select use cases that involve a small number of records. | Priority 2: Easy – choose something straightforward to implement. | Priority 3: Impact – ensure the use case creates noticeable value. | . Your initial use cases do not have to be the highest priority tasks but should be significant enough to demonstrate value. Successfully implementing these small use cases will help your team build confidence in your approach, processes, and tools. Use these early successes to demonstrate your team’s ability to deliver results and highlight CluedIn’s effectiveness to stakeholders. Once that is done, scale up gradually, and begin tackling your more substantial, real-world use cases. You can ingest millions of records into CluedIn, so you don’t have to worry about the load. Instead, think about business domains, rules, and what you want to produce. After that, the next natural step might involve processing all records or integrating one more source. This process will help you find balance between time and value. Aim to clean, deduplicate, enrich, and process smaller subsets of data thoroughly rather than handling massive datasets with minimal processing. Embrace a pragmatic approach by adapting to reality. Understand that sometimes compromises are necessary. You might not be able to enrich or deduplicate all data immediately, and that’s acceptable. CluedIn’s experience and support will help you find this balance to implement a successful project. ",
    "url": "/playbooks/prepare-for-your-cluedin-project#prepare-use-cases",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project#prepare-use-cases"
  },"365": {
    "doc": "Prepare for your CluedIn project",
    "title": "Prepare for your CluedIn project",
    "content": " ",
    "url": "/playbooks/prepare-for-your-cluedin-project",
    
    "relUrl": "/playbooks/prepare-for-your-cluedin-project"
  },"366": {
    "doc": "Pricing",
    "title": "On this page",
    "content": ". | Pay-as-you-go | Committed deal | . In this article, you will learn about our pricing options for CluedIn SaaS and PaaS. ",
    "url": "/deployment/pricing#on-this-page",
    
    "relUrl": "/deployment/pricing#on-this-page"
  },"367": {
    "doc": "Pricing",
    "title": "Pay-as-you-go",
    "content": "Our pay-as-you-go pricing model operates on a monthly invoicing system, where the cost is calculated based on the number of billable records recorded every hour. For more information, see Billable records. Additionally, you can download our billable record overview here. ",
    "url": "/deployment/pricing#pay-as-you-go",
    
    "relUrl": "/deployment/pricing#pay-as-you-go"
  },"368": {
    "doc": "Pricing",
    "title": "Committed deal",
    "content": "To find out about our committed deal pricing model, reach out to our sales team here. ",
    "url": "/deployment/pricing#committed-deal",
    
    "relUrl": "/deployment/pricing#committed-deal"
  },"369": {
    "doc": "Pricing",
    "title": "Pricing",
    "content": " ",
    "url": "/deployment/pricing",
    
    "relUrl": "/deployment/pricing"
  },"370": {
    "doc": "Terminology changes",
    "title": "Terminology changes",
    "content": "In the 2025.05 release of CluedIn, we have updated some of the terminology used in the platform. The terminology changes simplify CluedIn interface, making it more intuitive and better aligned with common industry concepts. The terminology changes only affect the terms, the functionality behind the terms remains the same. The following table provides a summary of terminology changes in CluedIn. | Old term | New term | Definition | . | Entity type | Business domain | A well-known business object that provides context for golden records. In CluedIn, all golden records must have a business domain to ensure systematic organization of data management processes. Learn more in Business domain. | . | Entity origin code | Primary identifier | A mechanism used in CluedIn to define the uniqueness of a golden record. It is configured during the mapping process. The primary identifier consists of a business domain, an origin, and a key that uniquely identifies the record. This combination allows you to achieve absolute uniqueness across all the data sources you interact with. Learn more in Identifiers. | . | Entity codes | Identifiers | Additional mechanism to define the uniqueness of a golden record. If a data set contains additional columns that can uniquely represent a record, these columns can also be used to generate identifiers. Learn more in Identifiers. | . ",
    "url": "/release-notes/terminology-changes",
    
    "relUrl": "/release-notes/terminology-changes"
  },"371": {
    "doc": "1.1.3. Building a single customer view",
    "title": "On this page",
    "content": ". | Part 1 | Part 2 | Part 3 | Part 4 | . ",
    "url": "/training/fundamentals/building-a-single-customer-view#on-this-page",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view#on-this-page"
  },"372": {
    "doc": "1.1.3. Building a single customer view",
    "title": "Part 1",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll focus on building a single customer view by identifying and merging duplicate records. We’ll explore the concept of primary identifiers in CluedIn and how to use them to proactively unify duplicate data into a single, accurate customer view. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Useful resources: . | Identifiers . | Origin . | Business domain . | . ",
    "url": "/training/fundamentals/building-a-single-customer-view#part-1",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view#part-1"
  },"373": {
    "doc": "1.1.3. Building a single customer view",
    "title": "Part 2",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll explore an alternative approach to identifying and merging duplicate records as part of building a single, unified customer view—a deduplication project. You’ll learn how to set up and work with a deduplication project, understand when this method is most effective, and see how it compares to other deduplication strategies, such as merging by identifiers. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Useful resources: . | Deduplication | . ",
    "url": "/training/fundamentals/building-a-single-customer-view#part-2",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view#part-2"
  },"374": {
    "doc": "1.1.3. Building a single customer view",
    "title": "Part 3",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll explore strategies for defining the uniqueness of records—particularly in situations where the property used to generate a primary or additional identifier is missing or empty. In CluedIn, when two or more records share the same identifier—whether it’s primary or additional—they will be merged into a single golden record. That’s why it’s important to always use valid, unique identifiers. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Useful resources: . | Identifiers | . ",
    "url": "/training/fundamentals/building-a-single-customer-view#part-3",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view#part-3"
  },"375": {
    "doc": "1.1.3. Building a single customer view",
    "title": "Part 4",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll learn how to identify and resolve data quality issues using CluedIn’s edit mode and validations. This helps fix invalid or incomplete identifiers before processing, ensuring more accurate matching and preventing unintended merges across your data. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Useful resources: . | Identifiers . | Modify source records . | Validations . | . ",
    "url": "/training/fundamentals/building-a-single-customer-view#part-4",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view#part-4"
  },"376": {
    "doc": "1.1.3. Building a single customer view",
    "title": "1.1.3. Building a single customer view",
    "content": " ",
    "url": "/training/fundamentals/building-a-single-customer-view",
    
    "relUrl": "/training/fundamentals/building-a-single-customer-view"
  },"377": {
    "doc": "Concept of mapping",
    "title": "On this page",
    "content": ". | Core mapping terms | Mapping process overview | Setting up the right identifiers | Main decisions in mapping | Next step | . | Audience | Time to read | . | Data Engineer, Data Analyst | 5 min | . You are here in the data journey . Before you start . | Make sure you have conducted the data impact workshop to understand what sources you want to use first. | Make sure you are familiar with the available tools for data ingestion and have picked the right tool for your use case. | Make sure you have ingested the data, and the records are available on the Preview tab of the data set. | . Now that you have ingested the data using the tool of your choice, the next step is to create mapping. However, before doing so, we recommend that you get acquainted with the overall concept of mapping. After all, starting the mapping process without understanding the core concepts can lead to issues later. Mapping is one of the most important steps in CluedIn, and we know it can be tricky initially. We understand that the learning curve is a bit steep, but hopefully with these playbooks you will be able to navigate it successfully. ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#on-this-page",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#on-this-page"
  },"378": {
    "doc": "Concept of mapping",
    "title": "Core mapping terms",
    "content": "The goal of mapping is to add a semantic layer to your records that allows CluedIn to better understand your records. This is a requirement for processing your records into golden records. The mapping uses multiple CluedIn terms that you will need to learn. While it may not be absolutely critical at the start of the project, we believe that a minimal level of understanding is necessary. The following table lists the most important mapping terms, their purposes, and links to relevant documentation. | Name | Purpose | Link to documentation | . | Business domain | The business domain represents a specific business object that describes the semantic meaning of your data. Read the documentation to understand how to choose a good business domain. | Link | . | Vocabulary and vocabulary keys | The vocabulary is used to define the semantic layer (metadata) for your data. The vocabulary contains vocabulary keys that describe the properties coming in from the data source. Read the documentation to learn about vocabulary usage. | Link | . | Identifiers | This is a mechanism that CluedIn uses to define the uniqueness of a golden record. Read the documentation to understand the concept of primary identifier and identifiers. | Link | . | Origin | The origin generally determines the source of golden records, and it is used in identifiers. Read the documentation to understand the importance of the origin. | Link | . ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#core-mapping-terms",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#core-mapping-terms"
  },"379": {
    "doc": "Concept of mapping",
    "title": "Mapping process overview",
    "content": "When you create mapping, you are essentially transforming your initial raw records into a format that CluedIn can understand. We call this format a clue. In terms of medallion architecture, a clue is a silver record. To get a good default mapping configuration, use the auto-mapping option. It is a great way to start and define the most important mapping attributes—business domain and vocabulary. You can learn more about the process of creating mapping and find step-by-step instructions in a dedicated article. For lineage purposes, we recommend keeping a vocabulary close to the source. Later, you can map the vocabulary keys to generic, shared vocabulary keys. When you have multiple sources with vocabulary keys mapped directly to the generic, shared vocabulary keys, it can become overwhelming to have more than 10 sources mapping directly to your golden records. In order to avoid confusion for those consuming the records, it is a good practice to map to the source vocabulary first. ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#mapping-process-overview",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#mapping-process-overview"
  },"380": {
    "doc": "Concept of mapping",
    "title": "Setting up the right identifiers",
    "content": "After you selected the right business domain and vocabulary, it’s time to choose the right key to produce the identifiers for your records. Poorly defined identifiers can have a truly unexpected impact. The most common pitfall of poorly defined identifiers is what we call over-merging. It happens when you set up an identifier that is not actually unique. Suppose you choose the country code as a key, then all records with the same country code will merge together into one record. For example, if you have 100,000 records with the country code of “DK”, then all those 100,000 records will end up as 1 record. Poorly defined identifiers can cause system slowdowns. While reverting is possible, it will take time as now CluedIn will need to split those records. At that point, the fastest solution is to remove those records from CluedIn and restart the mapping. With the country code example, it is easy to understand that using non-unique properties as identifiers is not a good choice. Sometimes, the key you consider unique is not in fact unique. For example, a SKU code—unique internal product code—can have the following issues: . | - as a value. | NULL as a text value. | N/A, N-A, or any other combinations. | Records sharing the same code, even if it is supposed to be unique. | Wrong value (for example, email in place of the actual value). | . Of course, CluedIn has ways to fix such data quality issues. However, if you blindly choose a key to produce an identifier, it may lead to issues. To avoid this, CluedIn tells you the potential duplicates you have in your records before processing. ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#setting-up-the-right-identifiers",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#setting-up-the-right-identifiers"
  },"381": {
    "doc": "Concept of mapping",
    "title": "Main decisions in mapping",
    "content": "This is an introductory article on the topic of mapping. We’ll add more detailed articles soon, as mapping is a significant part of the processes in CluedIn. As an outcome of this article, you know about 2 important decisions you have to make while creating mapping: . | Choose the right business domain. | Choose the right vocabulary that is close to the source. | . Additionally, you now know about the importance of choosing the unique key for producing the identifiers. If you are unsure whether the selected key is unique, you can use a deduplication project to detect and merge duplicates. ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#main-decisions-in-mapping",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#main-decisions-in-mapping"
  },"382": {
    "doc": "Concept of mapping",
    "title": "Next step",
    "content": "After creating the mapping, you can process your records to produce golden records. ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping#next-step",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping#next-step"
  },"383": {
    "doc": "Concept of mapping",
    "title": "Concept of mapping",
    "content": " ",
    "url": "/playbooks/data-ingestion-playbook/concept-of-mapping",
    
    "relUrl": "/playbooks/data-ingestion-playbook/concept-of-mapping"
  },"384": {
    "doc": "Local",
    "title": "Local",
    "content": "Learn how to run CluedIn locally and prepare to stream data to Microsoft SQL Server databases. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. CluedIn is being developed with a cloud-centric approach, which means that running CluedIn locally can pose some challenges. The local installation of CluedIn does not provide all the features available in cloud environment, such as auto-scaling, auto-restart, logging, monitoring, and more. However, you may still consider installing CluedIn locally for the following reasons: . | Cost efficiency. Running CluedIn in Azure incurs expenses, and for the purpose of simple testing, you may prefer to avoid these costs. | Writing CluedIn extensions. CluedIn is very flexible and can be extended through custom code. If you want to extend CluedIn by writing code, you can test your extensions locally within CluedIn, making the testing process easier. | No need to be Administrator in Azure. Running CluedIn in Azure requires elevated permissions within the Azure platform, which you might not have. Thus, having the ability to run CluedIn locally provides you with the advantage of evaluating, customizing, and developing with CluedIn without the need to deal with permissions in Azure. | . This section contains the instructions on how to run CluedIn locally with the SQL Server Connector extension. ",
    "url": "/deployment/local",
    
    "relUrl": "/deployment/local"
  },"385": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "On this page",
    "content": ". | AI agents | Copilot upgrade | Tag monitoring | Relations Explorer upgrade | Rule Builder: logging and affected records | Clean module: affected records | Schema protection for ingestion endpoints | Improved navigation experience | . This page describes an upcoming release. Features listed here are not yet available—stay tuned for updates when the release goes live. This article outlines new features and improvements in CluedIn 2025.09. Presentation: Download PPT . The following sections contain brief description of new features and links to related articles. ",
    "url": "/release-notes/2025-09#on-this-page",
    
    "relUrl": "/release-notes/2025-09#on-this-page"
  },"386": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "AI agents",
    "content": "You can now take the manual effort out of data quality with AI-powered agents that handle routine, time-consuming tasks for you, including: . | Detecting duplicate records. | Identifying and fixing data quality issues. | Suggesting and creating data quality rules. | . Two built-in agents are available out of the box, and you can also create custom agents tailored to your needs. Our AI agents work with all popular LLMs including OpenAI, so you can choose between free options or more advanced paid models. ",
    "url": "/release-notes/2025-09#ai-agents",
    
    "relUrl": "/release-notes/2025-09#ai-agents"
  },"387": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Copilot upgrade",
    "content": "The upgraded CluedIn Copilot takes automation even further, handling most master data management and data quality tasks that previously required manual work. Available actions include: . | Data mapping. | Detecting and flagging invalid values. | Fixing formatting and enrichment issues. | Generating rules from natural language. | . You can now launch entire workflows from a single prompt. ",
    "url": "/release-notes/2025-09#copilot-upgrade",
    
    "relUrl": "/release-notes/2025-09#copilot-upgrade"
  },"388": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Tag monitoring",
    "content": "The new Tag Monitoring module is now available under Governance. With this module, you can: . | Monitor in near real-time how the quality of your data evolves. | Track tag usage trends and identify recurring data quality issues. | . The module shows: . | A full list of tags used across the system. | Details about the golden records tagged. | Statistics on tag usage over time. | . The module doesn’t just provide insight, it empowers action. You can use our new AI agents to fix tagged records automatically and at scale. The Home page also features the Top tags by golden record count chart. It displays the top 5 most used tags based on the number of records flagged with them, making it easy to uncover recurring data issues at a glance. ",
    "url": "/release-notes/2025-09#tag-monitoring",
    
    "relUrl": "/release-notes/2025-09#tag-monitoring"
  },"389": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Relations Explorer upgrade",
    "content": "The redesigned Relations Explorer on the golden record page gives you a deeper, clearer view of your data connections. You can now: . | Filter data by direction (incoming, outgoing, or both), business domain, and relation type. | Search for golden records by name. | Load and explore multiple levels of golden records. | View detailed information about a record or its relations. | . ",
    "url": "/release-notes/2025-09#relations-explorer-upgrade",
    
    "relUrl": "/release-notes/2025-09#relations-explorer-upgrade"
  },"390": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Rule Builder: logging and affected records",
    "content": "The rule details page now includes two new tabs for greater visibility and oversight: . | Log – displays any issues that occur while a rule is processing the data, helping you quickly spot and resolve problems. | Affected Records – shows all golden records that were changed by the rule, helping you track and review the updates. | . ",
    "url": "/release-notes/2025-09#rule-builder-logging-and-affected-records",
    
    "relUrl": "/release-notes/2025-09#rule-builder-logging-and-affected-records"
  },"391": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Clean module: affected records",
    "content": "Every clean project now includes the new Affected Records tab, which shows all golden records changed as part of the clean project—helping you track and review the impact of your cleanup. ",
    "url": "/release-notes/2025-09#clean-module-affected-records",
    
    "relUrl": "/release-notes/2025-09#clean-module-affected-records"
  },"392": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Schema protection for ingestion endpoints",
    "content": "You can now use the new Schema Protection feature to ensure that incoming data matches the expected structure: . | When enabled, CluedIn automatically validates incoming records against the defined schema and field mappings. | Records that don’t conform are moved to quarantine, giving you full visibility to review and resolve the issues. | . For example, you expect employee data, but company data slips through—Schema Protection will catch it and send it to quarantine. Schema Protection can be enabled from the Process tab of any dataset imported through an ingestion endpoint. ",
    "url": "/release-notes/2025-09#schema-protection-for-ingestion-endpoints",
    
    "relUrl": "/release-notes/2025-09#schema-protection-for-ingestion-endpoints"
  },"393": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "Improved navigation experience",
    "content": "The new navigation menu makes it faster and easier to reach the areas you use most: . | Frequently used modules (such as Home, Business Domains, and so on) are now pinned to the top for one-click access. | Each such module includes a submenu for instant navigation to the exact page you need. | Modules that you haven’t used yet feature onboarding videos to help you get started. You can revisit the videos anytime on the documentation portal. | . ",
    "url": "/release-notes/2025-09#improved-navigation-experience",
    
    "relUrl": "/release-notes/2025-09#improved-navigation-experience"
  },"394": {
    "doc": "(Coming soon) Release 2025.09",
    "title": "(Coming soon) Release 2025.09",
    "content": " ",
    "url": "/release-notes/2025-09",
    
    "relUrl": "/release-notes/2025-09"
  },"395": {
    "doc": "Start your CluedIn project",
    "title": "On this page",
    "content": ". | PaaS CluedIn journey | SaaS CluedIn journey | Data journey | . | Audience | Time to read | . | Business User, Data Project Lead, Data Steward, Data Analyst, Data Architect, Data Engineer | 3 min | . This article outlines the main steps for starting your CluedIn project. Adopting the approach that we describe here will help you implement your project in an efficient manner. To streamline the process of starting your CluedIn project and provide a clear framework, we suggest dividing the project into 2 aspects: . | IT journey – involves CluedIn installation and other IT tasks for connecting CluedIn to external data sources and systems. | Data journey – involves data discovery followed by data ingestion, transformation, automation, export, and release to production. | . Depending on the cloud service offering that you choose—PaaS or SaaS—your IT journey will be different, but your data journey will be the same. Below, we’ll look at each aspect in more detail. ",
    "url": "/playbooks/start-your-cluedin-project#on-this-page",
    
    "relUrl": "/playbooks/start-your-cluedin-project#on-this-page"
  },"396": {
    "doc": "Start your CluedIn project",
    "title": "PaaS CluedIn journey",
    "content": "With CluedIn PaaS (platform as a service), you can install and manage CluedIn in your own Azure IT infrastructure, thus ensuring that your data stays in your own environment. We’ll work as part of your team throughout the whole PaaS CluedIn journey to ensure customer success. For more details about PaaS installation and IT journey, see Start your IT journey. ",
    "url": "/playbooks/start-your-cluedin-project#paas-cluedin-journey",
    
    "relUrl": "/playbooks/start-your-cluedin-project#paas-cluedin-journey"
  },"397": {
    "doc": "Start your CluedIn project",
    "title": "SaaS CluedIn journey",
    "content": "With CluedIn SaaS (software as a service), you can use CluedIn on a subscription basis. Instead of installing CluedIn on your own Azure IT infrastructure, you can access it in an isolated environment through a web browser. To help you achieve your goals more efficiently with guidance from our experts, consider obtaining a committed deal license and signing the Statement of Work (SOW). By doing so, we’ll work as part of your team throughout the entire SaaS journey, ensuring customer success. For more details about SaaS IT journey, see Start your IT journey. ",
    "url": "/playbooks/start-your-cluedin-project#saas-cluedin-journey",
    
    "relUrl": "/playbooks/start-your-cluedin-project#saas-cluedin-journey"
  },"398": {
    "doc": "Start your CluedIn project",
    "title": "Data journey",
    "content": "The data journey starts from the essential requirements workshop, where you discuss your objectives with a CluedIn expert so that we understand your use cases. The next step is the data discovery workshop, where you will collaborate with a CluedIn expert to discuss the data sources for your initial use case. Establishing and connecting to the data sources is the first step in the data quality virtuous cycle. The data quality virtuous cycle is a concept that emphasizes the continuous improvement of data quality through an iterative process of: . | Data ingestion – upload the data from the source systems into CluedIn and then map and process it to produce golden records. | Data transformation – identify and correct any issues in your data with the help of clean projects, establish criteria for finding and eliminating duplicates. | Data automation – create business rules to apply data transformations, capture data quality issues, and determine operational values. | Data export – send ready-to-use data from CluedIn to the external systems (also known as landing zones) where you usually perform tasks with business data. | Release to production – when all of the above steps are working as intended, release your first use case to production, and start with the next one. | . ",
    "url": "/playbooks/start-your-cluedin-project#data-journey",
    
    "relUrl": "/playbooks/start-your-cluedin-project#data-journey"
  },"399": {
    "doc": "Start your CluedIn project",
    "title": "Start your CluedIn project",
    "content": " ",
    "url": "/playbooks/start-your-cluedin-project",
    
    "relUrl": "/playbooks/start-your-cluedin-project"
  },"400": {
    "doc": "Upgrade",
    "title": "Upgrade",
    "content": "When signing up with CluedIn and taking on the Managed Services agreement, product upgrades will be handled for you by the CluedIn team. The team will reach out and work with your requirements to ensure that upgrades happen when it’s most convenient for your business. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. For some customers, a more tailored agreement exists whereby the management of the cluster and resources are managed by you, including upgrades. This section contains the guides for self-service upgrading of CluedIn. For details about the updates available in a specific release, see Release notes. ",
    "url": "/paas-operations/upgrade",
    
    "relUrl": "/paas-operations/upgrade"
  },"401": {
    "doc": "Operating cost reduction",
    "title": "On this page",
    "content": ". | Shut down environments in times of low or no usage | Reserve Azure instances | Run a local environment | . CluedIn runs on Azure Kubernetes Service (AKS), which uses node pools as the underlying resources. As part of the deployment, many Azure resources will be created in your environment, each with its own costs associated with it. The purpose of this guide is to explain what you can do to reduce the cost of CluedIn in your environment whilst maintaining a supported setup. To find out about the base cost, see CluedIn Azure costs. Additionally, keep the following considerations in mind: . | You do not need to have the production environment running during development. | You can reduce the cost of your production environment by reserving the VM: . | A 1-year reservation can save approximately 30% of the base cost. | A 3-year reservation can save up to 50% of the base cost. | . | We can provide and set up a script for scaling the environment up or down, potentially saving up to 40% of the base cost. | . ",
    "url": "/paas-operations/cost-reduction#on-this-page",
    
    "relUrl": "/paas-operations/cost-reduction#on-this-page"
  },"402": {
    "doc": "Operating cost reduction",
    "title": "Shut down environments in times of low or no usage",
    "content": "CluedIn is a product that is intended to run as part of your MDM pipeline. As a business, this may be during certain hours of the day (e.g. 0900-1700), which means that outside of these hours, the environments will be running and not being used. One easy way to reduce cost is to shut down your AKS during these hours and start the environment back up when it is needed. We only recommend these for your dev and test environments, and not production. Stopping the AKS cluster reduces costs, but you will still pay for other resources such as storage, networking, and so on. However, the overall cost will be significantly reduced. To stop AKS cluster . | Navigate to https://portal.azure. | Search for the Managed Resource Group (MRG) that houses your CluedIn instance. | Select the Kubernetes service resource. It usually starts with aks- followed by a unique string. | At the top, select Stop, which should then begin to power down your instance. | . If you require any further assistance, reach out to CluedIn support. The CluedIn team can help set this up in your environment by deploying an Automation Account along with a runbook and setting the schedule at your preferred downtime. ",
    "url": "/paas-operations/cost-reduction#shut-down-environments-in-times-of-low-or-no-usage",
    
    "relUrl": "/paas-operations/cost-reduction#shut-down-environments-in-times-of-low-or-no-usage"
  },"403": {
    "doc": "Operating cost reduction",
    "title": "Reserve Azure instances",
    "content": "Azure, like many cloud services, offers the ability to reserve virtual machines and other Azure resources upfront by committing to a year or more. This is one of the ways to save up to 72% on the virtual machine resources and is something we recommend when you are serious about CluedIn. For more information, see Microsoft documentation. ",
    "url": "/paas-operations/cost-reduction#reserve-azure-instances",
    
    "relUrl": "/paas-operations/cost-reduction#reserve-azure-instances"
  },"404": {
    "doc": "Operating cost reduction",
    "title": "Run a local environment",
    "content": "CluedIn is container-based and therefore can run on almost any kind of setup that can run containers. We support local installation, which eliminates the need for a full cluster setup for development purposes. It still requires quite a powerful machine but there are no running costs as a result. This is only recommended for experienced IT professionals due to the complexity of setting up a local running instance. For more information, refer to the local installation documentation or reach out to CluedIn support who will be happy to answer any questions. ",
    "url": "/paas-operations/cost-reduction#run-a-local-environment",
    
    "relUrl": "/paas-operations/cost-reduction#run-a-local-environment"
  },"405": {
    "doc": "Operating cost reduction",
    "title": "Operating cost reduction",
    "content": " ",
    "url": "/paas-operations/cost-reduction",
    
    "relUrl": "/paas-operations/cost-reduction"
  },"406": {
    "doc": "Delete CluedIn instance",
    "title": "On this page",
    "content": ". | Delete CluedIn PaaS . | Delete resource group | Delete managed application | . | Delete CluedIn SaaS . | Cancel CluedIn SaaS subscription | Delete resource group within subscription | . | . This article outlines how to delete your CluedIn instance if you no longer want to use it. Note that all data that you have in CluedIn will be permanently deleted. Therefore, consider making backups if needed. ",
    "url": "/deployment/delete-cluedin-instance#on-this-page",
    
    "relUrl": "/deployment/delete-cluedin-instance#on-this-page"
  },"407": {
    "doc": "Delete CluedIn instance",
    "title": "Delete CluedIn PaaS",
    "content": "If you no longer want to use CluedIn PaaS or if the installation of CluedIn PaaS failed, you can delete the instance. Deleting CluedIn PaaS instance will reset your CluedIn license. It means that you can install CluedIn again using the same license key. There are two ways to delete the CluedIn PaaS instance: . | Delete the resource group where you installed CluedIn. | Delete the managed application within the resource group where you installed CluedIn. | . Delete resource group . | In the Azure portal, navigate to the resource group where you installed CluedIn. | At the top of the page, select Delete resource group. | Confirm the deletion by typing the name of the resource group. | . Delete managed application . | In the Azure portal, navigate to the resource group where you installed CluedIn, and then find the corresponding managed application. | Open the managed application that you want to delete. | At the top of the page, select Delete, and then confirm your choice. | . ",
    "url": "/deployment/delete-cluedin-instance#delete-cluedin-paas",
    
    "relUrl": "/deployment/delete-cluedin-instance#delete-cluedin-paas"
  },"408": {
    "doc": "Delete CluedIn instance",
    "title": "Delete CluedIn SaaS",
    "content": "If you no longer want to use CluedIn SaaS, you can delete the instance in one of two ways: . | Cancel CluedIn SaaS subscription. | Delete the resource group within the CluedIn SaaS subscription. | . Cancel CluedIn SaaS subscription . | In the Azure portal, navigate to your CluedIn SaaS subscription. At the top of the subscription page, select Cancel Subscription. | Review the terms for cancelling the subscription and select the checkbox to confirm that you have read and understood the implications. Optionally, provide a reason for cancellation. Finally, select Cancel subscription. | . Delete resource group within subscription . | In the Azure portal, navigate to your CluedIn SaaS subscription. On the subscription page, in the Product and plan details section, select the resource group. | Select the checkbox next to the resource group. At the top of the page, select Delete. | Enter delete to confirm your choice, and then select Delete. | . ",
    "url": "/deployment/delete-cluedin-instance#delete-cluedin-saas",
    
    "relUrl": "/deployment/delete-cluedin-instance#delete-cluedin-saas"
  },"409": {
    "doc": "Delete CluedIn instance",
    "title": "Delete CluedIn instance",
    "content": " ",
    "url": "/deployment/delete-cluedin-instance",
    
    "relUrl": "/deployment/delete-cluedin-instance"
  },"410": {
    "doc": "1.1.4. Identifying and labelling incorrect data",
    "title": "1.1.4. Identifying and labelling incorrect data",
    "content": "Module: 1.1. Ingestion . Level: Beginner . In this session, we’ll learn how to identify and label invalid records using tags. We’ll explore different types of tags—such as structural, data quality, and business logic tags—and understand how each can be applied to highlight specific data issues. This approach helps streamline data cleanup, improve data quality, and ensure that invalid records are clearly marked for review or correction. Presented by: Jocelyn Ramirez, your Customer Success Manager, and Matthew Carter, your CluedIn AI Trainer . Presentation: Download PPT . Useful resources: . | Golden record rules . | Create rules . | . ",
    "url": "/training/fundamentals/identifying-and-labelling-incorrect-data",
    
    "relUrl": "/training/fundamentals/identifying-and-labelling-incorrect-data"
  },"411": {
    "doc": "Process data",
    "title": "On this page",
    "content": ". | Counting processed records | Pre-processing options | Types of processing | Next steps | . | Audience | Time to read | . | Data Engineer, Data Analyst | 5 min | . You are here in the data journey . Before you start . | Make sure you have conducted the data impact workshop to understand what sources you want to use first. | Make sure you are familiar with the available tools for data ingestion and have picked the right tool for your use case. | Make sure you have ingested the data, and the records are available on the Preview tab of the data set. | Make sure you have created the mapping for your records. | . Once your records are mapped, you can now process them. Processing is the final step of the data ingestion process in CluedIn. The act of processing means sending mapped records (clues) to the processing engine so it can either create new golden records or aggregate clues to the existing golden records. ",
    "url": "/playbooks/data-ingestion-playbook/process-data#on-this-page",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data#on-this-page"
  },"412": {
    "doc": "Process data",
    "title": "Counting processed records",
    "content": "If the number of ingested records does not match the number of golden records, it means you had merges across the source. The merge can happen because of the same codes or because of deduplication projects being run. Generally, it is a good thing to have less golden records than ingested records as it means that CluedIn found duplicates. What is a billable record? . A billable record is a new record that was processed from a data source. Learn more about billable records in a dedicated article. What record is considered a new record? . A new record is a unique record that comes from a source of data and is processed in CluedIn. If you repeatedly send the exact same record within the same source, it won’t be added to your billable records count. Each time CluedIn processes records, it keeps a hash code that represents the record. As some source systems do not support deltas, we do not want you to have your billable records increased due to this shortcoming. So, if the record within a source has the exact same hash code (same collection of attribute/value), it won’t be added to the billable records count. Keep in mind that changing the mapping can change the hash code of records, leading to those records being counted as new billable records. However, if you remove records from CluedIn, the number of billable records will decrease. So, if you require a major change in your mapping, make sure you remove the records you want to change to avoid having an increase in billable records. ",
    "url": "/playbooks/data-ingestion-playbook/process-data#counting-processed-records",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data#counting-processed-records"
  },"413": {
    "doc": "Process data",
    "title": "Pre-processing options",
    "content": "There are a number of actions you can do to change the records right before they are sent to the processing engine. These actions can be grouped into 2 categories: avoiding unwanted records and modifying the records before processing. Avoid unwanted records . If you do not want to send specific records from the data set to the processing pipeline, you can set up rules to send such records to quarantine. Learn more about quarantine in a dedicated article. Modify records before processing . | Advanced mapping – you can write a glue code in your data set to modify the records before they are processed. This is useful for complex and conditional mapping. Learn more about advanced mapping code here. | Property rules – improve the quality of records by normalizing, transforming, or changing property values before the records are processed. Learn more about property rules here. | Pre-process rules – execute some basic operations on the records before they are processed. Learn more about pre-process rules here. | . Generally, you should use the above-mentioned options to normalize or denormalize values in the records before they become golden records. Even though CluedIn is a tool to fix data quality issues, it can be useful to do some preparation before processing. This ensures that the advanced mapping code and rules remain close to the source and run only on the source records. If the records were already processed, the same activity would run on a large set of golden records. Therefore, if an issue appears only in a specific source, then it is a good idea to fix it with pre-processing rules, as the manipulation would be specific to that source. However, if the fix could benefit multiple sources, then do not add pre-processing rules, and add a classic rule instead. ",
    "url": "/playbooks/data-ingestion-playbook/process-data#pre-processing-options",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data#pre-processing-options"
  },"414": {
    "doc": "Process data",
    "title": "Types of processing",
    "content": "In CluedIn, there are two types of processing—manual and automated. Choosing the suitable type depends on how you ingest a source of data and how often you need to process records. | Source details | Suitable processing type | . | Source is ingested one time only | Manual | . | Source is a sample data | Manual | . | Source is ingested once a month | Manual | . | Source is ingested for the first time | Manual | . | Incremental batch of records | Automated* | . | ADF pipeline | Automated* | . * Switch to automated processing after you verify that the mapping and pre-processing actions work fine on a data sample. It is always a good idea to start with manual processing, especially at the initial stages of data ingestion. Since mapping has a significant influence on golden records, we recommend that you start with small data set and manual processing. This way you can verify that the golden records are accurate and then switch to automated processing. Keep in mind that CluedIn will skip the exact same records during processing—on the condition that mapping remains unchanged—so you can process the same data set multiple times. You can find step-by-step instruction for processing records in a dedicated article. Once the processing is done, you will see the result similar to the following. ",
    "url": "/playbooks/data-ingestion-playbook/process-data#types-of-processing",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data#types-of-processing"
  },"415": {
    "doc": "Process data",
    "title": "Next steps",
    "content": "Now that your records have turned to golden records, you can start searching for them using the main search. Processing concludes the data ingestion step of your data journey. Next, you can start the data transformation activities. ",
    "url": "/playbooks/data-ingestion-playbook/process-data#next-steps",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data#next-steps"
  },"416": {
    "doc": "Process data",
    "title": "Process data",
    "content": " ",
    "url": "/playbooks/data-ingestion-playbook/process-data",
    
    "relUrl": "/playbooks/data-ingestion-playbook/process-data"
  },"417": {
    "doc": "Start your IT journey",
    "title": "On this page",
    "content": ". | Main aspects of IT journey . | Authentication and authorization | Connection to the sources | Connection to enrichers | Connection to export targets | . | Types of IT journey . | SaaS IT journey | PaaS IT journey | Installation in a restrictive environment | . | . | Audience | Time to read | . | Administrator | 7 min | . This article outlines the key stages of your IT journey, from installing CluedIn and integrating it with necessary external systems and sources, to ensuring that user access aligns with your organization policies. ",
    "url": "/playbooks/start-your-it-journey#on-this-page",
    
    "relUrl": "/playbooks/start-your-it-journey#on-this-page"
  },"418": {
    "doc": "Start your IT journey",
    "title": "Main aspects of IT journey",
    "content": "There are several aspects of the IT journey that you should consider in order to configure CluedIn according to your business needs. The primary aspect is authentication and authorization to ensure that only legitimate users can access the system. Your business data typically resides in various sources, such as databases, ERP systems, CRM systems, and other applications. One of the key tasks in CluedIn is data ingestion, which requires establishing connections to these data sources. Additionally, to enrich your data with information from third-party sources, you can connect to the appropriate enrichers. Finally, to share ready-to-use data, CluedIn must connect to the external systems of your choice, often referred to as landing zones. All these tasks are part of the IT journey. In this section, you’ll find recommendations for the starting points of each aspect of the IT journey. Authentication and authorization . Establish the groundwork based on the rules of your IT organization. You will likely need to use your single sign-on (SSO) systems and roles in CluedIn. If you are starting with the development environment, perhaps SSO is not needed for it. If it is, CluedIn can assist, but always try to minimize steps to get started on your project. Connection to the sources . For your first use case, CluedIn will likely need to connect to the source. This generally involves IT steps managed by the system administrator. Here are some scenarios that explain the actions to take for different levels of access to the source: . | Full API access – discuss next steps with your CluedIn expert. | File export access – if full access is not possible, but you can get an export file, use that file. | No access – if you can’t get access, ask for the shape of the data and fake it. If you can’t even get this, escalate. Data is the fuel of your data transformation journey. CluedIn has encountered these situations many times, so let us know if you need help. | . Connection to enrichers . Enrichers are slightly more challenging because we can’t really fake them. You either have access or you don’t. If you don’t have access to one enricher, try another provided by CluedIn that doesn’t rely on IT (for example, Libpostal). It is important to see enrichers in action for a value perspective. It’s like having free, quality data at your disposal. But if you can’t access enrichers now, don’t wait and move forward. CluedIn is an agile platform, and you can always revisit this later. Connection to export targets . As a CluedIn user, you’ll want to share your improved data with other systems or places in your organization. To do this automatically, you’ll want to export directly to another source. As with any IT integration, you’ll need an owner ready to receive the data. If you face resistance or are blocked, there’s a trick: you can stream into the CluedIn database directly. This exercise proves that you are streaming the data to external destination. After all, it is better to have one stream than no stream. This will help you understand the kind of data you’re sending and the pace. ",
    "url": "/playbooks/start-your-it-journey#main-aspects-of-it-journey",
    
    "relUrl": "/playbooks/start-your-it-journey#main-aspects-of-it-journey"
  },"419": {
    "doc": "Start your IT journey",
    "title": "Types of IT journey",
    "content": "You have the flexibility to adopt CluedIn at your own pace. Since we offer 2 cloud models of CluedIn—PaaS and SaaS—the IT journey for each is different. In this section, you’ll find the details about IT journey for both SaaS and PaaS. SaaS IT journey . With CluedIn SaaS, the installation process is fully automated: we’ll provide everything you need to install CluedIn. What is more, if you have a committed SaaS deal, we can offer you a development or test environment at hosting cost. However, keep in mind that we’ll have to work together to implement the main aspects of IT journey in order to configure CluedIn for your business needs. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. Useful links to learn more about CluedIn SaaS: . | CluedIn SaaS installation . | Onboarding path for CluedIn SaaS . | . PaaS IT journey . With CluedIn PaaS, the installation process is automated, enabling you to install CluedIn in your own Azure infrastructure. The entire installation process can be simple and straightforward, especially if full access is provided to CluedIn and the IT team. Our experience in installing CluedIn in some of the most challenging environments ensures that we’re well-equipped to support you throughout the process. To facilitate a smooth installation process, we typically schedule meetings for each step: . | Pre-installation check . | Installation . | Post-installation . | . These meetings give you the opportunity to discuss any questions or concerns you may have with a CluedIn expert, ensuring that you’re fully informed and comfortable at every stage of your IT journey. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. The CluedIn PaaS IT journey timeline depends on two aspects: . | Privilege – this refers to the access level you have in the given resource group where CluedIn is installed (read-only, partial edit, full edit). | Access to resource – this refers to your access for connecting to the CluedIn cluster (you can connect, you can connect but require an approval, or you can’t connect and someone has to give access to CluedIn cluster). | . Based on your privileges and access to the resource group, there can be 3 IT journey scenarios: . | CluedIn PaaS managed service – with high privilege and access to the resource group, the installation and IT journey might take days. | CluedIn PaaS managed instance – with high privilege but limited access to the resource group, the installation and IT journey might take weeks. | CluedIn PaaS self-hosted – with no privilege and no access to the resource group, the installation and IT journey might take months. | . Installation in a restrictive environment . The entire IT journey can be smooth and easy, but it will depend on your IT infrastructure and the policies in place. Since CluedIn requires connections to various external systems and sources, you may need to take additional steps during IT journey. If your infrastructure is restrictive, you might face problems related to network restrictions, private and service endpoints, custom DNS settings, firewall policies, token expiration, and so on. If this is your case, we might have to work with you on the following tasks to set up the foundational infrastructure: . | Creation of a Virtual Network (VNet) with 2 subnets: . | AKS subnet – dedicated subnet for AKS resources. | Delegated subnet for CI – subnet specifically delegated for Container Instance (CI) operations. | . | Configuration of service endpoints for secure access to storage account and key vault. | Enabling the routing table and network security groups (NSGs) for network management and enhanced security. | Creation of user identity to manage access and permissions across the infrastructure. | . Depending on your security and infrastructure policies, we might have to implement some of the following changes: . | Remove automatic creation and assignment of user identities from the publisher onboard module to streamline security management. | Disable the setup of a private DNS zone in the storage account. | Configure the storage account and the key vault to use only private endpoints. | Other changes might be required to comply with your security and infrastructure policies. | . Additionally, your Azure administrator might be required to perform the following tasks to support infrastructure provisioning: . | Role assignments: . | Assign the Owner role to manage resources. | Assign the Storage File Data Contributor role at the storage account to manage file-level permissions. | Assign the Key Vault Administrator role to the user identity to enable Key Vault management and secret handling. | . | Private DNS Zone Vnet Link: . | Virtual network link enabled for privatelink.file.core.windows.net . | Virtual network link enabled for privatelink.blob.core.windows.net . | Virtual network link enabled for privatelink.vaultcore.azure.net . | . | . The tasks listed above are provided as an example. They may or may not fit your IT infrastructure and policies. You can discuss your requirements and plan the IT journey with a CluedIn expert.CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. Useful links to learn more about CluedIn PaaS: . | CluedIn PaaS installation . | Onboarding path for CluedIn PaaS . | CluedIn PaaS operations . | . ",
    "url": "/playbooks/start-your-it-journey#types-of-it-journey",
    
    "relUrl": "/playbooks/start-your-it-journey#types-of-it-journey"
  },"420": {
    "doc": "Start your IT journey",
    "title": "Start your IT journey",
    "content": " ",
    "url": "/playbooks/start-your-it-journey",
    
    "relUrl": "/playbooks/start-your-it-journey"
  },"421": {
    "doc": "Data ingestion playbook",
    "title": "Data ingestion playbook",
    "content": "| Audience | Time to read | . | Business User, Data Project Lead, Data Steward, Data Analyst, Data Architect, Data Engineer | 2 min | . You are here in the data journey . Data ingestion is the first step of your MDM project. In this playbook, you will learn how to organize your MDM project and communicate its complexities and associated risks to your managers and business stakeholders. By understanding these elements, you’ll be better equipped to: . | Deliver value by improving critical data and demonstrate project progress to stakeholders. Delivering value does not mean using all CluedIn features; it means having one end-to-end data flow running in CluedIn that has improved value. | Mitigate potential issues and ensure project success. | . The data ingestion process consists of several steps: data impact workshop, picking the right tool, ingesting, mapping, and processing. Before diving into the process of ingesting data into CluedIn, we recommend that you get acquainted with the foundational principles outlined in this playbook. In our experience, we’ve observed that many data teams tend to jump directly into the implementation phase without fully understanding the specificity of CluedIn. Additionally, they often overlook the critical step of defining which data will have the most significant impact on their objectives. If you have a small data team (one or two people), you may opt for a lighter approach. However, we still recommend following the guidelines from this playbook to better structure your MDM project. Main data ingestion principle for your first use case . If you have the opportunity, focus on the critical data you have access to and start with a simple implementation. Once completed, organize a demo for your stakeholders and then release it to your production environment. Why should you apply this principle? . | Improved data – you have one end-to-end data flow that improves a portion of your critical data. | Available in production – you went through your IT processes to deploy the production environment according to your organization policies. It means that from now on, it will only get easier. | Full cycle of data journey – by delivering a fairly simple use case into production, you’ve completed one full cycle of your MDM project, which you’ll need to repeat every time. | . Where to start with data ingestion? The first step of data ingestion is the data impact workshop. It is a collaborative session designed to help you identify your business domains, determine the data sources to ingest, and assess their criticality to your operations. Learn more in Data impact workshop. ",
    "url": "/playbooks/data-ingestion-playbook",
    
    "relUrl": "/playbooks/data-ingestion-playbook"
  },"422": {
    "doc": "CluedIn licensing agent",
    "title": "On this page",
    "content": ". | Prerequisites | Installation steps | . The CluedIn licensing agent is delivered as an Azure Managed Application and is installed via the Azure Marketplace. It enables CluedIn to bill customers directly through the Marketplace. The installation process is straightforward: . | You complete a short form, which then provisions a Managed Application in the customer’s Azure subscription. | Once installed, any agreed CluedIn charges can be billed to that subscription. | . ",
    "url": "/deployment/marketplace-licensing-agent#on-this-page",
    
    "relUrl": "/deployment/marketplace-licensing-agent#on-this-page"
  },"423": {
    "doc": "CluedIn licensing agent",
    "title": "Prerequisites",
    "content": ". | Azure account. If you already have an Azure account, you can proceed to the next prerequisite. If you do not have an Azure account, create a pay-as-you-go account using a valid credit card. | Marketplace purchases and Contributor role. Make sure that you have enabled Marketplace purchases and configured the required user permissions (at least Contributor) for the subscription where you want to store the CluedIn SaaS application. For more information, see Enable marketplace purchases in Azure. | . ",
    "url": "/deployment/marketplace-licensing-agent#prerequisites",
    
    "relUrl": "/deployment/marketplace-licensing-agent#prerequisites"
  },"424": {
    "doc": "CluedIn licensing agent",
    "title": "Installation steps",
    "content": ". | In Azure Marketplace, navigate to CluedIn PaaS offering. | Select Get It Now. | In the Create this app in Azure window, under Software plan, select CluedIn Licensing Agent. | Review your contact information, and then select Continue. | On the Basics tab, fill in the Project details section: . | In Subscription, select the Azure subscription where you want to install CluedIn. | In Resource group, create a new resource group. You may also select an existing resource group, but it must be empty. If the selected resource group contains other Azure resources, an error will occur. | . | Fill in the instance details: . | In Region, specify the Azure region where the resources will be created. Typically, this should match the region of your resource group. | Enter the following information: . | Name . | Company Name . | Contact Name . | Contact Email . | . | Verify that the Managed Application Details section is filled in correctly: . | Keep the application name as cluedinlicense. | The managed resource group is usually filled in by default, but you can make changes if needed. | . | . | On the License tab, keep the installation name as cluedinlicense. | On the Review + create tab, do the following: . | Review all settings and accept the terms and conditions. | When you are ready to deploy to your cloud environment, select Create. After a few minutes, the Azure portal will notify you once the installation is complete. | . | . ",
    "url": "/deployment/marketplace-licensing-agent#installation-steps",
    
    "relUrl": "/deployment/marketplace-licensing-agent#installation-steps"
  },"425": {
    "doc": "CluedIn licensing agent",
    "title": "CluedIn licensing agent",
    "content": " ",
    "url": "/deployment/marketplace-licensing-agent",
    
    "relUrl": "/deployment/marketplace-licensing-agent"
  },"426": {
    "doc": "Support scope",
    "title": "Support scope",
    "content": "This article outlines the scope of CluedIn support. The goal of this article is to answer the question of what happens if you change the default configuration installed using the Azure Marketplace offering. Any modifications made to the default configuration, except those explicitly documented as post-installation adjustments, will result in the termination of support. This includes Premium Support, Azure Managed Application, and any other customer support service. The reason for this is the high probability that the cluster may not operate correctly. Changes outside the documented scope are considered out of support, and CluedIn will not provide assistance or troubleshooting for configurations that deviate from the originally installed setup. Here is a list of allowed changes: . | Adjusting requests and limits on CluedIn operating pods for both CPU and memory. | Setting up and receiving assistance with the CluedIn backup solution. | Configuring single sign-on (SSO). | Setting up SMTP details for welcome emails. | Configuring extended alerts via CluedIn. This is related to CluedIn-specific alerts only, not customer-specific alerts. | Troubleshooting ad-hoc cluster failures, if determined not to be caused by the customer. | Upgrading environments. | Installing the product using our default configuration. | Usage of Azure Key Vault to provide secrets to the cluster. | General AKS maintenance on the default configuration. | . Any deviation from this list will be considered out of support, and assistance for the issue is not guaranteed and may incur charges. ",
    "url": "/paas-operations/support-scope",
    
    "relUrl": "/paas-operations/support-scope"
  },"427": {
    "doc": "Sync configuration between environments",
    "title": "On this page",
    "content": ". | CluedIn product toolkit | Local configuration migration . | Exporting configuration (Source system) | Importing configuration (Destination system) | . | Configuration migration using Azure DevOps (pipeline) | Final notes | . This document covers the process for migrating CluedIn configuration from one environment to another environment. This is especially useful when you are developing your foundation on one environment and want to promote it to production. Prerequisites . | PowerShell Core / 7+. | PowerShell CluedIn product toolkit module, which you can download it in the following section. | Service account to do the export and import (the script is provided in above toolkit). | Access to the front end, whether it’s public facing or private facing. | The user running the script must have full admin permissions with Accountable access level for every claim in CluedIn. | . PowerShell Core does not work with PowerShell ISE. To replicate the ISE experience, please use Visual Studio Code. ",
    "url": "/kb/config-migrate#on-this-page",
    
    "relUrl": "/kb/config-migrate#on-this-page"
  },"428": {
    "doc": "Sync configuration between environments",
    "title": "CluedIn product toolkit",
    "content": "Please find the latest releases on our GitHub repository . All future versions will be published on the CluedIn.Product.Toolkit repository. Make sure that you retrieve the version that relates to the version of CluedIn that you are running. | Version | Technical Version | Source Code | Link | . | 2024.12.00 | 4.4.0 | Source Code | Download | . | 2024.07 | 4.3.0 | Source Code | Download | . The product toolkit supports a number of methods of transferring configuration from one environment to another. The main two methods are running locally on your desktop or using a pipeline platform such as Azure DevOps. Below will guide you through both local and Azure DevOps, but you can technically use other methods. Please note that the product toolkit is not included in the CluedIn license. This is an open-source toolkit provided by the CluedIn team to help you migrate your configuration between environments if you need to do that. ",
    "url": "/kb/config-migrate#cluedin-product-toolkit",
    
    "relUrl": "/kb/config-migrate#cluedin-product-toolkit"
  },"429": {
    "doc": "Sync configuration between environments",
    "title": "Local configuration migration",
    "content": "This guide will be using the import and export scripts only. The toolkit also supports running the functions on their own, but this will not be covered as part of this documentation. To successfully run this, you will need the following information before proceeding: . | Full URL of your front end. | A backup path locally to store the JSON files exported. | IDs of the data you want to back up. | . To get the IDs of specific parameters, you will need to use the front end and navigate to the desired resource. In the URL, you will then notice the ID of a given object. This may be a GUID or integer depending on what is viewed. Exporting configuration (Source system) . | Open up a PowerShell. | In the PowerShell window, run the following: . Note: We will be using the ficticious front end https://cluedin-source.customer.com in the example below. $params = @{ BaseURL = 'customer.com' Organization = 'cluedin-source' BackupPath = '/path/to/export-20240403' } /path/to/CluedIn.Product.Toolkit/Scripts/Export-CluedInConfig.ps1 @params # The below are all optional. Please see what values are accepted below. If running without any of the below. # It will not export anything as everything defaults to export nothing. # # -BackupAdminSettings [switch] true | false (Default) # -SelectVocabularies [string] csv guids | None (Default) # -SelectDataSets [string] csv guids | All | None (Default) # -SelectRules [string] csv guids | All | None (Default) # -SelectExportTargets [string] csv guids | All | None (Default) # -SelectStreams [string] csv guids | All | None (Default) # -SelectGlossaries [string] csv guids | All | None (Default) # -SelectCleanProjects [string] csv guids | All | None (Default) . Note: /path/to/ will differ on your local system. Please update this to be the local respective path. | The script will launch and prepare to connect. You will be prompted for a username and password which will obtain a JWToken if successful. Once connected, it will attempt to export all the desired configuration specified above. Depending on how much configuration there is to export, this may take up to 15 minutes to complete. Once export has completed, move onto the import phase. | . Importing configuration (Destination system) . | Open up a new PowerShell session to avoid any conflict with environmental variables. | In the PowerShell window, run the following: . Note: We will be using the ficticious front end https://cluedin-destination.customer.com in the example below. $params = @{ BaseURL = 'customer.com' Organization = 'cluedin-destination' RestorePath = '/path/to/export-20240403' } /path/to/CluedIn.Product.Toolkit/Scripts/Import-CluedInConfig.ps1 @params . Note: /path/to/ will differ on your local system. Please update this to be the local respective path. | The script will launch and prepare to connect. When successful, it will iterate through the RestorePath and begin importing and/or correcting any configuration drift between the environments. The console will output any relevant changes that may have occurred during the import phase so that you can keep track of what has happened. A lot of the comparisons are based on Display Name lookup. If a display name matches and there is configuration drift, it will attempt to correct this during import time. It will not delete any configuration as that is not within scope. Depending on how much configuration there is to import, this may take up to 15 minutes to complete. | . This concludes the general process of how to export and import configuration locally from one environment to another. ",
    "url": "/kb/config-migrate#local-configuration-migration",
    
    "relUrl": "/kb/config-migrate#local-configuration-migration"
  },"430": {
    "doc": "Sync configuration between environments",
    "title": "Configuration migration using Azure DevOps (pipeline)",
    "content": "This guide will be using the import and export scripts only. The toolkit also supports running the functions on their own, but this will not be covered as part of this documentation. To successfully run this, you will need the following information before proceeding: . | Full URL of your front end. | Access to Azure DevOps with the pipelines setup. Please refer to the README within the CluedIn Product Toolkit. Failing to set up the pipelines correctly will result in failed configuration backups and restores. | IDs of the data you want to back up. | . To get the IDs of specific parameters, you will need to use the front end and navigate to the desired resource. In the URL, you will then notice the ID of a given object. This may be a GUID or integer depending on what is viewed. To transfer configuration . | Navigate to the backup pipeline and click on [Run Pipeline]. | Fill in parameters: . Note: We will be using the ficticious front end https://cluedin-source.customer.com and https://cluedin-destination.customer.com in the example below. | CluedIn Base URL (Source): This is in the format of customer.com without http(s)://. e.g. customer.com | CluedIn Organization (Source): This is the first part of your cluedin environment. e.g. cluedin-source | CluedIn Base URL (Destination): This is in the format of customer.com without http(s):// e.g. customer.com | CluedIn Organization (Destination): This is the first part of your cluedin environment. e.g. cluedin-destination | Admin Settings: Checkbox determines ‘True’ or ‘False’ | Vocabularies (guid, csv): Accepted values are ‘None’, or the guids seperated by a comma (,). All will not work for this one. | Data Sets (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Rules (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Export Targets (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Streams (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Glossaries (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Clean Projects (guid, csv): Accepted values are ‘All’, ‘None’, or guids seperated by a comma (,). | Push to repo: If set to true, it will push your configuration json files to the specified git repository in the pipeline. | . | When ready, click on [Run]. | . ",
    "url": "/kb/config-migrate#configuration-migration-using-azure-devops-pipeline",
    
    "relUrl": "/kb/config-migrate#configuration-migration-using-azure-devops-pipeline"
  },"431": {
    "doc": "Sync configuration between environments",
    "title": "Final notes",
    "content": "This guide only covers how to run the toolkit to migrate configuration from one environment to another. The steps to setup the Azure DevOps pipeline specifically must be followed from the document within the toolkit. If at any stage you have issues with trying to do a migration, don’t hesitate to reach out to CluedIn support who will be happy to assist. ",
    "url": "/kb/config-migrate#final-notes",
    
    "relUrl": "/kb/config-migrate#final-notes"
  },"432": {
    "doc": "Sync configuration between environments",
    "title": "Sync configuration between environments",
    "content": " ",
    "url": "/kb/config-migrate",
    
    "relUrl": "/kb/config-migrate"
  },"433": {
    "doc": "Data transformation playbook",
    "title": "Data transformation playbook",
    "content": "| Audience | Time to read | . | Data Steward | 4 minutes | . You are here in the data journey . Now that you have completed the data ingestion process and generated some golden records, it is time to start improving the quality of these records during data transformation. The data transformation process consists of three aspects: . | Improving the quality of values with the help of clean projects. | Decreasing the number of duplicates with the help of deduplication projects. | Extending the quality with values coming from trusted sources with the help of enrichers. | . The sequence of data transformation activities is flexible. You can start with data cleaning, then move to deduplication, and finally to enrichment. Alternatively, you can start with deduplication, then clean the data, and add enrichers last. The choice is yours as CluedIn doesn’t impose a strict order of data transformation activities. In this playbook, we will provide the details for each data transformation aspect. However, before diving into specifics, familiarize yourself with the key terms and features that will be useful during data transformation in CluedIn: . | Golden record . | Saved search . | Glossary term . | . For a smooth start of your data transformation journey in CluedIn, learn how to leverage saved search and glossary. Both of these tools help you identify a specific set of golden records for data transformation. However, each tool has its specific practical application. Why is saved search important for data transformation? . Saved search gives you an easy way to review golden records that match complex filters. Once you define the filters and add the needed vocabulary keys to the search page, you can save the current search configuration for future use. Next time when you need to review a specific set of golden records and their vocabulary keys, you can quickly open a saved search instead of configuring the search from scratch. That’s why saved search is a great tool for your day-to-day data transformation activities. Saved search can be personal (visible only for you) or shared (visible to your team members). If you want others to review a specific set of golden records, make sure you create a shared saved search. In addition to reviewing golden records, saved search is useful for creating clean projects. When you make a decision on which golden records you want to clean, you can register this decision in a saved search. This way you can quickly create a clean project and verify the results of your cleaning activities. Let’s consider an example of golden records that contain misspelled job titles. When configuring the search filters, you can list all misspelled values for the Job Title property. As a result, you will see all golden records that contain misspelled job titles. To be able to identify data quality issues at a glance, you can customize the search results page by adding or removing the columns (properties or vocabulary keys). When you reach the desired view of golden records on the search results page, it is time to save the current search configuration. After you retrieve the saved search, you can create a clean project in just a few clicks—the properties will be taken directly from the saved search. Another advantage is that you can quickly verify the results of your cleaning activities—simply open the saved search and if there are no results, it means that all data quality issues have been fixed. Why is glossary important for data transformation? . While glossary is similar to the saved search in terms of reviewing golden records that match complex filters, it gives you an additional advantage of documenting your golden records. It means that you can add descriptions, tags, ratings, and certification levels to the glossary term. All of these elements make it easier for your colleagues to understand: . | What golden records are included in the glossary term. | What you are trying to achieve with a specific collection of golden records. | What data transformation activities you have performed on a specific collection of golden records. | . Glossary can be used in filters in several places in CluedIn: search, clean projects, deduplication projects, and streams. Essentially, you can create a glossary with a specific set of golden records, then perform various data transformation activities on those golden records, and finally send ready-to-use and well-documented golden records to external systems. That’s why glossary is especially useful in in data-sharing activities—you can set up a stream with a specific glossary to send well-documented golden records to external systems. Where to start with data transformation? . Start by identifying a set of golden records that should be transformed. Register those golden records along with specific vocabulary keys using a saved search or glossary. As the order of data transformation activities is flexible, we suggest starting with clean projects. ",
    "url": "/playbooks/data-transformation-playbook",
    
    "relUrl": "/playbooks/data-transformation-playbook"
  },"434": {
    "doc": "Sync certificates and secrets from AKV to AKS",
    "title": "On this page",
    "content": ". | Guide | Technical notes | Known issues | . As of Q1 2024, synchronizing certificates and secrtes from AKV to AKS is not currently supported. However, beta versions are available, enabling you to set up and test this functionality before GA release. In this article, we will guide you through connecting your CluedIn instance to use the existing Azure Key Vault (AKV) to synchronize secrets and certificates to your Kubernetes cluster. The method described in this article uses the AKV provider for the secrets store CSI driver to facilitate this feature. Useful links: . | Azure Key Vault Provider for Secrets Store CSI Driver . | Use the Secrets Store CSI Driver for Kubernetes in an Azure Kubernetes Service (AKS) cluster (preview) . | . Prerequisites . | Powershell Core . | Azure CLI . | . ",
    "url": "/kb/sync-aks-to-akv#on-this-page",
    
    "relUrl": "/kb/sync-aks-to-akv#on-this-page"
  },"435": {
    "doc": "Sync certificates and secrets from AKV to AKS",
    "title": "Guide",
    "content": ". | Sign in to your tenant to enable the AKV add-on against the AKS. | Open up pwsh and sign in to your tenant with az login. | Set up your variables which will be used below. $aks = az aks show --name $aksName --resource-group $aksResourceGroup $subscription = '00000000-0000-0000-0000-000000000000' # This will be different guid on your end $tenantId = '00000000-0000-0000-0000-000000000000' # This will be different guid on your end $keyVaultName = 'kv-012345' # Use your desired key vault name $keyVaultRG = 'rg-kv' # Use the resource group the above key vault resides in . | Enable the add-on on the existing AKS Cluster. $params = @( '--addons', 'azure-keyvault-secrets-provider' '--name', $aks.name '--resource-group', $aks.resourceGroup ) az aks enable-addons --addons @params . This will deploy some additional pods to each available node. Once the add-on has been deployed, it will create a key vault managed identity that is used to communicate back to the AKV from the Kubernetes cluster. | To get the managed identity, run the following. $params = @( '--name', $aks.name '--resource-group', $aks.resourceGroup '--subscription', $subscription '--query', 'addonProfiles.azureKeyvaultSecretsProvider.identity.objectId' '--output', 'tsv' ) $kvManagedIdentity = az aks show @params . Depending on if you’re using RBAC or Key Vault access policy, you will need to update the appropriate area so the key vault managed identity can GET and LIST secrets and certificates. | Obtain the key vault details. $kv = az keyvault show --name $keyVaultName --resource-group $keyVaultRG | ConvertFrom-Json . This guide assumes you’ll use the pre-existing key vault deployed at install time. You may use an alternative key vault if preferred. In this case, update the values above to match your desired key vault. | Update access to your key vault. | RBAC $params = @( '--assignee', $kvManagedIdentity '--role', 'Key Vault Secret User' '--role', 'Key Vault Certificate User' '--scope', $kv.id ) az role assignment create @params . | Policy $params = @( '--name', $kv.name '--resource-group', $kv.resourceGroup '--certificate-permissions', 'get' '--secret-permissions', 'get' '--spn', $kvManagedIdentity ) az keyvault set-policy @params . | . With the above now set, it’s time to start uploading your secrets and certificates. | Navigate to the Key Vault in the Azure Portal instance and begin adding secrets and certificates. Depending on what you are uploading, select either Certificates or Secrets. | For Certificates: Upload a PFX of your choice along with a password. | For Secrets: Create a secret key:value pair. These will be used for synchronization from AKV to Kubernetes secrets. As Kubernetes secrets are an oject that contain multiple key:value pairs, ensure that your naming convention in AKV makes sense. | . Example: In Kubernetes, we may have a secret cluedin-login-details which contains 2 key:value pairs: Username, and Password. In AKV, it would be best to have these two secrets as cluedin-login-username and cluedin-login-password. When we get later into the guide, we’ll explain how to pair these back up into a single object. | With all your desired secrets and certificates now uploaded to AKV, it’s time to configure the Values.yaml to begin synchronizing. In your values file, add the following block of code into the global values: . global: keyvault: enabled: true # When enabled, it will deploy the secret store manifest to Kubernetes #frontendCrt: cluedin-sample-pfx # This must match the certificate name on the AKV end. When mounted, it will appear as `cluedin-frontend-crt` userAssignedIdentityID: $kvManagedIdentity # This is the guid for the kv managed identity keyvaultName: $kv.name # This is your key vault name tenantId: $tenantId # This is the guid of your tenant secretProviderClasses: cluedin-server: # For every key under `secretProviderClasses`, a new secret provider class will be created with the same name and `-sync` appended - secretName: cluedin-login-details # This is how the secret will appear within Kubernetes Secrets once synchronized secretType: # [OPTIONAL] Defaults to Opaque. But you can specify any supported secretType secretKeys: password: cluedin-login-password # The left side (Key) is the name in the Kubernetes secret object. The right side (Value) is the AKV reference which will grab its value username: cluedin-login-username infrastructure: cert-manager: #enabled: false # Only set to false if using an uploaded frontend certificate . If you are using the certificate upload as part of your setup, you must disable cert-manager by setting enabled: false. You also must set frontendCrt to a value. It will be mounted as cluedin-frontend-crt on cluedin-server. This secret name cannot change. | With all the secrets and certificates now done, the last step is to mount, map, and synchronize the secrets to Kubernetes. For every key under secretProviderClass, a new secret provider will be created with the same name with -sync appended. This gives you the flexibility to have secrets to share the same life cycle as an application, or to use a singular pod to synchronize the secrets. cluedin-server is the only server that will automount if keys are specified under the cluedin-server secretProviderClasses section. For everything else, a specific mount point will be required along with preventing local password creation by using the appropriate key. We highly recommend having secrets share life cycles with the application. We will cover only this scenario below. Please see the mapping table below. | Application | Secret | Helm Path | Mount Point | . | cluedin-server | cluedin-admin-secret | application.bootstrap.organization.existingSecret | auto-mounts | . |   | cluedin-email | application.email.secretRef |   | . |   | cluedin-frontend-crt | global.keyvault.frontendCrt |   | . | cluedin-sqlserver | cluedin-sqlserver-secret | infrastructure.mssql.existingSecret | infrastructure.mssql.extraVolumesinfrastructure.mssql.extraVolumeMounts | . |   | cluedin-sqlserver-clientuser-secret | application.sqlserver.users.clientUser.existingSecret |   | . | cluedin-elasticsearch | elasticsearch-credentials | infrastructure.elasticsearch.auth.existingSecret | infrastructure.elasticsearch.extraVolumesinfrastructure.elasticsearch.extraVolumeMounts | . | cluedin-redis | cluedin-redis | infrastructure.redis.auth.existingSecret | infrastructure.redis.master.extraVolumesinfrastructure.redis.master.extraVolumeMounts | . | cluedin-neo4j | cluedin-neo4j-secrets | infrastructure.neo4j.neo4j.passwordFromSecret | infrastructure.neo4j.additionalVolumesinfrastructure.neo4j.additionalVolumeMounts | . |   | cluedin-neo4j-auth | !! shared from above !! |   | . | cluedin-grafana | cluedin-grafana | infrastructure.monitoring.grafana.admin.existingSecret | infrastructure.monitoring.grafana.extraContainerVolumesinfrastructure.monitoring.grafana.sidecar.dashboards.extraMounts | . | cluedin-rabbitmq | cluedin-rabbitmq | infrastructure.rabbitmq.auth.existingPasswordSecret | infrastructure.rabbitmq.extraVolumesinfrastructure.rabbitmq.extraVolumeMounts | . |   | cluedin-rabbitmq-load-definition | !! shared from above !! |   | . Please find a sample yaml file here, which can be used as a reference for a complete setup. | . ",
    "url": "/kb/sync-aks-to-akv#guide",
    
    "relUrl": "/kb/sync-aks-to-akv#guide"
  },"436": {
    "doc": "Sync certificates and secrets from AKV to AKS",
    "title": "Technical notes",
    "content": "This section will explain some of the more technical bits: . | The secret csi driver is deployed at the Kubernetes level. This deploys some additional pods to each node to facilitate this. However, if your max pods limit is the default 30, you may run into an issue after deploying as the CluedIn application is on the borderline of this value. Please ensure this is checked before proceeding as it may cause the cluster to not function correctly. | The way the secrets are synchronized is by using the cluedin-server pod mounting the secret store or mapping and mounting on other pods. The sample file and table above explain how to achieve this. It’s important to note that the secrets synchronize only when the pod is active. It doesn’t need to be in a running state, but it must at least be pending. This is a limitation of the CSI driver itself rather than the solution provided by CluedIn. | The secrets synchronized do not overwrite existing secrets that are created by the CluedIn Installer. If your secret matches the same name (front-end certificate is mandatory here), you must remove the existing secret for the synchronized secret to take over. | Please ensure that you have enough resources. For example, by default the Neo4j and Elasticsearch pods consume a majority of the nodes they have been assigned. Having the additional Key Vault pods on these nodes may potentially prevent these from starting up even though the Key Vault pods request very little resource. | . ",
    "url": "/kb/sync-aks-to-akv#technical-notes",
    
    "relUrl": "/kb/sync-aks-to-akv#technical-notes"
  },"437": {
    "doc": "Sync certificates and secrets from AKV to AKS",
    "title": "Known issues",
    "content": "This section will explain some of the known issues. Issue 1 . Problem: When doing a migration of RabbitMQ from local kubernetes password to a synced password, you must do a sequence of steps due to the RabbitMQ charts logic. Solution: . | Do an initial deployment where the secret is mounted and mapped to RabbitMQ pod as well as the initial password being supplied in the User Supply Values for RabbitMQ. | Once deployed and you can see the secretProviderClass, delete the existing rabbitMQ secrets and then kill the RabbitMQ pod. A new pod should spawn and the secrets should then be mapped to the secretProviderClass. | Update User Supply Values to remove the password and update it to use an existing one. Redeploy the config, and this time it should succeed as the sychronised secret will exist at deployment time. | . Note: This is only an issue with migration. Fresh installs do not have this issue. Issue 2 . Problem: Not all secrets work from a synced source. Solution: Unfortunately, not all secrets will work from AKV. An example of this is the acr-registry-credentials secret. For a secret to be synchronized, it must first be mounted to a pod. However, you cannot pull the image for the pod if the secret does not exist. There are ways around this such as having a generic pod that is deployed pre-helm install, but this is not a scenario we support. ",
    "url": "/kb/sync-aks-to-akv#known-issues",
    
    "relUrl": "/kb/sync-aks-to-akv#known-issues"
  },"438": {
    "doc": "Sync certificates and secrets from AKV to AKS",
    "title": "Sync certificates and secrets from AKV to AKS",
    "content": " ",
    "url": "/kb/sync-aks-to-akv",
    
    "relUrl": "/kb/sync-aks-to-akv"
  },"439": {
    "doc": "Data export playbook",
    "title": "On this page",
    "content": ". | Data export models | Configuring a stream | Connector reference | . | Audience | Time to read | . | Data Engineer, Data Analyst | 4 min | . You are here in the data journey . Before you start . | Get acquainted with the overall concept of a stream in CluedIn. | Watch a video for a step-by-step explanation of how to export golden records in CluedIn. | . Now that you have completed the data transformation process and reached the desired quality of your golden records, it is time to push golden records to the target systems in your organization. To establish correlations between exported golden records in your target systems, use the primary identifier and additional identifiers instead of the entity ID. This is because the entity ID does not guarantee uniqueness, as records with different IDs could be merged. ",
    "url": "/playbooks/data-export-playbook#on-this-page",
    
    "relUrl": "/playbooks/data-export-playbook#on-this-page"
  },"440": {
    "doc": "Data export playbook",
    "title": "Data export models",
    "content": "CluedIn supports two data export models: . | Pull model – in this model, the target system pulls data from the source system (CluedIn) when needed. You have the option to use your own code or script to get golden records from CluedIn, or you can pull the data directly using GraphQL API. | Push model – in this model, the source system (CluedIn) actively sends data to the target system. You can use CluedIn connectors and configure streams to push golden records to the target system. So, the goal of streaming in MDM is to push golden records to the target system. For more information, see Streams. | . The following table outlines the pros and cons of each data export model. | Data export model | Pros | Cons | . | Pull model | - You have greater control over the load on source and target systems by deciding when to pull data (lazy loading).- You can manipulate records using tools like Microsoft Fabric.- You can export all golden record properties, as opposed to streams, where the set of properties is hardcoded. | You need to maintain the code and handle timeouts, retries, and any errors that may occur in your script. | . | Push model | - You can work in the CluedIn UI without the need for code or scripts.- CluedIn manages retries, timeouts, and issues, and reports them to you. | This model is more specific about the export format (golden record properties and vocabulary keys, with or without edges). | . When to use the pull model (CluedIn’s GraphQL API) . | When you have a small number of records and many Fabric users. | When you need to perform additional manipulations on the records or require a custom format before exporting. | When you have complex queries involving different business domains related to each other. | . The pull model is a good choice for use cases involving on-demand data queries, ad-hoc reporting, and integration with systems requiring real-time data access. This model allows for on-demand import from CluedIn using data pipelines, notebooks, or similar tools (GraphQL, Python SDK). When to use the push model (CluedIn stream) . | When you have a large number of records and deltas. | When you need to export golden records. | When no one on the project can write code. | . The push model is a good choice for use cases involving event-triggered updates and systems requiring regular, predictable data ingestion. This model allows for real-time export from CluedIn or export to a passive storage that can’t pull data. ",
    "url": "/playbooks/data-export-playbook#data-export-models",
    
    "relUrl": "/playbooks/data-export-playbook#data-export-models"
  },"441": {
    "doc": "Data export playbook",
    "title": "Configuring a stream",
    "content": "If you decide to implement the push data export model for your use case, then you’ll need to configure a stream. The process of configuring a stream consists of five steps as shown on the following diagram. Note that exporting edges is an optional step. Edges represent relations between golden records. If your golden records contain edges, you can export them to be able to correctly link related golden records in the target system. You can find detailed instructions for each step in the process in our article about creating a stream. ",
    "url": "/playbooks/data-export-playbook#configuring-a-stream",
    
    "relUrl": "/playbooks/data-export-playbook#configuring-a-stream"
  },"442": {
    "doc": "Data export playbook",
    "title": "Connector reference",
    "content": "If the push model is a suitable choice for you, get acquainted with the connectors supported in CluedIn. This section provides brief descriptions of each connector and links to configuration instructions. | Connector | Description | . | Azure Data Lake connector | Azure Data Lake ensures real-time access to trusted, high-quality golden records. Leveraging its massive scalability, secure storage, and advanced analytics capabilities, Azure Data Lake can efficiently process and analyze golden records for deeper insights and smarter decision-making. | . | Azure Dedicated SQL Pool connector | Use this connector when you need to integrate large volumes of structured data from your golden records into a high-performance, scalable data warehouse for advanced analytics, reporting, and business intelligence. However, this connector is not a good choice for frequent data pushes. | . | Azure Event Hub connector | Azure Event Hub enables real-time streaming of trusted golden records from CluedIn. It provides scalable, low-latency data distribution for seamless integration with downstream systems and analytics platforms. | . | Azure Service Bus connector | Azure Service Bus provides a reliable, scalable messaging platform for integrating golden records. It enables seamless communication and data exchange between systems and applications through asynchronous messaging. | . | Dataverse connector | Microsoft Dataverse enables the integration of trusted golden records from CluedIn and provides a secure, scalable, and unified data platform for business applications, automation, and analytics. | . | Http connector | This connector enables seamless integration with any third-party service or system that supports HTTP calls. It is a versatile tool that provides flexibility and ease of connectivity. | . | OneLake connector | This connector enables seamless integration of trusted golden records from CluedIn into Microsoft OneLake that provides unified storage, scalability, and easy access for analytics and AI workloads across the Microsoft Fabric ecosystem. | . | SQL Server connector | This connector offers a standardized and reliable way for sharing golden records. SQL Server ensures consistent, structured data that can be easily accessed and integrated by multiple downstream systems. | . ",
    "url": "/playbooks/data-export-playbook#connector-reference",
    
    "relUrl": "/playbooks/data-export-playbook#connector-reference"
  },"443": {
    "doc": "Data export playbook",
    "title": "Data export playbook",
    "content": " ",
    "url": "/playbooks/data-export-playbook",
    
    "relUrl": "/playbooks/data-export-playbook"
  },"444": {
    "doc": "Environment management playbook",
    "title": "On this page",
    "content": ". | Types of users | Development environment | Test environment | Production environment | . This article covers best practices for using development, test, and production environments of CluedIn. Here, you will learn how to make the most of your development and test environments to make sure your production environment is properly configured. The following diagram explains the purpose, data, users, and access privileges for each environment. ",
    "url": "/kb/env-management-playbook#on-this-page",
    
    "relUrl": "/kb/env-management-playbook#on-this-page"
  },"445": {
    "doc": "Environment management playbook",
    "title": "Types of users",
    "content": "Before dwelling into the details of each environment, it is important to mention the main types of users and CluedIn roles that should be assigned to them. These roles reflect the areas of users’ responsibility. Generally, there are 4 main roles: . | Administrator – this user is responsible for overall platform configuration as well as user and access management. | Data Architect – this user is responsible for setting up the overall data strategy, creating mapping, and configuring various data management elements (rules, glossary, streams, enrichers). | Data Steward – this user is responsible for conducting data cleaning and data management activities. | User – this user has view-only permissions and can view the elements in CluedIn, but cannot make any changes. | . To learn more about roles, check out our dedicated article. The reason we mention these roles is because they should have different privileges in each environment. ",
    "url": "/kb/env-management-playbook#types-of-users",
    
    "relUrl": "/kb/env-management-playbook#types-of-users"
  },"446": {
    "doc": "Environment management playbook",
    "title": "Development environment",
    "content": "The primary purpose of development environment is to provide a sandbox for experimentation, testing, and learning of CluedIn features. This is why development environment should be accessible to everyone in your organization who wants to use CluedIn. What is more, it is recommended that all users get high privileges without any restrictions to be able to fully explore the platform features. Due to the sandbox nature of the development environment, it is recommended to use fake, non-production data. This allows you to grant high privileges to all users without needing to restrict access to sensitive data. Essentially, development environment is a place for you to learn and make mistakes without affecting production data and configuration. After you get acquainted with CluedIn and define your use cases and data objectives, you can move to the test environment and start configuring the elements according to your business needs. It is not recommended to sync development and test environments. Your development environment should always run the latest version of CluedIn so that the users can explore all of the new features. ",
    "url": "/kb/env-management-playbook#development-environment",
    
    "relUrl": "/kb/env-management-playbook#development-environment"
  },"447": {
    "doc": "Environment management playbook",
    "title": "Test environment",
    "content": "Test environment is a place where you carry out the preparation for production environment. At this point, you already know your use cases and data, so you can start creating and configuring mapping, rules, streams, enrichers, glossary terms, roles, permissions, and other elements. It is recommended to keep this environment open for everyone in your organization to allow for collaboration. However, consider limiting privileges for view-only users to restrict their ability to create and modify elements in CluedIn. This approach helps mirror the access controls you would have in the production environment. For data in the test environment, we recommend using obfuscated data or data samples similar to production data, but not the production data itself. The reason for this is the same as in the development environment—to ensure flexibility and learning possibilities for all users. If you want to use sensitive data while preparing the configuration for the production environment, you might need a dedicated pre-production environment. If this is your case, reach out to CluedIn support at support@cluedin.com. After you configure all the elements you need for your data management tasks, you can move the configuration from the test environment to the production environment. You can sync test and production environments using CluedIn Production Toolkit available here. However, it is important to mention that the data as well as users and roles cannot be moved from the test to production environment during synchronization. ",
    "url": "/kb/env-management-playbook#test-environment",
    
    "relUrl": "/kb/env-management-playbook#test-environment"
  },"448": {
    "doc": "Environment management playbook",
    "title": "Production environment",
    "content": "After you sync test and production environments, make sure you assign the appropriate permissions to the users entrusted with daily data management tasks. Since you have configured all of the needed elements in the test environment, very often there is no need to modify them in the production environment. This means that you can limit privileges for all users, except Administrator. Essentially, as you move through the environments, you start with high privileges in development and move to very limited privileges in production. ",
    "url": "/kb/env-management-playbook#production-environment",
    
    "relUrl": "/kb/env-management-playbook#production-environment"
  },"449": {
    "doc": "Environment management playbook",
    "title": "Environment management playbook",
    "content": " ",
    "url": "/kb/env-management-playbook",
    
    "relUrl": "/kb/env-management-playbook"
  },"450": {
    "doc": "Quick feature tour",
    "title": "Ingestion",
    "content": "Find out how to bring data into CluedIn from a variety of data sources (such as files, databases, endpoints, or crawlers) and transform it into golden records. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"451": {
    "doc": "Quick feature tour",
    "title": "AI Agents",
    "content": "Discover how AI Agents proactively fix issues, find duplicates, and suggest data quality rules to save you time. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"452": {
    "doc": "Quick feature tour",
    "title": "Business Domain",
    "content": "Learn how business domains serve as the backbone for structuring, managing, and governing your golden records in CluedIn. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"453": {
    "doc": "Quick feature tour",
    "title": "Enricher",
    "content": "Explore how to add valuable external context to your records, turning partial data into complete golden records. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"454": {
    "doc": "Quick feature tour",
    "title": "Clean",
    "content": "Find out how the Clean module provides a safe way to manually correct data, ensuring accuracy with complete control. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"455": {
    "doc": "Quick feature tour",
    "title": "Deduplicate",
    "content": "Discover how to identify and merge duplicate records into unique, trusted golden records with full control. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"456": {
    "doc": "Quick feature tour",
    "title": "Rule Builder",
    "content": "Learn how to automate cleaning, normalization, tagging, and more with flexible rules that keep your data consistent and trusted. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"457": {
    "doc": "Quick feature tour",
    "title": "Stream",
    "content": "Explore how to set up near real-time data pipelines that automatically deliver golden records to your target systems. And more to come... ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"458": {
    "doc": "Quick feature tour",
    "title": "Quick feature tour",
    "content": "Welcome to the Quick Feature Tour of CluedIn. Here you’ll find short videos that walk you through the key modules of the platform—from cleaning and enriching data, to building rules, managing duplicates, setting up streams, and more. Each video gives you a fast, practical overview of how CluedIn helps you transform raw data into trusted, business-ready golden records. ",
    "url": "/quick-feature-tour",
    
    "relUrl": "/quick-feature-tour"
  },"459": {
    "doc": "Release to production playbook",
    "title": "On this page",
    "content": ". | Set up CI/CD process | Sync configuration between environments | Ingest production data | . | Audience | Time to read | . | Data Architect, Data Engineer | 2 min | . You are here in the data journey . Now that you have implemented your first use case in the test environment, it is time to release your solution to production. This process consists of three steps as shown on the following diagram. ",
    "url": "/playbooks/release-to-production-playbook#on-this-page",
    
    "relUrl": "/playbooks/release-to-production-playbook#on-this-page"
  },"460": {
    "doc": "Release to production playbook",
    "title": "Set up CI/CD process",
    "content": "Continuous integration (CI) pipeline should be implemented if you develop specific packages, such as export targets, crawlers, and enrichers. Once the package is ready, it must be integrated into CluedIn. You need to automate the process to update the configuration of your CluedIn instance with the new package version. Finally, you need to call Helm update commands to make the package available and operational. Continuous deployment (CD) pipeline should be implemented to automatically deploy new package versions or configuration changes. You can configure your CI/CD pipelines and automate your processes using Azure DevOps. Since your CluedIn instance is installed on an AKS cluster, which is part of Microsoft’s infrastructure, Azure DevOps is a good choice for implementing CI/CD. ",
    "url": "/playbooks/release-to-production-playbook#set-up-cicd-process",
    
    "relUrl": "/playbooks/release-to-production-playbook#set-up-cicd-process"
  },"461": {
    "doc": "Release to production playbook",
    "title": "Sync configuration between environments",
    "content": "You can sync the configuration of your CluedIn instance between environments with the help of our product toolkit available in Sync configuration between environments. Learn which elements of CluedIn configuration can and cannot be synced in Environment management playbook. Data, users, and roles cannot be migrated from one environment to another. This means that once you are in production, you need to ingest the data into your production instance and assign user access accordingly. ",
    "url": "/playbooks/release-to-production-playbook#sync-configuration-between-environments",
    
    "relUrl": "/playbooks/release-to-production-playbook#sync-configuration-between-environments"
  },"462": {
    "doc": "Release to production playbook",
    "title": "Ingest production data",
    "content": "After syncing configuration from test to production environments, you need to update your data ingestion pipelines to point to your CluedIn production instance. If you are using ingestion endpoints, you can update the URL in data pipelines or copy the data pipelines and update the URL. CluedIn can effectively process ingested data. Suppose you ingested 1 million records last week and are now ingesting data from the same source again. If you are using bridge mode, the ingestion and processing will be much faster because CluedIn will not store ingested data in the temporary storage on the Preview tab. The data will go directly into processing where it is checked for duplicates. If identical records are found, they will not be processed again. ",
    "url": "/playbooks/release-to-production-playbook#ingest-production-data",
    
    "relUrl": "/playbooks/release-to-production-playbook#ingest-production-data"
  },"463": {
    "doc": "Release to production playbook",
    "title": "Release to production playbook",
    "content": " ",
    "url": "/playbooks/release-to-production-playbook",
    
    "relUrl": "/playbooks/release-to-production-playbook"
  },"464": {
    "doc": "AKS",
    "title": "On this page",
    "content": ". | What are Kubernetes and Azure Kubernetes Services (AKS)? | Deploy AKS . | Role-Based Access Control (RBAC) | Azure CLI | ARM Template | . | Node pools sizing and configuration | Node pools . | system | general | data | dataneo | processing | . | Node Selectors | Requests and Limits | Disks | . ",
    "url": "/deployment/azure/aks#on-this-page",
    
    "relUrl": "/deployment/azure/aks#on-this-page"
  },"465": {
    "doc": "AKS",
    "title": "What are Kubernetes and Azure Kubernetes Services (AKS)?",
    "content": "Kubernetes is open-source software that helps deploy and manage containerized applications at scale. It orchestrates a cluster of Azure virtual machines, schedules containers, automatically manages service discovery, incorporates load balancing, and tracks resource allocation. It also checks the health of individual resources and heals apps with auto-restart and auto-replication. AKS provides a managed Kubernetes service with automated provisioning, upgrading, monitoring, and on-demand scaling. (Source: https://azure.microsoft.com/en-us/services/kubernetes-service/#faq) . There are several ways to install an AKS cluster: ARM templates, Azure CLI, Terraform - the choice is yours. To deploy Azure resources for CluedIn, you need to provide CluedIn Partner GUID (e64d9978-e282-4d1c-9f2e-0eccb50582e4 ). The way you provide the Partner GUID depends on the way you deploy AKS: . ",
    "url": "/deployment/azure/aks#what-are-kubernetes-and-azure-kubernetes-services-aks",
    
    "relUrl": "/deployment/azure/aks#what-are-kubernetes-and-azure-kubernetes-services-aks"
  },"466": {
    "doc": "AKS",
    "title": "Deploy AKS",
    "content": "Role-Based Access Control (RBAC) . To deploy and manage Azure resources, you need sufficient access rights. You can read more about it in Microsoft Documentation: Manage access to your Azure environment with Azure role-based access control Azure built-in roles You need a Contributor role on the Subscription level. If it’s not possible to have this role, you need to ask someone with enough permissions to create an AKS cluster. When you create a new AKS cluster in a particular resource group, Microsoft Azure automatically creates an infrastructure resource group (with “MC_” prefix) to keep AKS-related resources: disks, public IP, identity, etc. Therefore, you should have enough permissions to create resource groups in a given subscription to create a cluster. Then, to manage the cluster, you need to be a Contributor in two AKS resource groups - the group where you have created the AKS and the related infrastructure group. Azure CLI . Walkthrough (Microsoft Docs): https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough Microsoft’s instructions to deploy the Partner GUID: https://docs.microsoft.com/en-us/azure/marketplace/azure-partner-customer-usage-attribution#example-azure-cli . To install the Partner GUID, you need to add an environment variable to your terminal session. Bash: . export AZURE_HTTP_USER_AGENT='pid-e64d9978-e282-4d1c-9f2e-0eccb50582e4' ; echo AZURE_HTTP_USER_AGENT # should print pid-e64d9978-e282-4d1c-9f2e-0eccb50582e4 . PowerShell: . $env:AZURE_HTTP_USER_AGENT='pid-e64d9978-e282-4d1c-9f2e-0eccb50582e4' ; $env:AZURE_HTTP_USER_AGENT # should print pid-e64d9978-e282-4d1c-9f2e-0eccb50582e4 . ARM Template . Walkthrough (Microsoft Docs): https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-rm-template . Walkthrough with Partner GUID (CluedIn Docs): https://documentation.cluedin.net/kb/azure-customer-usage-attribution. Microsoft’s instructions to deploy the Partner GUID: https://docs.microsoft.com/en-us/azure/marketplace/azure-partner-customer-usage-attribution#add-a-guid-to-a-resource-manager-template . To deploy with the Partner GUID, you only need to add this deployment to the resources section: . { \"apiVersion\": \"2020-06-01\", \"name\": \"pid-e64d9978-e282-4d1c-9f2e-0eccb50582e4\", \"type\": \"Microsoft.Resources/deployments\", \"properties\": { \"mode\": \"Incremental\", \"template\": { \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"resources\": [] } } }, . ",
    "url": "/deployment/azure/aks#deploy-aks",
    
    "relUrl": "/deployment/azure/aks#deploy-aks"
  },"467": {
    "doc": "AKS",
    "title": "Node pools sizing and configuration",
    "content": "When you install CluedIn from Azure Marketplace, the AKS cluster is properly configured during the installation. However, you can use this setup as a reference for custom installs. ",
    "url": "/deployment/azure/aks#node-pools-sizing-and-configuration",
    
    "relUrl": "/deployment/azure/aks#node-pools-sizing-and-configuration"
  },"468": {
    "doc": "AKS",
    "title": "Node pools",
    "content": "In Azure Kubernetes Service (AKS), nodes of the same configuration are grouped together into node pools. These node pools contain the underlying VMs that run your applications. https://learn.microsoft.com/en-us/azure/aks/use-multiple-node-pools . See also: . | https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/ | https://azureprice.net/ | . The CluedIn configuration comes with a set of node pools listed below. The YAML snippets for each node type include only their unique features to make them more brief. system . Node count: 1 . apiVersion: v1 kind: Node metadata: labels: agentpool: system kubernetes.azure.com/mode: system kubernetes.cluedin.com/pooltype: system node.kubernetes.io/instance-type: Standard_DS2_v2 # General purpose compute, 2 vCPUs, 7 GiB RAM. spec: taints: - key: CriticalAddonsOnly value: 'true' effect: NoSchedule . The node pool runs critical add-on pods only. general . Node count: 2 . apiVersion: v1 kind: Node metadata: labels: agentpool: general kubernetes.azure.com/agentpool: general kubernetes.azure.com/mode: user kubernetes.cluedin.com/pooltype: general node.kubernetes.io/instance-type: Standard_D8s_v4 # General purpose compute, 8 vCPUs, 32 GiB RAM. The node pool runs more or less lightweight microservices but not databases or processing pods. data . Node count: 2 . apiVersion: v1 kind: Node metadata: labels: agentpool: data kubernetes.azure.com/agentpool: data kubernetes.azure.com/mode: user kubernetes.cluedin.com/pooltype: data node.kubernetes.io/instance-type: Standard_D8s_v4 # General purpose compute, 8 vCPUs, 32 GiB RAM. spec: taints: - key: kubernetes.cluedin.com/pool value: data effect: NoSchedule . The node pool runs databases and the message broker: SQL Server, Elasticsearch, RabbitMQ, Redis, but not Neo4j. dataneo . Node count: 1 . apiVersion: v1 kind: Node metadata: labels: agentpool: dataneo kubernetes.azure.com/agentpool: dataneo kubernetes.azure.com/mode: user kubernetes.cluedin.com/pooltype: data-neo node.kubernetes.io/instance-type: Standard_D8s_v4 # General purpose compute, 8 vCPUs, 32 GiB RAM. spec: taints: - key: kubernetes.cluedin.com/pool value: data-neo effect: NoSchedule . The node pool is dedicated to running Neo4j. processing . Node count: 1 . apiVersion: v1 kind: Node metadata: labels: agentpool: processing kubernetes.azure.com/agentpool: processing kubernetes.azure.com/mode: user kubernetes.cluedin.com/pooltype: processing node.kubernetes.io/instance-type: Standard_F8s_v2 # Compute optimized VMs, 8 vCPUs, 16 GiB RAM. spec: taints: - key: kubernetes.cluedin.com/pool value: processing effect: NoSchedule . The node pool runs processing pods. You can scale this node pool horizontally when needed. For example, during historical data loads or full reprocessing. ",
    "url": "/deployment/azure/aks#node-pools",
    
    "relUrl": "/deployment/azure/aks#node-pools"
  },"469": {
    "doc": "AKS",
    "title": "Node Selectors",
    "content": "infrastructure: elasticsearch: nodeSelector: kubernetes.cluedin.com/pooltype: data neo4j: nodeSelector: kubernetes.cluedin.com/pooltype: data-neo rabbitmq: nodeSelector: kubernetes.cluedin.com/pooltype: data redis: master: nodeSelector: kubernetes.cluedin.com/pooltype: data mssql: nodeSelector: kubernetes.cluedin.com/pooltype: data application: cluedin: roles: processing: nodeSelector: kubernetes.cluedin.com/pooltype: processing . ",
    "url": "/deployment/azure/aks#node-selectors",
    
    "relUrl": "/deployment/azure/aks#node-selectors"
  },"470": {
    "doc": "AKS",
    "title": "Requests and Limits",
    "content": "infrastructure: neo4j: core: resources: requests: cpu: \"7\" memory: \"28Gi\" limits: cpu: \"7\" memory: \"28Gi\" elasticsearch: resources: requests: cpu: \"1\" memory: \"2Gi\" limits: cpu: \"6\" memory: \"26Gi\" redis: master: resources: limits: cpu: \"1\" memory: \"2Gi\" mssql: resources: requests: cpu: \"1\" memory: \"2Gi\" limits: cpu: \"5\" memory: \"22Gi\" rabbitmq: resources: limits: cpu: \"2\" memory: \"6Gi\" application: cluedin: roles: main: resources: limits: cpu: \"2\" memory: \"12Gi\" processing: resources: limits: cpu: \"15\" memory: \"28Gi\" crawling: resources: limits: cpu: \"2\" memory: \"12Gi\" cluedincontroller: resources: limits: cpu: \"0.5\" memory: \"512Mi\" annotation: resources: limits: cpu: \"0.75\" memory: \"512Mi\" prepare: resources: limits: cpu: \"0.75\" memory: \"512Mi\" datasource: resources: limits: cpu: \"1\" memory: \"4Gi\" submitter: resources: limits: cpu: \"0.5\" memory: \"512Mi\" gql: resources: limits: cpu: \"0.5\" memory: \"2Gi\" ui: resources: limits: cpu: \"0.5\" memory: \"2Gi\" webapi: resources: limits: cpu: \"0.5\" memory: \"512Mi\" . ",
    "url": "/deployment/azure/aks#requests-and-limits",
    
    "relUrl": "/deployment/azure/aks#requests-and-limits"
  },"471": {
    "doc": "AKS",
    "title": "Disks",
    "content": "infrastructure: elasticsearch: volumeClaimTemplate: resources: requests: storage: \"500Gi\" mssql: persistence: dataSize: \"750Gi\" transactionLogSize: \"750Gi\" masterSize: \"128Gi\" neo4j: core: persistentVolume: size: \"500Gi\" rabbitmq: persistence: size: \"150Gi\" redis: master: persistence: size: \"32Gi\" . ",
    "url": "/deployment/azure/aks#disks",
    
    "relUrl": "/deployment/azure/aks#disks"
  },"472": {
    "doc": "AKS",
    "title": "AKS",
    "content": " ",
    "url": "/deployment/azure/aks",
    
    "relUrl": "/deployment/azure/aks"
  },"473": {
    "doc": "Azure requirements",
    "title": "Azure requirements",
    "content": "In this article, you will learn about the Azure requirements you need to meet to ensure successful installation of CluedIn SaaS. Azure account . The first requirement is an Azure account. If you already have an Azure account, you can proceed to the next requirement. If you don’t have an Azure account, you can easily create a pay-as-you-go account using a valid credit card. Marketplace purchases and Contributor role . Make sure that you have enabled marketplace purchases and configured the required user permissions (at least Contributor) for the subscription where you want to store the CluedIn SaaS application. For more information, see Enable marketplace purchases in Azure. ",
    "url": "/deployment/saas/requirements",
    
    "relUrl": "/deployment/saas/requirements"
  },"474": {
    "doc": "Processing pipeline",
    "title": "Processing pipeline",
    "content": "Introduction . Conceptually, you can think of CluedIn as a Streaming platform with multiple persistence stores. It just so happens that CluedIn also ships with a lot of functionality to integrate, manipulate, govern and steward this data on its way to downstream consumers. With this in mind, the question of how long does it take to process data from an input to its output arises. Naturally, it strongly depends on many factors, including: . | The number of CPU cores assigned to processing. | The amount of RAM assigned to processing. | The complexity and size of the data being processed. | What CluedIn features you are running on the processing pipeline (we will assume you are using all features) | . To provide some baselines and benchmarks, we will introduce some datasets coming from different types of platforms that speak to the complexity and size of data. CluedIn utilises an Enterprise Service Bus to queue the incoming data. This data is then monitored by many different “workers”. These “workers” are in-memory .net core processes that take a record at a time and run it through 100’s of validations, automated cleaners and more. The reason to explain this, is that, in its raw form, the Enterprise Service Bus can process incoming messages very fast, it is the 100’s of validations and automated processors that will (for example) take this rate from 30,000 messagess to 1000 messages per second i.e. the more the system automates, the slower it can process the data. CluedIn scales both vertically and horizontally, meaning that you can increase the speed of processing by either using bigger machines or more machines. Due to the stateless nature of the processing components of CluedIn, it means that you can have 0 processing services or 100 running. Although CluedIn can technically run using 1 CPU core, it is not optimal for any real workloads within CluedIn. The amount of CPU Cores and RAM that you assign to processing services is all set in your Helm charts. Within Kuberenets you will still need to allocate enough in your Node Pools to be able to scale your processing servers. Let’s start to talk about the data itself. We will typically talk about simple data vs complex data in CluedIn. Simple data is data that uses as little of the 100’s of inbuilt services as possible i.e. less work, means quicker processing. Good examples of simple data usually comes in the form of well structured and small records such as rows in a table or JSON/XML from a REST service. Complex data usually will come in the form of Physical files or large records in tables or JSON/XML packages. We refer to it as complex data due to the fact that it will be enabling a lot of the inbuilt processing functions and require their attention. NOTE: It has to be mentioned that you should still think about whether you need to bring in all columns in all tables from all sources. It is a hard question to answer, as it may be that the data you do not bring in, was the exact data you needed in a particular situation. What should be remembered is that you can always bring this data in later without the need for complex remodelling. There are many ways to monitor the performance of your processing pipeline, one of which is exposed to Administrators with the CluedIn User Interface itself. The Engine Room will give you a breakdown of the overall speed of the processing pipeline and then a zoom in on each processing pipeline step to show the rate of each step. This also gives you the idea that you can turn different processing pipelines if you want to. For example, if you are not wanting some of the more “expensive” processing pipeline steps then these can be disabled through configuration. ",
    "url": "/engine-room/processing-pipeline",
    
    "relUrl": "/engine-room/processing-pipeline"
  },"475": {
    "doc": "Concept of a stream",
    "title": "On this page",
    "content": ". | Stream configuration | Export target configuration | Sending golden records to export target | . In this article, you’ll learn about the process of sending golden records from CluedIn to the export target. We’ll focus on each step of the process so that you know what to expect when you create a stream yourself. ",
    "url": "/consume/streams/concept-of-a-stream#on-this-page",
    
    "relUrl": "/consume/streams/concept-of-a-stream#on-this-page"
  },"476": {
    "doc": "Concept of a stream",
    "title": "Stream configuration",
    "content": "This is the initial part where you determine which golden records will be sent to the export target. Using filters, you can precisely specify which golden records you want to send to the export target. All golden records matching the filters are displayed on the Preview tab of the stream. If you want to modify golden records that will be exported—for example, mask certain values—you can do it by adding actions. The changes applied by the actions do not affect golden records stored in CluedIn, only golden records that will be sent to the export target. These changes will be visible only in the export target. At this point, stream configuration is done, and it’s time to configure the export target—an external system that will receive golden records from CluedIn. ",
    "url": "/consume/streams/concept-of-a-stream#stream-configuration",
    
    "relUrl": "/consume/streams/concept-of-a-stream#stream-configuration"
  },"477": {
    "doc": "Concept of a stream",
    "title": "Export target configuration",
    "content": "This is the part where you configure the destination for golden records. First of all, you need to select the type of connector—an external system ready to receive golden records from CluedIn. Next, you need to define two connector properties: . | Target name – a name of a container that will receive golden records from CluedIn. For example, in SQL databases this container is a table, and in Elasticsearch databases it is the index. | Streaming mode – a mode that defines how golden records will be sent to the export target: . | Synchronized stream – with this mode, golden records in the export target will mirror golden records in CluedIn. It means that every time a golden record is changed in CluedIn, it is automatically changed in the export target. Also, if you delete a golden record in CluedIn, it is automatically deleted in the export target. | Event log stream – with this mode, each time an action occurs in CluedIn, a corresponding event is sent to the export target (for example, Create, Insert, Update, Delete). | . | . Now, it’s time to decide if you want to stream golden record relations, also known as edges. Relations give you the necessary information to link your data correctly in the export target. You can choose both outgoing relations and incoming relations. Finally, you need to select specific golden record properties that will be exported. You can automatically select all properties associated with golden records from stream configuration, or you can select specific properties. At this point, export target configuration is done, and it’s time to start sending golden records to the export target. ",
    "url": "/consume/streams/concept-of-a-stream#export-target-configuration",
    
    "relUrl": "/consume/streams/concept-of-a-stream#export-target-configuration"
  },"478": {
    "doc": "Concept of a stream",
    "title": "Sending golden records to export target",
    "content": "This is the final part of the process where you start sending golden records to the export target. After selecting Start, CluedIn starts sending golden records to the export target, and the export target starts receiving those golden records. ",
    "url": "/consume/streams/concept-of-a-stream#sending-golden-records-to-export-target",
    
    "relUrl": "/consume/streams/concept-of-a-stream#sending-golden-records-to-export-target"
  },"479": {
    "doc": "Concept of a stream",
    "title": "Concept of a stream",
    "content": " ",
    "url": "/consume/streams/concept-of-a-stream",
    
    "relUrl": "/consume/streams/concept-of-a-stream"
  },"480": {
    "doc": "Concept of workflows",
    "title": "Concept of workflows",
    "content": "To use the Workflow module in CluedIn, you need to configure Power Automate integration. Let’s explore the concept of Power Automate workflows in CluedIn using an example of rule modification. If you want to automate the approval process for rule changes, you can create a dedicated Power Automate workflow that will send approval requests to the appropriate users whenever a rule is modified. A user with at least Consulted access level to the Rule Builder claim can modify any rule. However, to ensure transparency and accuracy, the changes are not applied right away. Instead, an approval request is sent to the appropriate users. When one of the appropriate users approves changes, a notification in CluedIn is sent to the user who made the changes, and the changes are applied to the rule. Who are the users in charge of approval? . | In case of modifying an existing element, the users in charge of approval are the owners listed on the Owners tab of the element. Since both users and roles can be the owners, all users with the corresponding roles are also considered approvers. | . Is there a connection between external approvals and internal CluedIn change requests? . When you modify an element in CluedIn, both an internal change request and an external approval request are sent to the owners. The request that is processed first determines the outcome for the element. For example, if an owner approves the internal change request in CluedIn, the element is updated accordingly. However, if another owner rejects the external approval request in Outlook, this rejection is ignored because the element has already been updated. Essentially, the first processed request is applied to the element, and any subsequent request is ignored. ",
    "url": "/workflow/concept-of-approvals",
    
    "relUrl": "/workflow/concept-of-approvals"
  },"481": {
    "doc": "Concept of deduplication",
    "title": "Concept of deduplication",
    "content": "Let’s explore the concept of deduplication in CluedIn through an example. We have three duplicate records—Person 1, Person 2, and Person 3. How do we merge these three records into a single, consolidated golden record? . Note that the email is the same for Person 1 and Person 2, but different for Person 3. To begin with, we need to set up matching rules in the deduplication project: one rule to check for the same email and the other rule to check for the same first name and last name. Records meeting the criteria of either the first or second rule will be selected as possible duplicates. When you generate matches in the deduplication project, Person 1 and Person 2 are selected as possible duplicates because they have the same email address. At this point, the second rule did not identify any duplicates because the three records have different first and last names. When reviewing the group of duplicates, you have to choose the appropriate values among conflicting values. In this case, the conflicting values would be first name and last name. After merging, the duplicates result in a merged golden record—Merged Person 1 and 2—which combines consolidated and complete information. Merging does not create a new golden record; instead, it uses one of the duplicate records as a base and aggregates data from the others to form a comprehensive golden record. Now, after the first run of the deduplication project, we have two duplicate records: Merged Person 1 and 2 and Person 3. When a merged golden record appears, the rules that led to the merge need to be re-evaluated. It means that you need to generate matches in the deduplication project again. This time, the second rule comes into play: Merged Person 1 and 2 and Person 3 are selected as possible duplicates because they have the same first and last names. However, the email is identified as a conflicting value, so you have to choose the appropriate email for the merged golden record. After merging, the duplicates result in a merged golden record—Merged Person 1, 2, 3—with consolidated and complete information. Every time golden records are merged, it is crucial to re-evaluate them, as they may contain new information. This new information may align with criteria from matching rules, potentially making golden records eligible for subsequent merges. Essentially, each merging operation has the potential to produce golden records that could initiate further iterations of the merging process. This is why deduplication can be a somewhat time-consuming process, requiring multiple runs until no duplicates are identified, ensuring a comprehensive consolidation of data. ",
    "url": "/management/deduplication/concept-of-deduplication",
    
    "relUrl": "/management/deduplication/concept-of-deduplication"
  },"482": {
    "doc": "Concept of enricher",
    "title": "On this page",
    "content": ". | Enricher example | Enricher in data life cycle | . In this article, you will explore the concept of an enricher through an example and understand its place in the data life cycle in CluedIn. ",
    "url": "/preparation/enricher/concept-of-enricher#on-this-page",
    
    "relUrl": "/preparation/enricher/concept-of-enricher#on-this-page"
  },"483": {
    "doc": "Concept of enricher",
    "title": "Enricher example",
    "content": "Imagine you have a golden record—CluedIn—that contains several properties. How can you enrich this golden record with additional information available on the internet? . To begin with, you need to add the web enricher and specify the vocabulary key. The value of this vocabulary key will be used to retrieve information from external services. In this case, the vocabulary key is organization.website. The enricher then retrieves all available information it can find based on the specified website address. Finally, this new information is added to the initial golden record. In the next section, you’ll find out how the data from the enricher is incorporated into the corresponding golden record. ",
    "url": "/preparation/enricher/concept-of-enricher#enricher-example",
    
    "relUrl": "/preparation/enricher/concept-of-enricher#enricher-example"
  },"484": {
    "doc": "Concept of enricher",
    "title": "Enricher in data life cycle",
    "content": "The following diagram illustrates the place of enricher within the data life cycle in CluedIn. From golden record to enricher . To begin the process of enriching a golden record with additional information, you need to designate a vocabulary key. This vocabulary key serves as the basis for retrieving additional information about that particular golden record from the internet. If you don’t provide a vocabulary key, the enricher won’t be able to look for additional information. For more details about prerequisites for an enricher, see Connectors. From enricher to clue . When the enricher receives the vocabulary key value, it calls an external internet service, typically an API, to retrieve additional information associated with the specified vocabulary key. Then, the information obtained from the external service appears in CluedIn as a clue. For more information about the clue structure, see Clue reference. Now, how do we link a new clue to the golden record? . From clue to golden record . When a new clue appears in CluedIn from the enricher, it goes into the processing pipeline. It is important to note that such clue has the same primary identifier as the golden record. During processing, CluedIn transforms the clue into a data part and executes merging by identifiers to ensure that the new information seamlessly integrates with the existing golden record. To learn more about what happens to the clue during processing, see Data life cycle. The processing of a clue from an enricher follows the same steps as any other clue within the system. ",
    "url": "/preparation/enricher/concept-of-enricher#enricher-in-data-life-cycle",
    
    "relUrl": "/preparation/enricher/concept-of-enricher#enricher-in-data-life-cycle"
  },"485": {
    "doc": "Concept of enricher",
    "title": "Concept of enricher",
    "content": " ",
    "url": "/preparation/enricher/concept-of-enricher",
    
    "relUrl": "/preparation/enricher/concept-of-enricher"
  },"486": {
    "doc": "Concept of hierarchy",
    "title": "Concept of hierarchy",
    "content": "Let’s explore the concept of hierarchy through an example of Organization-Project-Employee relations. How can we visualize those relations in a hierarchy? . To begin, we need to create a hierarchy for the /Organization business domain. Because some organizations might be connected with others through parent-child relations, we select the corresponding edge type (/Parent). For convenience, we create multiple single-root projects, where each top-level root has its own hierarchy. As a result, two hierarchy projects are generated. We are going to use one of them to illustrate project and employee relations within an organization. Now, we need to visualize the relations between projects within organizations. However, as for now, only organizations are available in the hierarchy project. To load more golden records from other business domains, we need to edit the hierarchy properties and remove the /Organization business domain. Now, all golden records are available for selection, so we can load golden records that are connected to the organization by the specific type of relation (/PartOf). The next step is to add the employees who work on specific projects. To do that, we can load golden records that are connected to projects by the specific type of relation (/WorksFor). As a result, we leveraged the existing relations between golden records to visualize the connections between employees and projects within organizations. Following this approach, you can build complex hierarchies to discover hidden value from any data source. ",
    "url": "/management/hierarchy-builder/concept-of-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/concept-of-hierarchy"
  },"487": {
    "doc": "Configure a manual data entry project",
    "title": "On this page",
    "content": ". | Create a manual data entry project | Create form fields | Review and modify mapping configuration | Define quality of manual data entry project source | . In this article, you will learn how to create and configure a manual data entry project to be able to add the records manually directly in CluedIn. A manual data entry project is an underlying component of manual data entry in CluedIn. It contains the configuration, mapping, and permissions for records created within the project. The process of creating a manual data entry project consists of 4 parts: . | Creating a manual data entry project with basic configuration – defining the business domain and vocabulary for the golden records that will be produced, as well as setting up the option to send records for approval before processing. | Adding the form fields in a manual data entry project – defining the specific types of data that will be added manually in order to create a record. A field represents a property of a record. | Reviewing and modifying the mapping configuration – ensuring that the primary identifier as well as additional identifiers for records in manual data entry project are configured correctly. | Defining the quality of the manual data entry project source – defining quality is useful if you have survivorship rules that determine which value from multiple sources should be used in a golden record based on the quality of the source. | . ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project#on-this-page",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project#on-this-page"
  },"488": {
    "doc": "Configure a manual data entry project",
    "title": "Create a manual data entry project",
    "content": "You can create as many manual data projects as you need. Consider having separate projects for different types of business data. For example, you can create a project for contact data, a project for product data, and a project for customer data. This way, you can better organize your manual records and ensure that each type of information is handled appropriately. To create a manual data entry project . | On the navigation pane, go to Ingestion &gt; Manual Data Entry. | Select Create. | Enter the name of the manual data entry project. | Select the business domain for records that will be created in the manual data entry project. | Select the vocabulary that will be used in the manual data entry project. The vocabulary keys from this vocabulary will be available for selection when you create the form fields. | If you want to send the records created in the manual data entry project by non-owner users for approval, select the checkbox in Require records approval. If you enable records approval, you, as the project owner, and any other owners will receive notifications when non-owner users try to add new records to the project. The owner needs to approve these records before they can be processed. For more information, see Approval. | (Optional) Enter the description of the manual data entry project. | Select Create. The manual data entry project page opens where you can proceed to add the form fields. | . ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project#create-a-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project#create-a-manual-data-entry-project"
  },"489": {
    "doc": "Configure a manual data entry project",
    "title": "Create form fields",
    "content": "A form field is an element in a manual data entry project that represents a property of a record. For example, if you created a manual data entry project for contact data, your form fields may include ID, First Name, Last Name, Email, and Phone Number. Essentially, the process of adding a manual record consists of entering values into the defined form fields. To create a form field . | In the manual data entry project, go to the Form Fields tab. | Select Create Form Field. | In the Vocabulary Key section, expand the dropdown list, and select the vocabulary key that will be used as a field for manual data entry. By default, the list includes the vocabulary keys that belong to the vocabulary that you selected when creating the project. If you want to add a vocabulary key from another vocabulary, clear the checkmark and then find and select the needed vocabulary key. | Review the Label of the form field. This is the name of the field that will be displayed when adding a manual record. The label is added automatically based on the vocabulary key selected in the previous step. You can modify the label if needed. If you modify the label, the changes will only be visible in the manual data entry project, not in the vocabulary key. | Select the Form Field Type: . | Text Field – an input type where you can enter any text in the field. | Pick List – an input type where you can select an option from a predefined list. If you select this form field type, you need to add the options for the list. | Toggle – an input type where you can switch between two states—true or false. | . | If you selected Pick List in the previous step, add the pick list items: . | In the Pick list items field, enter an option for the list, and then select + Add item. The pick list item appears in the Added pick list items section. | To add more pick list items, repeat step 6.1. | . | If the field is required in a record, turn on the toggle for Is Required. | If you want to restrict the input in the field to only existing values from the vocabulary key, turn on the toggle for Only Existing Values. This option is available only for the Text Field type only. | If you want to use the field for producing the identifier for a record, turn on the toggle for Use as identifier. | (Optional) Enter the description of the form field. | Select Create. The form field is added to the Form Fields tab of the manual data entry project. | To add more form fields, repeat steps 1–11. Once you have created all the form fields you need, review the mapping configuration for the records that will be created in a project and modify it as needed. | . On the Form Fields tab, the fields are displayed in the order in which they appear when you add a record. You can change the order of the fields if needed. To do this, on the right side of the row, open the three-dot menu, and then select where you want to move the row. ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project#create-form-fields",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project#create-form-fields"
  },"490": {
    "doc": "Configure a manual data entry project",
    "title": "Review and modify mapping configuration",
    "content": "The mapping configuration for records from the manual data entry project is created automatically based on the details you provide when adding form fields. By default, the primary identifier and the origin are auto-generated to ensure uniqueness of the records. Additionally, if you turned on the Use as identifier toggle for a form field, this field will be used to produce additional identifiers for the records. Note that the default mapping configuration does not include the name for the records. This is the name that is displayed during search as well as on the golden record details page. If you do not select the field for producing the name, CluedIn will use automatically generated record ID. To review and modify mapping configuration . | In the manual data entry project, go to the Map tab. You will see the default mapping configuration. | Select Edit mapping. | In the General details section, select the field that will be used to produce the name for a record once it has been processed. You can add multiple fields. | In the Primary identifier section, review the default configuration for producing a primary identifier: origin and property (field). You can select another origin and field if needed. | In the Identifiers section, review the default configuration for producing additional identifiers. You can edit or delete the default identifier as well as add new additional identifiers. | Select Finish. Once you have reviewed and modified the mapping configuration as needed, you can proceed to define the quality of the manual data entry project source. This is only necessary if you use survivorship rules that determine the winning value based on the quality of the source. If you do not use such survivorship rules, you can proceed to add the records manually. | . ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project#review-and-modify-mapping-configuration",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project#review-and-modify-mapping-configuration"
  },"491": {
    "doc": "Configure a manual data entry project",
    "title": "Define quality of manual data entry project source",
    "content": "If you have survivorship rules that determine which value from multiple sources should be used in a golden record based on the quality of the source, then you need to define the quality of the manual data entry project. If you believe that the values from the manual data entry project are of higher quality and more trustworthy than those from other sources, you can assign a higher quality rating for the manual data entry project. This way, in case of conflicting values between the manual data entry project and another source, CluedIn will prioritize the value from a manual data entry project. To define quality of manual data entry project source . | In the manual data entry project, go to the Quality tab. | In the Source section, select the category that best describes the manual data entry project. | In the Source Quality section, define the quality rating for the source by dragging the slider towards Lower Quality or Higher Quality. | Select Save. The quality of the manual data entry project is updated. Next, you can proceed to add the records manually. | . ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project#define-quality-of-manual-data-entry-project-source",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project#define-quality-of-manual-data-entry-project-source"
  },"492": {
    "doc": "Configure a manual data entry project",
    "title": "Configure a manual data entry project",
    "content": " ",
    "url": "/integration/manual-data-entry/configure-a-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/configure-a-manual-data-entry-project"
  },"493": {
    "doc": "Connect CluedIn to Microsoft Fabric",
    "title": "On this page",
    "content": ". | Load data | Explore data | Create schema | . In this article, you will learn how to load data from CluedIn to a Microsoft Fabric notebook, do basic data exploration and transformation, and save data in a Delta Lake table. Prerequisites . To connect CluedIn to Microsoft Fabric, you need an active API token. You can create an API token in CluedIn in Administration &gt; API tokens. ",
    "url": "/microsoft-integration/fabric/connect-cluedin-to-fabric#on-this-page",
    
    "relUrl": "/microsoft-integration/fabric/connect-cluedin-to-fabric#on-this-page"
  },"494": {
    "doc": "Connect CluedIn to Microsoft Fabric",
    "title": "Load data",
    "content": "In the following procedure, we’ll use a CluedIn instance that has 601_222 entities of /IMDb/Title entity type. To load data from CluedIn to a Microsoft Fabric notebook . | Install the cluedin library. %pip install cluedin==2.2.0 . | Import the following libraries. import pandas as pd import matplotlib.pyplot as plt import cluedin . | Provide the URL of your CluedIn instance and the API token. # CluedIn URL: https://foobar.mycluedin.com/: # - foobar is the organization's name # - mycluedin.com is the domain name cluedin_context = { 'domain': 'mycluedin.com', 'org_name': 'foobar', 'access_token': '(your token)' } . | Pull one row from CluedIn to see what data you have. # Create a CluedIn context object. ctx = cluedin.Context.from_dict(cluedin_context) # GraphQL query to pull data from CluedIn. query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query cursor: $cursor pageSize: $pageSize sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} ) { totalResults cursor entries { id name entityType properties } } } \"\"\" # Fetch the first record from the `cluedin.gql.entries` generator. next(cluedin.gql.entries(ctx, query, { 'query': 'entityType:/IMDb/Title', 'pageSize': 1 })) . You will get an output similar to the following. {'id': '00001e32-9bae-53b9-a30f-cf30ed66c360', 'name': 'Murder, Money and a Dog', 'entityType': '/IMDb/Title', 'properties': {'attribute-type': '/Metadata/KeyValue', 'property-imdb.title.endYear': '\\\\N', 'property-imdb.title.genres': 'Comedy,Drama,Thriller', 'property-imdb.title.isAdult': '0', 'property-imdb.title.originalTitle': 'Murder, Money and a Dog', 'property-imdb.title.primaryTitle': 'Murder, Money and a Dog', 'property-imdb.title.runtimeMinutes': '65', 'property-imdb.title.startYear': '2010', 'property-imdb.title.tconst': 'tt1664719', 'property-imdb.title.titleType': 'movie'}} . For performance reasons and to avoid collisions, sort the results by a unique field—for example, Entity ID—in the GraphQL query. sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} . | Pull the whole data set in a pandas DataFrame. To make it compatible with the Spark schema, flatten the properties, remove unnecessary property name prefixes, and replace dots with underscores. ctx = cluedin.Context.from_dict(cluedin_context) query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query sort: FIELDS cursor: $cursor pageSize: $pageSize sortFields: {field: \"id\", direction: ASCENDING} ) { totalResults cursor entries { id properties } } } \"\"\" def flatten_properties(d): for k, v in d['properties'].items(): if k == 'attribute-type': continue if k.startswith('property-'): k = k[9:] # len('property-') == 9 k = k.replace('.', '_') d[k] = v del d['properties'] return d df_titles = pd.DataFrame( map( flatten_properties, cluedin.gql.entries(ctx, query, { 'query': 'entityType:/IMDb/Title', 'pageSize': 10_000 }))) df_titles.head() . You will get an output similar to the following. | Set the DataFrame’s index to Entity ID. df_titles.set_index('id', inplace=True) df_titles.head() . You will get an output similar to the following. | . ",
    "url": "/microsoft-integration/fabric/connect-cluedin-to-fabric#load-data",
    
    "relUrl": "/microsoft-integration/fabric/connect-cluedin-to-fabric#load-data"
  },"495": {
    "doc": "Connect CluedIn to Microsoft Fabric",
    "title": "Explore data",
    "content": "You can display some summaries and visualizations of the data in a Microsoft Fabric notebook. In the following example, you can see the visualization of movies by genre. df_titles['imdb_title_genres'].str.split(',', expand=True).stack().value_counts().plot(kind='bar') plt.title('Distribution of genres') plt.xlabel('Genres') plt.ylabel('Count') plt.show() . ",
    "url": "/microsoft-integration/fabric/connect-cluedin-to-fabric#explore-data",
    
    "relUrl": "/microsoft-integration/fabric/connect-cluedin-to-fabric#explore-data"
  },"496": {
    "doc": "Connect CluedIn to Microsoft Fabric",
    "title": "Create schema",
    "content": "To view the data in the Catalog, you need to create a schema. Mind that in the following example, imdb_title_genres is a string, not an array, so we need to split it. from pyspark.sql import SparkSession from pyspark.sql.types import StructType,StructField, StringType, ArrayType, IntegerType from pyspark.sql.functions import split spark = SparkSession.builder.getOrCreate() schema = StructType([ StructField('id', StringType(), True), StructField('imdb_title_endYear', StringType(), True), StructField('imdb_title_genres', ArrayType(StringType()), True), StructField('imdb_title_isAdult', StringType(), True), StructField('imdb_title_originalTitle', StringType(), True), StructField('imdb_title_primaryTitle', StringType(), True), StructField('imdb_title_runtimeMinutes', StringType(), True), StructField('imdb_title_startYear', StringType(), True), StructField('imdb_title_tconst', StringType(), True), StructField('imdb_title_titleType', StringType(), True) ]) df_spark_titles = spark.createDataFrame(df_titles) df_spark_titles = df_spark_titles.withColumn('imdb_title_genres', split(df_spark_titles.imdb_title_genres, ',')) spark.sql('CREATE DATABASE IF NOT EXISTS cluedin') df_spark_titles.write.mode('overwrite').format('parquet').saveAsTable('cluedin.imdb_titles', schema=schema) display(df_spark_titles) . Now, you can view the data in the Catalog. ",
    "url": "/microsoft-integration/fabric/connect-cluedin-to-fabric#create-schema",
    
    "relUrl": "/microsoft-integration/fabric/connect-cluedin-to-fabric#create-schema"
  },"497": {
    "doc": "Connect CluedIn to Microsoft Fabric",
    "title": "Connect CluedIn to Microsoft Fabric",
    "content": " ",
    "url": "/microsoft-integration/fabric/connect-cluedin-to-fabric",
    
    "relUrl": "/microsoft-integration/fabric/connect-cluedin-to-fabric"
  },"498": {
    "doc": "Connect to CluedIn cluster",
    "title": "Connect to CluedIn cluster",
    "content": "Azure Cloud Shell provides an effective environment and the tools you need to interact with your AKS cluster without significant configuration or effort. To connect to your CluedIn cluster in Azure Cloud Shell . | In the Azure portal, navigate to your AKS resource. | On the menu bar, select Connect. In the right pane, you’ll see the connection details and instructions that you can use to connect to your CluedIn cluster. | . For additional details, please refer to Microsoft Documentation. ",
    "url": "/deployment/infra-how-tos/connect-to-cluedin",
    
    "relUrl": "/deployment/infra-how-tos/connect-to-cluedin"
  },"499": {
    "doc": "Connector reference",
    "title": "On this page",
    "content": ". | Azure Data Lake | Azure Dedicated SQL Pool | Azure Event Hub | Azure Service Bus | Dataverse | HTTP | OneLake | Sql Server | . In this article, you will find reference information about built-in connectors in CluedIn. Please note that the connectors are not included in the CluedIn license. Each connector is an open-source package provided by the CluedIn team for free to help you send your golden records to external systems. ",
    "url": "/consume/export-targets/connector-reference#on-this-page",
    
    "relUrl": "/consume/export-targets/connector-reference#on-this-page"
  },"500": {
    "doc": "Connector reference",
    "title": "Azure Data Lake",
    "content": "The Azure Data Lake connector allows you to publish data to Azure Data Lake Storage Gen2. | Package name | Package version | Link to source code | . | CluedIn.Connector.AzureDataLake | 4.4.0 | Source code | . | CluedIn.Connector.AzureDataLake | 4.3.3 | Source code | . | CluedIn.Connector.AzureDataLake | 4.3.2 | Source code | . | CluedIn.Connector.AzureDataLake | 4.3.0 | Source code | . | CluedIn.Connector.AzureDataLake | 4.0.0 | Source code | . ",
    "url": "/consume/export-targets/connector-reference#azure-data-lake",
    
    "relUrl": "/consume/export-targets/connector-reference#azure-data-lake"
  },"501": {
    "doc": "Connector reference",
    "title": "Azure Dedicated SQL Pool",
    "content": "The Azure Dedicated SQL Pool connector allows you to publish data to Microsoft’s Dedicated SQL Pool, which is a part of Azure Synapse Analytics platform. | Package name | Package version | Link to source code | . | CluedIn.Connector.AzureDedicatedSqlPool | 4.0.2 | - | . ",
    "url": "/consume/export-targets/connector-reference#azure-dedicated-sql-pool",
    
    "relUrl": "/consume/export-targets/connector-reference#azure-dedicated-sql-pool"
  },"502": {
    "doc": "Connector reference",
    "title": "Azure Event Hub",
    "content": "The Azure Event Hub connector allows you to publish data to Azure Event Hubs. | Package name | Package version | Link to source code | . | CluedIn.Connector.AzureEventHubs | 4.0.1 | Source code | . | CluedIn.Connector.AzureEventHubs | 4.0.0 | Source code | . ",
    "url": "/consume/export-targets/connector-reference#azure-event-hub",
    
    "relUrl": "/consume/export-targets/connector-reference#azure-event-hub"
  },"503": {
    "doc": "Connector reference",
    "title": "Azure Service Bus",
    "content": "The Azure Service Bus connector allows you to publish data to Azure Service Bus. | Package name | Package version | Link to source code | . | CluedIn.Connector.AzureServiceBus | 4.0.2 | Source code | . | CluedIn.Connector.AzureServiceBus | 4.0.1 | Source code | . | CluedIn.Connector.AzureServiceBus | 4.0.0 | Source code | . ",
    "url": "/consume/export-targets/connector-reference#azure-service-bus",
    
    "relUrl": "/consume/export-targets/connector-reference#azure-service-bus"
  },"504": {
    "doc": "Connector reference",
    "title": "Dataverse",
    "content": "The Dataverse connector allows you to publish data to Microsoft Dataverse. | Package name | Package version | Link to source code | . | CluedIn.Connector.Dataverse | 4.4.0 | - | . ",
    "url": "/consume/export-targets/connector-reference#dataverse",
    
    "relUrl": "/consume/export-targets/connector-reference#dataverse"
  },"505": {
    "doc": "Connector reference",
    "title": "HTTP",
    "content": "The HTTP connector allows you to publish data to your own API endpoint. | Package name | Package version | Link to source code | . | CluedIn.Connector.Http | 4.0.0 | Source code | . ",
    "url": "/consume/export-targets/connector-reference#http",
    
    "relUrl": "/consume/export-targets/connector-reference#http"
  },"506": {
    "doc": "Connector reference",
    "title": "OneLake",
    "content": "The OneLake connector allows you to publish data to Microsoft’s OneLake. | Package name | Package version | Link to source code | . | CluedIn.Connector.OneLake | 4.4.0 | - | . ",
    "url": "/consume/export-targets/connector-reference#onelake",
    
    "relUrl": "/consume/export-targets/connector-reference#onelake"
  },"507": {
    "doc": "Connector reference",
    "title": "Sql Server",
    "content": "The SQL Server connector allows you to publish data to a Microsoft SQL Server database. | Package name | Package version | Link to source code | . | CluedIn.Connector.SqlServer | 4.1.1 | Source code | . | CluedIn.Connector.SqlServer | 4.1.0 | Source code | . | CluedIn.Connector.SqlServer | 4.0.0 | Source code | . ",
    "url": "/consume/export-targets/connector-reference#sql-server",
    
    "relUrl": "/consume/export-targets/connector-reference#sql-server"
  },"508": {
    "doc": "Connector reference",
    "title": "Connector reference",
    "content": " ",
    "url": "/consume/export-targets/connector-reference",
    
    "relUrl": "/consume/export-targets/connector-reference"
  },"509": {
    "doc": "Create a stream",
    "title": "On this page",
    "content": ". | Create a stream | Configure an export target | . In this section, you will learn how to set up a live stream of golden records from CluedIn to a specific external target (for example, a Microsoft SQL Server database). Prerequisite: Make sure you have added and configured the connection to the export target where you want to send the records from CluedIn. For more information, see Export targets. ",
    "url": "/consume/streams/create-a-stream#on-this-page",
    
    "relUrl": "/consume/streams/create-a-stream#on-this-page"
  },"510": {
    "doc": "Create a stream",
    "title": "Create a stream",
    "content": "The first step to send the records from CluedIn to an export target is to create a stream and define what golden records you want to share. To create a stream . | On the navigation pane, go to Consume &gt; Streams. | Select Create Stream. Enter the name of the stream, and then select Create. The stream details page opens, where you can configure the stream and the export target for the stream, as well as view all stream-related data. | In the Filters section, select Add First Filter, and then specify which golden records you want to send to the export target. For more information on how to set up a filer, see Filters. If you want to view the golden records matching the filter, select Preview Condition. | If you want to modify golden records before sending them to the export target—for example, mask certain values—you can do it in the Actions section. Select Add Action, and then configure the action that will be performed on the records matching the stream’s filter. For more information about the available actions, see Actions in data part rules. You can add multiple actions. The changes applied by the actions do not affect the records stored in CluedIn, only the records that are sent to the export target. | (Optional) In the Description section, enter the details about the stream. | In the upper-right corner of the stream details page, select Save. You created the stream. Now, you are the owner of the stream. For more information, see Ownership. Next, configure the export target for the stream and select which properties from the golden records will be shared. | . ",
    "url": "/consume/streams/create-a-stream",
    
    "relUrl": "/consume/streams/create-a-stream"
  },"511": {
    "doc": "Create a stream",
    "title": "Configure an export target",
    "content": "You can configure the export target for the stream on the Export Target Configuration tab of the stream details page. To configure the export target for the stream: . | On the Choose connector tab, select the export target where the records will be sent. If the export target is Unhealthy, you cannot select it in the export target configuration for a stream. | Near the upper-right corner, select Next. | On the Connector properties tab, provide the following details: . | In the Target name field, enter the name of the container that will receive the records sent from CluedIn. For example, in SQL databases this container is a table and in Elasticsearch databases it is the index. | In the Streaming mode section, select the option for sending the records to the export target: . | Synchronized stream – select this option if you want the records in the export target to mirror the records in CluedIn. | Event log stream – select this option if you want to send data as events (for example, Create, Insert, Update, Delete) each time an action occurs in CluedIn. | . | Near the upper-right corner, select Next. | . | On the Properties to export tab, provide the following details: . | In the Export edges section, select whether you want to send the linked data together with the golden records: . | Outgoing – turn on this toggle if you want to send the data for which the stream’s records are the target. | Incoming – turn on this toggle if you want to send the data for which the stream’s records are the source. | . | In the Properties to export section, select which properties of the golden records you need to send to the export target. | . By default, certain properties of the golden records will be sent to the export target. These default properties depend on the export target (for example, for SQL Server Connector the default properties include Id, PersistVersion, PersistHash, OriginEntityCode, EntityType, Timestamp). However, you can send other properties as well. To do that, select Add Property &gt; Add Entity Property, and then select the needed properties. In a similar way, you can add the vocabulary keys. If you want to send all vocabulary keys associated with the records matching the stream’s filters, select Auto-select. All vocabulary keys will be displayed in the table. If you don’t want to send a particular vocabulary key, select the checkbox next to it, and then select Remove Property. | Near the upper-right corner, select Save. You configured the export target for the stream and defined what properties will be sent to the export target. Now, you’re ready to start streaming records to the external system. | . ",
    "url": "/consume/streams/create-a-stream#configure-an-export-target",
    
    "relUrl": "/consume/streams/create-a-stream#configure-an-export-target"
  },"512": {
    "doc": "Create access control policy",
    "title": "On this page",
    "content": ". | Create an access control policy | Impact of access control on system features | . In this article, you will learn how to create an access control policy to configure reliable and secure access to data in CluedIn. ",
    "url": "/management/access-control/create-access-control-policy#on-this-page",
    
    "relUrl": "/management/access-control/create-access-control-policy#on-this-page"
  },"513": {
    "doc": "Create access control policy",
    "title": "Create an access control policy",
    "content": "Since access control is not enabled by default, first you need to enable it: . | In the navigation pane, go to Administration &gt; Settings. | In the Data Access section, turn on the toggle for Access Control setting. | Save your changes. | . Now that the access control feature is enabled, you can create access control policies. To create an access control policy . | In the navigation pane, go to Management &gt; Access Control. | Select Create Policy. | Enter the name of the policy, and then select Create Policy. | In the Filters section, set up a filter to define the golden records to which the policy will apply. | In the Rules section, select Add Policy Rule, and then define the policy rule: . | Enter the name of the policy rule. | (Optional) In Conditions, set up additional criteria on top of the filters to define which golden records will be affected by the specific policy rule. | In Action, select the type of access to golden record properties: view, mask, or add/edit. Learn more in Access control reference. | In Members, select the users or roles to which the policy rule will apply. | If you want to allow access to all vocabulary keys in the golden records affected by the access control policy, select the All Vocabulary Keys checkbox. | If you want to allow access only to specific vocabularies or vocabulary keys in the golden records affected by the access control policy, select the needed vocabulary keys or vocabularies. | Select Add Policy Rule. | . You can add multiple policy rules. | Save your changes and then turn on the status toggle to activate the policy. It might take up to 1 minute to apply the policy across the system. If the source control is enabled, make sure the users from the Members section of the policy rule have permissions to the source of golden records. If the users don’t have such permissions, they can’t view golden records from that source at all. For more information about source control, see Data access. | . ",
    "url": "/management/access-control/create-access-control-policy#create-an-access-control-policy",
    
    "relUrl": "/management/access-control/create-access-control-policy#create-an-access-control-policy"
  },"514": {
    "doc": "Create access control policy",
    "title": "Impact of access control on system features",
    "content": "Access control policies are applied to golden records and vocabulary keys throughout the system. If you don’t have access to specific golden records, then you won’t be able to view them at all. If you have access to golden records but lack access to some vocabulary keys associated with them, you will see a Limited details label on the golden record page. This label indicates that not all properties of the golden record are visible to you. The information on all tabs of the golden record is displayed in accordance with access control policies. For example, on the History tab, you can only view only those records that generated vocabulary keys you have access to. Access control and data catalog . You can view only those vocabularies and vocabulary keys that you have access to according to the source control policies. Also, if an access control policy is applied to a vocabulary key that is mapped to another vocabulary key, then the policy is also applied to the mapped key. Access control and streams . In the stream, you can view only those golden records and vocabulary keys that you have access to according to the source control policies. However, when you start the stream, all data matching the stream configuration will be exported regardless of your access. Access policy and rules . Rules are applied throughout the system regardless of your access to the affected golden records and vocabulary keys. So, you can set up rules for the records that you don’t have access to. ",
    "url": "/management/access-control/create-access-control-policy#impact-of-access-control-on-system-features",
    
    "relUrl": "/management/access-control/create-access-control-policy#impact-of-access-control-on-system-features"
  },"515": {
    "doc": "Create access control policy",
    "title": "Create access control policy",
    "content": " ",
    "url": "/management/access-control/create-access-control-policy",
    
    "relUrl": "/management/access-control/create-access-control-policy"
  },"516": {
    "doc": "Create a clean project",
    "title": "On this page",
    "content": ". | From the search results page | From the Preparation module | From the glossary term | . A clean project is where you specify the data to be cleaned, retrieve it from CluedIn, perform the cleaning process, and then send the cleaned values back to golden records. You can create a clean project from several places in CluedIn. This article will guide you through the different methods for creating a clean project, helping you select the most convenient one to suit your needs. There are three ways to create a clean project: . | From the search results page – this is the most convenient option as you can define filters once and then reuse them to run the clean projects. | From the Preparation module – this is a bit laborious option as you have to define filters every time you need to run the clean project. | From the glossary term – this is another convenient option as you can rely on previously defined filters that include the records that need to be cleaned. | . ",
    "url": "/preparation/clean/create-clean-project#on-this-page",
    
    "relUrl": "/preparation/clean/create-clean-project#on-this-page"
  },"517": {
    "doc": "Create a clean project",
    "title": "From the search results page",
    "content": "If you want to regularly clean your data, we recommend setting up and saving search filters for quick retrieval of the needed data. The main benefit of creating a clean project from the saved search is that you define the vocabulary keys that need to be cleaned just once, and then you can reuse this filter definition multiple times. To create a clean project from the search results page . | In the search field, select the search icon. Depending on whether you have the saved search that retrieves the data you need to clean, do one of the following: . | If you have the saved search, in the upper-right corner of the page, select the vertical ellipsis button, and then select Saved Searches. Find and open the needed saved search. Make sure that the vocabulary keys to be cleaned are displayed on the search results page. This way, you don’t have to specify the vocabulary keys in the clean project configuration. To learn how to add vocabulary keys to the search results page, see Add columns. | If you don’t have the saved search, define the data that you want to clean using filters. Then, add the vocabulary keys to be cleaned to the search results page following the Add columns procedure. If you need to clean the same set of data in future, save the current filter configuration. | . | In the upper-right corner of the page, select the vertical ellipsis button, and then select Clean. The Create Clean Project pane opens for you to configure the clean project. Note that the Filters section is already filled in with the information from the search filter definition. | Enter the name of the clean project. Then, in the lower-right corner, select Next. | Review the properties that will be loaded to the clean project. These properties are taken from the column options of your search results page. If you do not want to load some properties to the clean project, choose the needed properties, and then select Remove Property. | In the lower-right corner, select Create. You created the clean project from the search results page. The next step is to generate results of the clean project. | . ",
    "url": "/preparation/clean/create-clean-project#from-the-search-results-page",
    
    "relUrl": "/preparation/clean/create-clean-project#from-the-search-results-page"
  },"518": {
    "doc": "Create a clean project",
    "title": "From the Preparation module",
    "content": "Creating a clean project from the Preparation module requires more effort than using other options. The reason for this is that you need to set filters each time you want to clean data. To create a clean project from the Preparation module . | On the navigation pane, go to Preparation &gt; Clean. | Select Create Clean Project. The Create Clean Project pane opens for you to configure the clean project. | On the Configure tab, do the following: . | Enter the name of the clean project. | In the Filters section, define what type of data you want to load to the clean project. Then, in the lower-right corner, select Next. | . | On the Choose Vocabulary Keys tab, do the following: . | Select Add Property, and then choose what type of property—record property or vocabulary property—you want to load to the clean project. | Depending on the type of property that you chose, select the record properties or find and select the vocabulary keys that you want to load to the clean project. | In the lower-right corner, select Save Selection. | . | In the lower-right corner, select Create. You created the clean project from the Preparation module. The next step is to generate results of the clean project. | . ",
    "url": "/preparation/clean/create-clean-project#from-the-preparation-module",
    
    "relUrl": "/preparation/clean/create-clean-project#from-the-preparation-module"
  },"519": {
    "doc": "Create a clean project",
    "title": "From the glossary term",
    "content": "You can create a glossary term to collect a group of records that meet certain criteria, and then clean such records. Creating a clean project from the glossary term is quick and convenient because you don’t have to define filters in the clean project configuration. To create a clean project from the glossary term . | On the navigation pane, go to Management &gt; Glossary. | Find and open the needed glossary term. Then, select Create Clean Project. The Create Clean Project pane opens for you to configure the clean project. | On the Configure tab, do the following: . | Enter the name of the clean project. | In the Filters section, choose the operation for defining what to do with the records belonging to the glossary term. Choosing Is True will load all records belonging to the term, and choosing Is Not True will load all records that do not belong to the glossary term. Then, in the lower-right corner, select Next. | . | On the Choose Vocabulary Keys tab, do the following: . | Select Add Property, and then choose what type of property—record property or vocabulary property—you want to load to the clean project. | Depending on the type of property that you chose, select the record properties or find and select the vocabulary keys that you want to load to the clean project. | In the lower-right corner, select Save Selection. | . | In the lower-right corner, select Create. You created the clean project from the glossary. The next step is to generate results of the clean project. | . ",
    "url": "/preparation/clean/create-clean-project#from-the-glossary-term",
    
    "relUrl": "/preparation/clean/create-clean-project#from-the-glossary-term"
  },"520": {
    "doc": "Create a clean project",
    "title": "Create a clean project",
    "content": " ",
    "url": "/preparation/clean/create-clean-project",
    
    "relUrl": "/preparation/clean/create-clean-project"
  },"521": {
    "doc": "Data access",
    "title": "On this page",
    "content": ". | Source control | Access control | Combinations of source and access control | . In this article, you will learn about the main settings for configuring secure and reliable access to the data in CluedIn—source control and access control. The operations that users can perform with data depend on the roles assigned to the users. For more information, see Roles. Both source control and access control settings are located in Administration &gt; Settings. This is the starting point where you specify if you want to restrict access to the data in CluedIn. By restricting access, we mean turning on the toggle under Source Control and Access Control. Exception in data access settings . The table below contains the combination of section-claim-access level that grants users access to data regardless of whether the source control and access control are turned on or not. | Section | Claim | Minimal access level | . | Admin | Data Management | Informed | . | Management | Deduplication Project Management | Informed | . | Management | Deduplication Review | Informed | . The reason for this exception is to allow users working on deduplication, clean, or hierarchy projects to view all data in order to properly perform their tasks. Imagine processing a group of duplicates where you don’t have access to some of the conflicting properties; in this case, you won’t be able to fix conflicts at all. ",
    "url": "/administration/user-access/data-access#on-this-page",
    
    "relUrl": "/administration/user-access/data-access#on-this-page"
  },"522": {
    "doc": "Data access",
    "title": "Source control",
    "content": "With source control, you can manage access to data from a specific source—a data source, a manual data entry project, an integration, and an enricher. Every source has the Permissions tab with a list of users or roles who have access the records produced from that source. Initially, only the user who created the source has access to the records produced from it. However, you can give permission to the source to other users or roles. If you select a role, then all users who have that role will be granted permission to the source. To give permission to the source . | Open the needed source, and then go to the Permissions tab. | Select Add &gt; Add Users or Add Roles. | Select the checkboxes next to the users or roles who will be granted permission to the source. | In the upper-right corner, select Grant Access. | . When the users or roles appear on the Permissions tab of the source, they are granted access to the records produced from that source. However, if the access control is enabled, those users or roles must also be added to the access control policies in order to view the records. ",
    "url": "/administration/user-access/data-access#source-control",
    
    "relUrl": "/administration/user-access/data-access#source-control"
  },"523": {
    "doc": "Data access",
    "title": "Access control",
    "content": "With access control, you can manage access to entire golden records or specific vocabulary keys. To do that, you create access control policies consisting of the following: . | A filter that defines the golden records to which the policy will be applied. | Users or roles who will be granted access to golden records. | Vocabularies or vocabulary keys to which the users or roles will be granted access. | . Learn how to create an access control policy in the dedicated article. ",
    "url": "/administration/user-access/data-access#access-control",
    
    "relUrl": "/administration/user-access/data-access#access-control"
  },"524": {
    "doc": "Data access",
    "title": "Combinations of source and access control",
    "content": "Since both access control and source control regulate access to data, it is important to understand how they work together. Both source control and access control are disabled . When both source control and access control are disabled, any user with any role can view all data in CluedIn without any restrictions. If you have sensitive data that requires protection, we recommend enabling source control and/or access control to ensure secure access. Both source control and access control are enabled . When both source control and access control are enabled, access to data for all users and roles is restricted according to source permissions and access control policies. To view specific golden records, the user has to meet the following requirements: . | The user or the role assigned to the user should be added to the Permissions tab of the source from which golden records were produced. | The user or the role assigned to the user should be added to the Members section of the access control policy rule that regulates access to golden records. | . Only when both requirements are met, the user can view specific golden records. If either requirement is not met, or if no requirements are met at all, the user will not be able to view golden records. For example, if the user has permissions to the source of golden records but is not included in the access control policy that regulates access to such golden records, then the user cannot view the golden records. Similarly to the previous example, if the user is included in the access control policy that regulates access to golden records but does not have permissions to the source of such golden records, then the user cannot view the golden records. For reliable and secure access to golden records in CluedIn, you have to configure both source permissions and access control policies. One of the data access settings is enabled and the other is disabled . When one of the data access settings is enabled and the other is disabled, access to data is restricted for all users and/or roles according to the enabled data access setting. For example, when source control is enabled and access control is disabled, access to data for all users and/or roles is restricted according to source permissions. Thus, users and/or roles can view golden records only if they are added to the Permissions tab of the source from which such golden records were produced. On the contrary, when source control is disabled and access control is enabled, access to data for all users and roles is restricted according to access control policies. Thus, users and/or roles can view golden records only if they are added to the access control policies that regulate access to such golden records. ",
    "url": "/administration/user-access/data-access#combinations-of-source-and-access-control",
    
    "relUrl": "/administration/user-access/data-access#combinations-of-source-and-access-control"
  },"525": {
    "doc": "Data access",
    "title": "Data access",
    "content": " ",
    "url": "/administration/user-access/data-access",
    
    "relUrl": "/administration/user-access/data-access"
  },"526": {
    "doc": "Data life cycle",
    "title": "On this page",
    "content": ". | Data life cycle overview | From record to clue | From clue to golden record | . In this article, you will learn about the stages your data goes through to become a golden record. Understanding this process helps you assess the accuracy, reliability, and overall quality of your data, enabling better decision-making and more efficient usage of resources within the system. ",
    "url": "/key-terms-and-features/data-life-cycle#on-this-page",
    
    "relUrl": "/key-terms-and-features/data-life-cycle#on-this-page"
  },"527": {
    "doc": "Data life cycle",
    "title": "Data life cycle overview",
    "content": "When you ingest the data into CluedIn—either from a file, an ingestion endpoint, a database, or an integration—it enters the first stage of the life cycle. A record represents the data in its basic, raw format as it was in the source system. When you create the mapping, the record becomes a clue. It is an object model that CluedIn generates for your records. Essentially, mapping provides a semantic layer that allows CluedIn to understand the nature of your data. For more information, see the Clue reference article. When you process the data set, the clue becomes a golden record. It is a consolidated representation of a data subject, derived from multiple sources. The following diagram illustrates the transition from an ingested record to a golden record, including various actions that take place along the way. For more information about the concept of data life cycle in CluedIn, download this file. ",
    "url": "/key-terms-and-features/data-life-cycle#data-life-cycle-overview",
    
    "relUrl": "/key-terms-and-features/data-life-cycle#data-life-cycle-overview"
  },"528": {
    "doc": "Data life cycle",
    "title": "From record to clue",
    "content": "The goal of mapping is to create the clues in the format that you set up. After you create the mapping, you can modify or improve the records by using the following options: . | Property rules – to improve the quality of mapped records by normalizing and transforming property values. | Pre-process rules – to apply changes to mapped records. For example, add tags or aliases to the records or send the records to quarantine. | Advanced mapping code – to introduce changes to the clues programmatically. You can perform similar actions as with property and pre-process rules but you gain greater flexibility to apply complex conditions. | . The clues reflect all changes that you introduce to the mapped records. ",
    "url": "/key-terms-and-features/data-life-cycle#from-record-to-clue",
    
    "relUrl": "/key-terms-and-features/data-life-cycle#from-record-to-clue"
  },"529": {
    "doc": "Data life cycle",
    "title": "From clue to golden record",
    "content": "To become a golden record, the clue has to go through processing, which consists of two steps: data part processing and golden record processing. On this stage, the actions are applied across all data sources in CluedIn. This means that the system analyzes the existing golden records and determines whether the current record should create a new golden record or become a part of another golden record. During data part processing, CluedIn analyzes the clue and turns it into a data part by applying the semantic layer. One clue always produces one data part. Often, data parts have the same vocabulary keys as the corresponding clues. However, there might be cases when the vocabulary keys are different due to vocabulary key mapping in the data catalogue. The data part processing step involves the following actions: . | Applying data part rules – to modify the values in data parts that come from different sources. Data part rules are mostly used for normalization and transformation of values on the vocabulary key level. | Triggering the enrichment – to improve the clues by providing additional details from external services. | Merging identical clues by identifiers – to reduce the number of duplicates in the system by merging clues that have identical primary identifiers or additional identifiers. For more information, see Identifiers. | . During golden record processing, CluedIn analyzed a data part and determines whether it will produce a new golden record or aggregate into the existing golden record. During this step, the following actions take place: . | Establishing relations between golden records – to indicate how golden records are related or interact with each other. | Applying survivorship rules – to determine which values from the data parts contribute to the golden record. | Producing the golden record – to create a new golden record from the data part or aggregate the data part into the existing golden record. | Applying golden record rules – to facilitate easy identification and retrieval of golden records within the system (for example, by adding tags). For more information about rules, see Rule types. | . The golden records can then be cleaned, deduplicated, and streamed. As a result, you’ll get accurate and reliable master data that you can use for collaboration, reporting, decision-making, and optimizing your business processes.​ . ",
    "url": "/key-terms-and-features/data-life-cycle#from-clue-to-golden-record",
    
    "relUrl": "/key-terms-and-features/data-life-cycle#from-clue-to-golden-record"
  },"530": {
    "doc": "Data life cycle",
    "title": "Data life cycle",
    "content": " ",
    "url": "/key-terms-and-features/data-life-cycle",
    
    "relUrl": "/key-terms-and-features/data-life-cycle"
  },"531": {
    "doc": "Disaster recovery plan",
    "title": "On this page",
    "content": ". | Default backup | Geo-redundant backup | Geo-redundant backup with cold switch-over | Geo-redundant backup with live switch-over | . This article provides a high-level overview of the available disaster recovery options for the CluedIn PaaS instance. The purpose of this article is to help you understand the available disaster recovery options so you can choose the one that best meets your needs. Depending on your agreement with CluedIn and your IT policy, the disaster recovery option that you choose can be managed either by CluedIn or by yourself. If you need to customize the selected option, additional costs may occur on the services side. In case of any questions, contact our support team at support@cluedin.com. To begin with, it is important to emphasize that each CluedIn PaaS instance is fully isolated, both in terms of clusters and networks. This means that each instance operates within its own dedicated Azure Kubernetes Service (AKS) cluster located in the specific Azure region. Keep in mind that your CluedIn cluster is set up with high availability (HA) by default, ensuring it remains operational even if some components fail. This means that critical pods are replicated—if one pod fails, the other one is ready to take over. These critical pods include cluedin-server-processing, cluedin-datasource-processing, cluedin-gql, cluedin-server, and cluedin-ui. So, you don’t need to implement high availability on top of the cluster. CluedIn PaaS uses the standard tier for AKS cluster management. This means that AKS cluster is guaranteed to be operational and available more than 99% of time. For more information about uptime SLA for AKS cluster management, see Microsoft documentation. ",
    "url": "/kb/disaster-recovery-plan#on-this-page",
    
    "relUrl": "/kb/disaster-recovery-plan#on-this-page"
  },"532": {
    "doc": "Disaster recovery plan",
    "title": "Default backup",
    "content": "The default backup is a disaster recovery strategy where your CluedIn instance and backup vault are running in the same Azure region. In the default backup scenario, we take incremental snapshots of your CluedIn instance every 4 hours. These snapshots are stored in a backup vault within the same Azure region. Additionally, we take a full snapshot of your CluedIn instance once a day, at night. Pros: . | Cost efficiency – you pay only for one CluedIn instance and a backup vault in the Azure region of your choice. | . Cons: . | No geo-redundancy – you won’t be able to use CluedIn or access the backup during a regional outage or other disaster. | No live switch-over – you won’t be able to use CluedIn or access the backup during a regional outage or other disaster. | . Recovery time: . | Approximately 4 hours, depending on Azure quotas in the region of your CluedIn installation. This is the estimated time to have a fully operational CluedIn instance with the latest backup. | . ",
    "url": "/kb/disaster-recovery-plan#default-backup",
    
    "relUrl": "/kb/disaster-recovery-plan#default-backup"
  },"533": {
    "doc": "Disaster recovery plan",
    "title": "Geo-redundant backup",
    "content": "The geo-redundant backup is a disaster recovery strategy where your CluedIn instance and a backup vault are running in the primary Azure region, and an additional backup vault is running in the secondary Azure region. In this scenario, you get access to the backup vault in the secondary Azure region, unlike the default backup scenario. With access to this backup, you can restore it in a different Azure region. Pros: . | Cost efficiency – you pay only for one CluedIn instance and a backup vault in the primary Azure region and for a backup vault in the secondary region. | Geo-redundant backup – you are protected against outages that affect backup vault in the primary region. | . Cons: . | No live failover – you will need to set up a new CluedIn instance in a different Azure region. | . Recovery time: . | Approximately 4 hours, depending on Azure quotas in the region of your CluedIn installation. This is the estimated time to have a fully operational CluedIn instance with the latest backup. However, generally, AKS is back online even before the backup is restored. | . ",
    "url": "/kb/disaster-recovery-plan#geo-redundant-backup",
    
    "relUrl": "/kb/disaster-recovery-plan#geo-redundant-backup"
  },"534": {
    "doc": "Disaster recovery plan",
    "title": "Geo-redundant backup with cold switch-over",
    "content": "The geo-redundant backup with cold switch-over is a disaster recovery strategy where your CluedIn instance and a backup vault are running in the primary Azure region, and an additional backup vault and inactive CluedIn instance are running in the secondary Azure region. In this scenario, your CluedIn instance in the secondary Azure region is configured but remains inactive. If your CluedIn instance in the primary Azure region fails, then the instance in the secondary region will be activated using the backup vault in its own region. Pros: . | Geo-redundant backup – you are protected against outages that affect backup vault in the primary region. | Ready to switch over – you can switch over to another CluedIn instance in case of regional outage or other disaster in the primary Azure region. | . Cons: . | No live failover – switching to the CluedIn instance in the secondary Azure region might take some time. | Increased cost – you will incur more expenses due to the need for an additional cluster. | . Recovery time: . | Approximately 1 hour. This is the estimated time to have a fully operational CluedIn instance with the latest backup. You don’t need to worry about Azure quotas since you already have the reserved machines, ready to take over as soon as something happens. However, generally, by the time the secondary CluedIn instance takes over, the primary CluedIn instance has already recovered. | . ",
    "url": "/kb/disaster-recovery-plan#geo-redundant-backup-with-cold-switch-over",
    
    "relUrl": "/kb/disaster-recovery-plan#geo-redundant-backup-with-cold-switch-over"
  },"535": {
    "doc": "Disaster recovery plan",
    "title": "Geo-redundant backup with live switch-over",
    "content": "The geo-redundant backup with live switch-over is a disaster recovery strategy where your CluedIn instance and a backup vault are running in the primary Azure region, and an additional CluedIn instance and backup vault are running in the secondary Azure region. In this scenario, if your CluedIn instance in the primary Azure region fails, then your CluedIn instance in the secondary Azure region will take over the operation within minutes. Pros: . | Geo-redundant backup – you are protected against outages that affect backup vault in the primary region. | Live switch over – you can switch over to another CluedIn instance immediately in case of regional outage or other disaster in the primary Azure region. | . Cons: . | Increased cost and complexity – the price of infrastructure might be three times higher than in the default backup scenario due to some requirements on the database level. | . Recovery time: . | Approximately 5 minutes. The instance with the latest backup becomes available immediately once activated. This is the estimated time to have a fully operational CluedIn instance with the latest backup. However, generally, by the time the secondary CluedIn instance takes over, the primary CluedIn instance has already recovered. | . ",
    "url": "/kb/disaster-recovery-plan#geo-redundant-backup-with-live-switch-over",
    
    "relUrl": "/kb/disaster-recovery-plan#geo-redundant-backup-with-live-switch-over"
  },"536": {
    "doc": "Disaster recovery plan",
    "title": "Disaster recovery plan",
    "content": " ",
    "url": "/kb/disaster-recovery-plan",
    
    "relUrl": "/kb/disaster-recovery-plan"
  },"537": {
    "doc": "Disaster recovery runbook",
    "title": "On this page",
    "content": ". | Automation account | Input parameters | Process | . The disaster recovery runbook can be triggered manually or on a set schedule. It is responsible for orchestrating the backup &gt; copy &gt; restore process. This runbook is a PowerShell script, which CluedIn will provide as needed. Prerequisites . | An active CluedIn cluster with a valid license (and/or a passive CluedIn cluster) | The runbook script | An automation account | The backup runbook | The copy runbook | The restore runbook | A storage account | Sufficient permissions | . ",
    "url": "/paas-operations/automation/disaster-recovery-runbook#on-this-page",
    
    "relUrl": "/paas-operations/automation/disaster-recovery-runbook#on-this-page"
  },"538": {
    "doc": "Disaster recovery runbook",
    "title": "Automation account",
    "content": "An automation account must be provided. The runbook will be installed into the the automation account. The three dependent runbooks must already be installed. Typically, the runbook should be scheduled to run once a day outside of office hours. ",
    "url": "/paas-operations/automation/disaster-recovery-runbook#automation-account",
    
    "relUrl": "/paas-operations/automation/disaster-recovery-runbook#automation-account"
  },"539": {
    "doc": "Disaster recovery runbook",
    "title": "Input parameters",
    "content": "| Parameter | Default | Description | . | CustomerName | required | Name of customer | . | Subscription | required | ID of the Azure subscription | . | SourceResourceGroup | required | Name of resource group where source AKS cluster is located | . | TargetResourceGroup | required` | Name of resource group where target AKS cluster is located | . | TargetStorageAccount | required | Name of target storage account | . | SourceSnapshotResourceGroup | required | Name of resource group containing source snapshots | . | TargetSnapshotResourceGroup | required | Name of resource group containing copied snapshots | . | ScaledownDR | true | Scales down a DR cluster after a successful restoer | . ",
    "url": "/paas-operations/automation/disaster-recovery-runbook#input-parameters",
    
    "relUrl": "/paas-operations/automation/disaster-recovery-runbook#input-parameters"
  },"540": {
    "doc": "Disaster recovery runbook",
    "title": "Process",
    "content": ". Scaling down of the DR environment following a successful restore is optional. ",
    "url": "/paas-operations/automation/disaster-recovery-runbook#process",
    
    "relUrl": "/paas-operations/automation/disaster-recovery-runbook#process"
  },"541": {
    "doc": "Disaster recovery runbook",
    "title": "Disaster recovery runbook",
    "content": " ",
    "url": "/paas-operations/automation/disaster-recovery-runbook",
    
    "relUrl": "/paas-operations/automation/disaster-recovery-runbook"
  },"542": {
    "doc": "Entity page layout",
    "title": "On this page",
    "content": ". | Overview of entity page layouts | Layout details page | Create a layout | Manage a layout | . An entity page layout is the way in which information about a golden record is arranged on the golden record overview page. In this article, you will learn about the types of layouts and how they are applied to golden records. ",
    "url": "/administration/entity-page-layout#on-this-page",
    
    "relUrl": "/administration/entity-page-layout#on-this-page"
  },"543": {
    "doc": "Entity page layout",
    "title": "Overview of entity page layouts",
    "content": "The entity page layout is assigned to the business domain. This ensures that all golden records belonging to that business domain consistently display relevant information on the golden record overview page. All layouts are stored in Administration &gt; Entity page layouts. CluedIn contains several built-in entity page layouts: . | Default layout . | Discussion layout . | Document layout . | Organization layout . | Person layout . | . You cannot edit built-in layouts, but you can create custom layouts. When you create a new business domain while creating mapping, the default layout is assigned to such business domain. You change the layout later by editing the business domain. The layouts consist of the following elements: . | Nav – a container for the following information about a golden record: entity properties, core vocabularies, source vocabularies, information sources, sources, and relations. | Main – a container for the following information about a golden record: quality metrics and suggested searches. | . The following screenshot displays the arrangement of nav (a) and main (b) containers on the golden record overview page. ",
    "url": "/administration/entity-page-layout#overview-of-entity-page-layouts",
    
    "relUrl": "/administration/entity-page-layout#overview-of-entity-page-layouts"
  },"544": {
    "doc": "Entity page layout",
    "title": "Layout details page",
    "content": "On the layout details page, you can view relevant information about the entity page layout and take other actions to manage the layout. Configuration . This tab contains general information about the layout, including: . | Layout template that defines the arrangement of elements on the golden record overview page (the placement of main and nav containers). | Elements that are displayed on the golden record overview page (all or custom core and non-core vocabulary keys, suggested search, and quality metrics). | . Business domains . This tab contains all business domains that use the current layout. You can also add more business domains to which the layout will be assigned. ",
    "url": "/administration/entity-page-layout#layout-details-page",
    
    "relUrl": "/administration/entity-page-layout#layout-details-page"
  },"545": {
    "doc": "Entity page layout",
    "title": "Create a layout",
    "content": "If built-in layouts are not sufficient for you, you create your own layout. To create a layout . | On the navigation pane, go to Administration &gt; Entity page layouts. | Select Create layout. | Enter the display name of the layout. | Choose the layout template for organizing the information on the golden record overview page: simple, two column, header, or two column inverse. | Select Create. Alternatively, you can create a layout from the business domain page. You can do it when editing the business domain. After the layout is created, you can assign it to the business domain. As a result, the information on the Overview tab of all golden records belonging to the business domain will be arranged according to the selected layout. | . ",
    "url": "/administration/entity-page-layout#create-a-layout",
    
    "relUrl": "/administration/entity-page-layout#create-a-layout"
  },"546": {
    "doc": "Entity page layout",
    "title": "Manage a layout",
    "content": "You can change the layout configuration and choose the elements that should be displayed on the golden record overview page. Note that you cannot delete the layout. To edit a layout . | On the navigation pane, go to Administration &gt; Entity page layouts. | Open the layout that you want to edit, and then select Edit. The layout template opens in edit mode and contains the following sections: . | Display name – you can change the layout name. | Select layout template – you can choose another layout template. | Configure layout – you can choose what elements you want to be displayed on the golden record overview page. By default, all core vocabulary keys and non-core vocabulary keys that the golden record contains are displayed on the golden record overview page, together with quality metrics and suggested searches. You can select custom core and non-core vocabulary keys or disable the quality metrics and suggested searches. | . | To add custom core or non-core vocabulary keys, in the corresponding section, choose the Custom option, and then do the following: . | Select Add vocabulary keys. | Find and select the vocabulary keys to be displayed on the golden record overview page. | Select Add. | . The vocabulary keys that you added are displayed on the page. | Make other changes as needed. | After completing your edits to the layout, select Save. The changes will be automatically applied to the overview pages of all golden records that belong to the business domain associated with the current layout. | . ",
    "url": "/administration/entity-page-layout#manage-a-layout",
    
    "relUrl": "/administration/entity-page-layout#manage-a-layout"
  },"547": {
    "doc": "Entity page layout",
    "title": "Entity page layout",
    "content": " ",
    "url": "/administration/entity-page-layout",
    
    "relUrl": "/administration/entity-page-layout"
  },"548": {
    "doc": "How to fix address data",
    "title": "On this page",
    "content": ". | Fixing address data using Libpostal enricher | Fixing address data using Google Maps enricher | Fixing address data using AI-based rule action | . In this article, you will learn how to fix address data using various methods: . | Enrichment via Libpostal – use this method to parse and standardize the address. | Enrichment via Google Maps – use this method to enhance your golden records with precise address details and comprehensive business information. This method requires a valid Google Maps API key. | Enrichment via Azure Maps – use this method to enhance your golden records with precise address details and comprehensive geographic information. This method requires a valid Azure Maps API key. The Azure Maps enricher is not yet available as an open-source package. If you would like to use it, reach out to CluedIn support at support@cluedin.com. The results of enrichment using Google Maps and Azure Maps are similar. Depending on the API key you have, use the corresponding method to enhance your golden records. | Validation and enhancement via AI-based rules – use this method to enrich any field in your golden records by leveraging AI-driven text prompts. For example, you can find an address based on the company name or retrieve the full postal code using the company name and address. | . ",
    "url": "/kb/how-to-fix-address-data#on-this-page",
    
    "relUrl": "/kb/how-to-fix-address-data#on-this-page"
  },"549": {
    "doc": "How to fix address data",
    "title": "Fixing address data using Libpostal enricher",
    "content": "The Libpostal enricher is particularly useful when you need to parse, standardize, and normalize addresses from various formats. It is a powerful tool that can break down a single address string into its components, such as street, city, and postal code. This helps in standardizing addresses and improving data quality. The Libpostal enricher has its pros and cons, which can help you decide if it will solve your use case. Pros of Libpostal: . | It is open source and free to use, with no licensing costs. | It is great at parsing and address standardization. | It supports multiple languages and address formats. | . Cons of Libpostal: . | It lacks advanced enrichment features, thereby returning fewer details. | It may experience performance issues when processing large volumes of data. | . Example of using Libpostal . Let’s consider an example of a CluedIn golden record that contains the address property. This property includes street address details all in one value. To ensure data accuracy and usability, we will use the Libpostal enricher to validate and parse this address. To configure the Libpostal enricher, specify the business domain of golden records that you want to enrich and the vocabulary key that contains the initial address. When the enricher is configured, trigger the enrichment for golden records. You can trigger the enrichment for all golden records of the specific business domain using the GraphQL tool. Alternatively, you can trigger the enrichment manually for each golden record. To do this, on the golden record page, select More &gt; Trigger external enrichment. As a result, the CluedIn golden record now contains parsed address with such components as house, house number, level, road, and unit. ",
    "url": "/kb/how-to-fix-address-data#fixing-address-data-using-libpostal-enricher",
    
    "relUrl": "/kb/how-to-fix-address-data#fixing-address-data-using-libpostal-enricher"
  },"550": {
    "doc": "How to fix address data",
    "title": "Fixing address data using Google Maps enricher",
    "content": "The Google Maps enricher can help you add valuable context about a specific place, such as its name, address, phone number, website, user reviews, and ratings. To use this enricher, you need to have a valid Google Maps API key. The Google Maps enricher has its pros and cons, which can help you decide if it will solve your use case. Pros of Google Maps: . | It provides a comprehensive location data—extensive address validation, geocoding, and enrichment capabilities. | It provides a global coverage and supports address validation in multiple countries and different languages. | . Cons of Google Maps: . | To use the Google Maps enricher, you need an API key from the Google Maps Platform. | Google Maps API keys are insecure if unrestricted. | High usage of Google Maps can become expensive. | . Example of using Google Maps . Let’s consider an example of the same CluedIn golden record. To add more context about the company, we will use the Google Maps enricher to retrieve additional information. We are using the same golden record to illustrate the difference between two enrichers. To configure the Google Maps enricher, specify the following details: . | API Key for retrieving information from the Google Maps Platform. | Accepted Business Domain (previously entity type) to define which golden records will be enriched. | Vocabulary Key used to control whether it should be enriched to indicate if the golden record should be enriched. If the value of the vocabulary key is true, then the golden record will be enriched. Otherwise, the golden record will not be enriched. | Organization Name Vocab Key to define company names that will be used for searching the Google Maps Platform. | Organization Address Vocab Key to define company addresses that will be used for searching the Google Maps Platform. | . When the enricher is configured, trigger the enrichment for golden records. You can trigger the enrichment for all golden records of the specific business domain using the GraphQL tool, or you can trigger the enrichment manually for each golden record. Alternatively, you can trigger the enrichment manually for each golden record. To do this, on the golden record page, select More &gt; Trigger external enrichment. As a result, the CluedIn golden record now contains a variety of company details from Google Maps, such as administrative area level 1 and 2, business status, country code, and more. ",
    "url": "/kb/how-to-fix-address-data#fixing-address-data-using-google-maps-enricher",
    
    "relUrl": "/kb/how-to-fix-address-data#fixing-address-data-using-google-maps-enricher"
  },"551": {
    "doc": "How to fix address data",
    "title": "Fixing address data using AI-based rule action",
    "content": "The AI-based rule action can be used to enhance golden records with additional information based on their existing properties. The AI-based rule action has its pros and cons, which can help you decide if it will solve your use case. Pros of AI-based rule action: . | No need to prepare or configure third-party enrichers. | AI-based rule action leverages AI prompts for free text queries. | It provides full control over what to parse, standardize, transform, and use from the results within the same rule and prompt. | . Cons of AI-based rule action: . | It requires access to additional resources, such as Azure OpenAI Service for access to OpenAI’s models. | Processing large volumes of prompts can be expensive. | . Example of AI-based rule action . We’ll use a Google golden record to illustrate how to validate the address and get full postal address based on the existing address data.   . To validate the address, create a data part rule and select an action to add value with CluedIn AI. In the action, do the following: . | Select the address vocabulary key that AI engine will use as an input. | Enter the prompt for the AI engine explaining what to do with the address. For example, Check the address in {vocabulary:trainingcompany.address} and company name in {vocabulary:trainingcompany.name} and return the full postal address only. | Enter the name of Azure OpenAI Service model—gpt-35-turbo-instruct. | Select the field for AI-generated full postal address. | . Next, save, activate and re-process the rule. To verify that the rule has been applied, find and open the Google golden record. As a result, new field—AI Address—containing full postal address has been added. ",
    "url": "/kb/how-to-fix-address-data#fixing-address-data-using-ai-based-rule-action",
    
    "relUrl": "/kb/how-to-fix-address-data#fixing-address-data-using-ai-based-rule-action"
  },"552": {
    "doc": "How to fix address data",
    "title": "How to fix address data",
    "content": " ",
    "url": "/kb/how-to-fix-address-data",
    
    "relUrl": "/kb/how-to-fix-address-data"
  },"553": {
    "doc": "Get access to CluedIn Copilot",
    "title": "Get access to CluedIn Copilot",
    "content": "In this article, you will learn what you need to do to make the AI assistant available in CluedIn. For Azure Administrators and CluedIn Organization Administrators . CluedIn does not provide an LLM or API token to interact with Copilot. So, you need to have an API token to interact with one of the available Azure OpenAI models. We recommend using the GPT-3.5-16K token. You cannot use different models for different skills. To make the AI assistant available in CluedIn, you need to fulfill several prerequisites: . | In your Azure OpenAI resource, make sure that you have created a deployment that uses the gpt-35-turbo model. | Enable development features for your CluedIn instance. Depending on the type of CluedIn installation, do one of the following: . | If you have CluedIn AMA, contact CluedIn support to get this feature in your system. | If you are running the local installation of CluedIn, in the .env file for your environment, set ENABLE_DEVELOPMENT_FEATURES to true, and then stop and start your CluedIn instance. | . | In CluedIn, go to Administration &gt; Feature Flags, and then enable the AI Copilot feature. | In CluedIn, go to Administration &gt; Settings. Scroll down to the Copilot section and complete the following fields: . | Base Url – you can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Keys and Endpoint, and then get the value from the Endpoint field. Alternatively, you can find this value in Azure OpenAI Studio by going to Playground &gt; View code. | API Key – you can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Keys and Endpoint. You can use either KEY 1 or KEY 2. | Deployment – specify the custom name you chose for your deployment when you deployed a model. In Azure portal, go to Resource Management &gt; Deployments. Alternatively, you can find this value in Azure OpenAI Studio by going to Management &gt; Deployments. | Deployment Max Tokens – specify the number of tokens that correspond to the model used. The minimum number should be 16384. | Minimum Skill Development Stage – specify which skills—alpha, beta, or production—you want to have available in CluedIn. Currently, almost all skills are alpha. For more information about the required variables, see Microsoft documentation. | . | . ",
    "url": "/microsoft-integration/copilot-integration/get-access-to-copilot",
    
    "relUrl": "/microsoft-integration/copilot-integration/get-access-to-copilot"
  },"554": {
    "doc": "How to get CluedIn",
    "title": "On this page",
    "content": ". | Onboarding path for CluedIn SaaS . | CluedIn SaaS pay-as-you-go | CluedIn SaaS committed deal | . | Onboarding path for CluedIn PaaS . | CluedIn PaaS pay-as-you-go | CluedIn PaaS committed deal | . | Next steps | . CluedIn makes doing business easy by giving you flexibility to adopt our platform at your own pace. We are offering 2 cloud models of CluedIn: . | SaaS (software as a service) – allows you to use CluedIn on a subscription basis. Instead of installing CluedIn in your own Azure IT infrastructure, you can access it in an isolated environment through a web browser. As a software provider, we are responsible for hosting, managing updates, and ensuring security. For details about the available service levels and support options, see our CluedIn SaaS SLA. | PaaS (platform as a service) – allows you to install and manage CluedIn in your own Azure IT infrastructure, thus ensuring that your data stays in your own environment. As a software provider, we are managing updates and ensuring security. However, hosting the platform is your responsibility, giving you flexibility and control over your infrastructure while benefiting from our managed services. | . For more details about the difference between CluedIn installation options, see Installation. Where can I find information about pricing models? . | To learn about our pay-as-you pricing model, see Pricing. | To learn about our committed deal pricing model, book a meeting with our sales team representative here. | . Can I change my pricing model? . Yes, you can change your pricing model from pay-as-you-go to committed deal and vice versa. However, a change from committed deal to pay-as-you-go is governed by the terms of the contract. If you decide to change the pricing model, contact our support team by email support@cluedin.com. Can I move from SaaS to PaaS or vice versa? . Yes, but there is a migration fee for this process. If you decide to change the cloud service model, contact our support team by email support@cluedin.com. ",
    "url": "/get-cluedin#on-this-page",
    
    "relUrl": "/get-cluedin#on-this-page"
  },"555": {
    "doc": "How to get CluedIn",
    "title": "Onboarding path for CluedIn SaaS",
    "content": "By onboarding path, we mean the process of purchasing and installing CluedIn. After completing the onboarding steps, you’ll get a link to your CluedIn instance and you can start using the system. CluedIn SaaS pay-as-you-go . This is the easiest and quickest way to start with CluedIn. You don’t need an IT team to install and start using CluedIn. Who can start CluedIn SaaS pay-as-you-go onboarding? . Depending on whether you have the authority to create a pay-as-you-go Azure account with your business credit card, there are two options: . | If yes, you and your team can drive the CluedIn project in your company on your own. | If no, you need to meet several Azure requirements. Generally, you would need to find a person in your company who can make you a contributor for an Azure subscription where CluedIn will be stored. | . What are the steps of CluedIn SaaS pay-as-you-go onboarding? . | Make sure you meet all of the prerequisites for SaaS installation. See our Azure requirements guide. | Complete the subscription process. See our SaaS installation guide. | Receive an email with the link to your CluedIn instance and get started. | . CluedIn SaaS committed deal . This option gives you access to our experts and a lower price than pay-as-you-go. You do not need an IT team to install and start using CluedIn. Who can start CluedIn SaaS committed deal onboarding? . It depends on your procurement and legal processes, as well as on who your final decision maker is. CluedIn ships with Microsoft Standard Contract, and if you are a Microsoft user, your legal department has probably already approved this Standard Contract. We are fast on getting the committed deal through, so the timeline for completing the onboarding depends on your internal processes. What are the steps of CluedIn SaaS committed deal onboarding? . | Get in touch with our sales team here. | Discuss your use cases and draft the contract with us. | Sign the contract. | Make sure you meet all of the prerequisites for SaaS installation. See our Azure requirements guide. | Complete the subscription process using the license key that you receive from us. See our SaaS installation guide. | Receive an email with the link to your CluedIn instance and get started. | . ",
    "url": "/get-cluedin#onboarding-path-for-cluedin-saas",
    
    "relUrl": "/get-cluedin#onboarding-path-for-cluedin-saas"
  },"556": {
    "doc": "How to get CluedIn",
    "title": "Onboarding path for CluedIn PaaS",
    "content": "By onboarding path, we mean the process of purchasing and installing CluedIn. After completing onboarding steps, you’ll have CluedIn running in your own cloud environment. CluedIn PaaS pay-as-you-go . This model gives you the possibility to test CluedIn in your infrastructure with 10K records for free. You’ll get full control of CluedIn as you’ll be running the platform in your own environment. What are the steps of CluedIn PaaS pay-as-you-go onboarding? . | Make business decisions: get acquainted with billing, review terms and conditions, choose Azure Administrator on your side to perform the installation. See our Business decisions guide. | Perform pre-installation checks. See our Pre-installation guide. | Perform the installation process. See our Installation guide. | Customize CluedIn according to your company’s needs. See our Post-installation guide. | . CluedIn PaaS committed deal . This option gives you full control of CluedIn as you’ll be running the platform in your own environment. Also, it has a lower price than pay-as-you-go. What are the steps of CluedIn PaaS committed deal onboarding? . | Get in touch with our sales team here. | Discuss your use cases and draft the contract with us. | Sign the contract. | Make business decisions: choose Azure Administrator on your side to perform the installation. See our Business decisions guide. | Perform pre-installation checks. See our Pre-installation guide. | Perform the installation process. See our Installation guide. | Customize CluedIn according to your company’s needs. See our Post-installation guide. | . ",
    "url": "/get-cluedin#onboarding-path-for-cluedin-paas",
    
    "relUrl": "/get-cluedin#onboarding-path-for-cluedin-paas"
  },"557": {
    "doc": "How to get CluedIn",
    "title": "Next steps",
    "content": "Now that you are familiar with our onboarding process, you can choose the pricing and cloud service model that are suitable for you. Click the link in the table to start with the option of your choice. |   | Pay-as-you-go | Committed deal | . | SaaS | Send SaaS request | Contact sales team | . | PaaS | Send PaaS request | Contact sales team | . ",
    "url": "/get-cluedin#next-steps",
    
    "relUrl": "/get-cluedin#next-steps"
  },"558": {
    "doc": "How to get CluedIn",
    "title": "How to get CluedIn",
    "content": " ",
    "url": "/get-cluedin",
    
    "relUrl": "/get-cluedin"
  },"559": {
    "doc": "CluedIn for Developers",
    "title": "CluedIn for Developers — Build, Integrate, Automate",
    "content": "Audience: Application developers, integration engineers, platform/DevOps engineers Goal: Give developers a practical, end‑to‑end handbook to build against CluedIn: auth, APIs/SDK patterns, ingestion producers, export consumers, webhooks, cleaning/validation-as-code, AI Agents, dedup, logging/audit, testing, CI/CD, and secure operations. Assumptions: Your org uses SSO, keeps config as code, and prefers automated ingestion (streams/batches) over manual file uploads. ",
    "url": "/kb/cluedin-for-developers#cluedin-for-developers--build-integrate-automate",
    
    "relUrl": "/kb/cluedin-for-developers#cluedin-for-developers--build-integrate-automate"
  },"560": {
    "doc": "CluedIn for Developers",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "Access &amp; Tools . | Sign in via SSO; confirm a Developer/Engineer role with needed scopes. | Get a short‑lived token (OAuth client) or scoped PAT for local dev. | Install: HTTP client (curl/Postman), language SDKs, and repo with config‑as‑code. | . Hello, CluedIn . | Call /api/me to test auth. | Create a sandbox Ingestion Endpoint and push a sample record. | Create a staging Export and run an end‑to‑end smoke. | . Guardrails . | Set correlation_id conventions for all requests. | Configure retries with jitter and idempotency keys. | Wire a dead‑letter path and a simple replay tool. | . ",
    "url": "/kb/cluedin-for-developers#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-for-developers#0-your-first-48-hours-checklist"
  },"561": {
    "doc": "CluedIn for Developers",
    "title": "1) Architecture (Developer View)",
    "content": "[Producer] → Ingestion Endpoint → Raw Store → Mapping → Cleaning/Validation → Dedup/Golden → Exports (tables/topics/APIs/files) → [Consumers/Apps/BI] ↓ AI Agents (analysis/suggestions) . You primarily write: . | Producers that push data in (HTTP/Kafka/Batch/CDC). | Consumers that react to Exports or Webhooks. | Jobs or functions that call CluedIn APIs (AI Agents, dedup, policies). | Infra glue (CI/CD, secrets, logging, alerts). | . ",
    "url": "/kb/cluedin-for-developers#1-architecture-developer-view",
    
    "relUrl": "/kb/cluedin-for-developers#1-architecture-developer-view"
  },"562": {
    "doc": "CluedIn for Developers",
    "title": "2) Authentication &amp; Identity",
    "content": "2.1 OAuth 2.0 Client (preferred for services) . POST /oauth/token { \"grant_type\": \"client_credentials\", \"client_id\": \"&lt;ID&gt;\", \"client_secret\": \"&lt;SECRET&gt;\", \"scope\": \"ingest:write export:read policy:read\" } . 2.2 Personal Access Tokens (PAT) (for local dev) . | Create in Admin → API Tokens with minimal scopes and expiry. | Store in a secret manager; never commit to git. | . 2.3 Request Hygiene . Always send: . | Authorization: Bearer &lt;token&gt; | X-Correlation-Id: &lt;uuid&gt; | Content-Type: application/json (for JSON payloads) | . ",
    "url": "/kb/cluedin-for-developers#2-authentication--identity",
    
    "relUrl": "/kb/cluedin-for-developers#2-authentication--identity"
  },"563": {
    "doc": "CluedIn for Developers",
    "title": "3) API Patterns (Paging, Filtering, Errors)",
    "content": "Paging . GET /api/entities?type=Person&amp;limit=200&amp;cursor=&lt;token&gt; → 200 + { \"items\": [...], \"next_cursor\": \"...\" } . Filtering . GET /api/exports/runs?name=warehouse-contacts-v1&amp;since=2025-08-20T00:00:00Z . Common errors (illustrative) . | 400 invalid schema / missing required fields | 401/403 auth/permission issues | 409 conflict (idempotency violation) | 429 rate limit → backoff and retry | 5xx transient → exponential backoff with jitter | . Retry policy (pseudo) . const base = 250; // ms retry = attempt =&gt; Math.min(30_000, base * 2 ** attempt) + rand(0, 200); . ",
    "url": "/kb/cluedin-for-developers#3-api-patterns-paging-filtering-errors",
    
    "relUrl": "/kb/cluedin-for-developers#3-api-patterns-paging-filtering-errors"
  },"564": {
    "doc": "CluedIn for Developers",
    "title": "4) Ingestion Producers",
    "content": "4.1 HTTP/Webhook (JSON lines) . curl -X POST \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"X-Correlation-Id: $(uuidgen)\" \\ -H \"Content-Type: application/json\" \\ -d '{\"source\":\"crm-contacts\",\"payload\":{\"id\":\"c_123\",\"email\":\"a@example.com\",\"updated_at\":\"2025-08-22T12:00:00Z\"}}' \\ https://&lt;HOST&gt;/api/ingest . Best practices . | Idempotency: include stable id and updated_at. | Batching: prefer small batches (e.g., 1–5k records). | Compression: Content-Encoding: gzip for large payloads. | DLQ: send parse failures to a durable store with replay tooling. | . 4.2 Node.js example . import axios from \"axios\"; const client = axios.create({ baseURL: process.env.CLUE_HOST }); async function ingestContact(c) { const r = await client.post(\"/api/ingest\", { source: \"crm-contacts\", payload: c, }, { headers: { Authorization: `Bearer ${process.env.CLUE_TOKEN}`, \"X-Correlation-Id\": crypto.randomUUID() }, timeout: 10000 }); return r.data; } . 4.3 Python example . import os, uuid, requests, json host = os.environ[\"CLUE_HOST\"]; token = os.environ[\"CLUE_TOKEN\"] payload = {\"source\":\"crm-contacts\",\"payload\":{\"id\":\"c_123\",\"email\":\"a@example.com\"}} r = requests.post(f\"{host}/api/ingest\", json=payload, headers={\"Authorization\":f\"Bearer {token}\",\"X-Correlation-Id\":str(uuid.uuid4())}, timeout=10) r.raise_for_status() print(r.json()) . 4.4 Kafka/Event Hub . | One topic per domain/entity; include schema version in headers. | Ensure producer idempotence and ordered keys (e.g., id). | . 4.5 Batch (S3/Blob/ADLS) . | Land JSON/CSV/Parquet on schedule; register a bucket‑watch endpoint. | Keep manifest files for completeness and replay. | . ",
    "url": "/kb/cluedin-for-developers#4-ingestion-producers",
    
    "relUrl": "/kb/cluedin-for-developers#4-ingestion-producers"
  },"565": {
    "doc": "CluedIn for Developers",
    "title": "5) Mapping, Cleaning, Validation as Code",
    "content": "5.1 Minimal Mapping (pseudo‑YAML) . entity: Person source: \"crm-contacts\" fields: id: $.id email: $.email first_name: $.first_name last_name: $.last_name updated_at: $.updated_at . 5.2 Cleaning Project . project: normalize_contacts schedule: \"0 * * * *\" steps: - name: normalize_email action: set field: email value: lower(trim(email)) - name: e164_phone when: phone is not null action: set field: phone value: to_e164(phone, default_country=\"AU\") . 5.3 Validations . rule: email_regex entity: Person check: { regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" } severity: high on_fail: { action: \"flag\" } . Dev loop: edit YAML → PR → staging export diff → promote. ",
    "url": "/kb/cluedin-for-developers#5-mapping-cleaning-validation-as-code",
    
    "relUrl": "/kb/cluedin-for-developers#5-mapping-cleaning-validation-as-code"
  },"566": {
    "doc": "CluedIn for Developers",
    "title": "6) Exports &amp; Contracts",
    "content": "6.1 Export Config (table) . { \"name\": \"warehouse-contacts-v1\", \"type\": \"sql-table\", \"options\": { \"connection\": \"analytics-warehouse\", \"schema\": \"mdm\", \"table\": \"contacts_v1\", \"mode\": \"upsert\", \"primary_key\": [\"contact_id\"] }, \"mapping\": { \"contact_id\": \"Person.id\", \"email\": \"Person.email\", \"first_name\": \"Person.first_name\", \"last_name\": \"Person.last_name\", \"updated_at\": \"Person.updated_at\" }, \"schedule\": \"0 * * * *\" } . 6.2 Contract (YAML) . name: contacts_v1 primary_key: contact_id delivery: { type: sql-table, schedule: hourly } sla: { freshness_p95_minutes: 60 } compatibility: additive_only . Versioning: breaking changes → _v2; run both until consumers migrate. ",
    "url": "/kb/cluedin-for-developers#6-exports--contracts",
    
    "relUrl": "/kb/cluedin-for-developers#6-exports--contracts"
  },"567": {
    "doc": "CluedIn for Developers",
    "title": "7) Webhooks &amp; Eventing",
    "content": "7.1 Registering a Webhook (pseudo) . POST /api/webhooks { \"name\": \"export-success-teams\", \"events\": [\"export.succeeded\"], \"url\": \"https://example.com/hooks/export\", \"secret\": \"&lt;HMAC_SECRET&gt;\" } . 7.2 Verify Signatures . import crypto from \"crypto\"; function verify(sig, body, secret) { const h = crypto.createHmac(\"sha256\", secret).update(body).digest(\"hex\"); return crypto.timingSafeEqual(Buffer.from(sig,\"hex\"), Buffer.from(h,\"hex\")); } . Retry model: Webhooks are retried on non‑2xx; make handlers idempotent. ",
    "url": "/kb/cluedin-for-developers#7-webhooks--eventing",
    
    "relUrl": "/kb/cluedin-for-developers#7-webhooks--eventing"
  },"568": {
    "doc": "CluedIn for Developers",
    "title": "8) AI Agents (Programmatic Use)",
    "content": "8.1 Run an Analysis . POST /api/ai/agents/run { \"agent\": \"dq-analyzer\", \"target\": { \"entity\": \"Person\" }, \"mode\": \"analysis\", // analysis | suggest | auto_fix (guarded) \"options\": { \"sample\": 10000 } } . 8.2 Retrieve Findings . GET /api/ai/agents/runs/&lt;run_id&gt;/findings → { \"issues\":[{\"field\":\"email\",\"type\":\"invalid\",\"examples\":[...]}, ...] } . Guardrails . | Restrict to masked views for PII. | Treat auto‑fixes as code changes (reviewable, reversible). | . ",
    "url": "/kb/cluedin-for-developers#8-ai-agents-programmatic-use",
    
    "relUrl": "/kb/cluedin-for-developers#8-ai-agents-programmatic-use"
  },"569": {
    "doc": "CluedIn for Developers",
    "title": "9) Dedup APIs",
    "content": "9.1 Create Rules (deterministic first) . rules: - name: exact_email when: lower(email) == lower(other.email) confidence: 0.98 - name: phone_e164 when: e164(phone) == e164(other.phone) confidence: 0.95 auto_approve_threshold: 0.97 queue_threshold: 0.85 . 9.2 Merge/Unmerge (pseudo) . POST /api/dedup/merge { \"entity\":\"Person\", \"ids\":[\"p1\",\"p2\"] } POST /api/dedup/unmerge { \"entity\":\"Person\", \"id\":\"p1\" } . Log decisions; update survivorship config. ",
    "url": "/kb/cluedin-for-developers#9-dedup-apis",
    
    "relUrl": "/kb/cluedin-for-developers#9-dedup-apis"
  },"570": {
    "doc": "CluedIn for Developers",
    "title": "10) Logs, Audit &amp; Observability",
    "content": "10.1 Fetch Logs (illustrative) . GET /api/logs?category=export&amp;name=warehouse-contacts-v1&amp;since=2025-08-23T00:00:00Z . 10.2 Audit Events . | SSO sign‑ins, role grants, token lifecycle, policy changes, export promotions, merges. | . GET /api/audit?action=policy.update&amp;since=2025-08-01 . 10.3 Correlation &amp; Tracing . | Pass a correlation_id end‑to‑end and include it in logs and errors. | Emit metrics: success/failure counts, latency, row counts, DLQ size. | . ",
    "url": "/kb/cluedin-for-developers#10-logs-audit--observability",
    
    "relUrl": "/kb/cluedin-for-developers#10-logs-audit--observability"
  },"571": {
    "doc": "CluedIn for Developers",
    "title": "11) Testing &amp; Local Dev",
    "content": "11.1 Unit Tests . | Validate normalization helpers and schema mappers. | Golden files for tricky encodings, null behavior, and long strings. | . 11.2 Contract Tests . | Assert export schema and SLA (freshness). | . 11.3 Stubs &amp; Mocks . | Spin a mock CluedIn (OpenAPI stub) for local dev. | Record/replay HTTP using tools like vcrpy/nock. | . 11.4 Example PyTest . def test_email_normalization(): assert normalize_email(\" A@Example.COM \") == \"a@example.com\" . ",
    "url": "/kb/cluedin-for-developers#11-testing--local-dev",
    
    "relUrl": "/kb/cluedin-for-developers#11-testing--local-dev"
  },"572": {
    "doc": "CluedIn for Developers",
    "title": "12) CI/CD &amp; Promotion",
    "content": "12.1 GitHub Actions (sketch) . name: cluedin-pipelines on: [push, pull_request] jobs: validate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - run: ./tools/validate-config.sh # lint YAML/JSON - run: pytest -q # unit tests deploy-staging: needs: validate if: github.ref == 'refs/heads/main' steps: - run: ./tools/apply.sh env/test deploy-prod: needs: deploy-staging steps: - run: ./tools/apply.sh env/prod # change window only . 12.2 Release Notes . | Summarize mapping/cleaning/export diffs, risk, rollback, owners, metrics to watch. | . ",
    "url": "/kb/cluedin-for-developers#12-cicd--promotion",
    
    "relUrl": "/kb/cluedin-for-developers#12-cicd--promotion"
  },"573": {
    "doc": "CluedIn for Developers",
    "title": "13) Security Essentials",
    "content": ". | Least privilege scopes for tokens; rotate ≤ 90 days. | Secrets in vaults, never in code or logs. | Mask PII by default in non‑prod and in AI prompts. | Validate webhook signatures and protect endpoints with allowlists. | Log who, what, where (IP), and when for sensitive ops. | . ",
    "url": "/kb/cluedin-for-developers#13-security-essentials",
    
    "relUrl": "/kb/cluedin-for-developers#13-security-essentials"
  },"574": {
    "doc": "CluedIn for Developers",
    "title": "14) Performance &amp; Cost",
    "content": ". | Prefer Parquet for batch; partition by date/time. | Tune batch size and parallelism; avoid tiny files. | Cache reference data; precompute hot aggregates. | Run heavy dedup or fuzzy steps off‑peak. | . ",
    "url": "/kb/cluedin-for-developers#14-performance--cost",
    
    "relUrl": "/kb/cluedin-for-developers#14-performance--cost"
  },"575": {
    "doc": "CluedIn for Developers",
    "title": "15) Cookbooks",
    "content": "15.1 Build an Ingestion Microservice (Node.js) . | Read from CRM API delta endpoint. | Transform minimal fields; add updated_at. | POST to /api/ingest with retries+DLQ. | Emit metrics and pass X-Correlation-Id. | . 15.2 Export‑Driven Reverse ETL (Python) . | Poll /api/exports/runs?name=contacts_v1&amp;status=success. | Diff changed rows since last watermark. | Upsert to downstream SaaS/CX via their API. | Log audit record with counts and latency. | . 15.3 Webhook to Teams . | Register export.succeeded webhook. | Verify HMAC, format message card, post to Teams webhook URL. | . 15.4 Backfill CLI . | Accept start/end timestamps and entity type. | Read source snapshots; push batches with rate limit and progress bar. | Tag runs with correlation_id and write a resumable state file. | . ",
    "url": "/kb/cluedin-for-developers#15-cookbooks",
    
    "relUrl": "/kb/cluedin-for-developers#15-cookbooks"
  },"576": {
    "doc": "CluedIn for Developers",
    "title": "16) Templates",
    "content": "16.1 .env.example . CLUE_HOST=https://your-cluedin-host CLUE_TOKEN= LOG_LEVEL=info . 16.2 Makefile . lint: ; ./tools/validate-config.sh test: ; pytest -q run: ; node src/index.js . 16.3 Dockerfile (Node) . FROM node:20-alpine WORKDIR /app COPY package*.json ./ RUN npm ci --omit=dev COPY . CMD [\"node\",\"src/index.js\"] . ",
    "url": "/kb/cluedin-for-developers#16-templates",
    
    "relUrl": "/kb/cluedin-for-developers#16-templates"
  },"577": {
    "doc": "CluedIn for Developers",
    "title": "17) Troubleshooting",
    "content": "401/403: token scope/expiry; SSO group‑to‑role mapping. 429: respect Retry-After; backoff with jitter; reduce concurrency. Schema null explosion: check mapping field paths and cleaning order. Webhook storms: dedupe by event id; idempotent handlers; collapse alerts. Export drift: switch to staging export; diff schemas; version bump if breaking. ",
    "url": "/kb/cluedin-for-developers#17-troubleshooting",
    
    "relUrl": "/kb/cluedin-for-developers#17-troubleshooting"
  },"578": {
    "doc": "CluedIn for Developers",
    "title": "18) Operating Rhythm",
    "content": "Daily: glance pipeline health, DLQ, last export run, top errors. Weekly: ship 1–2 improvements; tighten tests; review costs. Monthly: token/secrets rotation; access review; deprecate old exports. ",
    "url": "/kb/cluedin-for-developers#18-operating-rhythm",
    
    "relUrl": "/kb/cluedin-for-developers#18-operating-rhythm"
  },"579": {
    "doc": "CluedIn for Developers",
    "title": "19) Definition of Done (Dev)",
    "content": ". | Config in repo with PR review &amp; audit links. | Staging run green; prod run green ×3. | DQ metrics same or better. | Runbook updated; alert routed; rollback noted. | . You’re set: build producers and consumers, codify mapping/cleaning/validations, leverage AI Agents with guardrails, program the dedup lifecycle, observe everything, and ship safely with versioned exports and CI/CD. ",
    "url": "/kb/cluedin-for-developers#19-definition-of-done-dev",
    
    "relUrl": "/kb/cluedin-for-developers#19-definition-of-done-dev"
  },"580": {
    "doc": "CluedIn for Developers",
    "title": "CluedIn for Developers",
    "content": " ",
    "url": "/kb/cluedin-for-developers",
    
    "relUrl": "/kb/cluedin-for-developers"
  },"581": {
    "doc": "CluedIn for Data Engineers",
    "title": "CluedIn for Data Engineers — Build &amp; Operate Handbook",
    "content": "Audience: Data Engineers, Analytics Engineers, MLEs Goal: Provide a practical, end‑to‑end playbook for building reliable, secure, and observable pipelines in CluedIn—from ingestion through mapping, cleaning, exports, CI/CD, and operations. This handbook favors configuration-as-code, small incremental releases, strong observability, and close collaboration with Admins and Stewards. ",
    "url": "/kb/cluedin-data-engineers#cluedin-for-data-engineers--build--operate-handbook",
    
    "relUrl": "/kb/cluedin-data-engineers#cluedin-for-data-engineers--build--operate-handbook"
  },"582": {
    "doc": "CluedIn for Data Engineers",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "Access &amp; Environment . | Sign in via SSO; verify Data Engineer role permissions. | Identify dev/test/prod workspaces and which one you own. | Configure the CLI/API credentials (short‑lived if possible). | . Repos &amp; Config-as-Code . | Clone the platform-config repo (mappings, policies, projects). | Set up a branching model (feature → PR → staging → prod). | Install pre-commit linters for YAML/JSON schema validations. | . Pipelines . | Create one Ingestion Endpoint for your first source (stream or scheduled batch). | Wire one Export Target (table/topic/API) for an end‑to‑end path. | Add a staging export to test mapping/cleaning changes safely. | . Observability . | Pin dashboards for ingestion/export success, latency, rows. | Learn where to pull logs and audit logs with correlation_id. | Set baseline alerts (failures, volume anomalies, schema drift). | . ",
    "url": "/kb/cluedin-data-engineers#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-data-engineers#0-your-first-48-hours-checklist"
  },"583": {
    "doc": "CluedIn for Data Engineers",
    "title": "1) Role Scope (What You Own)",
    "content": ". | Data ingress (HTTP/Kafka/webhooks/batch/CDC) and reliability (retries, DLQ, replay). | Mapping of sources → CluedIn entities/relationships; schema evolution. | Cleaning projects and validations implementation with Stewards. | Exports to warehouses, lakes, topics, APIs; versioning and contracts. | Automation (schedules, webhooks) and integration (Power Platform, Purview). | Observability (logs, metrics, tracing), cost/perf, and incident response. | Security (secrets, tokens, PII handling) in partnership with Admins. | . ",
    "url": "/kb/cluedin-data-engineers#1-role-scope-what-you-own",
    
    "relUrl": "/kb/cluedin-data-engineers#1-role-scope-what-you-own"
  },"584": {
    "doc": "CluedIn for Data Engineers",
    "title": "2) Environments &amp; Config as Code",
    "content": "2.1 Workspace Layout . | dev: fast iteration, broad logs, feature flags on. | test/staging: PR validation, near‑prod data scale, alerts to engineers. | prod: tight scopes, change windows, alerts to on‑call. | . 2.2 Config Repository . Keep mappings, policies, cleaning projects, and export configs in a Git repo: . /cluedin/ mappings/ cleaning/ exports/ policies/ ai-agents/ env/ dev/ test/ prod/ . Use overlay variables per env (URIs, secrets references, schedules). 2.3 Promotion . | PR → staging export diff → approval → prod. | Always include: change summary, risk, rollback, owner, metrics to watch. | . ",
    "url": "/kb/cluedin-data-engineers#2-environments--config-as-code",
    
    "relUrl": "/kb/cluedin-data-engineers#2-environments--config-as-code"
  },"585": {
    "doc": "CluedIn for Data Engineers",
    "title": "3) Ingestion: Endpoints, Streams, Batches",
    "content": "Prefer automated producers; avoid manual uploads after day 1. 3.1 HTTP/Webhook (JSON lines) . curl -X POST \\ -H \"Authorization: Bearer $CLUEDIN_TOKEN\" \\ -H \"Content-Type: application/json\" \\ -d '{\"source\":\"crm-contacts\",\"payload\":{\"id\":\"c_123\",\"email\":\"a@example.com\",\"updated_at\":\"2025-08-22T12:00:00Z\"}}' \\ https://&lt;YOUR_INGESTION_ENDPOINT&gt;/ingest . 3.2 Kafka / Event Hub (stream) . | Topic per domain or entity; include schema version in headers. | Enforce idempotent producers; include event_id and updated_at. | . 3.3 Batch (S3/Blob/ADLS) . { \"name\": \"s3-sales-orders\", \"type\": \"s3-bucket-watch\", \"options\": { \"bucket\": \"acme-prod-orders\", \"prefix\": \"daily/\", \"file_types\": [\"json\",\"csv\",\"parquet\"], \"schedule\": \"cron(0 * * * ? *)\" } } . 3.4 CDC (Databases) . | Use Debezium/Log-based CDC to stream inserts/updates/deletes. | Normalize op codes upstream; ensure primary keys present. | . 3.5 Reliability Patterns . | Retries with exponential backoff; cap attempts. | DLQ/Quarantine for poison messages; add a replay tool. | Backfills: separate lane (lower priority), preserve current SLAs. | . ",
    "url": "/kb/cluedin-data-engineers#3-ingestion-endpoints-streams-batches",
    
    "relUrl": "/kb/cluedin-data-engineers#3-ingestion-endpoints-streams-batches"
  },"586": {
    "doc": "CluedIn for Data Engineers",
    "title": "4) Mapping: From Raw to Entities",
    "content": "4.1 Principles . | Start minimal: keys, names, timestamps, core relations. | Keep heavy standardization in cleaning projects, not mapping. | Make mapping versioned and revertible. | . 4.2 Example Mapping (pseudo-YAML) . entity: Person source: \"crm-contacts\" fields: id: $.id email: $.email first_name: $.first_name last_name: $.last_name updated_at: $.updated_at relationships: - type: \"EMPLOYED_BY\" to_entity: Organization from: $.org_id to: $.organization.id . 4.3 Schema Evolution . | Additive first (new nullable fields). | For breaking changes, create _v2 mapping and staging export to test. | . ",
    "url": "/kb/cluedin-data-engineers#4-mapping-from-raw-to-entities",
    
    "relUrl": "/kb/cluedin-data-engineers#4-mapping-from-raw-to-entities"
  },"587": {
    "doc": "CluedIn for Data Engineers",
    "title": "5) Cleaning Projects &amp; Validations",
    "content": "5.1 Cleaning (incremental, idempotent) . project: normalize_contacts schedule: \"0 * * * *\" # hourly steps: - name: normalize_email action: set field: email value: lower(trim(email)) - name: e164_phone when: phone is not null action: set field: phone value: to_e164(phone, default_country=\"US\") . 5.2 Validations (guardrails) . rule: email_regex entity: Person check: regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" severity: high on_fail: { action: \"flag\" } . Work with Stewards for rules and thresholds; engineers implement safely. ",
    "url": "/kb/cluedin-data-engineers#5-cleaning-projects--validations",
    
    "relUrl": "/kb/cluedin-data-engineers#5-cleaning-projects--validations"
  },"588": {
    "doc": "CluedIn for Data Engineers",
    "title": "6) Exports: Tables, Topics, APIs, Files",
    "content": "6.1 Contract &amp; Versioning . Define a data contract per export (schema, keys, SLA, semantics). Bump version on breaking changes (contacts_v1 → contacts_v2). 6.2 Example Export Config (pseudo-JSON) . { \"name\": \"warehouse-contacts-v1\", \"type\": \"sql-table\", \"options\": { \"connection\": \"analytics-warehouse\", \"schema\": \"mdm\", \"table\": \"contacts_v1\", \"mode\": \"upsert\", \"primary_key\": [\"contact_id\"] }, \"mapping\": { \"contact_id\": \"Person.id\", \"email\": \"Person.email\", \"first_name\": \"Person.first_name\", \"last_name\": \"Person.last_name\", \"updated_at\": \"Person.updated_at\" }, \"schedule\": \"0 * * * *\" } . 6.3 Reliability . | Upserts require stable primary keys. | Add row count and null ratio checks per run. | Emit correlation_id to trace in logs. | . ",
    "url": "/kb/cluedin-data-engineers#6-exports-tables-topics-apis-files",
    
    "relUrl": "/kb/cluedin-data-engineers#6-exports-tables-topics-apis-files"
  },"589": {
    "doc": "CluedIn for Data Engineers",
    "title": "7) Orchestration &amp; Triggers",
    "content": ". | Use CluedIn schedules for simple jobs. | For complex DAGs, trigger CluedIn via webhooks from Airflow/ADF. | Emit success/failure webhooks to Power Automate or incident channels. | . ",
    "url": "/kb/cluedin-data-engineers#7-orchestration--triggers",
    
    "relUrl": "/kb/cluedin-data-engineers#7-orchestration--triggers"
  },"590": {
    "doc": "CluedIn for Data Engineers",
    "title": "8) Testing Strategy",
    "content": "8.1 Unit &amp; Transform Tests . | Functions: normalization, parsing, enrichment. | Golden datasets for edge cases (UTF‑8, nulls, long strings). | . 8.2 Contract Tests . | Validate input fields/types/required keys. | Validate export schema and SLA (latency, freshness). | . 8.3 Regression Diffs . | Compare staging vs prod exports for row counts, nulls, distribution drift. | . 8.4 CI Example (pseudo-YAML) . name: cluedin-ci on: [pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - run: pip install -r requirements.txt - run: pytest -q - run: ./tools/validate-config.sh # schema check for YAML/JSON . ",
    "url": "/kb/cluedin-data-engineers#8-testing-strategy",
    
    "relUrl": "/kb/cluedin-data-engineers#8-testing-strategy"
  },"591": {
    "doc": "CluedIn for Data Engineers",
    "title": "9) Observability: Logs, Metrics, Tracing",
    "content": "9.1 What to Log . | Ingestion: status, bytes, parse errors, DLQ counts. | Mapping/Cleaning: records touched, rule breaches. | Exports: run duration, rows out, schema diffs. | . 9.2 Metrics to Track . | Freshness/latency per pipeline stage. | Completeness/validity/duplicates (with Stewards). | Failure rate by category. | Cost signals (records processed, reprocess volume). | . 9.3 Correlation . | Pass a correlation_id end‑to‑end; include it in user‑visible errors. | . ",
    "url": "/kb/cluedin-data-engineers#9-observability-logs-metrics-tracing",
    
    "relUrl": "/kb/cluedin-data-engineers#9-observability-logs-metrics-tracing"
  },"592": {
    "doc": "CluedIn for Data Engineers",
    "title": "10) Performance &amp; Cost Tuning",
    "content": ". | Choose columnar formats for heavy batch (Parquet). | Partition by time/entity; avoid small files. | Tune batch size and parallelism; monitor backpressure. | Cache reference data; avoid N+1 lookups in hot loops. | For dedup, start deterministic; schedule fuzzy phases off‑peak. | . ",
    "url": "/kb/cluedin-data-engineers#10-performance--cost-tuning",
    
    "relUrl": "/kb/cluedin-data-engineers#10-performance--cost-tuning"
  },"593": {
    "doc": "CluedIn for Data Engineers",
    "title": "11) Reliability Engineering",
    "content": ". | Idempotency: dedupe by event_id + updated_at. | Retries with jitter; circuit‑break noisy sources. | DLQ for poison data; add a replay CLI that tags correlation_id. | Backfills: run in read‑only mode for exports first; promote after QA. | Schema evolution: additive default; versioned breaking changes. | . ",
    "url": "/kb/cluedin-data-engineers#11-reliability-engineering",
    
    "relUrl": "/kb/cluedin-data-engineers#11-reliability-engineering"
  },"594": {
    "doc": "CluedIn for Data Engineers",
    "title": "12) Security &amp; Compliance (Engineer View)",
    "content": ". | Use least‑privilege tokens; scope to project/export. | Store secrets in a vault; never in repo or logs. | Respect labels: mask PII in staging and during AI prompts. | Use audit logs to evidence changes; link them in PRs. | Coordinate with Admins for role and policy changes. | . ",
    "url": "/kb/cluedin-data-engineers#12-security--compliance-engineer-view",
    
    "relUrl": "/kb/cluedin-data-engineers#12-security--compliance-engineer-view"
  },"595": {
    "doc": "CluedIn for Data Engineers",
    "title": "13) AI Agents for Engineers",
    "content": ". | Ask Agents to profile data, suggest validations, and generate test cases. | Use Agents to propose mapping diffs and survivorship rules. | Restrict to masked views when fields carry PII/Restricted labels. | Keep auto‑fixes reversible; log before/after samples. | . ",
    "url": "/kb/cluedin-data-engineers#13-ai-agents-for-engineers",
    
    "relUrl": "/kb/cluedin-data-engineers#13-ai-agents-for-engineers"
  },"596": {
    "doc": "CluedIn for Data Engineers",
    "title": "14) Collaboration Model",
    "content": ". | Stewards: define rules, thresholds, glossary; review DQ impacts. | Admins: SSO/roles, feature toggles, secrets, policies. | Engineers: implement, test, ship; own runtime health. | . Change cadence: small PRs, clear runbooks, explicit rollback steps. ",
    "url": "/kb/cluedin-data-engineers#14-collaboration-model",
    
    "relUrl": "/kb/cluedin-data-engineers#14-collaboration-model"
  },"597": {
    "doc": "CluedIn for Data Engineers",
    "title": "15) Runbooks (Common Incidents)",
    "content": "A) Ingestion 4xx/5xx spike . | Identify source + correlation_id. | Check auth/quotas; inspect DLQ samples. | Throttle producer or widen batch; hotfix schema parse. | Reprocess DLQ after fix; confirm metrics normal. | . B) Export schema drift . | Freeze prod export; route to staging. | Diff mapping since last green; revert or bump v2. | Notify consumers; run backfill if required. | . C) Duplicate surge . | Raise auto‑approve threshold; pause auto‑merge. | Add deterministic rule; schedule fuzzy off‑peak. | Review precision/recall; promote new rules. | . D) SLA breach (freshness) . | Identify slow stage (ingest, clean, export). | Scale parallelism or adjust schedule. | Add alert on pre‑cursor metrics (queue depth). | . ",
    "url": "/kb/cluedin-data-engineers#15-runbooks-common-incidents",
    
    "relUrl": "/kb/cluedin-data-engineers#15-runbooks-common-incidents"
  },"598": {
    "doc": "CluedIn for Data Engineers",
    "title": "16) Templates &amp; Snippets",
    "content": "16.1 Export Contract (YAML) . name: contacts_v1 primary_key: contact_id delivery: type: sql-table schedule: hourly sla: freshness_minutes_p95: 60 schema: - name: contact_id type: string required: true - name: email type: string - name: updated_at type: timestamp labels: [\"PII:email\"] . 16.2 Policy Hook (mask PII for non-owners) . policy: mask_email_for_non_owners target: entity:Person.field:email actions: [read] effect: allow_with_mask mask: \"partial_email\" unless: - role_in: [\"Data Steward\",\"Administrator\"] . 16.3 Makefile Helpers . lint: \\t./tools/validate-config.sh deploy-staging: \\t./tools/apply.sh env/test deploy-prod: \\t./tools/apply.sh env/prod . 16.4 SQL Smoke Checks . -- Row count sanity vs yesterday (±10%) SELECT CASE WHEN ABS((today.c - yday.c) / NULLIF(yday.c,0)) &lt;= 0.10 THEN 'OK' ELSE 'ALERT' END AS status FROM (SELECT COUNT(*) c FROM mdm.contacts_v1 WHERE _load_date = CURRENT_DATE) today, (SELECT COUNT(*) c FROM mdm.contacts_v1 WHERE _load_date = CURRENT_DATE - INTERVAL '1 day') yday; . ",
    "url": "/kb/cluedin-data-engineers#16-templates--snippets",
    
    "relUrl": "/kb/cluedin-data-engineers#16-templates--snippets"
  },"599": {
    "doc": "CluedIn for Data Engineers",
    "title": "17) Operating Rhythm",
    "content": "Daily . | Check pipeline health + last run metrics (2–5 min). | Triage DLQ; scan error logs by category. | Review DQ dashboard with Stewards. | . Weekly . | Ship 1–2 mapping/cleaning improvements. | Tighten tests for any incident class seen. | Cost/perf review: partitions, parallelism, batch sizes. | . Monthly . | Access &amp; token review (with Admins). | Evaluate dedup thresholds (precision/recall). | Retire old export versions (_v0) after consumers migrate. | . ",
    "url": "/kb/cluedin-data-engineers#17-operating-rhythm",
    
    "relUrl": "/kb/cluedin-data-engineers#17-operating-rhythm"
  },"600": {
    "doc": "CluedIn for Data Engineers",
    "title": "18) What “Good” Looks Like",
    "content": ". | Green end‑to‑end runs with alert noise &lt; 1%. | Additive schema changes dominate; breaking changes are versioned. | DQ metrics improving quarter‑over‑quarter. | Runbooks exercised; MTTR trending down. | Config-as-code with clear history, rollbacks, and audit links. | . You now have the Data Engineer blueprint: land data reliably, map minimally then iterate, clean &amp; validate safely, export with contracts, test and observe everything, and collaborate tightly with Stewards and Admins. Ship small, ship often, and keep the pipeline healthy. ",
    "url": "/kb/cluedin-data-engineers#18-what-good-looks-like",
    
    "relUrl": "/kb/cluedin-data-engineers#18-what-good-looks-like"
  },"601": {
    "doc": "CluedIn for Data Engineers",
    "title": "CluedIn for Data Engineers",
    "content": " ",
    "url": "/kb/cluedin-data-engineers",
    
    "relUrl": "/kb/cluedin-data-engineers"
  },"602": {
    "doc": "CluedIn For Data Stewards",
    "title": "CluedIn for Data Stewards — Field Guide",
    "content": "Audience: Data Stewards, Data Quality Leads, Business Owners Goal: Give stewards a clear, practical playbook for operating CluedIn: profiling, validation, cleaning, dedup review, metadata &amp; glossary, and day‑to‑day rhythms with engineers and admins. This is the “how we actually work” guide. It favors repeatable workflows, small changes shipped often, and evidence from metrics and audit trails. ",
    "url": "/kb/cluedin-for-data-stewards#cluedin-for-data-stewards--field-guide",
    
    "relUrl": "/kb/cluedin-for-data-stewards#cluedin-for-data-stewards--field-guide"
  },"603": {
    "doc": "CluedIn For Data Stewards",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "Get access &amp; context . | Sign in via SSO; verify your Steward role permissions. | Join the #data-quality and #cluedin-ops channels (or equivalent). | Open the Entities Explorer and Exports you own. | Skim the Runbook and Use‑Case Brief for your domain. | . Profile &amp; label . | Run profiling on top entities (completeness, validity, uniqueness). | Apply labels/classifications (PII, Restricted, Confidential). | Note top 3 issues (e.g., invalid emails, duplicate orgs). | . Set foundations . | Draft validation rules for the highest‑impact fields. | Create a Cleaning Project with 1–2 safe normalizations. | Review Dedup queue; define deterministic auto‑approve rules (with Admin/Engineer). | . Observability . | Pin DQ metrics and set initial thresholds. | Learn where to find logs and audit logs for your domain. | . ",
    "url": "/kb/cluedin-for-data-stewards#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-for-data-stewards#0-your-first-48-hours-checklist"
  },"604": {
    "doc": "CluedIn For Data Stewards",
    "title": "1) What a Steward Does in CluedIn",
    "content": ". | Profiling &amp; Monitoring — Understand data shapes, trends, outliers. | Validation &amp; Policy — Encode business rules as validations/policies. | Cleaning — Build small, idempotent improvements that run on a cadence. | Dedup &amp; Golden Records — Review matches, define survivorship, and unmerge safely. | Metadata &amp; Glossary — Maintain definitions, semantics, classifications, and lineage notes. | Governance &amp; Evidence — Track DQ metrics, create change records, use audit logs. | Collaboration — Work with Engineers (pipelines/mapping) and Admins (access/roles). | . Principle: Ship smaller changes more often. Each step should be low‑risk, easy to revert, and measurable. ",
    "url": "/kb/cluedin-for-data-stewards#1-what-a-steward-does-in-cluedin",
    
    "relUrl": "/kb/cluedin-for-data-stewards#1-what-a-steward-does-in-cluedin"
  },"605": {
    "doc": "CluedIn For Data Stewards",
    "title": "2) Finding and Understanding Your Data",
    "content": "2.1 Entities Explorer &amp; Catalog . | Browse Entities (e.g., Person, Organization, Order) and their attributes. | Check relationships (e.g., Person ↔ Organization) and lineage from sources to exports. | Use labels (PII, Restricted) to drive access and masking policies. | . 2.2 Profiling (What to look at) . | Completeness: non‑null %, blank %, cardinality. | Validity: regex/domain compliance (emails, country codes). | Uniqueness: duplicates by natural/business keys. | Distributions: skew, min/max, outliers. | Timeliness: update latency vs SLAs. | . Quick profiling questions . | Which 5 fields most impact downstream decisions? | Where are the biggest drops in completeness/validity? | Which sources disagree (consistency check)? | . ",
    "url": "/kb/cluedin-for-data-stewards#2-finding-and-understanding-your-data",
    
    "relUrl": "/kb/cluedin-for-data-stewards#2-finding-and-understanding-your-data"
  },"606": {
    "doc": "CluedIn For Data Stewards",
    "title": "3) Writing Validations (Guardrails)",
    "content": "3.1 Validation Rule Template . rule: email_must_be_valid entity: Person when: - field: email is_not_null: true check: regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" severity: high on_fail: action: \"flag\" # flag | quarantine | auto_fix message: \"Invalid email format\" labels: [\"PII\",\"contactability\"] . 3.2 Cross‑Field Rules . rule: order_dates_consistent entity: Order check: expression: \"order_date &lt;= ship_date\" severity: medium on_fail: { action: \"flag\", message: \"Ship date precedes order date\" } . 3.3 Domain Lists &amp; Reference Data . rule: country_in_iso3166 entity: Address check: in_list: field: country_code list: [\"US\",\"AU\",\"GB\",\"DE\",\"FR\",\"JP\",\"NZ\"] on_fail: { action: \"flag\" } . Tips . | Start allow‑listing (valid patterns), avoid complex negative logic. | Add labels to rules to group them by policy (e.g., privacy, finance). | Prefer flag or quarantine first; adopt auto_fix only when safe and deterministic. | . ",
    "url": "/kb/cluedin-for-data-stewards#3-writing-validations-guardrails",
    
    "relUrl": "/kb/cluedin-for-data-stewards#3-writing-validations-guardrails"
  },"607": {
    "doc": "CluedIn For Data Stewards",
    "title": "4) Cleaning Projects (Small, Safe, Scheduled)",
    "content": "4.1 Design Principles . | Idempotent: Running twice doesn’t double‑change. | Observable: Log counts changed, keep before/after examples. | Revertible: Version the project; easy rollback. | Scoped: 1–3 steps per release; avoid big‑bang refactors. | . 4.2 Starter Project . project: normalize_contacts schedule: \"0 * * * *\" # hourly steps: - name: trim_names action: set field: first_name value: trim(first_name) - name: normalize_email action: set field: email value: lower(trim(email)) - name: drop_impossible_birthdates when: \"birthdate &lt; '1900-01-01' or birthdate &gt; now()\" action: set field: birthdate value: null observability: sample_before_after: 25 emit_metrics: true . 4.3 Approval &amp; Promotion . | Test in staging export. Compare row counts, nulls, and schema. | Peer review from another Steward/Engineer. | Promote with a short changelog entry and link to metrics. | . ",
    "url": "/kb/cluedin-for-data-stewards#4-cleaning-projects-small-safe-scheduled",
    
    "relUrl": "/kb/cluedin-for-data-stewards#4-cleaning-projects-small-safe-scheduled"
  },"608": {
    "doc": "CluedIn For Data Stewards",
    "title": "5) Deduplication &amp; Golden Records",
    "content": "5.1 Review Workflow . | Open Dedup queue for your entity (e.g., Person). | Start with high‑confidence candidates; verify examples. | Approve merges that are deterministic; send ambiguous ones to manual review with notes. | If needed, unmerge with documented rationale. | . 5.2 Deterministic First, then Fuzzy . rules: - name: exact_email when: lower(email) == lower(other.email) confidence: 0.98 - name: phone_e164 when: e164(phone) == e164(other.phone) confidence: 0.95 auto_approve_threshold: 0.97 queue_threshold: 0.85 . 5.3 Survivorship (Golden Record) . Define how fields are chosen on merge: . survivorship: precedence: - source: \"crm\" - source: \"support\" - source: \"marketing\" recency_win: true # prefer latest updated_at when sources tie field_overrides: email: \"most_recent_non_null\" phone: \"most_trusted_source_first\" . | Maintain a decision log for merges/unmerges. | Update dedup rules after recurring manual decisions to reduce future toil. | . ",
    "url": "/kb/cluedin-for-data-stewards#5-deduplication--golden-records",
    
    "relUrl": "/kb/cluedin-for-data-stewards#5-deduplication--golden-records"
  },"609": {
    "doc": "CluedIn For Data Stewards",
    "title": "6) Mapping &amp; Schema with a Steward’s Lens",
    "content": ". | Propose new fields/semantics based on business needs. | Keep business definitions (glossary) aligned with mapped fields. | Avoid heavy transformation in mapping; use Cleaning Projects for standardization. | When adding fields, coordinate with Engineers to run staging exports and diff results. | . Change Checklist . | Impacted dashboards identified | Export contracts updated (if needed) | Backfill plan (if required) | Rollback tested | . ",
    "url": "/kb/cluedin-for-data-stewards#6-mapping--schema-with-a-stewards-lens",
    
    "relUrl": "/kb/cluedin-for-data-stewards#6-mapping--schema-with-a-stewards-lens"
  },"610": {
    "doc": "CluedIn For Data Stewards",
    "title": "7) Metadata, Glossary &amp; Classifications",
    "content": "7.1 Glossary Entries . Include: business definition, calculation notes, owner, example, data quality caveats. term: \"Active Customer\" definition: \"Customer with at least one completed order in the last 90 days\" owner: \"Sales Ops\" sources: [\"orders_v1\",\"subscriptions_v2\"] dq_notes: \"Relies on order status != 'cancelled'\" . 7.2 Classifications &amp; Policy Hooks . | Tag fields/entities with PII, Restricted, Confidential, or Public. | Policies can mask, hash, or deny reads based on labels. | Steward task: keep labels accurate as new fields arrive. | . ",
    "url": "/kb/cluedin-for-data-stewards#7-metadata-glossary--classifications",
    
    "relUrl": "/kb/cluedin-for-data-stewards#7-metadata-glossary--classifications"
  },"611": {
    "doc": "CluedIn For Data Stewards",
    "title": "8) Data Quality Metrics &amp; Dashboards",
    "content": "Track few, meaningful KPIs per entity: . | Completeness: pct_non_null(email) | Validity: pct_match(email, regex) | Uniqueness: 1 - duplicate_rate(email) | Timeliness: p95(now() - updated_at) | . Thresholds . metrics: email_validity: { warn: \"&gt;= 0.97\", fail: \"&lt; 0.95\" } duplicate_rate_email: { warn: \"&lt;= 0.03\", fail: \"&gt; 0.05\" } alerts: - metric: email_validity action: \"notify #data-quality\" . Habit: Improve by small, frequent increments (e.g., +0.5–1.0% per week). ",
    "url": "/kb/cluedin-for-data-stewards#8-data-quality-metrics--dashboards",
    
    "relUrl": "/kb/cluedin-for-data-stewards#8-data-quality-metrics--dashboards"
  },"612": {
    "doc": "CluedIn For Data Stewards",
    "title": "9) Reading Logs &amp; Audit Logs (Steward View)",
    "content": "9.1 Operational Logs . | Cleaning logs for before/after counts, failures. | Validation logs for rule breaches and quarantines. | Export logs for schema diffs and row counts. | . Grab the correlation_id from UI errors and filter logs by it when triaging. 9.2 Audit Logs . | Who approved a merge? Who changed a rule? When was a policy updated? | Use audit logs to evidence changes and for post‑incident analysis. | Include audit links in your changelog notes. | . ",
    "url": "/kb/cluedin-for-data-stewards#9-reading-logs--audit-logs-steward-view",
    
    "relUrl": "/kb/cluedin-for-data-stewards#9-reading-logs--audit-logs-steward-view"
  },"613": {
    "doc": "CluedIn For Data Stewards",
    "title": "10) Working with AI Agents (Your Co‑pilot)",
    "content": "10.1 Analysis Prompt Starters . “Analyze Person for missingness, invalid formats, and duplicates. Propose validation rules with example failing records. Suggest deterministic dedup keys first.” . “Given recent invalid email spikes, propose a stricter validation with back‑compat and estimate false positive rate.” . 10.2 Safe Auto‑Fix Patterns . | Normalize casing/whitespace. | Standardize ISO codes (countries, states). | Convert phone numbers to E.164 (with default country). | . Guardrails . | Keep auto‑fixes reversible; log original values. | Restrict AI write scope to masked views where needed. | . ",
    "url": "/kb/cluedin-for-data-stewards#10-working-with-ai-agents-your-copilot",
    
    "relUrl": "/kb/cluedin-for-data-stewards#10-working-with-ai-agents-your-copilot"
  },"614": {
    "doc": "CluedIn For Data Stewards",
    "title": "11) Collaboration &amp; Change Management",
    "content": ". | Submit PRs or change requests for new validations/cleaning steps. | Pair with Engineers for mapping/export impacts. | Ask Admins for labels/policy updates and role changes. | Document user impact and rollback in each change. | . Promotion flow . | Draft in staging → 2. Peer review → 3. Scheduled deploy → 4. Post‑deploy checks → 5. Changelog &amp; metric snapshot. | . ",
    "url": "/kb/cluedin-for-data-stewards#11-collaboration--change-management",
    
    "relUrl": "/kb/cluedin-for-data-stewards#11-collaboration--change-management"
  },"615": {
    "doc": "CluedIn For Data Stewards",
    "title": "12) Day‑to‑Day Operating Rhythm",
    "content": "Daily (5–10 minutes) . | DQ dashboard glance; triage new alerts. | Dedup queue: approve high‑confidence; flag edge cases. | Check last cleaning run; scan error logs. | . Weekly . | Ship 1–3 cleaning improvements. | Review validation breach trends; tighten where safe. | Update glossary entries if semantics changed. | . Monthly . | Access review for your domain (with Admins). | Revisit dedup thresholds; sample precision/recall. | Retire deprecated fields/exports. | . ",
    "url": "/kb/cluedin-for-data-stewards#12-daytoday-operating-rhythm",
    
    "relUrl": "/kb/cluedin-for-data-stewards#12-daytoday-operating-rhythm"
  },"616": {
    "doc": "CluedIn For Data Stewards",
    "title": "13) Steward Runbook (Common Situations)",
    "content": "Spike in invalid emails . | Confirm via validation logs and sample records. | Identify source; check recent mapping/cleaning changes. | Add temporary quarantine rule; propose stricter regex. | Communicate downstream impact; backfill if needed. | . Duplicate surge after new source onboarded . | Lower auto‑approve threshold; pause auto‑merge. | Add deterministic rule (e.g., exact phone or customer_id). | Review queue; update survivorship precedence with the new source. | . Export schema drift detected . | Compare staging vs prod exports; find mapping change. | Coordinate rollback with Engineer; file a changelog entry. | Add a validation to catch the offending pattern next time. | . ",
    "url": "/kb/cluedin-for-data-stewards#13-steward-runbook-common-situations",
    
    "relUrl": "/kb/cluedin-for-data-stewards#13-steward-runbook-common-situations"
  },"617": {
    "doc": "CluedIn For Data Stewards",
    "title": "14) Templates &amp; Snippets",
    "content": "14.1 Validation Pack (copy/paste) . - rule: phone_is_e164_or_null entity: Person when: [{ field: phone, is_not_null: true }] check: { regex: \"^\\\\+\\\\d{8,15}$\" } severity: medium on_fail: { action: \"flag\" } - rule: name_min_length entity: Person check: { expression: \"len(trim(first_name)) &gt;= 1 and len(trim(last_name)) &gt;= 1\" } severity: low on_fail: { action: \"flag\" } . 14.2 Cleaning Steps . - name: fix_common_email_typos action: set field: email value: replace_multi(lower(email), {\"gmal.com\":\"gmail.com\",\"hotnail.com\":\"hotmail.com\"}) . 14.3 Dedup Reviewer Notes (template) . pair_id: \"dup_7f3a\" decision: \"approve\" evidence: - \"exact_email\" - \"matching phone_e164\" comment: \"Same person from CRM and Support; safe to auto‑approve rule next time.\" . 14.4 Changelog Entry . date: \"2025-08-24\" change: \"Added phone E.164 normalization; tightened email regex\" impact: \"Duplicate rate down from 5.2% → 2.1%; validity up 96.8% → 98.9%\" links: [\"run logs\", \"audit event #1432\"] . ",
    "url": "/kb/cluedin-for-data-stewards#14-templates--snippets",
    
    "relUrl": "/kb/cluedin-for-data-stewards#14-templates--snippets"
  },"618": {
    "doc": "CluedIn For Data Stewards",
    "title": "15) What Good Looks Like",
    "content": ". | DQ metrics trending up, no long‑lived red alerts. | Small, frequent improvements with audit evidence. | Glossary &amp; labels kept up to date. | Dedup queue under control; high precision merges; rare unmerges. | Change notes clear enough that a new Steward can follow the story. | . ",
    "url": "/kb/cluedin-for-data-stewards#15-what-good-looks-like",
    
    "relUrl": "/kb/cluedin-for-data-stewards#15-what-good-looks-like"
  },"619": {
    "doc": "CluedIn For Data Stewards",
    "title": "Appendix — Steward Permission Profile (Typical)",
    "content": "| Capability | Viewer | Steward | Engineer | Admin | . | Read entities/exports | ✅ | ✅ | ✅ | ✅ | . | Edit validations | ❌ | ✅ | ✅ | ✅ | . | Edit cleaning projects | ❌ | ✅ | ✅ | ✅ | . | Review dedup &amp; approve merges | ❌ | ✅ | ✅ | ✅ | . | Configure ingestion/export | ❌ | ❌ | ✅ | ✅ | . | Manage roles/policies | ❌ | ❌ | ❌ | ✅ | . | Toggle features | ❌ | ❌ | ❌ | ✅ | . If you need elevated access temporarily, request a time‑boxed role with Admin approval. You now have the Steward playbook: profile, validate, clean, dedup, document, measure—then repeat. Keep changes small, keep evidence, and keep the pipeline healthy. ",
    "url": "/kb/cluedin-for-data-stewards#appendix--steward-permission-profile-typical",
    
    "relUrl": "/kb/cluedin-for-data-stewards#appendix--steward-permission-profile-typical"
  },"620": {
    "doc": "CluedIn For Data Stewards",
    "title": "CluedIn For Data Stewards",
    "content": " ",
    "url": "/kb/cluedin-for-data-stewards",
    
    "relUrl": "/kb/cluedin-for-data-stewards"
  },"621": {
    "doc": "CluedIn for Data Modellers",
    "title": "CluedIn for Data Modellers — Canonical &amp; Export Schema Playbook",
    "content": "Audience: Data Modellers, Semantic Layer Owners, Analytics Architects Goal: Provide a practical handbook to model data in CluedIn—canonical entities, relationships, semantics, temporal patterns, export schemas, contracts, and safe iteration with Stewards, Engineers, Admins, and Governance. Assumption: CluedIn runs ELT—land raw first, then model/standardize in-platform. Your superpower is designing schemas that evolve safely and power trustworthy exports. ",
    "url": "/kb/cluedin-data-modellers#cluedin-for-data-modellers--canonical--export-schema-playbook",
    
    "relUrl": "/kb/cluedin-data-modellers#cluedin-for-data-modellers--canonical--export-schema-playbook"
  },"622": {
    "doc": "CluedIn for Data Modellers",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "Context &amp; Access . | Sign in via SSO; verify your role (Model/Engineer/Steward). | Review the Use‑Case Brief, Runbook, and Export Contracts already defined. | Browse Entities Explorer and current Exports for your domain. | . Model Scope . | Identify the canonical entities (e.g., Person, Organization, Order, Product). | List critical attributes and business keys per entity. | Document relationships (cardinality, optionality). | . Standards . | Agree on naming conventions, data types, timezones/temporal fields. | Align with labels/classifications (PII, Restricted). | Pick SCD strategy per entity (SCD1 vs SCD2). | . Change Safety . | Confirm staging export for rehearsing model changes. | Set up schema diff and contract tests. | Define a versioning policy for exports (e.g., _v1, _v2). | . ",
    "url": "/kb/cluedin-data-modellers#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-data-modellers#0-your-first-48-hours-checklist"
  },"623": {
    "doc": "CluedIn for Data Modellers",
    "title": "1) Role Scope (What You Own)",
    "content": ". | Canonical entity &amp; relationship design; attribute semantics and units. | Keys &amp; identity strategy (surrogates, natural keys, source keys). | Temporal modeling (SCD types, event time vs processing time). | Reference/master data modeling and code lists. | Semantics for metrics and derived attributes (definitions &amp; lineage). | Export schemas (star/wide/event) and contracts with consumers. | Change governance: backward compatibility, deprecations, and documentation. | . You partner with: . | Stewards (quality rules, dedup survivorship, glossary), | Engineers (ingestion, mapping, cleaning, exports), | Admins/Governance (access, labels, approvals). | . ",
    "url": "/kb/cluedin-data-modellers#1-role-scope-what-you-own",
    
    "relUrl": "/kb/cluedin-data-modellers#1-role-scope-what-you-own"
  },"624": {
    "doc": "CluedIn for Data Modellers",
    "title": "2) Modelling Principles in CluedIn",
    "content": ". | Start minimal; evolve rapidly. Model only what you need for current exports. | Prefer additive changes. Breakers require new versioned exports. | Separate concerns: mapping ≠ cleaning ≠ export shaping. | Make identity explicit. Always carry source_system, source_id, updated_at. | Document semantics adjacent to schema (glossary + YAML spec). | Model for query patterns. Choose star/wide/event based on consumers. | Label sensitive fields. PII/Restricted labels drive masking/access policies. | . ",
    "url": "/kb/cluedin-data-modellers#2-modelling-principles-in-cluedin",
    
    "relUrl": "/kb/cluedin-data-modellers#2-modelling-principles-in-cluedin"
  },"625": {
    "doc": "CluedIn for Data Modellers",
    "title": "3) Canonical Entities &amp; Attributes",
    "content": "3.1 Naming &amp; Types . | snake_case for fields, UpperCamel for entities (e.g., Person). | Use typed timestamps with timezone (*_at in UTC). | Store IDs as strings unless arithmetic is required. | Enumerations use UPPER_SNAKE values (document the domain list). | . 3.2 Keys . | Surrogate key: person_id (stable, synthetic). | Business keys: email, customer_number (may change). | Source link: {source_system, source_id} as unique pair. | . 3.3 Example Canonical Spec . entity: Person keys: surrogate: person_id natural: [email] # optional; can be multi sources: - system: \"crm\" id_field: \"id\" attributes: person_id: { type: string, required: true, semantics: \"stable synthetic id\" } email: { type: string, labels: [\"PII\"], constraints: { regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" } } first_name: { type: string } last_name: { type: string } phone_e164: { type: string, labels: [\"PII\"] } updated_at: { type: timestamp, required: true } lineage_fields: [source_system, source_id, source_updated_at] . ",
    "url": "/kb/cluedin-data-modellers#3-canonical-entities--attributes",
    
    "relUrl": "/kb/cluedin-data-modellers#3-canonical-entities--attributes"
  },"626": {
    "doc": "CluedIn for Data Modellers",
    "title": "4) Relationships",
    "content": "4.1 Patterns . | 1:M (Organization → Person) via foreign key organization_id. | M:M (Person ↔ Product) via bridge PersonProduct with role/effective dates. | Hierarchies (e.g., org trees) via parent_id + path/level fields. | . 4.2 Relationship Spec . relationship: EMPLOYED_BY from: Person to: Organization cardinality: many_to_one navigability: from_to keys: from_fk: organization_id semantics: \"Employment at current employer\" . Record direction, cardinality, and whether the relationship is current or historical. ",
    "url": "/kb/cluedin-data-modellers#4-relationships",
    
    "relUrl": "/kb/cluedin-data-modellers#4-relationships"
  },"627": {
    "doc": "CluedIn for Data Modellers",
    "title": "5) Temporal Modeling (SCD)",
    "content": "5.1 Choose per entity . | SCD1 (overwrite): current snapshot only. Good for operational exports. | SCD2 (history): track changes with effective_from, effective_to, is_current. | SCD3 (limited history): carry previous value fields for comparison. | . 5.2 SCD2 Template . entity: Organization_dim scd: type2 keys: { surrogate: organization_sk, business: [organization_id] } temporal: effective_from: valid_from effective_to: valid_to current_flag: is_current defaults: valid_to: \"9999-12-31T23:59:59Z\" . 5.3 Event vs Snapshot . | Event models (append‑only) are ideal for BI with time series. | Snapshot exports provide easy point‑in‑time queries. | . ",
    "url": "/kb/cluedin-data-modellers#5-temporal-modeling-scd",
    
    "relUrl": "/kb/cluedin-data-modellers#5-temporal-modeling-scd"
  },"628": {
    "doc": "CluedIn for Data Modellers",
    "title": "6) Reference &amp; Master Data",
    "content": "6.1 Code Lists . Document controlled vocabularies and map source variants during cleaning: . reference: country_codes canonical: ISO_3166_1_ALPHA2 values: [\"US\",\"AU\",\"GB\",\"DE\",\"FR\",\"JP\",\"NZ\"] aliases: \"United States\": \"US\" \"U.S.\": \"US\" . 6.2 Master Data (Golden Records) . | Use dedup rules (deterministic first) to converge sources. | Define survivorship (source precedence, recency, field overrides). | . survivorship: precedence: [\"crm\",\"support\",\"marketing\"] recency_win: true field_overrides: email: \"most_recent_non_null\" . ",
    "url": "/kb/cluedin-data-modellers#6-reference--master-data",
    
    "relUrl": "/kb/cluedin-data-modellers#6-reference--master-data"
  },"629": {
    "doc": "CluedIn for Data Modellers",
    "title": "7) Mapping Strategy (Raw → Canonical)",
    "content": "7.1 Principles . | Map minimum attributes for v0 exports. | Normalize in cleaning projects (case, formats, code lists). | Keep source fields for traceability. | . 7.2 Example Mapping (pseudo) . entity: Person source: \"crm-contacts\" fields: person_id: $.id # or synthetic from source id email: $.email first_name: $.first_name last_name: $.last_name phone_e164: normalize_phone($.phone, \"AU\") updated_at: $.updated_at stash: source_system: \"crm\" source_id: $.id source_updated_at: $.updated_at . ",
    "url": "/kb/cluedin-data-modellers#7-mapping-strategy-raw--canonical",
    
    "relUrl": "/kb/cluedin-data-modellers#7-mapping-strategy-raw--canonical"
  },"630": {
    "doc": "CluedIn for Data Modellers",
    "title": "8) Export Schema Patterns",
    "content": "8.1 Star Schema (BI) . | Facts (events/measures) with keys to Dims. | Dims carry attributes (SCD2 dims for history). | . fact: orders_fct_v1 grain: \"order_line\" dimensions: [date_dim, customer_dim, product_dim] measures: - name: gross_amount type: decimal(18,2) - name: net_amount type: decimal(18,2) . 8.2 Wide Tables (Operational Analytics) . | One row per business entity; denormalize child counts or latest states. | . 8.3 Event Streams . | Append-only export (e.g., to Kafka/Event Hub) with event_time + correlation_id. | . 8.4 Export Contract . name: customers_v1 primary_key: customer_id delivery: { type: sql-table, schedule: hourly } schema: - name: customer_id ; type: string ; required: true - name: email ; type: string ; labels: [\"PII\"] - name: status ; type: string ; domain: [\"ACTIVE\",\"INACTIVE\"] sla: freshness_p95_minutes: 60 compatibility: additive_only . ",
    "url": "/kb/cluedin-data-modellers#8-export-schema-patterns",
    
    "relUrl": "/kb/cluedin-data-modellers#8-export-schema-patterns"
  },"631": {
    "doc": "CluedIn for Data Modellers",
    "title": "9) Semantics &amp; Metric Definitions",
    "content": ". | Write metric specs with business logic, filters, and grain. | Align with BI/semantic layer (e.g., Power BI/Looker definitions). | . metric: active_customers_90d grain: \"day\" entity: Person definition: \"Customers with ≥1 completed order in last 90 days\" filters: { order_status: \"COMPLETED\" } owner: \"Sales Ops\" . ",
    "url": "/kb/cluedin-data-modellers#9-semantics--metric-definitions",
    
    "relUrl": "/kb/cluedin-data-modellers#9-semantics--metric-definitions"
  },"632": {
    "doc": "CluedIn for Data Modellers",
    "title": "10) Constraints, Validations &amp; Labels",
    "content": ". | Enumerations (status, country_code). | Regex constraints (email, phone). | Cross-field checks (order_date &lt;= ship_date). | Labels to tie to policies (PII, Restricted). | . constraints: - field: status in: [\"ACTIVE\",\"INACTIVE\",\"PROSPECT\"] labels: - field: email add: [\"PII\"] . ",
    "url": "/kb/cluedin-data-modellers#10-constraints-validations--labels",
    
    "relUrl": "/kb/cluedin-data-modellers#10-constraints-validations--labels"
  },"633": {
    "doc": "CluedIn for Data Modellers",
    "title": "11) Internationalization (i18n) &amp; Addresses",
    "content": ". | Store addresses as structured fields (line1, line2, city, region, postal_code, country_code). | Use ISO codes; normalize during cleaning. | Timezones: store UTC plus timezone field if needed for local rendering. | . ",
    "url": "/kb/cluedin-data-modellers#11-internationalization-i18n--addresses",
    
    "relUrl": "/kb/cluedin-data-modellers#11-internationalization-i18n--addresses"
  },"634": {
    "doc": "CluedIn for Data Modellers",
    "title": "12) Performance &amp; Scale Considerations",
    "content": ". | Use columnar formats (Parquet) for batch exports. | Avoid excessive high‑cardinality dimensions in star schemas; pre-aggregate where needed. | Partition facts by event_date; cluster by keys frequently filtered. | For wide tables, cap column count; offload blobs/json to sidecar tables if necessary. | . ",
    "url": "/kb/cluedin-data-modellers#12-performance--scale-considerations",
    
    "relUrl": "/kb/cluedin-data-modellers#12-performance--scale-considerations"
  },"635": {
    "doc": "CluedIn for Data Modellers",
    "title": "13) Change Management &amp; Versioning",
    "content": "13.1 Backward Compatibility . | Additive changes (new nullable columns) are safe for minor versions. | Breaking changes (rename, type change, remove) → new _vN export. | . 13.2 Deprecation Policy . | Announce N+1 version availability; run both for at least one cycle. | Share diffs and migration notes; set a sunset date. | . 13.3 Review Checklist . | Contract updated &amp; docs linked | Staging diff reviewed (row counts, nulls) | DQ metrics unaffected or improved | Policies/labels reviewed (PII exposure unchanged) | Backfill strategy and rollback plan | . ",
    "url": "/kb/cluedin-data-modellers#13-change-management--versioning",
    
    "relUrl": "/kb/cluedin-data-modellers#13-change-management--versioning"
  },"636": {
    "doc": "CluedIn for Data Modellers",
    "title": "14) Tests for Models",
    "content": ". | Schema tests: required fields, types. | Domain tests: enum membership, regex validity. | Referential tests: FK integrity for relationships. | Contract tests: export schema + SLA. | Regression tests: distribution drift checks between versions. | . tests: - name: fk_person_org type: referential from: Person.organization_id to: Organization.organization_id on_fail: error . ",
    "url": "/kb/cluedin-data-modellers#14-tests-for-models",
    
    "relUrl": "/kb/cluedin-data-modellers#14-tests-for-models"
  },"637": {
    "doc": "CluedIn for Data Modellers",
    "title": "15) Collaboration Patterns",
    "content": ". | With Stewards: agree validations, classifications, dedup rules; co-own glossary. | With Engineers: design mapping inputs &amp; export outputs; stage &amp; compare. | With Governance: approvals for PII exports, policy impacts; DPIA inputs. | With Admins: roles and access for model editors; feature toggles. | . Change cadence: small PRs, peer review, staging rehearsal, measured rollout. ",
    "url": "/kb/cluedin-data-modellers#15-collaboration-patterns",
    
    "relUrl": "/kb/cluedin-data-modellers#15-collaboration-patterns"
  },"638": {
    "doc": "CluedIn for Data Modellers",
    "title": "16) Common Anti‑Patterns (Avoid)",
    "content": ". | Big‑bang models aiming to cover all sources before shipping anything. | Encoding complex business logic in mapping instead of cleaning/exports. | Breaking changes without versioning and consumer migration. | Overusing free‑text fields for coded values (lose constraints). | Ignoring temporal requirements—no way to do point‑in‑time analysis. | Missing source link fields → poor traceability and incident triage. | . ",
    "url": "/kb/cluedin-data-modellers#16-common-antipatterns-avoid",
    
    "relUrl": "/kb/cluedin-data-modellers#16-common-antipatterns-avoid"
  },"639": {
    "doc": "CluedIn for Data Modellers",
    "title": "17) Operating Rhythm",
    "content": "Daily . | Skim DQ dashboard for your entities; validate last export. | Chat with Stewards on new rule breaches; raise PRs for tweaks. | . Weekly . | Add 1–2 additive attributes or a new relationship. | Review export usage &amp; feedback; plan next iteration. | . Monthly . | Version review: candidates for _vN due to breaking needs. | Glossary audit; ensure CDEs are complete and accurate. | . ",
    "url": "/kb/cluedin-data-modellers#17-operating-rhythm",
    
    "relUrl": "/kb/cluedin-data-modellers#17-operating-rhythm"
  },"640": {
    "doc": "CluedIn for Data Modellers",
    "title": "18) “What Good Looks Like”",
    "content": ". | Canonical model simple, well‑documented, and stable. | Exports versioned, with clear contracts and predictable SLAs. | DQ metrics trending up; few long‑lived red alerts. | Consumers can self‑serve because semantics are explicit. | Change velocity stays high without breaking downstreams. | . ",
    "url": "/kb/cluedin-data-modellers#18-what-good-looks-like",
    
    "relUrl": "/kb/cluedin-data-modellers#18-what-good-looks-like"
  },"641": {
    "doc": "CluedIn for Data Modellers",
    "title": "19) Templates &amp; Snippets",
    "content": "19.1 Canonical Entity Template . entity: &lt;EntityName&gt; description: \"&lt;short purpose&gt;\" keys: surrogate: &lt;entity&gt;_id natural: [] attributes: &lt;field&gt;: { type: string|int|decimal|timestamp|bool, required: false, labels: [], constraints: {} } relationships: [] labels: [] lineage_fields: [source_system, source_id, source_updated_at] . 19.2 Relationship Bridge (M:M) . entity: PersonProduct keys: { surrogate: person_product_id } attributes: person_id: { type: string, required: true } product_id: { type: string, required: true } role: { type: string, domain: [\"OWNER\",\"VIEWER\",\"BUYER\"] } valid_from: { type: timestamp } valid_to: { type: timestamp } constraints: - fk: { from: person_id, to: Person.person_id } - fk: { from: product_id, to: Product.product_id } . 19.3 Export Contract (Ops Wide) . name: customers_wide_v1 primary_key: customer_id delivery: { type: sql-table, schedule: daily } schema: - name: customer_id ; type: string ; required: true - name: primary_email ; type: string ; labels: [\"PII\"] - name: total_orders ; type: int - name: last_order_at ; type: timestamp compatibility: additive_only consumers: [\"crm_ops\",\"support_reporting\"] . 19.4 Enum Definition . enum: order_status values: [\"PENDING\",\"PAID\",\"SHIPPED\",\"CANCELLED\",\"REFUNDED\"] . 19.5 Changelog Entry . date: \"2025-08-24\" change: \"Add order_status enum and relationship Person ↔ Organization\" impact: \"Enables churn analysis by employer\" links: [\"PR#314\", \"staging diff\", \"audit event #1578\"] . Summary: As a Data Modeller on CluedIn, design minimal but powerful canonical entities, encode semantics, choose the right temporal pattern, and publish consumer‑friendly exports with contracts—then iterate safely. Keep identity, labels, and lineage front‑and‑center, and version any breaking change. ",
    "url": "/kb/cluedin-data-modellers#19-templates--snippets",
    
    "relUrl": "/kb/cluedin-data-modellers#19-templates--snippets"
  },"642": {
    "doc": "CluedIn for Data Modellers",
    "title": "CluedIn for Data Modellers",
    "content": " ",
    "url": "/kb/cluedin-data-modellers",
    
    "relUrl": "/kb/cluedin-data-modellers"
  },"643": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "CluedIn for Data Governance Managers — Operating Manual",
    "content": "Audience: Data Governance Managers, Chiefs of Data/Privacy, Risk &amp; Compliance leads Goal: Provide a practical, prescriptive playbook to govern data on CluedIn—policy/controls, ownership, privacy &amp; consent, access &amp; retention, DQ governance, lineage, AI use, audit evidence, and operating cadence. This manual is tool-aware (CluedIn), but policy-oriented. It favors governance-as-code, minimal bureaucracy, and measurable outcomes. ",
    "url": "/kb/cluedin-data-governance-manager#cluedin-for-data-governance-managers--operating-manual",
    
    "relUrl": "/kb/cluedin-data-governance-manager#cluedin-for-data-governance-managers--operating-manual"
  },"644": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "People &amp; Ownership . | Confirm domain owners and data stewards for top entities (e.g., Person, Organization, Order). | Stand up the Data Governance Council (DGC) rhythm and RACI. | Publish the use‑case brief and policy pack for day‑1 scope. | . Policy &amp; Access . | Approve classification scheme (PII/Restricted/Confidential/Public). | Enable SSO‑only, group‑based roles, least privilege defaults. | Define SoD (Segregation of Duties) and approval thresholds. | . Controls &amp; Observability . | Turn on audit log export and set retention ≥ your compliance need. | Require export contracts and PII masking policies by default. | Select DQ KPIs and set initial thresholds/alerts. | . Privacy &amp; Retention . | Map legal bases and purpose for PII processing. | Approve retention schedules and deletion/hold workflows. | Validate DSR (Data Subject Request) process with runbook. | . ",
    "url": "/kb/cluedin-data-governance-manager#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-data-governance-manager#0-your-first-48-hours-checklist"
  },"645": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "1) Governance Model &amp; Roles",
    "content": "1.1 RACI Snapshot . | Area | DGC | Governance Manager | Admin | Data Steward | Data Engineer | Legal/Privacy |—|—|—|—|—|—|—| Classifications &amp; policies | A/R | R | C | C | C | C/A | Roles &amp; access | A/R | R | R | C | C | C | DQ KPIs &amp; SLAs | A | R | C | R | R | C | Retention &amp; deletion | A | R | C | C | C | R | AI guardrails | A | R | C | C | C | R | Audit evidence | A | R | C | C | C | C | . A = Accountable, R = Responsible, C = Consulted . 1.2 Governance Objects . | Classification scheme (labels/tiers). | Policies (masking, row/column access, export approvals). | Export contracts (schema, semantics, SLA). | DQ metrics &amp; thresholds, issue register. | Lineage and glossary (CDEs—Critical Data Elements). | Retention schedule and legal hold register. | AI policy and model/agent registry. | . ",
    "url": "/kb/cluedin-data-governance-manager#1-governance-model--roles",
    
    "relUrl": "/kb/cluedin-data-governance-manager#1-governance-model--roles"
  },"646": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "2) Classification &amp; Policy-as-Code",
    "content": "2.1 Standard Labels . labels: - name: PII description: \"Personal data subject to privacy rules\" - name: Restricted description: \"Sensitive business data; limited access\" - name: Confidential - name: Public . 2.2 Masking &amp; Access Policies (CluedIn-style pseudo) . policy: mask_email_default target: entity:Person.field:email actions: [read] effect: allow_with_mask mask: \"partial_email\" # e.g., a***@example.com unless: - role_in: [\"Data Steward\",\"Administrator\"] labels_required: [\"PII\"] . policy: row_filter_region target: entity:Order actions: [read] effect: allow_when when: \"record.region in user.allowed_regions\" applies_to: - roles: [\"Analyst\",\"Viewer\"] . 2.3 Export Approval for Sensitive Data . policy: export_requires_approval target: export:* actions: [promote] # moving to prod effect: require_approval when: \"export.contains_label('PII') or export.contains_label('Restricted')\" approvers: [\"Data Governance Manager\",\"Data Protection Officer\"] . Principle: Policies must be machine-evaluable, versioned, and reviewed via PRs. ",
    "url": "/kb/cluedin-data-governance-manager#2-classification--policy-as-code",
    
    "relUrl": "/kb/cluedin-data-governance-manager#2-classification--policy-as-code"
  },"647": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "3) Access Control &amp; Segregation of Duties (SoD)",
    "content": ". | Group to Role mapping from IdP; no direct user grants unless time-boxed. | Least privilege defaults; read-mostly for wide audiences. | SoD matrix to prevent a single actor from authoring and approving sensitive changes. | . 3.1 Example SoD . | Action | Allowed | Requires Approval |—|—|—| Create export with PII | Engineer/Steward | Governance Manager | Change masking policy | Admin | Governance Manager + DPO | Grant Admin role | Admin | DGC chair approval | Create long-lived API token | Admin/Engineer | Governance Manager | . ",
    "url": "/kb/cluedin-data-governance-manager#3-access-control--segregation-of-duties-sod",
    
    "relUrl": "/kb/cluedin-data-governance-manager#3-access-control--segregation-of-duties-sod"
  },"648": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "4) Data Quality Governance",
    "content": "4.1 Select KPIs (by entity/CDE) . | Completeness (required fields non-null %) | Validity (regex/domain rules) | Uniqueness (duplicate rate) | Consistency (cross-field rules) | Timeliness (source→export latency) | . 4.2 KPI Template . entity: Person kpis: completeness_email: { warn: \"&gt;= 0.98\", fail: \"&lt; 0.95\" } validity_email_regex: { warn: \"&gt;= 0.98\", fail: \"&lt; 0.95\" } duplicate_rate_email: { warn: \"&lt;= 0.03\", fail: \"&gt; 0.05\" } alerts: - metric: validity_email_regex action: \"notify #data-quality\" review_cadence: \"weekly\" . 4.3 Issue Management . | Central DQ issue register with owner, ETA, and business impact. | SLAs: high-severity breach triage within 24h, fix within 7 days or approved waiver. | Evidence: charts, logs, and audit events linked to each closure. | . ",
    "url": "/kb/cluedin-data-governance-manager#4-data-quality-governance",
    
    "relUrl": "/kb/cluedin-data-governance-manager#4-data-quality-governance"
  },"649": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "5) Privacy, Consent &amp; Purpose Limitation",
    "content": "5.1 Legal Bases &amp; Purposes . Maintain a register of processing purposes and legal bases (GDPR Art. 6, CCPA/CPRA analogs). Map fields/entities to purposes. purpose: \"Customer Support\" legal_basis: \"Legitimate Interests\" entities: [\"Person\",\"Ticket\"] fields: Person.email: [\"PII\"] retention: \"3 years from last activity\" . 5.2 DSR (Access/Deletion/Correction) . | Standard DSR runbook with time targets (e.g., 30 days). | Use CluedIn search and policies to locate, mask, or delete records. | Soft-delete first; schedule hard-delete post-hold checks. | Record audit trail for every DSR. | . 5.3 Anonymization &amp; Pseudonymization . | Prefer masking/hasing for analytics where possible. | Track re-identification risk; document controls. | . ",
    "url": "/kb/cluedin-data-governance-manager#5-privacy-consent--purpose-limitation",
    
    "relUrl": "/kb/cluedin-data-governance-manager#5-privacy-consent--purpose-limitation"
  },"650": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "6) Retention, Deletion &amp; Legal Hold",
    "content": "6.1 Retention Schedules . Create per-entity schedules with legal references; encode as policy. policy: retention_person target: entity:Person actions: [delete] effect: schedule_delete when: \"now() &gt; record.last_activity + duration('3 years')\" exceptions: - \"legal_hold == true\" . 6.2 Legal Hold . | Admin can set legal_hold=true at entity or export scope. | Prevents deletion; logs an audit event with case ID. | Review holds quarterly with Legal. | . 6.3 Destruction Evidence . | Produce destruction certificates with counts, time window, and correlation IDs. | Store centrally with retention policy evidence. | . ",
    "url": "/kb/cluedin-data-governance-manager#6-retention-deletion--legal-hold",
    
    "relUrl": "/kb/cluedin-data-governance-manager#6-retention-deletion--legal-hold"
  },"651": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "7) Catalog, Glossary &amp; Lineage",
    "content": "7.1 Glossary &amp; CDEs . | Each CDE has definition, owner, calculation notes, and DQ caveats. | Stewards maintain; Governance approves changes. | . term: \"Active Customer\" definition: \"Customer with at least one completed order in the last 90 days\" owner: \"Sales Ops\" cdes: [\"Person.id\",\"Order.completed_at\"] dq_notes: \"Exclude orders with status in ('cancelled','fraud')\" . 7.2 Lineage &amp; Purview . | Ensure exports are scannable by Purview or push Atlas lineage after runs. | Require lineage for all prod exports; no “unknown source” data in BI. | . ",
    "url": "/kb/cluedin-data-governance-manager#7-catalog-glossary--lineage",
    
    "relUrl": "/kb/cluedin-data-governance-manager#7-catalog-glossary--lineage"
  },"652": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "8) AI Governance on CluedIn",
    "content": "8.1 Allowed Uses . | Read‑only analysis by Agents on masked datasets by default. | Suggestion workflows (validations, dedup rules) require human approval. | Auto-fix limited to deterministic transformations with rollback. | . 8.2 Guardrails (policy sketch) . policy: ai_mask_pii target: ai:agents actions: [read] effect: allow_when when: \"dataset.view == 'masked' and agent.mode in ['analysis','suggest']\" . | Log prompts &amp; outputs; retain for investigation period. | Maintain an AI model/agent registry with owner, purpose, and data scope. | . ",
    "url": "/kb/cluedin-data-governance-manager#8-ai-governance-on-cluedin",
    
    "relUrl": "/kb/cluedin-data-governance-manager#8-ai-governance-on-cluedin"
  },"653": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "9) Audit, Evidence &amp; Compliance",
    "content": "9.1 Audit Log Requirements . | Retention ≥ policy (e.g., 3–7 years). | Immutable storage (WORM) where mandated. | Coverage: SSO events, role grants, token lifecycle, policy changes, export promotions, dedup merges, retention deletes. | . Audit record (illustrative) . { \"ts\": \"2025-08-23T11:02:44Z\", \"actor\": \"tiw@cluedin.com\", \"action\": \"policy.update\", \"target\": \"mask_email_default\", \"old\": {\"mask\": \"none\"}, \"new\": {\"mask\": \"partial_email\"}, \"ip\": \"203.0.113.5\", \"correlation_id\": \"a6c9-...-4f\" } . 9.2 Control Library &amp; Evidence Plan . | Map controls → evidence (log queries, screenshots, configs). | Pre‑build audit packets (SSO config, RBAC matrices, policy files, retention jobs, DQ dashboards, incident postmortems). | . ",
    "url": "/kb/cluedin-data-governance-manager#9-audit-evidence--compliance",
    
    "relUrl": "/kb/cluedin-data-governance-manager#9-audit-evidence--compliance"
  },"654": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "10) Change Management &amp; Exceptions",
    "content": ". | All policy/config changes via PR with risk notes and rollback. | CAB/DGC approves sensitive changes (PII exports, masking off). | Exception register with owner, expiry, and mitigation; auto‑review cadence. | . Change ticket template . change: \"Enable dedup auto-approve @ 0.97 for Person\" risk: \"Potential false merges\" mitigation: \"Raise reviewer sampling, add unmerge runbook\" rollback: \"Set threshold to 1.0; disable auto-approve\" approvers: [\"Governance Manager\",\"DPO\"] . ",
    "url": "/kb/cluedin-data-governance-manager#10-change-management--exceptions",
    
    "relUrl": "/kb/cluedin-data-governance-manager#10-change-management--exceptions"
  },"655": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "11) Incident Response (Data Incidents/Breaches)",
    "content": "Trigger examples: PII exposure, policy disabled in prod, unauthorized token use, export schema exposing restricted fields. Runbook (condensed) . | Identify: Alert triage, correlation_id, scope quantification. | Contain: Re-enable policies, revoke tokens, pause exports. | Eradicate: Fix mapping/cleaning/policy root cause. | Recover: Backfill corrected outputs; notify consumers. | Notify: Legal assesses regulatory notifications (GDPR 72h, etc.). | Review: Post-incident with preventive controls &amp; tests. | . ",
    "url": "/kb/cluedin-data-governance-manager#11-incident-response-data-incidentsbreaches",
    
    "relUrl": "/kb/cluedin-data-governance-manager#11-incident-response-data-incidentsbreaches"
  },"656": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "12) Operating Cadence",
    "content": "Weekly DGC (30–45 min) . | DQ breaches &amp; trends; top 5 risks. | Policy changes awaiting approval. | Export lineage gaps and consumers onboarded. | Retention/DSR stats; exceptions expiring. | . Monthly Governance Review . | KPI scorecard by domain, audit log sampling results, AI usage review, access recertification outcomes. | . Quarterly . | Maturity assessment; roadmap; control gap remediation. | . ",
    "url": "/kb/cluedin-data-governance-manager#12-operating-cadence",
    
    "relUrl": "/kb/cluedin-data-governance-manager#12-operating-cadence"
  },"657": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "13) KPIs &amp; Scorecard (Examples)",
    "content": ". | % CDEs with owner, glossary entry, lineage ✅ | DQ: completeness/validity at or above threshold for top entities ✅ | Access: % users via group‑based roles; # exceptions open ↓ | Privacy: DSR SLA hit rate; # PII exports with approvals ✅ | Retention: % due-for-delete completed; # holds reviewed ✅ | AI governance: % agents constrained to masked datasets ✅ | . scorecard: cde_coverage: { target: \"100%\", actual: \"92%\" } pii_export_approvals: { target: \"100%\", actual: \"100%\" } dsr_sla: { target: \"&gt;= 95%\", actual: \"97%\" } . ",
    "url": "/kb/cluedin-data-governance-manager#13-kpis--scorecard-examples",
    
    "relUrl": "/kb/cluedin-data-governance-manager#13-kpis--scorecard-examples"
  },"658": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "14) Templates &amp; Artifacts",
    "content": "14.1 Classification Policy (excerpt) . tiers: - Public - Confidential - Restricted labels: PII: description: \"Personal data\" defaults: mask_read: true export_requires_approval: true . 14.2 Export Contract (governance fields) . name: contacts_v1 owner: \"Sales Ops\" pii: true approval_ids: [\"chg-2025-1032\"] sla: freshness_p95_minutes: 60 lineage_required: true . 14.3 Access Request Workflow . workflow: request_export_access steps: - submit: requester -&gt; manager_approval - review: governance -&gt; approve_or_deny - provision: admin -&gt; role_grant(group) - audit: evidence_link + expiry_date . 14.4 DPIA Checklist (snapshot) . | Purpose &amp; legal basis documented | Data categories &amp; flows mapped | Risks &amp; mitigations listed | Residual risk accepted by DPO | Re‑assessment date set | . ",
    "url": "/kb/cluedin-data-governance-manager#14-templates--artifacts",
    
    "relUrl": "/kb/cluedin-data-governance-manager#14-templates--artifacts"
  },"659": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "15) Maturity Roadmap",
    "content": "Level 1 → 2 . | Classifications applied to top 3 entities; basic masking; manual approvals. | DQ KPIs defined; weekly review established. | . Level 2 → 3 . | Policies as code; PR reviews; automated approvals with conditions. | Full lineage coverage; DSR automation; retention jobs live. | . Level 3 → 4 . | Predictive DQ &amp; anomaly detection; auto‑fix playbooks with guardrails. | AI agents widely used on masked data; real‑time policy observability. | . ",
    "url": "/kb/cluedin-data-governance-manager#15-maturity-roadmap",
    
    "relUrl": "/kb/cluedin-data-governance-manager#15-maturity-roadmap"
  },"660": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "16) Quick Reference (Who to Call)",
    "content": ". | Admin: SSO/RBAC, tokens, feature toggles. | Engineer: mappings, cleaning, exports, incident on-call. | Steward: validations, dedup, glossary, labels. | Legal/Privacy: DPIA, DSR, notifications, holds. | You (Gov Mgr): policies, approvals, exceptions, audits, council. | . Bottom line: Treat governance as a product: versioned configs, small safe changes, clear owners, and measurable outcomes. With CluedIn, encode policies, label data, control access, automate retention, prove lineage, and keep your audit shelf ready. ",
    "url": "/kb/cluedin-data-governance-manager#16-quick-reference-who-to-call",
    
    "relUrl": "/kb/cluedin-data-governance-manager#16-quick-reference-who-to-call"
  },"661": {
    "doc": "CluedIn for Data Governance Manager",
    "title": "CluedIn for Data Governance Manager",
    "content": " ",
    "url": "/kb/cluedin-data-governance-manager",
    
    "relUrl": "/kb/cluedin-data-governance-manager"
  },"662": {
    "doc": "CluedIn for System Integrators",
    "title": "CluedIn for System Integrators — Integration Cookbook",
    "content": "Audience: System Integrators, Professional Services, SI practice leads Goal: Provide proven patterns, templates, and guardrails for integrating CluedIn with the wider ecosystem: Data Lakes, Data Engineering tools, Data Governance tools, Power Platform &amp; AI stacks (Power Automate, Power Apps, Copilot, Azure AI Foundry, OpenAI, Claude, Ollama), and Dataverse. Principles: ELT-first, event-friendly, contracts &amp; policies as code, least privilege, and observability by default. ",
    "url": "/kb/cluedin-system-integrators#cluedin-for-system-integrators--integration-cookbook",
    
    "relUrl": "/kb/cluedin-system-integrators#cluedin-for-system-integrators--integration-cookbook"
  },"663": {
    "doc": "CluedIn for System Integrators",
    "title": "0) Engagement Blueprint (First Week)",
    "content": ". | Day 1–2: Confirm SSO/roles, network allowlists, non-prod workspace, export destinations. | Day 3–4: Stand up one Ingestion Endpoint (stream or batch) + one Export Target (table or file). | Day 5: Wire catalog scan/lineage, alerts, and a demo to Power BI/Apps. | Deliverables: Diagrams (L0/L1), export contract, policy/masking file, runbook, and CI pipeline. | . ",
    "url": "/kb/cluedin-system-integrators#0-engagement-blueprint-first-week",
    
    "relUrl": "/kb/cluedin-system-integrators#0-engagement-blueprint-first-week"
  },"664": {
    "doc": "CluedIn for System Integrators",
    "title": "1) Patterns Overview",
    "content": "| Integration Area | Primary Pattern | Protocols | Artifacts to Deliver | . | Data Lakes | Export Parquet/Delta, partitioned | S3/ADLS/GCS | Export config, partition spec, retention policy | . | Data Engineering | Orchestrate deploy/run/monitor | Airflow/ADF/Fabric/Glue | DAG/pipeline JSON/YAML, webhook triggers, CI | . | Data Governance | Catalog scan + lineage push | Purview/Collibra/Atlas | Scan config, lineage job, label mapping | . | Power Platform &amp; AI | Event in/out, API calls, guarded prompts | HTTP/Kafka/Webhooks/OAuth | Flow/App sketches, HMAC verify, AI guardrails | . | Dataverse | Delta → CluedIn, exports back to tables | Dataverse APIs, Power Automate | Connector flows, backfill scripts, SLA docs | . ",
    "url": "/kb/cluedin-system-integrators#1-patterns-overview",
    
    "relUrl": "/kb/cluedin-system-integrators#1-patterns-overview"
  },"665": {
    "doc": "CluedIn for System Integrators",
    "title": "2) Data Lakes (ADLS / S3 / GCS)",
    "content": "2.1 Export Shapes . | Columnar files: Parquet (preferred), optional Delta/iceberg in your lake. | Partitioning: by load_date or business date (e.g., event_date=YYYY-MM-DD). | Layout: s3://bucket/mdm/customers_wide_v1/event_date=2025-08-24/part-000.parquet | . 2.2 Export Config (pseudo-JSON) . { \"name\": \"customers_wide_v1\", \"type\": \"file-parquet\", \"options\": { \"connection\": \"s3://company-analytics\", \"path\": \"mdm/customers_wide_v1/\", \"partition_by\": [\"event_date\"], \"compression\": \"snappy\", \"overwrite\": false }, \"schedule\": \"0 * * * *\" } . 2.3 Lake Guardrails . | Small files kill performance: use 128–512MB target file sizes. | Schema evolution: additive by default; break → new _vN path. | Retention: lifecycle policies in S3/ADLS; document RPO/RTO. | . ",
    "url": "/kb/cluedin-system-integrators#2-data-lakes-adls--s3--gcs",
    
    "relUrl": "/kb/cluedin-system-integrators#2-data-lakes-adls--s3--gcs"
  },"666": {
    "doc": "CluedIn for System Integrators",
    "title": "3) Data Engineering Tools",
    "content": "3.1 Airflow DAG (sketch) . from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime import requests, os def trigger_export(**ctx): r = requests.post( f\"{os.environ['CLUE_HOST']}/api/exports/run\", json={\"name\":\"customers_wide_v1\"}, headers={ \"Authorization\": f\"Bearer {os.environ['CLUE_TOKEN']}\", \"X-Correlation-Id\": ctx['run_id'] }, timeout=30) r.raise_for_status() with DAG(\"cluedin_customers_export\", start_date=datetime(2025,8,1), schedule=\"@hourly\", catchup=False) as dag: run = PythonOperator(task_id=\"run_export\", python_callable=trigger_export) . 3.2 Azure Data Factory / Fabric Data Pipelines . | Use Web activity to POST to CluedIn (ingest/export). | Handle retry with exponential backoff; bubble up non-2xx. | Emit correlation_id from pipeline run ID. | . ADF Web activity body (example) . { \"name\": \"warehouse-contacts-v1\" } . 3.3 AWS Glue / GCP . | Use Job step to call CluedIn APIs or read CluedIn exports. | Favor Parquet reads; push metrics back to CloudWatch/Stackdriver. | . 3.4 dbt / Semantic Layers . | dbt models consume CluedIn exports; keep contracts aligned. | Avoid heavy transformations in CluedIn mapping when dbt is authoritative downstream. | . ",
    "url": "/kb/cluedin-system-integrators#3-data-engineering-tools",
    
    "relUrl": "/kb/cluedin-system-integrators#3-data-engineering-tools"
  },"667": {
    "doc": "CluedIn for System Integrators",
    "title": "4) Data Governance Tools",
    "content": "4.1 Purview / Collibra Catalog Scans . | Register warehouse/lake connections where CluedIn writes. | Schedule hourly/daily scans; tag datasets with owners and classifications. | . 4.2 Lineage (Atlas-style) . POST https://&lt;purview&gt;/api/atlas/v2/lineage { \"process\": { \"typeName\": \"cluedin_export\", \"attributes\": { \"name\":\"customers_wide_v1\",\"qualifiedName\":\"cluedin.export.customers_wide_v1\"} }, \"inputs\": [{\"qualifiedName\":\"cluedin.entity.Person\"}], \"outputs\": [{\"qualifiedName\":\"s3://company-analytics/mdm/customers_wide_v1\"}] } . 4.3 Classification Sync . | Map CluedIn labels (PII, Restricted) to catalog classifications. | Drive masking policies consistently in BI and lake security layers. | . ",
    "url": "/kb/cluedin-system-integrators#4-data-governance-tools",
    
    "relUrl": "/kb/cluedin-system-integrators#4-data-governance-tools"
  },"668": {
    "doc": "CluedIn for System Integrators",
    "title": "5) Power Platform &amp; AI Stacks",
    "content": "5.1 Power Automate (Flow) → CluedIn Ingestion . | Trigger: When a row is added in Dataverse (or any connector). | Action: HTTP POST to CluedIn /api/ingest with OAuth token. | . Flow HTTP action body . { \"source\": \"dataverse-contacts\", \"payload\": { \"id\": \"@{triggerBody()?['contactid']}\", \"email\": \"@{triggerBody()?['emailaddress1']}\", \"updated_at\": \"@{utcNow()}\" } } . 5.2 CluedIn Webhook → Flow / Teams . | Register export.succeeded webhook → verify HMAC → post to Teams. | . Node verify sketch . const crypto = require(\"crypto\"); function verify(sig, raw, secret){ const h = crypto.createHmac(\"sha256\", secret).update(raw).digest(\"hex\"); return crypto.timingSafeEqual(Buffer.from(sig,\"hex\"), Buffer.from(h,\"hex\")); } . 5.3 Power Apps . | Stewarding app over CluedIn APIs (dedup review, fix invalids). | Use service principal; enforce role checks server-side. | . 5.4 Copilot (Power Platform) . | Feed versioned exports with clear semantics. | Mask PII for prompts; restrict to masked views for exploratory agents. | . 5.5 Azure AI Foundry / OpenAI / Claude / Ollama . Pattern: AI Agents with Guardrails . | Read scope: masked datasets or sample subsets. | Modes: analysis and suggest; no auto_fix until reviewed. | Prompt logging: store prompt/response IDs with correlation_id. | PII/Secrets: redact before sending to APIs; keep model configs in code. | . Example: calling an AI analysis job (pseudo) . POST /api/ai/agents/run { \"agent\": \"dq-analyzer\", \"target\": { \"entity\": \"Person\" }, \"mode\": \"analysis\", \"options\": { \"sample\": 5000 } } . Local inference (Ollama) . | Use Ollama for in‑VPC/in‑laptop development; treat it like any external AI: masked data only and clear retention policy. | . ",
    "url": "/kb/cluedin-system-integrators#5-power-platform--ai-stacks",
    
    "relUrl": "/kb/cluedin-system-integrators#5-power-platform--ai-stacks"
  },"669": {
    "doc": "CluedIn for System Integrators",
    "title": "6) Dataverse Integration",
    "content": "6.1 Inbound (to CluedIn) . | Dataverse → Flow → CluedIn via HTTP ingestion for deltas. | For backfills, use Dataverse Web API paging and push batches to CluedIn. | Emit idempotency key (id + updated_at) and correlation_id. | . 6.2 Outbound (to Dataverse) . | Consume CluedIn export (wide table) and Upsert to Dataverse entity via Web API. | Respect concurrency (If‑Match ETag) and retry on 429s. | . Dataverse upsert (HTTP) . PATCH https://&lt;org&gt;.crm.dynamics.com/api/data/v9.2/contacts(&lt;guid&gt;) If-Match: * Content-Type: application/json { \"emailaddress1\": \"a@example.com\", \"firstname\": \"Ada\", \"lastname\": \"Lovelace\" } . 6.3 Identity &amp; Ownership . | Align Person/Account keys with Dataverse GUIDs or keep a link table in the export. | Document survivorship rules in the runbook for conflicts. | . ",
    "url": "/kb/cluedin-system-integrators#6-dataverse-integration",
    
    "relUrl": "/kb/cluedin-system-integrators#6-dataverse-integration"
  },"670": {
    "doc": "CluedIn for System Integrators",
    "title": "7) Security &amp; Compliance Playbook",
    "content": ". | SSO-only with OIDC/SAML; group‑based roles mapped to least privilege. | Tokens: short-lived; rotate ≤ 90 days; store in a vault; audit usage. | Policies: column masking (PII), row filters (region/tenant), export promotion approvals for sensitive labels. | Webhooks: HMAC signatures; allowlist source IPs; idempotent handlers. | AI: masked datasets by default; prompt logging; model registry with owner/purpose. | Audit: retain logs for 1–7 years; include SSO, role changes, tokens, policy updates, merges, and export promotions. | . ",
    "url": "/kb/cluedin-system-integrators#7-security--compliance-playbook",
    
    "relUrl": "/kb/cluedin-system-integrators#7-security--compliance-playbook"
  },"671": {
    "doc": "CluedIn for System Integrators",
    "title": "8) Observability &amp; SLAs",
    "content": ". | Metrics: ingestion success, DLQ size, export rows/latency, DQ KPIs (validity, completeness, duplicates), webhook retries. | Dashboards: per domain/export; include top error classes and freshness. | Alerts: export failure, schema drift, DQ breach, 429 storms, unusual token usage. | Runbooks: incident steps—contain, diagnose, rollback, backfill—with correlation_id examples. | . ",
    "url": "/kb/cluedin-system-integrators#8-observability--slas",
    
    "relUrl": "/kb/cluedin-system-integrators#8-observability--slas"
  },"672": {
    "doc": "CluedIn for System Integrators",
    "title": "9) CI/CD &amp; Config-as-Code",
    "content": ". | Keep mappings, cleaning, validations, policies, and exports in Git. | Pipeline: PR → staging export diff → approval → prod deploy. | Validate with schema/domain/FK tests and export contract checks; publish release notes. | . GitHub Actions sketch . name: cluedin-deploy on: [pull_request, push] jobs: validate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - run: ./tools/validate-config.sh deploy-staging: if: github.ref == 'refs/heads/main' steps: - run: ./tools/apply.sh env/test deploy-prod: needs: deploy-staging steps: - run: ./tools/apply.sh env/prod . ",
    "url": "/kb/cluedin-system-integrators#9-cicd--config-as-code",
    
    "relUrl": "/kb/cluedin-system-integrators#9-cicd--config-as-code"
  },"673": {
    "doc": "CluedIn for System Integrators",
    "title": "10) Ready-to-Use Templates",
    "content": "10.1 Export Contract (with governance) . name: customers_wide_v1 owner: \"Sales Ops\" primary_key: customer_id delivery: { type: file-parquet, schedule: hourly } sla: { freshness_p95_minutes: 60 } labels: [\"PII:email\"] compatibility: additive_only lineage_required: true approval_required_when_labels: [\"PII\",\"Restricted\"] . 10.2 Policy: Mask Email for Non-Owners . policy: mask_email_default target: entity:Person.field:email actions: [read] effect: allow_with_mask mask: \"partial_email\" unless: [{ role_in: [\"Data Steward\",\"Administrator\"] }] . 10.3 Webhook Registration . POST /api/webhooks { \"name\": \"teams-export-success\", \"events\": [\"export.succeeded\"], \"url\": \"https://example.com/hooks/teams\", \"secret\": \"&lt;HMAC&gt;\" } . 10.4 Airflow Backfill Operator (sketch) . def backfill(start, end, step=\"1d\"): # call CluedIn backfill endpoint or replay DLQ in windows ... ",
    "url": "/kb/cluedin-system-integrators#10-ready-to-use-templates",
    
    "relUrl": "/kb/cluedin-system-integrators#10-ready-to-use-templates"
  },"674": {
    "doc": "CluedIn for System Integrators",
    "title": "11) Common Pitfalls &amp; How to Avoid Them",
    "content": ". | Pre-cleaning upstream → breaks lineage and duplicates work. Use CluedIn cleaning projects. | No contracts/versioning → downstream breakage. Always publish contracts and bump _vN on breakers. | Tiny Parquet files → slow analytics. Batch/compact. | Unbounded AI access → privacy risk. Enforce masked views + logs. | Webhook handlers not idempotent → duplicate side effects. Store event IDs and use upserts. | . ",
    "url": "/kb/cluedin-system-integrators#11-common-pitfalls--how-to-avoid-them",
    
    "relUrl": "/kb/cluedin-system-integrators#11-common-pitfalls--how-to-avoid-them"
  },"675": {
    "doc": "CluedIn for System Integrators",
    "title": "12) One-Page SI Checklist",
    "content": ". | Ingestion endpoints automated; no manual uploads. | One end‑to‑end in→map→clean→dedup→out path live. | Export contract + partitioning + retention documented. | Catalog scan + lineage push configured. | Policies/labels wired; approvals enforced for PII exports. | Dashboards + alerts + runbooks active. | CI/CD promotion path with rollback tested. | Power Platform/AI demos working end‑to‑end. | Dataverse upsert path proven with concurrency + rate limits. | . Outcome: With these patterns and templates, you can plug CluedIn into lakes, warehouses, orchestration, catalogs, Power Platform &amp; AI stacks, and Dataverse quickly—while preserving security, lineage, and SLAs. Clone the snippets, fill in environment details, and ship a thin slice first; iterate safely with contracts and policies. ",
    "url": "/kb/cluedin-system-integrators#12-one-page-si-checklist",
    
    "relUrl": "/kb/cluedin-system-integrators#12-one-page-si-checklist"
  },"676": {
    "doc": "CluedIn for System Integrators",
    "title": "CluedIn for System Integrators",
    "content": " ",
    "url": "/kb/cluedin-system-integrators",
    
    "relUrl": "/kb/cluedin-system-integrators"
  },"677": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Go Live in 14 Days with CluedIn: A Step‑by‑Step Playbook",
    "content": "Audience: Data engineers, analytics engineers, solutions architects Goal: Get a working, production‑minded slice of your use case live in 14 days—with real data flowing in via Ingestion Endpoints and out via Export Targets, while building the habits that make CluedIn successful over time. ",
    "url": "/kb/14-days-live#go-live-in-14-days-with-cluedin-a-stepbystep-playbook",
    
    "relUrl": "/kb/14-days-live#go-live-in-14-days-with-cluedin-a-stepbystep-playbook"
  },"678": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Why this works",
    "content": "This plan embraces ELT and iteration: . | Push data as‑is (raw) into CluedIn. Don’t pre‑clean. Let CluedIn’s schema inference, lineage, mapping, and cleaning projects do the heavy lifting. | Start small (one incoming asset), ship data out early, and improve mapping and quality as you learn. | Automate ingestion (streaming or scheduled batches) so you never have to re‑upload files manually. | Lean on AI Agents to spot quality issues, suggest validations, and surface duplicates. | Measure progress with data quality metrics. Improve frequently in small increments. | Practice mapping changes: try, test, revert. Lower the cost of change. | . ",
    "url": "/kb/14-days-live#why-this-works",
    
    "relUrl": "/kb/14-days-live#why-this-works"
  },"679": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "14‑Day Overview (at a glance)",
    "content": "| Day | Theme | Outcome | . | 1 | Business slice &amp; target destinations | A crisp “thin slice” of the use case and where data will be exported | . | 2 | Ingestion Endpoints (stream/batch) | Endpoints created for one single source | . | 3 | First raw data in | One asset flowing into CluedIn “as‑is” | . | 4 | Minimal mapping v0 | Just enough mapping to recognize entities and relationships | . | 5 | Export Targets v0 | First export wired to BI / ops surface | . | 6 | End‑to‑end smoke test | Data reliably moving in → out on a schedule | . | 7 | Retrospective &amp; plan | Add tasks for Week 2 improvements | . | 8 | AI Agents for QA | Findings list: missingness, invalids, duplicates | . | 9 | Cleaning projects | First incremental fixes running on a cadence | . | 10 | Data quality metrics | Baselines + alerts; choose KPIs and thresholds | . | 11 | Iterative mapping | Practice change/test/rollback; improve coverage | . | 12 | Dedup projects | Deterministic rules + auto‑approve high‑confidence | . | 13 | Second asset | Expand model; regression check exports | . | 14 | Go‑live checklist | Runbook, ownership, on‑call and next 30/60/90 | . Scope guardrails: One source → one export in week 1. In week 2, add fixes, dedup, and one more source. That’s enough to go live credibly and safely. ",
    "url": "/kb/14-days-live#14day-overview-at-a-glance",
    
    "relUrl": "/kb/14-days-live#14day-overview-at-a-glance"
  },"680": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Prerequisites",
    "content": ". | CluedIn workspace access and a service account (API key) with Ingestion and Export permissions. | A target Export destination (e.g., data warehouse table, analytics lake, CRM, reverse‑ETL). | One initial source you can stream or batch on a schedule (e.g., Kafka topic, webhook producer, S3/Blob folder, database CDC). | A lightweight success metric (e.g., “Daily pipeline success + record count within ±5%”, “&lt;2% invalid emails”, “&lt;1% duplicates”). | . ",
    "url": "/kb/14-days-live#prerequisites",
    
    "relUrl": "/kb/14-days-live#prerequisites"
  },"681": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Core principles you’ll follow",
    "content": ". | Push to CluedIn via Ingestion Endpoints. Prefer live streams (Kafka/webhooks) or automated batches (S3/Blob schedulers). Avoid manual uploads after day 1. | Drop data “as‑is.” Don’t pre‑clean or reshape. CluedIn is ELT: land first, then transform/clean in place. | Don’t perfect mapping up front. Get data in and out first. Iteratively refine mapping as you learn. | Start small—one asset. Don’t model the world. Bring data in, then let requirements inform the model. | Lean on AI Agents. Use them to find issues, propose validations, and surface duplicate candidates. | Cleaning projects &gt; one‑off fixes. Ship frequent, small improvements on a schedule. | Measure constantly. Set DQ baselines now; raise the bar gradually. | Make mapping change cheap. Practice modify → test → revert. Embrace versioning. | Deduplicate safely. Start deterministic; auto‑approve only high‑confidence matches. | . ",
    "url": "/kb/14-days-live#core-principles-youll-follow",
    
    "relUrl": "/kb/14-days-live#core-principles-youll-follow"
  },"682": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Day‑by‑Day Guide",
    "content": "Day 1 — Define the thin slice and “data out” first . Objectives . | Pick one concrete question you want to answer or workflow to power. | Define the Export Target and format: table, topic, API, or file layout. | Identify one source (asset) that supports the outcome, even if imperfect. | . Artifacts . | Use‑case brief (1 page): purpose, users, downstream surface, SLAs. | Data contract (minimal) for the export: the final fields and semantics. | Runbook skeleton (who owns what, escalation, observability). | . Tip: If the export is a table, decide primary key, update strategy (upsert vs. append), and schema evolution policy now. Day 2 — Create Ingestion Endpoints (stream or batch) . Objectives . | Create an Ingestion Endpoint for one asset, using a streaming or scheduled path: . | Streaming: Kafka topic / webhook that produces JSON lines | Batch: S3/Blob folder with daily/hourly drops (CSV/JSON/Parquet) | . | . Example: HTTP ingestion (webhook) . curl -X POST \\ -H \"Authorization: Bearer &lt;CLUEDIN_API_KEY&gt;\" \\ -H \"Content-Type: application/json\" \\ -d '{\"source\":\"crm-contacts\",\"payload\":{\"id\":\"c_123\",\"email\":\"a@example.com\",\"first_name\":\"Ada\",\"last_name\":\"Lovelace\",\"updated_at\":\"2025-08-22T12:00:00Z\"}}' \\ https://&lt;YOUR_INGESTION_ENDPOINT_URL&gt;/ingest . Example: S3/Blob batch registration (pseudo‑JSON) . { \"name\": \"s3-sales-orders\", \"type\": \"s3-bucket-watch\", \"options\": { \"bucket\": \"acme-prod-orders\", \"prefix\": \"daily/\", \"file_types\": [\"json\",\"csv\",\"parquet\"], \"schedule\": \"cron(0 * * * ? *)\" // hourly } } . Checklist . | Endpoint created for a single asset | Producer (or object drop) automated | Authentication/authorization confirmed | Observability: ingestion success counter + dead‑letter path | . Rule: From here on, no manual uploads. Everything arrives via the endpoint. Day 3 — Land the first raw data “as‑is” . Objectives . | Send a small but real sample through the endpoint. | Verify CluedIn can parse and persist the records. | Capture lineage and metadata (source system, timestamps, version). | . Validation . | Ingestion dashboard shows new records | Sample record previewed (raw fields visible) | No pre‑cleaning done upstream | . Anti‑pattern: Don’t pause to cleanse/massage data before landing it. That’s what cleaning projects are for later this week. Day 4 — Minimal mapping v0 (don’t perfect it) . Objectives . | Create just‑enough entity mapping to recognize core entities (e.g., Person, Organization, Order) and a few relationships. | Avoid exotic field logic; capture IDs, names, keys, timestamps. | . Tactics . | Start with 5–10 high‑value fields only. | Mark known primary keys and foreign keys. | Add a default namespace for unmapped attributes. | . Success looks like . | Records appear as proper entities | Core joins/relationships visible | No blocking errors preventing export | . Remember: You will change this mapping many times. That’s normal. Day 5 — Wire up Export Targets v0 . Objectives . | Create the first export to your chosen destination (warehouse table, topic, API, file). Aim for a stable daily/hourly schedule. | . Example: table export config (pseudo‑JSON) . { \"name\": \"warehouse-contacts-v0\", \"type\": \"sql-table\", \"options\": { \"connection\": \"analytics-warehouse\", \"schema\": \"mdm\", \"table\": \"contacts_v0\", \"mode\": \"upsert\", \"primary_key\": [\"contact_id\"] }, \"mapping\": { \"contact_id\": \"Person.id\", \"email\": \"Person.email\", \"first_name\": \"Person.first_name\", \"last_name\": \"Person.last_name\", \"updated_at\": \"Person.updated_at\" }, \"schedule\": \"0 * * * *\" // hourly } . Validation . | Export task succeeds end‑to‑end | Downstream surface can query/use the output | Row counts within expected bounds | . Day 6 — End‑to‑end smoke test on a schedule . Objectives . | Run the pipeline in → through → out a few times on an automatic schedule. | Add alerts for failures and volume anomalies (±X% vs baseline). | . Checklist . | End‑to‑end schedule in place | Alerting/notifications configured | Runbook updated with failure modes and triage steps | . Day 7 — Retrospective &amp; Week‑2 plan . Discuss (30–45 min) . | What slowed us down? | What parts of mapping are confusing downstream? | Which fields are most error‑prone? | What small fixes would materially improve trust? | . Plan . | Pick 3–5 fixes for Week 2 (AI Agents, cleaning, metrics, dedup). | Nominate owners and define “done” for each. | . Day 8 — Use AI Agents to analyze data quality . Objectives . | Run AI Agents to scan entities for: . | Missingness, invalid formats, out‑of‑range values | Suggested validations (e.g., regex, domain lists, referential checks) | Duplicate candidates (e.g., person/org dedup) | . | . Workflow . | Run an analysis prompt or playbook against your entity set. | Review the findings list with counts and examples. | Accept or tailor proposed validation rules and dedup hints. | Open issues/tasks directly from findings. | . Deliverables . | Findings doc with top 5 issues | Proposed validation rules drafted | Candidate dedup keys identified | . Day 9 — Create cleaning projects (incremental fixes) . Objectives . | Build cleaning projects that remediate the top issues. Examples: . | Standardize phone/email formats | Trim/normalize names | Resolve country/state codes | Impute defaults (only where safe) | Enforce referential integrity | . | . Practices . | Small, re-runnable steps | Idempotent logic (safe to run multiple times) | Versioned transformations; review &amp; rollback friendly | Scheduled execution (hourly/daily) | . Success . | Cleaning pipeline runs on a cadence | Changes land back in CluedIn entities | No breaking changes to the export schema | . Day 10 — Establish data quality metrics &amp; alerts . Objectives . | Define and baseline key metrics for your use case: . | Completeness (non‑null %, required fields) | Validity (regex/domain compliance) | Uniqueness (duplicate rate) | Consistency (cross‑field rules) | Timeliness (SLA latency from source → export) | . | . Actions . | Capture baselines per entity/table. | Configure thresholds and alerts. | Add a dashboard panel to your downstream surface. | . Rule of thumb . | Improve by small increments frequently (e.g., +0.5–1.0% per day). | . Day 11 — Iterate on mapping (practice change/revert) . Objectives . | Evolve mapping to add fields demanded by downstream users. | Practice safe change: . | Branch/version your mapping | Apply change to a staging export | Compare outputs (row count, nulls, schema drift) | Promote or revert | . | . Checklist . | Version history for mapping changes | Staging export used for trials | Rollback tested at least once | . Day 12 — Set up deduplication projects . Objectives . | Create dedup projects for key entities (Person/Organization) with a deterministic first pass. | Auto‑approve only high‑confidence matches; queue the rest for review. | . Example deterministic rules (pseudo) . rules: - name: exact_email when: lower(email) matches lower(other.email) confidence: 0.98 - name: phone_e164 when: e164(phone) == e164(other.phone) confidence: 0.95 - name: org_name_address when: norm(name)==norm(other.name) and norm(addr)==norm(other.addr) confidence: 0.92 auto_approve_threshold: 0.97 queue_threshold: 0.85 . Success . | High‑confidence duplicates auto‑merged or flagged for auto‑action | Reviewer queue created for the “gray area” | Export logic updated to respect master records | . Day 13 — Add a second asset, adapt the model . Objectives . | Onboard the second source that enriches the same entities or introduces a related one. | Validate that mapping, cleaning, and dedup logic still holds. | . Regression checks . | Row counts within expected bounds | DQ metrics didn’t regress | Export schema unchanged (or managed evolution with version bump) | . Day 14 — Go‑live checklist &amp; handover . Go‑live gates . | End‑to‑end schedule stable for 3 consecutive runs | Alerts firing to the right channel with owners | DQ metrics tracked and improving | Dedup auto‑approve threshold validated | Runbook complete (how to pause/retry/backfill; RTO/RPO notes) | Ownership clear: source, mapping, cleaning, export | . Handover packet . | Use‑case brief, data contract, runbook, dashboard link, “what changed” log, next 30/60/90 roadmap. | . ",
    "url": "/kb/14-days-live#daybyday-guide",
    
    "relUrl": "/kb/14-days-live#daybyday-guide"
  },"683": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "How‑To Details &amp; Templates",
    "content": "A. Minimal export data contract (template) . name: contacts_v0 primary_key: contact_id delivery: type: sql-table schedule: hourly fields: - name: contact_id type: string semantics: stable synthetic id from source required: true - name: email type: string constraints: - regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" - name: first_name type: string - name: last_name type: string - name: updated_at type: timestamp semantics: last update from any contributing source . B. AI Agent prompt starter . “Analyze the Person entity for missingness, invalid formats, and duplicates. Suggest concrete validation rules with examples and propose deterministic dedup keys. Prioritize fixes that unlock downstream usage of the contacts_v0 export.” . C. Cleaning project sketch . project: normalize_contacts schedule: \"0 * * * *\" # hourly steps: - name: trim_names action: set field: first_name value: trim(first_name) - name: normalize_email action: set field: email value: lower(trim(email)) - name: e164_phone when: phone is not null action: set field: phone value: to_e164(phone, default_country=\"US\") - name: drop_impossible_dates when: birthdate &lt; \"1900-01-01\" or birthdate &gt; now() action: set field: birthdate value: null . D. Data quality metric tracker (example) . entity: Person metrics: completeness: email_non_null: pct_non_null(email) name_non_null: pct_non_null(first_name) &amp; pct_non_null(last_name) validity: email_regex_ok: pct_match(email, \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\") uniqueness: email_unique: 1 - duplicate_rate(email) timeliness: export_latency_minutes: p95(now() - updated_at) alerts: - metric: email_regex_ok threshold: \"&gt;= 0.98\" action: \"notify #data-quality\" . E. Mapping iteration checklist . | Create mapping branch/version | Add 1–3 fields only | Run staging export &amp; compare | Roll forward or revert | Update runbook &amp; data contract | . F. Dedup reviewer guide . | Approve only when you’d be comfortable auto‑approving next time. | Leave notes; encode your decision as a new rule where possible. | Track precision (false merge rate) and recall (missed dupes). | . ",
    "url": "/kb/14-days-live#howto-details--templates",
    
    "relUrl": "/kb/14-days-live#howto-details--templates"
  },"684": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Troubleshooting",
    "content": "Ingestion fails intermittently . | Check auth tokens, rate limits, and payload size. | Use a dead‑letter queue/folder and replay tool. | . Mapping produces nulls . | Verify source field names and paths (case/array/indexing). | Add defaults in cleaning; avoid lossy transforms early. | . Export breaks schema . | Switch to staging target and diff schemas. | Use versioned exports (_v1, _v2) and deprecate gracefully. | . Duplicate cascades . | Tighten deterministic rules and raise auto‑approve threshold. | Add secondary keys (email+phone, name+DOB, org name+address). | . ",
    "url": "/kb/14-days-live#troubleshooting",
    
    "relUrl": "/kb/14-days-live#troubleshooting"
  },"685": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Operating Rhythm (after go‑live)",
    "content": ". | Daily: Pipeline run checks + key DQ metrics glance (2 mins). | Weekly: Review AI Agent findings; ship 2–3 cleaning tweaks. | Bi‑weekly: Add/adjust a mapping field; test rollback. | Monthly: Revisit dedup thresholds; expand export consumers. | . ",
    "url": "/kb/14-days-live#operating-rhythm-after-golive",
    
    "relUrl": "/kb/14-days-live#operating-rhythm-after-golive"
  },"686": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Summary",
    "content": "In two weeks you can have a resilient, automated in→through→out slice in CluedIn, built on repeatable habits: . | Automated ingestion (no manual uploads) | As‑is landing (ELT) | Exports first, mapping evolves with needs | AI‑assisted quality and dedup | Incremental cleaning projects | Metrics‑driven improvement | Cheap mapping changes you can revert anytime | . Ship small, ship often—and keep the pipeline warm. ",
    "url": "/kb/14-days-live#summary",
    
    "relUrl": "/kb/14-days-live#summary"
  },"687": {
    "doc": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "title": "Go Live in 14 Days with CluedIn - A Step‑by‑Step Playbook",
    "content": " ",
    "url": "/kb/14-days-live",
    
    "relUrl": "/kb/14-days-live"
  },"688": {
    "doc": "Implement CluedIn with AI",
    "title": "CluedIn: The 100% AI-First Operating Model",
    "content": "Scope: How to run CluedIn with AI at the center—using AI Agents, AI Rules, AI Enricher, AI Mapping, and Copilot—from onboarding to daily operations, with safety, observability, and governance. TL;DR: Automate suggestions and safe auto-fixes end‑to‑end, keep humans in the approval loop for high‑impact changes, and measure everything. Start read‑only, then progress to suggest → gated auto‑fix → auto‑promotion with rollback. ",
    "url": "/kb/cluedin-ai#cluedin-the-100-ai-first-operating-model",
    
    "relUrl": "/kb/cluedin-ai#cluedin-the-100-ai-first-operating-model"
  },"689": {
    "doc": "Implement CluedIn with AI",
    "title": "1) Principles of an AI‑First CluedIn",
    "content": ". | ELT + AI loops: Land raw data, then let AI map, profile, validate, clean, dedup, and enrich inside CluedIn. | Guardrails by default: AI reads masked views; approvals required for PII exposure or schema‑breaking actions. | Human‑on‑the‑loop: AI proposes → you approve (or promote) with clear thresholds and rollbacks. | Config as code: Store Agent playbooks, Rules, Enricher templates, and Mapping diffs in Git with PR review. | Measure everything: Acceptance rate, precision/recall of fixes, incident rate, time‑to‑adopt suggestions, rollback frequency. | Fail safe: Prefer flag/quarantine &gt; auto‑fix; require extra proof for structural changes (mappings, exports). | . ",
    "url": "/kb/cluedin-ai#1-principles-of-an-aifirst-cluedin",
    
    "relUrl": "/kb/cluedin-ai#1-principles-of-an-aifirst-cluedin"
  },"690": {
    "doc": "Implement CluedIn with AI",
    "title": "2) Reference Architecture (AI Control Loop)",
    "content": "Ingestion → AI Mapping → Entities ↘ ↗ AI Agents (profile/validate/dedup plan) ↓ AI Rules (validations, policies, fixes) ↓ AI Enricher (lookup/classify/summarize) ↓ Cleaning &amp; Dedup (auto/queued) ↓ Exports → Consumers (BI/Apps) ↓ Copilot (chat + actions + change PRs) ↺ Feedback (metrics, audit, prompts) . | AI Agents: Orchestrate analysis, generate proposals, triage issues. | AI Rules: Convert proposals into executable validations/policies/transform steps. | AI Enricher: Add attributes (classifications, categories, standardizations). | AI Mapping: Propose/maintain source→entity mappings and diffs. | Copilot: Conversational control surface to run, inspect, and ship changes safely. | . ",
    "url": "/kb/cluedin-ai#2-reference-architecture-ai-control-loop",
    
    "relUrl": "/kb/cluedin-ai#2-reference-architecture-ai-control-loop"
  },"691": {
    "doc": "Implement CluedIn with AI",
    "title": "3) Quick Start (2 Days)",
    "content": "Day 1 . | Enable AI features; point to your provider(s). | Run AI Mapping on one source; accept minimal mapping. | Run AI Agents (read‑only) to produce DQ &amp; dedup findings. | Generate AI Rules draft pack; keep all as flag/quarantine only. | . Day 2 . | Turn on AI Enricher for 1–2 low‑risk enrichments (e.g., phone E.164, country codes). | Schedule Agents nightly; export findings to a review queue. | Wire Copilot to create PRs for mapping/cleaning changes; no direct prod writes. | . ",
    "url": "/kb/cluedin-ai#3-quick-start-2-days",
    
    "relUrl": "/kb/cluedin-ai#3-quick-start-2-days"
  },"692": {
    "doc": "Implement CluedIn with AI",
    "title": "4) AI Agents — Analysis &amp; Automation Brain",
    "content": "4.1 What to use them for . | Profiling: completeness, validity, uniqueness, timeliness. | Rule synthesis: propose validations, cleaning steps, dedup keys. | Risk detection: schema drift, PII leakage, broken joins. | Change notes: auto‑draft PR descriptions and runbooks. | . 4.2 Agent Run (pseudo‑API) . POST /api/ai/agents/run { \"agent\": \"dq-analyzer\", \"target\": { \"entity\": \"Person\" }, \"mode\": \"analysis\", \"options\": { \"sample\": 10000, \"masked\": true } } . 4.3 Outputs you expect . { \"issues\": [ {\"field\":\"email\",\"type\":\"invalid_regex\",\"rate\":0.031,\"examples\":[\"a@x\",\"b@x\"]}, {\"field\":\"phone\",\"type\":\"format_inconsistent\",\"rate\":0.22} ], \"proposals\": { \"validations\":[ \"...yaml...\" ], \"cleaning\":[ \"...yaml...\" ], \"dedup_rules\":[ \"...yaml...\" ] } } . 4.4 Promotion Policy . | Auto‑create PR with the proposals → staging run → diff &amp; metrics. | Auto‑promote only if: tests pass, metrics improve, and risk tag ≠ high. | Always retain correlation_id and prompt hash for audit. | . ",
    "url": "/kb/cluedin-ai#4-ai-agents--analysis--automation-brain",
    
    "relUrl": "/kb/cluedin-ai#4-ai-agents--analysis--automation-brain"
  },"693": {
    "doc": "Implement CluedIn with AI",
    "title": "5) AI Rules — From Idea to Executable Guardrails",
    "content": "5.1 What they encode . | Validations (regex, domain lists, cross‑field checks). | Quarantine conditions &amp; auto‑fix suggestions. | Policy hints (masking, row filters) for Governance to review. | . 5.2 Template (generated by AI, reviewed by humans) . # ai_rules/person-email.yaml rule: email_must_match_regex entity: Person severity: high when: [{ field: email, is_not_null: true }] check: { regex: \"^[^@\\\\s]+@[^@\\\\s]+\\\\.[^@\\\\s]+$\" } on_fail: action: flag # start with flag; escalate later labels: [\"PII\",\"contactability\"] tests: - sample_invalids: [\"a@x\",\"b@x\"] - sample_valids: [\"ada@lovelace.org\"] . 5.3 Lifecycle . | Agent proposes → PR opens. | CI validates schema &amp; runs sample tests. | Staging export diff; DQ metrics must not regress. | Human approves or requests changes. | Optional: auto‑promote next similar rules if precision &gt; threshold. | . ",
    "url": "/kb/cluedin-ai#5-ai-rules--from-idea-to-executable-guardrails",
    
    "relUrl": "/kb/cluedin-ai#5-ai-rules--from-idea-to-executable-guardrails"
  },"694": {
    "doc": "Implement CluedIn with AI",
    "title": "6) AI Enricher — Add Signals Safely",
    "content": "6.1 Use cases . | Normalize: phone → E.164, emails → lowercase/trim. | Classify: industry, product category, sentiment, language. | Extract: keywords, geocodes, brand names from free‑text. | Summarize: latest ticket or notes for Copilot context. | . 6.2 Enricher Config (example) . enricher: standardize_contacts target: Person schedule: \"0 * * * *\" steps: - name: email_normalize type: ai_enricher mode: transform field: email prompt: \"Normalize to lowercase and trim whitespace.\" reversible: true - name: phone_e164 type: ai_enricher mode: transform field: phone prompt: \"Convert to E.164 with default country 'AU' when missing.\" guardrails: deny_if_changes_pct_over: 0.6 observability: emit_metrics: true sample_before_after: 25 . Guardrails . | Reversible writes; keep original in shadow fields. | Rate limits and caching for any external lookups. | PII: redact in prompts or use masked views. | . ",
    "url": "/kb/cluedin-ai#6-ai-enricher--add-signals-safely",
    
    "relUrl": "/kb/cluedin-ai#6-ai-enricher--add-signals-safely"
  },"695": {
    "doc": "Implement CluedIn with AI",
    "title": "7) AI Mapping — Source → Canonical, On Autopilot (With Brakes)",
    "content": "7.1 Capabilities . | Suggest entity classification for a new source. | Propose field‑level mappings with confidence scores. | Generate diffs as sources evolve (rename/add/remove). | . 7.2 Example Proposal (diff) . mapping: Person source: \"crm-contacts\" proposal: add: - field: first_name ; from: \"$.firstName\" ; confidence: 0.94 - field: last_name ; from: \"$.lastName\" ; confidence: 0.94 change: - field: email ; from: \"$.emailAddr\" ; confidence: 0.88 unknown: - \"$.nickname\" ; note: \"stash in attributes.* for now\" risk: medium . 7.3 Acceptance Flow . | Open PR with mapping diff + staging export. | Auto‑accept low‑risk adds (nullable fields) if tests pass. | Require human approval for type changes, renames, or key semantics. | Provide one‑click rollback and keep version history. | . ",
    "url": "/kb/cluedin-ai#7-ai-mapping--source--canonical-on-autopilot-with-brakes",
    
    "relUrl": "/kb/cluedin-ai#7-ai-mapping--source--canonical-on-autopilot-with-brakes"
  },"696": {
    "doc": "Implement CluedIn with AI",
    "title": "8) Copilot — Conversational Control Surface",
    "content": "8.1 What Copilot should do . | “Profile Person and propose top 5 fixes.” | “Draft dedup rules deterministic first, then fuzzy.” | “Create an export customers_wide_v1 (upsert, hourly) with these fields.” | “Explain last export failure and suggest a safe rollback.” | . 8.2 System Prompt (sketch) . You are CluedIn Copilot. Prefer minimal, reversible changes. Never expose raw PII in responses; use masked views or aggregates. For structural changes (mapping/exports/policies), open a PR with tests and staging runs. Include a rollback block. 8.3 Action Binding (examples) . | /copilot create-export customers_wide_v1 … → opens PR + staging run. | /copilot dedup-plan Person → posts rule YAML and sampling plan. | /copilot dq-report Person --since 7d → posts charts + issues. | . ",
    "url": "/kb/cluedin-ai#8-copilot--conversational-control-surface",
    
    "relUrl": "/kb/cluedin-ai#8-copilot--conversational-control-surface"
  },"697": {
    "doc": "Implement CluedIn with AI",
    "title": "9) Governance, Safety &amp; Observability for AI",
    "content": "9.1 Policy hooks . policy: ai_read_masked_by_default target: ai:agents actions: [read] effect: allow_when when: \"dataset.view == 'masked'\" . policy: ai_auto_promotion_guard target: ai:proposals actions: [promote] effect: require_approval when: \"proposal.impact in ['schema','pii','survivorship']\" approvers: [\"Data Governance Manager\",\"Administrator\"] . 9.2 Metrics to track . | Suggestion acceptance rate (weekly). | Precision of auto‑fixes (sampled QA). | False‑positive rate of validations. | Rollback count &amp; time to rollback. | DQ KPI trend post‑adoption (validity, duplicates, completeness). | Prompt drift: changes in outputs for same inputs. | . 9.3 Audit &amp; Evidence . | Store prompt, parameters, model/version, proposal diff, approver, correlation_id. | Snapshot before/after samples for fixes. | Export audit packets for reviews. | . ",
    "url": "/kb/cluedin-ai#9-governance-safety--observability-for-ai",
    
    "relUrl": "/kb/cluedin-ai#9-governance-safety--observability-for-ai"
  },"698": {
    "doc": "Implement CluedIn with AI",
    "title": "10) Maturity Ladder (AI Adoption Path)",
    "content": ". | Read‑Only Insights: Agents analyze; Rules generate as drafts; Enricher off. | Human‑Gated Fixes: Enricher on for low‑risk transforms; Rules flag/quarantine; Mapping diffs PR‑only. | Targeted Auto‑Fix: Auto‑approve low‑risk Rules/Enrichers with &gt;98% precision; Copilot opens PRs for structure. | Auto‑Promotion: Time‑boxed auto‑promotion for repetitive, proven changes (e.g., mapping adds); continuous monitoring &amp; fast rollback. | Self‑Optimizing: Agents retune thresholds based on QA feedback; A/B test competing rule sets. | . ",
    "url": "/kb/cluedin-ai#10-maturity-ladder-ai-adoption-path",
    
    "relUrl": "/kb/cluedin-ai#10-maturity-ladder-ai-adoption-path"
  },"699": {
    "doc": "Implement CluedIn with AI",
    "title": "11) End‑to‑End Playbooks",
    "content": "11.1 New Source Onboarding (AI‑led) . | AI Mapping proposes entity &amp; field map → PR. | Agent profiles early load → proposes validations/cleaning. | Rules merged with flag/quarantine only. | Enricher normalizes low‑risk fields. | Copilot scaffolds an export contract and staging run. | Promote once DQ KPIs are stable; set alerts. | . 11.2 Quality Lift Sprint (1 week) . | Day 1: Agent report; pick top 3 issues. | Day 2–3: Rules + Enricher steps; staging QAs. | Day 4: Partial rollout with alerts; sample QA. | Day 5: Measure lift; capture lessons; adjust thresholds. | . 11.3 Dedup Program . | Agent proposes deterministic keys → queue. | Rules codified; auto‑approve high‑confidence merges. | Copilot generates survivorship spec &amp; reviewer guide. | Weekly precision/recall sampling; refine rules. | . ",
    "url": "/kb/cluedin-ai#11-endtoend-playbooks",
    
    "relUrl": "/kb/cluedin-ai#11-endtoend-playbooks"
  },"700": {
    "doc": "Implement CluedIn with AI",
    "title": "12) CI/CD &amp; Testing for AI Changes",
    "content": ". | Unit tests for prompts: stable I/O examples. | Golden datasets for validation drift. | Contract tests to prevent schema break. | Staging export diffs with row/null distribution comparisons. | Release notes: link to audit events and dashboards. | . GitHub Actions sketch . name: cluedin-ai-changes on: [pull_request] jobs: validate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - run: ./tools/validate-yaml.sh ./ai_rules ./ai_mapping ./ai_enricher - run: pytest -q # prompt/golden tests stage: needs: validate steps: - run: ./tools/apply.sh env/test --ai . ",
    "url": "/kb/cluedin-ai#12-cicd--testing-for-ai-changes",
    
    "relUrl": "/kb/cluedin-ai#12-cicd--testing-for-ai-changes"
  },"701": {
    "doc": "Implement CluedIn with AI",
    "title": "13) Operating Rhythm",
    "content": "Daily: Agent reports glance, new proposals triage, failed jobs check. Weekly: Ship 2–3 AI Rules/Enricher improvements; measure acceptance/precision. Monthly: Review auto‑promotion thresholds; audit sample prompts; renew keys/secrets. ",
    "url": "/kb/cluedin-ai#13-operating-rhythm",
    
    "relUrl": "/kb/cluedin-ai#13-operating-rhythm"
  },"702": {
    "doc": "Implement CluedIn with AI",
    "title": "14) “What Good Looks Like”",
    "content": ". | 95% of low‑risk fixes applied automatically, with &lt;1% rollback. | Mapping changes are additive and auto‑promoted with staged safety. | DQ KPIs trend up; duplicate rate drops steadily. | Copilot PRs are the default path for structural change. | Audit shelf is always ready: prompts, diffs, metrics, approvals. | . ",
    "url": "/kb/cluedin-ai#14-what-good-looks-like",
    
    "relUrl": "/kb/cluedin-ai#14-what-good-looks-like"
  },"703": {
    "doc": "Implement CluedIn with AI",
    "title": "15) Copy‑Paste Library",
    "content": "15.1 Agent Request (DQ) . { \"agent\":\"dq-analyzer\",\"target\":{\"entity\":\"Person\"},\"mode\":\"analysis\",\"options\":{\"sample\":10000,\"masked\":true} } . 15.2 Rule: Cross‑Field Consistency . rule: order_date_precedes_ship_date entity: Order check: { expression: \"order_date &lt;= ship_date\" } severity: medium on_fail: { action: flag } . 15.3 Enricher: Country Codes . - name: normalize_country_code type: ai_enricher field: country_code prompt: \"Map common names to ISO_3166_1_ALPHA2. If ambiguous, leave null.\" . 15.4 Mapping Diff Template . proposal_id: \"map-2025-08-24-001\" entity: Person changes: add: [{ field: \"middle_name\", from: \"$.middleName\", confidence: 0.91 }] change: [] remove: [] risk: low . 15.5 Copilot Command Examples . /copilot dq-report Person --since 7d /copilot propose-mapping crm-contacts --entity Person /copilot create-export customers_wide_v1 --upsert --hourly --fields id,email,updated_at /copilot dedup-plan Person --deterministic-first . Bottom line: A 100% AI approach to CluedIn works when you combine Agent‑generated proposals, Rule &amp; Enricher execution, Mapping diffs, and Copilot‑driven PRs—all wrapped in policy guardrails, metrics, and fast rollback. Start small, ship often, and let AI handle the toil while humans set the rules of the game. ",
    "url": "/kb/cluedin-ai#15-copypaste-library",
    
    "relUrl": "/kb/cluedin-ai#15-copypaste-library"
  },"704": {
    "doc": "Implement CluedIn with AI",
    "title": "Implement CluedIn with AI",
    "content": " ",
    "url": "/kb/cluedin-ai",
    
    "relUrl": "/kb/cluedin-ai"
  },"705": {
    "doc": "CluedI For Administrators",
    "title": "CluedIn for Administrators — The Practical Handbook",
    "content": "Audience: Platform admins, security engineers, data platform owners Goal: Give administrators a clear, actionable view of how to operate CluedIn securely at scale—identity, access, integrations, features, observability, and governance. This handbook focuses on how to run CluedIn day-to-day and what good looks like. It includes checklists, templates, and examples you can adapt to your environment. ",
    "url": "/kb/cluedin-for-administrators#cluedin-for-administrators--the-practical-handbook",
    
    "relUrl": "/kb/cluedin-for-administrators#cluedin-for-administrators--the-practical-handbook"
  },"706": {
    "doc": "CluedI For Administrators",
    "title": "0) Your First 48 Hours (Checklist)",
    "content": "Identity &amp; Access . | Configure SSO (OIDC or SAML 2.0). | (Optional) Enable SCIM user/group provisioning. | Map IdP groups → CluedIn roles (least privilege). | Enforce SSO-only sign-in and MFA at your IdP. | . Access Control &amp; Security . | Review built-in roles and create custom roles as needed. | Define data access policies using classifications/labels (PII, Restricted). | Set session timeout and token lifetimes. | Create API tokens with minimal scopes; store secrets centrally. | . Features &amp; Integrations . | Turn on only the features you need for day‑1; keep others off. | Connect Microsoft Purview (catalog/lineage) via scanning or push. | Connect Power Automate/Power Apps to CluedIn APIs or webhooks. | (Optional) Configure AI integrations/AI Agents with guardrails. | . Observability . | Decide your log retention strategy; export logs to your SIEM. | Enable audit log export and alerts on high‑risk events. | Create an Admin dashboard: pipeline health, ingestion/export SLAs. | Document a runbook and an incident response checklist. | . ",
    "url": "/kb/cluedin-for-administrators#0-your-first-48-hours-checklist",
    
    "relUrl": "/kb/cluedin-for-administrators#0-your-first-48-hours-checklist"
  },"707": {
    "doc": "CluedI For Administrators",
    "title": "1) Identity, SSO &amp; Provisioning",
    "content": "CluedIn supports enterprise SSO via OIDC or SAML 2.0 and optional SCIM for provisioning. 1.1 OIDC (OpenID Connect) Setup . In your IdP (e.g., Microsoft Entra ID / Okta) . | Register a Web application. | Set the redirect URI to your CluedIn callback (from CluedIn SSO settings). | Issue a Client ID and Client Secret. | Add scopes/claims you need mapped to roles and groups (e.g., email, name, groups). | Assign users/groups to the app. | . In CluedIn → Admin → Authentication . { \"type\": \"oidc\", \"issuer_url\": \"https://login.microsoftonline.com/&lt;tenant&gt;/v2.0\", \"client_id\": \"&lt;CLIENT_ID&gt;\", \"client_secret\": \"&lt;CLIENT_SECRET&gt;\", \"redirect_uri\": \"https://&lt;your-cluedin-host&gt;/auth/oidc/callback\", \"scopes\": [\"openid\",\"profile\",\"email\",\"groups\"], \"group_claim\": \"groups\", \"enforce_sso_only\": true } . Tip: Prefer OIDC where possible—simpler operations, modern token formats, and easy group claims. 1.2 SAML 2.0 Setup . In your IdP . | Create a SAML application. | Upload the SP metadata (from CluedIn) or configure the ACS (Assertion Consumer Service) URL manually. | Map attributes: email, firstName, lastName, and a group attribute (e.g., memberOf). | Download IdP metadata (XML). | . In CluedIn → Admin → Authentication . &lt;!-- Paste IdP Metadata --&gt; &lt;EntityDescriptor entityID=\"https://idp.example.com/metadata\"&gt; &lt;!-- ... --&gt; &lt;/EntityDescriptor&gt; . { \"type\": \"saml2\", \"acs_url\": \"https://&lt;your-cluedin-host&gt;/auth/saml/acs\", \"entity_id\": \"https://&lt;your-cluedin-host&gt;/auth/saml/metadata\", \"email_attribute\": \"email\", \"groups_attribute\": \"memberOf\", \"enforce_sso_only\": true } . 1.3 Group-to-Role Mapping . Map IdP groups to CluedIn roles. Keep mappings in code/config for repeatability. # cluedin-role-mapping.yaml mappings: - idp_group: \"cluedin-admins\" roles: [\"Administrator\"] - idp_group: \"cluedin-data-engineers\" roles: [\"Data Engineer\"] - idp_group: \"cluedin-stewards\" roles: [\"Data Steward\"] - idp_group: \"cluedin-viewers\" roles: [\"Viewer\"] defaults: # Users with SSO but no mapped group get these roles (or none) roles: [\"Viewer\"] . Least privilege: give broad read where required, but restrict write/config permissions to a small, accountable set. 1.4 SCIM Provisioning (Optional) . Enable SCIM to automate user/group lifecycle: . | Create/Update/Deactivate users in CluedIn based on IdP. | Sync group membership so role mappings stay current. | . IdP → SCIM config (example) . { \"scim_base_url\": \"https://&lt;your-cluedin-host&gt;/scim/v2\", \"bearer_token\": \"&lt;SCIM_PROVISIONING_TOKEN&gt;\", \"sync_interval_minutes\": 15 } . Operational tips . | Treat SCIM like code: change via PR, audit regularly. | Test deprovisioning; confirm tokens and sessions are revoked. | . ",
    "url": "/kb/cluedin-for-administrators#1-identity-sso--provisioning",
    
    "relUrl": "/kb/cluedin-for-administrators#1-identity-sso--provisioning"
  },"708": {
    "doc": "CluedI For Administrators",
    "title": "2) Connecting Systems: Purview, Power Automate, Power Apps, AI",
    "content": "2.1 Microsoft Purview (Catalog &amp; Lineage) . You have two main patterns—pick one (or both): . 1) Scan exports that CluedIn writes to your lake/warehouse. | Register the storage/database in Purview. | Schedule scans so Purview catalogs the tables/files produced by CluedIn. | Pro: simple, no custom code. Con: lineage may be coarse. | . 2) Push lineage/metadata into Purview (Atlas-compatible APIs). | Use a job to publish processes/entities/lineage after your exports run. | Pro: explicit lineage from source → CluedIn → export. Con: you own jobs. | . Minimal lineage push (pseudo) . POST https://&lt;purview-account&gt;/api/atlas/v2/lineage { \"process\": { \"typeName\": \"cluedin_export\", \"attributes\": { \"name\": \"warehouse-contacts-v1\", \"qualifiedName\": \"cluedin.export.warehouse-contacts-v1\" } }, \"inputs\": [{\"qualifiedName\": \"cluedin.entity.Person\"}], \"outputs\": [{\"qualifiedName\": \"sql.mdm.contacts_v1\"}] } . 2.2 Power Automate (Flows) &amp; Power Apps . Pattern A — Call CluedIn APIs from a Flow . | Use HTTP action with OAuth2 or PAT (preferred: OAuth client). | Trigger on events (e.g., a new record in Dataverse) → call CluedIn ingestion endpoint or AI Agent. | . POST https://&lt;your-cluedin-host&gt;/api/ingest/crm-contacts Authorization: Bearer &lt;ACCESS_TOKEN&gt; Content-Type: application/json {\"id\":\"c_123\",\"email\":\"a@example.com\",\"updated_at\":\"2025-08-22T12:00:00Z\"} . Pattern B — Webhooks from CluedIn → Flow . | Register a webhook in CluedIn (on export success, DQ alert, dedup queue event). | The Flow receives the payload and notifies Teams/updates a ticket/starts approvals. | . Pattern C — Power Apps UI + CluedIn APIs . | Build a simple app for data stewarding (approve merges, fix invalids). | Use a service principal for API access and enforce role checks server-side. | . 2.3 AI Integrations . | Configure your AI provider (e.g., Azure OpenAI) in CluedIn AI settings. | Scope what entities and fields AI Agents can read/write. | Restrict PII access, enable redaction/masking for prompts and logs. | Start with read-only analysis Agents; expand to suggestion/auto-fix flows after review. | . ",
    "url": "/kb/cluedin-for-administrators#2-connecting-systems-purview-power-automate-power-apps-ai",
    
    "relUrl": "/kb/cluedin-for-administrators#2-connecting-systems-purview-power-automate-power-apps-ai"
  },"709": {
    "doc": "CluedI For Administrators",
    "title": "3) Turning Features On/Off",
    "content": "Navigate to Admin → Features (or Workspace Settings → Features). Best practices . | Keep non-essential features off until you need them. | Maintain separate dev/test/prod workspaces. | Use change windows and a rollback plan for feature toggles. | Document compatibility (some features require certain roles/integrations). | . Example (pseudo) . { \"features\": { \"ai_agents\": true, \"dedup_projects\": true, \"experimental_mappers\": false, \"webhooks\": true, \"custom_roles\": true } } . ",
    "url": "/kb/cluedin-for-administrators#3-turning-features-onoff",
    
    "relUrl": "/kb/cluedin-for-administrators#3-turning-features-onoff"
  },"710": {
    "doc": "CluedI For Administrators",
    "title": "4) Access Control for Data",
    "content": "CluedIn uses RBAC (roles → permissions) and can layer policy‑based controls for row/column access (ABAC-style). 4.1 Levels of Control . | Workspace/project level: who can configure ingestion, mapping, cleaning, exports. | Entity/Dataset level: who can read/write specific entities or exports. | Field/Column level: mask, hash, or hide selected attributes (e.g., PII). | Row level: filter rows by attributes (region, tenant, ownership). | . 4.2 Example Policies . Column masking . policy: mask_pii_email target: entity:Person.field:email actions: [read] effect: allow_with_mask mask: \"partial_email\" # e.g., a***@example.com conditions: - role_in: [\"Viewer\",\"Analyst\"] . Row-level filter . policy: restrict_region target: entity:Order actions: [read] effect: allow_when when: \"record.region in user.allowed_regions\" applies_to: - roles: [\"Analyst\"] - groups: [\"finance-emea\"] . Deny write to exports for non-owners . policy: export_write_guardrail target: export:mdm.contacts_v1 actions: [write,configure] effect: deny unless: - role_in: [\"Administrator\",\"Data Engineer\"] . Order of evaluation: explicit deny should override broad allows; test with staging identities. 4.3 Classifications &amp; Tags . Apply labels (PII, Restricted, Confidential, Public) at the entity/field level and drive policies from labels: . | Auto-mask PII for non‑steward roles. | Require approval for exports that include Restricted fields. | . ",
    "url": "/kb/cluedin-for-administrators#4-access-control-for-data",
    
    "relUrl": "/kb/cluedin-for-administrators#4-access-control-for-data"
  },"711": {
    "doc": "CluedI For Administrators",
    "title": "5) Managing API Tokens",
    "content": "CluedIn supports Personal Access Tokens (PATs) and/or OAuth clients for service‑to‑service access. 5.1 Creating Tokens . | Go to Admin → API Tokens. | Create a token with the minimal scopes and a short expiry. | Tag tokens by purpose (power-automate-flow-42). | . Token example (pseudo) . { \"name\": \"power-automate-contact-sync\", \"scopes\": [\"ingest:write\",\"export:read\"], \"expires_at\": \"2025-12-31T23:59:59Z\" } . Usage . curl -H \"Authorization: Bearer &lt;TOKEN&gt;\" https://&lt;host&gt;/api/exports/status . 5.2 Rotation &amp; Hygiene . | Rotate on a 90‑day cadence (or faster). | Revoke tokens immediately on user departure (SCIM + audit). | Keep secrets in a vault; never in code or chat. | Log who/what uses tokens (user agent, IP), alert on anomalies. | . ",
    "url": "/kb/cluedin-for-administrators#5-managing-api-tokens",
    
    "relUrl": "/kb/cluedin-for-administrators#5-managing-api-tokens"
  },"712": {
    "doc": "CluedI For Administrators",
    "title": "6) Reading Logs",
    "content": "CluedIn exposes logs across categories; forward them to your SIEM/Observability stack for long‑term analytics. 6.1 Types . | Ingestion logs: HTTP status, schema/parse issues, dead‑letter entries. | Mapping &amp; cleaning logs: transformation steps, validation failures. | Export logs: schedule triggers, job durations, row counts, schema diffs. | System logs: auth, feature toggles, config changes. | . Example log (pseudo) . { \"ts\": \"2025-08-23T10:15:08Z\", \"category\": \"export\", \"export\": \"warehouse-contacts-v1\", \"status\": \"success\", \"duration_ms\": 4213, \"records_out\": 15234, \"correlation_id\": \"a6c9-...-4f\", \"actor\": \"system@scheduler\" } . 6.2 Practical Use . | Always capture correlation_id from UI/API; thread it through pipelines. | Build alerts for error rate spikes and duration regressions. | Keep log levels sane in prod; switch to debug only during incidents. | . ",
    "url": "/kb/cluedin-for-administrators#6-reading-logs",
    
    "relUrl": "/kb/cluedin-for-administrators#6-reading-logs"
  },"713": {
    "doc": "CluedI For Administrators",
    "title": "7) Reading Audit Logs",
    "content": "Audit logs track who did what, when, and from where—critical for compliance. Common events . | SSO sign‑ins, failed logins. | Role/permission changes; token creation/revocation. | Feature toggles; workspace settings changes. | Policy updates; data export configuration changes. | Bulk actions (dedup merges, cleaning jobs with write effects). | . Sample audit record (pseudo) . { \"ts\": \"2025-08-23T11:02:44Z\", \"actor\": {\"type\":\"user\",\"id\":\"tiw@cluedin.com\"}, \"action\": \"role.update\", \"target\": {\"type\":\"role\",\"id\":\"Data Steward\"}, \"old\": {\"permissions\":[\"entity.read\",\"dq.view\"]}, \"new\": {\"permissions\":[\"entity.read\",\"dq.view\",\"dedup.review\"]}, \"ip\": \"203.0.113.5\" } . Best practices . | Export audit logs daily; retain for 1–7 years per policy. | Monitor high‑risk actions (token create, role grant, export schema change). | Automate tickets/approvals for sensitive actions. | . ",
    "url": "/kb/cluedin-for-administrators#7-reading-audit-logs",
    
    "relUrl": "/kb/cluedin-for-administrators#7-reading-audit-logs"
  },"714": {
    "doc": "CluedI For Administrators",
    "title": "8) Controlling Access to Functionality (Roles &amp; Users)",
    "content": "Start with built‑in roles and add custom roles only when needed. 8.1 Example Permission Matrix (excerpt) . | Capability | Viewer | Data Steward | Data Engineer | Administrator |—|—:|—:|—:|—:| Read entities/exports | ✅ | ✅ | ✅ | ✅ | Approve dedup merges | ❌ | ✅ | ✅ | ✅ | Edit cleaning projects | ❌ | ✅ (limited) | ✅ | ✅ | Configure ingestion/export | ❌ | ❌ | ✅ | ✅ | Manage roles &amp; policies | ❌ | ❌ | ❌ | ✅ | Manage feature toggles | ❌ | ❌ | ❌ | ✅ | Create API tokens | ❌ | ❌ | ✅ (scoped) | ✅ | . Use scoped custom roles to carve out precise abilities (e.g., “Export Maintainer” can edit exports in project sales, but nowhere else). 8.2 Custom Role Definition (template) . role: \"Export Maintainer\" description: \"Manage exports for Sales project only\" permissions: - export.read:project:sales - export.write:project:sales - policy.read constraints: - deny: [\"feature.toggle\",\"role.manage\"] . 8.3 Change Management . | Approvals for role grants beyond Viewer/Steward. | Time‑boxed elevated roles (auto‑expire admin for break‑glass). | Quarterly access reviews with audit evidence. | . ",
    "url": "/kb/cluedin-for-administrators#8-controlling-access-to-functionality-roles--users",
    
    "relUrl": "/kb/cluedin-for-administrators#8-controlling-access-to-functionality-roles--users"
  },"715": {
    "doc": "CluedI For Administrators",
    "title": "9) Operational Runbooks",
    "content": "Daily . | Check pipeline health (last run status, latency, volumes). | Review alerts (ingestion failures, DQ thresholds). | Triage dedup review queues (if enabled). | . Weekly . | Review audit log highlights and token usage. | Patch/rotate secrets and service principals as needed. | Validate Purview lineage completeness for top datasets. | . Monthly . | Access reviews; role/permission drift check. | Capacity planning (storage/compute) and cost review. | Disaster recovery tabletop: restore from backup or re‑build exports. | . Incident (example) . | Identify: Error rate spike on export, correlation_id X. | Contain: Pause affected schedules; toggle feature if implicated. | Diagnose: Compare config (git/PR), check last mapping/cleaning changes. | Remediate: Rollback mapping or re-run cleaning; backfill export. | Review: Post‑incident notes; add alert/test; update runbook. | . ",
    "url": "/kb/cluedin-for-administrators#9-operational-runbooks",
    
    "relUrl": "/kb/cluedin-for-administrators#9-operational-runbooks"
  },"716": {
    "doc": "CluedI For Administrators",
    "title": "10) Security &amp; Compliance Quick Wins",
    "content": ". | Enforce SSO-only and MFA at IdP. | Use least-privilege roles; prefer group-based access. | Mask PII by default; require approvals to export PII. | Short-lived tokens; rotate frequently; monitor token use. | Immutable audit logs with long retention. | Secrets in a vault, never in pipelines or notebooks. | Data residency and encryption: document and validate with your infra team. | . ",
    "url": "/kb/cluedin-for-administrators#10-security--compliance-quick-wins",
    
    "relUrl": "/kb/cluedin-for-administrators#10-security--compliance-quick-wins"
  },"717": {
    "doc": "CluedI For Administrators",
    "title": "Appendix A — SSO Attribute Mapping (examples)",
    "content": "attributes: email: \"user.email || user.userprincipalname\" first_name: \"user.given_name\" last_name: \"user.family_name\" groups: \"user.groups\" # or saml:memberOf . ",
    "url": "/kb/cluedin-for-administrators#appendix-a--sso-attribute-mapping-examples",
    
    "relUrl": "/kb/cluedin-for-administrators#appendix-a--sso-attribute-mapping-examples"
  },"718": {
    "doc": "CluedI For Administrators",
    "title": "Appendix B — SCIM Field Mapping (examples)",
    "content": "user: id: \"id\" userName: \"mail\" active: \"accountEnabled\" name.givenName: \"givenName\" name.familyName: \"surname\" emails[0].value: \"mail\" group: displayName: \"displayName\" members[].value: \"members[].id\" . ",
    "url": "/kb/cluedin-for-administrators#appendix-b--scim-field-mapping-examples",
    
    "relUrl": "/kb/cluedin-for-administrators#appendix-b--scim-field-mapping-examples"
  },"719": {
    "doc": "CluedI For Administrators",
    "title": "Appendix C — Power Automate Flow (HTTP) Sketch",
    "content": "trigger: \"When a row is added in Dataverse\" steps: - name: Compose payload action: compose inputs: id: \"@{triggerBody()?['contactid']}\" email: \"@{triggerBody()?['emailaddress1']}\" updated_at: \"@{utcNow()}\" - name: POST to CluedIn action: http inputs: method: POST uri: \"https://&lt;host&gt;/api/ingest/crm-contacts\" headers: Authorization: \"Bearer @{parameters('CLUE_TOKEN')}\" Content-Type: \"application/json\" body: \"@{outputs('Compose payload')}\" . ",
    "url": "/kb/cluedin-for-administrators#appendix-c--power-automate-flow-http-sketch",
    
    "relUrl": "/kb/cluedin-for-administrators#appendix-c--power-automate-flow-http-sketch"
  },"720": {
    "doc": "CluedI For Administrators",
    "title": "Appendix D — Sample Policies (copy/paste)",
    "content": "D1. Require approval for exports with PII . policy: export_pii_guard target: export:* # any export actions: [promote] effect: require_approval when: \"export.contains_label('PII')\" approvers: [\"Data Protection Officer\",\"Administrator\"] . D2. Deny field read of government_id to all but Stewards/Admins . policy: hide_government_id target: entity:Person.field:government_id actions: [read] effect: deny unless: - role_in: [\"Data Steward\",\"Administrator\"] . D3. Limit AI access to masked views . policy: ai_masking target: ai:agents actions: [read] effect: allow_when when: \"agent.mode == 'analysis' and dataset.view == 'masked'\" . ",
    "url": "/kb/cluedin-for-administrators#appendix-d--sample-policies-copypaste",
    
    "relUrl": "/kb/cluedin-for-administrators#appendix-d--sample-policies-copypaste"
  },"721": {
    "doc": "CluedI For Administrators",
    "title": "Final Notes",
    "content": ". | Keep configuration as code (YAML/JSON in a repo) where possible to enable PR reviews, versioning, and quick rollbacks. | Separate people permissions (roles) from data policies (labels/rules); you’ll evolve both independently. | Start with simple integrations and tighten controls as you scale. | . You’ve now got the admin view: set up identity, connect the Microsoft ecosystem, control features and data access, run with observability, and govern with audit + policy. Clone the templates, fill in your env details, and you’re production‑ready. ",
    "url": "/kb/cluedin-for-administrators#final-notes",
    
    "relUrl": "/kb/cluedin-for-administrators#final-notes"
  },"722": {
    "doc": "CluedI For Administrators",
    "title": "CluedI For Administrators",
    "content": " ",
    "url": "/kb/cluedin-for-administrators",
    
    "relUrl": "/kb/cluedin-for-administrators"
  },"723": {
    "doc": "CluedIn for Solution Architects",
    "title": "CluedIn for Solution Architects — Integration &amp; Reference Architecture Guide",
    "content": "Audience: Solution/Enterprise Architects, Platform Owners, Domain Architects Goal: Explain where CluedIn fits in the wider data landscape, which architectural patterns it enables, and how to integrate it cleanly with surrounding systems (sources, lakes/warehouses, catalogs, BI/AI/Apps, and governance). Assumptions: You favor event-driven, ELT, infrastructure-as-code, and least-privilege security. This guide is platform-agnostic with callouts for Azure/AWS/GCP and the Microsoft Power Platform. ",
    "url": "/kb/cluedin-solution-architects#cluedin-for-solution-architects--integration--reference-architecture-guide",
    
    "relUrl": "/kb/cluedin-solution-architects#cluedin-for-solution-architects--integration--reference-architecture-guide"
  },"724": {
    "doc": "CluedIn for Solution Architects",
    "title": "1) What CluedIn Is (and Isn’t)",
    "content": ". | Is: a data unification + quality platform that ingests raw data, maps to canonical entities, enables cleaning, validation, dedup/golden records, and exports reliable outputs to your downstream systems. It supports streaming and batch, policy-based access, and auditability, with AI Agents for analysis/suggestions. | Is not: a heavy ETL monolith, BI semantic layer, or a general-purpose orchestrator. It plays with those systems, not instead of them. | . Positioning vs common components . | With ETL/ELT tools (ADF/Airflow/Glue/Fabric): CluedIn owns entity modeling, quality, unification; your orchestrator triggers/schedules. | With Lake/Warehouse (ADLS/S3/GCS + Fabric/Synapse/BigQuery/Snowflake): CluedIn exports golden/wide/star data to these stores. | With MDM: CluedIn’s dedup + survivorship can act as operational MDM-lite or feed/consume a dedicated MDM if present. | With Catalog/Lineage (Purview/Data Catalog/Collibra): CluedIn publishes metadata/lineage and respects classifications. | With Reverse ETL / Apps (Power Apps/Automate, Kafka topics, APIs): CluedIn provides operationalized, trustworthy datasets and events. | . ",
    "url": "/kb/cluedin-solution-architects#1-what-cluedin-is-and-isnt",
    
    "relUrl": "/kb/cluedin-solution-architects#1-what-cluedin-is-and-isnt"
  },"725": {
    "doc": "CluedIn for Solution Architects",
    "title": "2) Reference Architectures",
    "content": "2.1 High-Level (Hybrid Streaming + Batch) . [SaaS/DB/Events] ──► (Producers: CDC/HTTP/Kafka/S3) ──► CluedIn Ingestion Endpoints | │ | Raw Landing (ELT) | │ └─────────────► (Optional Lake Zone) ◄───────────────┘ │ Mapping ⇢ Cleaning ⇢ Validations ⇢ Dedup/Golden │ ┌─────────────────────┴─────────────────────┐ │ │ Export Targets (Tables/Files) Export Targets (Topics/APIs) ► Warehouse/Lake (Snowflake/Fabric/ ► Kafka/Event Hub/Webhooks/Reverse‑ETL BigQuery/Synapse/Databricks) ► Apps/Operational CX │ │ BI/AI/Analytics Apps/Workflows/Automation │ │ Catalog/Lineage (Purview/Collibra) Observability (SIEM, APM, Dashboards) . 2.2 Microsoft-Centric Blueprint . Dataverse/D365/Apps → Power Automate (Flows) → CluedIn Ingestion (HTTP/Webhook/Event Hub) ↑ │ Teams/Approvals AI Agents (masked) ADLS/Fabric OneLake ← Exports (Parquet/SQL) ← CluedIn │ Power BI / Fabric Semantic Model ↑ │ Purview Catalog &amp; Lineage ◄────────────┴────────────── Metadata/Lineage Push ◄┘ . 2.3 Data Mesh Placement . | CluedIn can be a domain data product factory: each domain owns ingestion→golden→export with governance guardrails and shared platform controls (SSO, policies, audit, catalog). | . ",
    "url": "/kb/cluedin-solution-architects#2-reference-architectures",
    
    "relUrl": "/kb/cluedin-solution-architects#2-reference-architectures"
  },"726": {
    "doc": "CluedIn for Solution Architects",
    "title": "3) Integration Patterns",
    "content": "3.1 Ingestion Into CluedIn . | Streaming: Kafka/Event Hubs; prefer JSON lines, include event_id, updated_at, schema_version headers; key by business ID for ordering. | HTTP/Webhooks: simple producers (Power Automate/Apps, custom services) POST to ingestion endpoints; include idempotency keys. | Batch/Landing: S3/ADLS/GCS watchers for CSV/JSON/Parquet on a schedule; use manifests for completeness; automate backfills. | CDC: Debezium/ADF/Fabric Data Pipelines to stream DB changes; normalize op codes. | . Architectural note: Keep producers dumb (no pre-clean). CluedIn is ELT: land first, transform inside via cleaning projects and policies. 3.2 Outbound from CluedIn . | Tables/Views: export to warehouses (Fabric/Synapse, Snowflake, BigQuery, Databricks). Choose upsert with stable PKs; version breaking changes (_vN). | Files: Parquet to lake zones for batch analytics; partition by date/entity. | Events: publish to Kafka/Event Hubs when exports finish or when golden records change (perfect for Reverse ETL patterns). | APIs/Webhooks: push updates to SaaS; sign webhooks (HMAC) and design idempotent handlers. | . 3.3 Catalog &amp; Lineage . | Scan exported datasets from lake/warehouse with your catalog (Purview/Collibra/Glue). | Push process lineage (source → CluedIn → export) via Atlas/REST where supported. | Synchronize classifications/labels (PII/Restricted) and use them to drive policies in CluedIn and your BI/lake security layers. | . 3.4 Power Platform Integration . | Power Automate: trigger on business events → call CluedIn ingestion endpoints; or receive CluedIn webhooks (DQ alerts, export complete) → route to Teams/Dataverse. | Power Apps: stewarding UIs (review duplicates, fix invalids) over CluedIn APIs. | Power BI/Fabric: consume versioned exports (star/wide); manage semantic models separately but aligned with CluedIn contracts. | . ",
    "url": "/kb/cluedin-solution-architects#3-integration-patterns",
    
    "relUrl": "/kb/cluedin-solution-architects#3-integration-patterns"
  },"727": {
    "doc": "CluedIn for Solution Architects",
    "title": "4) Solution Design Decisions (Playbook)",
    "content": "4.1 Choosing Export Shapes . | Star schema → BI at scale, facts + SCD2 dims. | Wide operational tables → CX apps, CRM backfills, support tooling. | Event streams → near‑real‑time syncs and automation. | API endpoints → per‑entity CRUD patterns are not the goal; favor batch/event outputs for scalability. | . 4.2 Identity &amp; Golden Records . | Start with deterministic dedup (email/phone/customer_id). | Define survivorship: source precedence + recency. | Emit stable entity IDs to downstreams to avoid churn. | . 4.3 Temporal Strategy . | Decide SCD1 vs SCD2 per entity; BI usually needs SCD2 for dims. | For events, carry event_time (business) and ingest_time (processing). | . 4.4 Contracts &amp; Versioning . | Every export has a contract (schema, keys, SLA, labels). | Breaking changes → new _vN and a dual‑run window with migration notes. | . 4.5 Governance Hooks . | Labels (PII, Restricted) at field/entity drive masking and approval workflows (e.g., PII export requires Governance approval). | Audit logs must cover promotions, policy changes, merges, and deletions. | . ",
    "url": "/kb/cluedin-solution-architects#4-solution-design-decisions-playbook",
    
    "relUrl": "/kb/cluedin-solution-architects#4-solution-design-decisions-playbook"
  },"728": {
    "doc": "CluedIn for Solution Architects",
    "title": "5) End‑to‑End Blueprints",
    "content": "5.1 Real‑Time Customer 360 (Ops + BI) . | Sources: CRM, Support, Marketing streams. | Ingest: Kafka topics into CluedIn; CDC for legacy DB. | Unify: Mapping + cleaning + deterministic dedup → Person golden. | Exports: . | customers_wide_v1 (ops wide) → CRM/Support via Reverse ETL. | customers_dim_v1 + orders_fct_v1 → warehouse for BI. | . | Catalog: Purview scan + lineage push. | Apps/AI: Power Apps for stewarding; AI Agent reports DQ trends weekly. | . 5.2 B2B Product Catalog Harmonization . | Ingest vendor feeds (S3/HTTP) + ERP/Kanban events. | Standardize attributes (units, categories), deduplicate SKUs. | Exports to commerce PIM and search index; event stream on SKU changes. | DQ KPIs: completeness of attributes, invalid GTIN rate, duplicate rate. | . 5.3 “Data Quality Firewall” for Downstream . | Ingest raw; validate with rules (regex, referential, domain lists). | Quarantine failures; webhook to ticketing; weekly auto‑fixes via cleaning projects. | Export only passing records to a downstream operational DB. | Metrics: validity %, quarantine count, MTTR per issue type. | . ",
    "url": "/kb/cluedin-solution-architects#5-endtoend-blueprints",
    
    "relUrl": "/kb/cluedin-solution-architects#5-endtoend-blueprints"
  },"729": {
    "doc": "CluedIn for Solution Architects",
    "title": "6) Security, Privacy &amp; Compliance (Architect View)",
    "content": ". | SSO‑only with OIDC/SAML; SCIM for lifecycle; group‑to‑role mapping. | Least privilege roles; feature toggles isolated per env. | Data policies: column masking, row filters, deny‑by‑default for sensitive exports. | PII handling: masked views for AI Agents and non‑prod environments. | Secrets in a vault; short‑lived tokens; IP allowlists on webhooks. | Retention &amp; legal holds: encode as policies; export destruction evidence. | Multi‑region: active/active for ingestion; exports regionally localized; clear RPO/RTO. | . ",
    "url": "/kb/cluedin-solution-architects#6-security-privacy--compliance-architect-view",
    
    "relUrl": "/kb/cluedin-solution-architects#6-security-privacy--compliance-architect-view"
  },"730": {
    "doc": "CluedIn for Solution Architects",
    "title": "7) Non‑Functional Requirements (NFRs)",
    "content": "| NFR | Guidance | . | SLA (freshness) | Export contracts with freshness_p95_minutes; alarms on breach. | . | Latency | For near‑real‑time, prefer event pipelines with small batches. | . | Throughput | Partition events; scale consumers; avoid tiny files. | . | Resilience | DLQ + replay; idempotent producers; backpressure control. | . | Observability | Correlation IDs; structured logs; dashboards for volumes/latency/errors. | . | Cost | Columnar formats; partition strategy; right‑size schedules. | . | Compliance | Immutable audit logs; approvals for PII exports; evidence packs. | . ",
    "url": "/kb/cluedin-solution-architects#7-nonfunctional-requirements-nfrs",
    
    "relUrl": "/kb/cluedin-solution-architects#7-nonfunctional-requirements-nfrs"
  },"731": {
    "doc": "CluedIn for Solution Architects",
    "title": "8) Orchestration &amp; CI/CD",
    "content": ". | Use Airflow/ADF/Fabric Data Pipelines/GitHub Actions to deploy configs (mappings, cleaning, policies, exports) as code. | Promotion: PR → staging diff → prod, with a change window and rollback plan. | Tests: schema/domain/FK/contract checks; regression diffs across exports. | Automate lineage publishing post‑export and catalog scans. | . ",
    "url": "/kb/cluedin-solution-architects#8-orchestration--cicd",
    
    "relUrl": "/kb/cluedin-solution-architects#8-orchestration--cicd"
  },"732": {
    "doc": "CluedIn for Solution Architects",
    "title": "9) Observability &amp; Runbooks",
    "content": ". | Dashboards: ingestion success/latency, export success/rows, DQ KPIs, dedup queue size. | Alerts: export failure, schema drift, DQ breach (validity/duplicate spikes), webhook retry storms. | Runbooks: ingestion 4xx/5xx spikes, export schema drift, duplicate surges, SLA breaches; each with contain/diagnose/rollback/backfill steps. | . ",
    "url": "/kb/cluedin-solution-architects#9-observability--runbooks",
    
    "relUrl": "/kb/cluedin-solution-architects#9-observability--runbooks"
  },"733": {
    "doc": "CluedIn for Solution Architects",
    "title": "10) Decision Helpers (When to…)",
    "content": ". | Use CluedIn mapping vs ETL transforms? . | Entity semantics and unification → CluedIn. | Heavy joins/enrichment external to CluedIn or compute‑intensive transforms → ETL/warehouse. | . | Star vs Wide exports? . | BI → Star (SCD2 dims + facts). | Operational syncs/apps → Wide. | . | Streaming vs Batch ingestion? . | Event‑driven, low‑latency needs → Streaming. | Bulk daily snapshots/backfills → Batch. | . | Deterministic vs Fuzzy dedup? . | Start deterministic; add fuzzy rules off‑peak once precision is measured. | . | . ",
    "url": "/kb/cluedin-solution-architects#10-decision-helpers-when-to",
    
    "relUrl": "/kb/cluedin-solution-architects#10-decision-helpers-when-to"
  },"734": {
    "doc": "CluedIn for Solution Architects",
    "title": "11) Templates (Copy/Paste)",
    "content": "11.1 Export Contract (with governance fields) . name: customers_wide_v1 owner: \"Sales Ops\" primary_key: customer_id delivery: { type: sql-table, schedule: hourly } sla: { freshness_p95_minutes: 60 } labels: [\"PII:email\"] compatibility: additive_only lineage_required: true approval_required_when_labels: [\"PII\",\"Restricted\"] . 11.2 Policy Sketches . policy: mask_email_default target: entity:Person.field:email actions: [read] effect: allow_with_mask mask: \"partial_email\" unless: [{ role_in: [\"Data Steward\",\"Administrator\"] }] . policy: export_requires_approval target: export:* actions: [promote] effect: require_approval when: \"export.contains_label('PII')\" approvers: [\"Governance Manager\",\"DPO\"] . 11.3 Webhook Registration (pseudo) . POST /api/webhooks { \"name\": \"teams-export-success\", \"events\": [\"export.succeeded\"], \"url\": \"https://example.com/hooks/teams\", \"secret\": \"&lt;HMAC&gt;\" } . ",
    "url": "/kb/cluedin-solution-architects#11-templates-copypaste",
    
    "relUrl": "/kb/cluedin-solution-architects#11-templates-copypaste"
  },"735": {
    "doc": "CluedIn for Solution Architects",
    "title": "12) Operating Cadence (Architecture Guild)",
    "content": "Bi‑weekly design review: incoming sources, export changes, lineage completeness, PII exposure checks. Monthly scorecard: export contract coverage, DQ trend, incident classes, cost, catalog lineage %, AI usage review. Quarterly roadmap: domains onboarding, deprecations of _v0, platform upgrades, governance maturity. ",
    "url": "/kb/cluedin-solution-architects#12-operating-cadence-architecture-guild",
    
    "relUrl": "/kb/cluedin-solution-architects#12-operating-cadence-architecture-guild"
  },"736": {
    "doc": "CluedIn for Solution Architects",
    "title": "13) Anti‑Patterns to Avoid",
    "content": ". | Designing a grand unified model before shipping any export. | Pre‑cleaning upstream; defeating ELT and lineage transparency. | Shipping breaking changes without contracts/versioning and a dual‑run window. | Treating AI Agents as unrestricted data readers; avoid unmasked access. | Ignoring catalog/lineage; producing orphaned BI datasets. | . ",
    "url": "/kb/cluedin-solution-architects#13-antipatterns-to-avoid",
    
    "relUrl": "/kb/cluedin-solution-architects#13-antipatterns-to-avoid"
  },"737": {
    "doc": "CluedIn for Solution Architects",
    "title": "14) One‑Page Checklist (Print This)",
    "content": ". | Ingestion endpoints (stream/batch) automated, no manual uploads. | Minimal mapping → exports live → iterate mapping/cleaning. | Dedup deterministic first; survivorship clarified. | Contracts for each export; _vN for breakers. | Policies/labels on PII; approvals enforced. | Catalog &amp; lineage integrated; Purview scan/push verified. | Dashboards + alerts + runbooks in place. | CI/CD promotion path; rollback tested; audit logs retained. | . Bottom line for architects: Place CluedIn at the heart of unification and quality, bridging raw sources and trustworthy consumer outputs. Keep it event‑friendly, contract‑driven, catalog‑connected, and governed by policy. Integrate tightly with your orchestrator, lake/warehouse, catalog, Power Platform/Apps, and SIEM—so the rest of your architecture can move faster with confidence. ",
    "url": "/kb/cluedin-solution-architects#14-onepage-checklist-print-this",
    
    "relUrl": "/kb/cluedin-solution-architects#14-onepage-checklist-print-this"
  },"738": {
    "doc": "CluedIn for Solution Architects",
    "title": "CluedIn for Solution Architects",
    "content": " ",
    "url": "/kb/cluedin-solution-architects",
    
    "relUrl": "/kb/cluedin-solution-architects"
  },"739": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": "In this section, we’ll walk you through the fundamental steps to get started with CluedIn. Before proceeding with the getting started guides, make sure that you have CluedIn up and running. This step is essential to ensure that you can follow along seamlessly and fully experience the basic features. If you don’t have CluedIn installed, check the following guides: . | CluedIn SaaS installation . | CluedIn PaaS installation . | Local installation of CluedIn . | . The guides in this section provide quick, hands-on instructions for the basic operations in CluedIn, such as: . | Ingesting the data into CluedIn. | Cleaning and deduplicating the records. | Streaming the records to a Microsoft SQL database. | Creating rules, hierarchies, glossary terms, and relations between golden records. | . While CluedIn contains a multitude of features, we’ll focus on these fundamental operations that are key to comprehending and using the platform. The primary objective of CluedIn is to empower you to customize the data flow according to your preferences, without imposing any strict rules and structures. That is why the sequence in which you choose to explore the guides within this section is flexible. However, we recommend starting by ingesting a file into CluedIn, as this will make it easier to view the outcomes from other record management operations, such as cleaning, deduplication, and streaming. CluedIn serves as a zero-modelling master data management platform, enabling you to immediately improve the quality of your data even before defining the approach for producing and sharing your golden records. After ingesting the data, you are free to perform other operations in any order you want. For instance, you may want to stream the records to a Microsoft SQL database. By doing so, you can promptly observe CluedIn’s ability to keep the stream synchronized with any actions performed on the records within the platform. Each getting-started guide includes a video and a set of step-by-step procedures designed to help you become acquainted with the basic features of CluedIn and acquire the skills for performing essential tasks. ",
    "url": "/getting-started",
    
    "relUrl": "/getting-started"
  },"740": {
    "doc": "GraphQL",
    "title": "Examples",
    "content": "To help you get upskilled on our GraphQL implementation, here are some examples for you to play with. ",
    "url": "/consume/graphql#examples",
    
    "relUrl": "/consume/graphql#examples"
  },"741": {
    "doc": "GraphQL",
    "title": "Get an entity by Id",
    "content": "Obtain an example id by searching for your admin user: . { entity(id:\"c2d38278-6886-532d-8405-98e97298298f\") { name } } . ",
    "url": "/consume/graphql#get-an-entity-by-id",
    
    "relUrl": "/consume/graphql#get-an-entity-by-id"
  },"742": {
    "doc": "GraphQL",
    "title": "Get entities by search",
    "content": "{ search(query:\"Test\") { entries { name } } } . ",
    "url": "/consume/graphql#get-entities-by-search",
    
    "relUrl": "/consume/graphql#get-entities-by-search"
  },"743": {
    "doc": "GraphQL",
    "title": "Get entities by particular vocabulary keys",
    "content": "{ search(query:\"properties.user.firstName:Alix\") { entries { name } } } . ",
    "url": "/consume/graphql#get-entities-by-particular-vocabulary-keys",
    
    "relUrl": "/consume/graphql#get-entities-by-particular-vocabulary-keys"
  },"744": {
    "doc": "GraphQL",
    "title": "Get entities by a combination of vocabulary keys",
    "content": "{ search(query: \"+properties.user.firstName:Alix +properties.user.lastName:Freke\") { entries { name } } } . ",
    "url": "/consume/graphql#get-entities-by-a-combination-of-vocabulary-keys",
    
    "relUrl": "/consume/graphql#get-entities-by-a-combination-of-vocabulary-keys"
  },"745": {
    "doc": "GraphQL",
    "title": "Get all entities that have a value for a certain property",
    "content": "{ search(query: \"properties.user.firstName:A*\") { totalResults entries { name } } } . or just find out how many records match . { search(query: \"properties.user.firstName:A*\") { totalResults } } . ",
    "url": "/consume/graphql#get-all-entities-that-have-a-value-for-a-certain-property",
    
    "relUrl": "/consume/graphql#get-all-entities-that-have-a-value-for-a-certain-property"
  },"746": {
    "doc": "GraphQL",
    "title": "Change what properties come back in the results, 4 records at a time",
    "content": "{ search(query: \"properties.user.firstName:A*\", pageSize:4) { totalResults entries { name createdDate displayName properties } } } . ",
    "url": "/consume/graphql#change-what-properties-come-back-in-the-results-4-records-at-a-time",
    
    "relUrl": "/consume/graphql#change-what-properties-come-back-in-the-results-4-records-at-a-time"
  },"747": {
    "doc": "GraphQL",
    "title": "Change what metadata comes out of the properties",
    "content": "{ search(query: \"properties.user.firstName:A*\", pageSize:4) { totalResults entries { name properties(propertyNames:[\"user.lastName\", \"user.gender\"]) } } } . ",
    "url": "/consume/graphql#change-what-metadata-comes-out-of-the-properties",
    
    "relUrl": "/consume/graphql#change-what-metadata-comes-out-of-the-properties"
  },"748": {
    "doc": "GraphQL",
    "title": "Explore Edges",
    "content": "{ search(query: \"user.firstName:*\", pageSize: 4) { entries { name edges { outgoingOfType(entityType: \"/Organization\", edgeType: \"/PartOf\") { entries { name } } } } } } . ",
    "url": "/consume/graphql#explore-edges",
    
    "relUrl": "/consume/graphql#explore-edges"
  },"749": {
    "doc": "GraphQL",
    "title": "GraphQL",
    "content": "CluedIn provides GraphQL as its way to pull and query data from it. The CluedIn GraphQL endpoint uses a combination of the different datastores to service the result of the query in question. You might find that a particular GraphQL query uses the Search, Graph and Blob Datastore to render the results. This is due to the query optimiser of CluedIn that determines the right datastore to serve the different parts of your query. This also allows immense flexibility with querying the data. An example would be that if we wanted to find all entities that are of a specific business domain and have a particular value for a property then you will find that the Search Store will service both these parts of the query and hence CluedIn will only ask it to service the query. If you then ask it to run this query, but return the full history of the records then CluedIn will run the search against the Search Store, but then using the results from the Search it will then ask the Blob Store to fetch the full object history out if it. Likewise, if you asked it to also return the records that are connected to these results of type Person, then it will most likely ask the Graph Store to fulfil that part of the query. The GraphQL endpoint has many different operations, including the ability to: . | Lookup entities by Id | Lookup entities using a Full Text search | Lookup entities using property value matches. | Lookup Metrics of Data | . All value lookups are case sensitive by default and hence if you were to use our GraphQL search point to lookup organisation.industry:Banking then if you had a value in an entity that was “banking” (note the lower case “b”) then you would not match this data. Although this behaviour can be changed, there is a better way to handle this. One of the main ideas behind CluedIn is that we are going to give downstream consumers a standard representation of the data and hence “banking” and “Banking” are different variations of essentially the same value. We would rather that you use CluedIn Clean to normalise these values and standardise as a business on the way that you will represent values for downstream consumers. It is perfectly fine to not propagate these changes back to the sources using the Mesh API, but downstream consumers should receive a standardised representation of values. If you decide that you would like to enabled case incentive values, you will need to extend the inbuilt ElasticEntity model within CluedIn and add in your own properties with their respective analysers to achieve that. ",
    "url": "/consume/graphql",
    
    "relUrl": "/consume/graphql"
  },"750": {
    "doc": "History",
    "title": "On this page",
    "content": ". | Golden record history . | Data part view | Golden record view | Filters | . | Data part statuses | Relations history | . History is an important aspect of a golden record because it shows which data parts make up a golden record, what values each data part has, and what changes were made to those data parts. In this article, you will learn how the history of a golden record works and how to interpret changes made to golden records. Note: Changes made by golden record rules, data part rules, and survivorship rules are not included in the golden record history. Depending on which aspect of the history you want to look at, you can switch between the two: . | Golden record history – here, you can view detailed information about all data parts that make up a golden record. | Relations history – here, you can view detailed information about all outgoing relations (also referred to as edges) of a golden record. | . ",
    "url": "/key-terms-and-features/golden-records/history#on-this-page",
    
    "relUrl": "/key-terms-and-features/golden-records/history#on-this-page"
  },"751": {
    "doc": "History",
    "title": "Golden record history",
    "content": "A golden record is made up of many data parts. These data parts can appear from clean projects, deduplication projects, files, crawlers, or manual modifications. For example, if you create a clean project and it affects a golden record property, then a new data part is added to the golden record history, showing the affected property along with other metadata properties. Similarly, if you manually edit a golden record property, then a new data part is added to the golden record history, showing the new property value. Every time a data part appears in the history, a branch is created. Branches help organize data parts by sources and easily retrieve data parts that were created from one source. For example, data parts that appeared from one clean project belong to one branch, and data parts from a different clean project belong to a different branch. Each branch has a head that contains values from the data parts belonging to that branch. The same golden record property can have different values in each head. By default, the latest value coming to CluedIn or the latest manually modified value is used in the golden record. If you want to determine which value should be used in a golden record property, create a survivorship rule. The following diagram illustrates the process of establishing a golden record. By default, the History page displays the data parts arranged by the sort date (this is the date when CluedIn received the data part). You can explore the data parts using the following views: . | Data part view . | Golden record view . | . Data part view . The data part view is the default view that shows all data parts that make up a golden record. Each data part has the following attributes: . | A color circle that represents a branch. A branch contains the data part versions that appeared from the same source. For example, if you have a crawler that changes some properties of the data part, these changes will be added to the branch as separate data part versions. By selecting the color circle, you can view the branch (1) and its versions (2, 3). | An icon that represents a source. A source is a place where the data part is coming from. A source can be a clean project, a deduplication project, a manual data entry project, a file, a crawler, and so on. | A title of the data part that consists of the source name and additional details. For example, if the source is the clean project, then the title also includes the clean project name. You can view the source by selecting the title. | A data part ID (also referred to as record ID). You can view the data part properties (vocabulary keys) by selecting the ID number. These data part properties are used to produce a head, which is then processed by survivorship rules to determine the operational values for the golden record. | Data part details, such as the user who created the data part and dates associated with the data part. For more information about what each date means, see Filters. | Data part status that indicates the type of change made on the data part level. For more information about each status, see Dat part statuses. | . By expanding the data part, you can view a table with the metadata properties and vocabulary keys associated with the data part. Each property is marked with the change type label, which indicates whether the property is newly added or changed from one data part version to another. Each data part is an individual entity, and it is not linked to other data parts in any way, except if these data parts belong to one branch. Because each data part is individual, the first data part in the branch is marked with the Added status and does not contain previous values. If the subsequent data part within the branch contain some changes compared to the first data part in the branch, then such data parts are marked with the Changed status. Golden record view . The golden record view appears when you select any metadata properties or vocabulary keys on the filters pane on the left side of the History page. This view shows all records (1) where the selected property or vocabulary key is used, along with the value (2) that is used in the golden record (this is the value that is shown on the Property tab of the golden record details page). By looking at the sort date, you can track the historical changes in the property value across all data parts. Part ID from the golden record view and Record ID from the data part view are the same. Filters . The History page contains various filters to help you navigate through the data parts: . | Data source filter – you can filter the data parts by the data source that indicates a place where a data part was produced. By default, the data parts from all data sources are displayed on the History page. | Date filter – you can filter the data parts by one of the following dates: . | Sort date – this date is determined by selecting the first available date among modified, created, and discovery dates. In most cases, this is the date when CluedIn received the data. | Modified date – the date when the data part was last modified in the source system. | Created date – the date when the data part was originally created in the source system. | Discovery date – the date when the data part was created in CluedIn. | . | Metadata filter – you can select any metadata property and see the data parts where it is used. | Vocabulary key filter – you can select any vocabulary key and see the data parts where it is used, as well as the value chosen for the golden record. The medal icon next to the vocabulary key signifies that this vocabulary key is used in the golden record. | . ",
    "url": "/key-terms-and-features/golden-records/history#golden-record-history",
    
    "relUrl": "/key-terms-and-features/golden-records/history#golden-record-history"
  },"752": {
    "doc": "History",
    "title": "Data part statuses",
    "content": "You can view the data part statuses when you’re on the data part view of the History page. | Status | Description | . | Discovered | A data part has been added for the first time in CluedIn. Such data parts do not have the Created Date value. The Previous Value column in such data part is always empty because this is the first data part in the history. | . | Added | A new data part has been added, creating a new branch. Such data part has the Created Date value. The Previous Value column is always empty because this is the first data part in a branch. | . | Changed | A new data part has been added within the existing branch. The Previous Value column contains the value from the previous data part in the branch. | . ",
    "url": "/key-terms-and-features/golden-records/history#data-part-statuses",
    
    "relUrl": "/key-terms-and-features/golden-records/history#data-part-statuses"
  },"753": {
    "doc": "History",
    "title": "Relations history",
    "content": "The relations history page contains detailed information about all outgoing relations (edges) of a golden record. While outgoing edges are included in the golden record history page, viewing them on the relations history page is more convenient. Each relation is presented as a separate table entry that contains the source, ID, type, and properties. To view the relation properties, simply expand the dropdown in the Properties column. You can filter relations by two criteria: . | Edge Type – the type of outgoing relation that a golden record has. | Properties – the properties that an outgoing relation of a golden record has. | . If you added an outgoing relation by mistake or if you no longer need an outgoing relation, you can delete it. To do this, select the part ID in the table, and then select Delete. ",
    "url": "/key-terms-and-features/golden-records/history#relations-history",
    
    "relUrl": "/key-terms-and-features/golden-records/history#relations-history"
  },"754": {
    "doc": "History",
    "title": "History",
    "content": " ",
    "url": "/key-terms-and-features/golden-records/history",
    
    "relUrl": "/key-terms-and-features/golden-records/history"
  },"755": {
    "doc": "What are Integrations",
    "title": "Introduction",
    "content": "The first thing you need to do when CluedIn is running, is to feed it with data. You need to choose which data you want to add to CluedIn. Data is pushed into CluedIn via integrations. You have two options: . | Installing an existing integration | Building a custom integration | . When you install CluedIn, you gain access to out-of-the-box integrations. Feel free to contact us for assistance with configuring and enabling these integrations. Additionally, we can implement custom integrations upon request. There are two main types of integrations: providers and enrichers. Providers . These integrations allow you to add data into CluedIn. They can connect to cloud tools, databases, file systems, etc. extract the data you want to send to CluedIn and assemble it in a format CluedIn will understand. There are many providers available in our GitHub, but alternatively you can also build your own to cater for your specific requirements. In order to do this though, you will require some C# coding experience. Some of the most common providers are the following: . | Crawlers | Connectors | . Enrichers . Their mission is to add extra information to improve data that is already in CluedIn. Data in CluedIn is structured in entities; these are similar to records. They can contain information about a person, a company, a task, etc. An enricher will use the existing information CluedIn to then query other external systems to try to find out more information about that entity, i.e. enrich it. We have a list of available enrichers in our GitHub, but you can also build your own, as long as you have some C# coding experience. ",
    "url": "/integration/introduction#introduction",
    
    "relUrl": "/integration/introduction#introduction"
  },"756": {
    "doc": "What are Integrations",
    "title": "What are Integrations",
    "content": " ",
    "url": "/integration/introduction",
    
    "relUrl": "/integration/introduction"
  },"757": {
    "doc": "Modeling approaches",
    "title": "On this page",
    "content": ". | Data-first approach | Model-first approach | . In this article, you will learn about two data modeling approaches in CluedIn: agile data-first approach and traditional model-first approach. ",
    "url": "/management/data-catalog/modeling-approaches#on-this-page",
    
    "relUrl": "/management/data-catalog/modeling-approaches#on-this-page"
  },"758": {
    "doc": "Modeling approaches",
    "title": "Data-first approach",
    "content": "The data-first approach focuses on agility, flexibility, and faster time-to-value. It provides the opportunity to ingest your data first and then dynamically create a business domain and vocabulary as needed. Unlike a predefined schema, the data-first approach uses flexible data structures to adapt to changing requirements. This allows your data models to iteratively evolve based on feedback, data analysis, and the dynamic nature of business needs. The following video shows an example of creating a shared model using vocabulary key mapping and survivorship rules. ",
    "url": "/management/data-catalog/modeling-approaches#data-first-approach",
    
    "relUrl": "/management/data-catalog/modeling-approaches#data-first-approach"
  },"759": {
    "doc": "Modeling approaches",
    "title": "Model-first approach",
    "content": "Traditional model-first approach involves the creation of a comprehensive data model that defines the structure, relationships, and constraints of master data. This approach requires a thorough understanding of the business domain, data requirements, and anticipated data usage scenarios. You need to think about all potential scenarios within your data beforehand. When it comes to practice, it may turn out that the model you’ve built does not align with your requirements, particularly when unforeseen possibilities and cases arise. After preparing the model, it may turn out that certain vocabulary keys are missing or that the model is not ideally suited for your specific dataset. That’s why the data-first approach is the preferred one. ",
    "url": "/management/data-catalog/modeling-approaches#model-first-approach",
    
    "relUrl": "/management/data-catalog/modeling-approaches#model-first-approach"
  },"760": {
    "doc": "Modeling approaches",
    "title": "Modeling approaches",
    "content": " ",
    "url": "/management/data-catalog/modeling-approaches",
    
    "relUrl": "/management/data-catalog/modeling-approaches"
  },"761": {
    "doc": "Plan the upgrade",
    "title": "On this page",
    "content": ". | Get familiar with the versioning scheme | Schedule the upgrade window | Inform the stakeholders | Perform a full backup | Review upgrade-related documentation | Prepare and test custom packages | . Careful planning ensures that the upgrade process is efficient, minimizes downtime, and accounts for dependencies across your environment. This page covers stage 1 of the CluedIn upgrade process. It outlines the decisions and preparations you should make before executing the upgrade. ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#on-this-page"
  },"762": {
    "doc": "Plan the upgrade",
    "title": "Get familiar with the versioning scheme",
    "content": "CluedIn releases version numbers follow this format: {year}.{month}.{patch}. For example, 2025.05.02. For details about the updates available in a specific release, see Release notes. ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#get-familiar-with-the-versioning-scheme",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#get-familiar-with-the-versioning-scheme"
  },"763": {
    "doc": "Plan the upgrade",
    "title": "Schedule the upgrade window",
    "content": ". | Schedule downtime – Plan the upgrade during a period when the application is not in use. The application will be unavailable throughout the upgrade process. | Allocate sufficient time – Ensure you have enough time for the full upgrade and, if required, disaster recovery. | . ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#schedule-the-upgrade-window",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#schedule-the-upgrade-window"
  },"764": {
    "doc": "Plan the upgrade",
    "title": "Inform the stakeholders",
    "content": "It is essential to keep stakeholders informed at every stage of the upgrade process: . | Before the upgrade – Provide advance notice of the planned upgrade window, expected downtime, and potential business impact. | During the upgrade – Provide updates if the upgrade takes longer than anticipated or if issues arise. | After the upgrade – Confirm completion, communicate any changes affecting users, and explain how to report issues. | . ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#inform-the-stakeholders",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#inform-the-stakeholders"
  },"765": {
    "doc": "Plan the upgrade",
    "title": "Perform a full backup",
    "content": "Back up the following: . | All persistent disks . | All user values . | . This step is critical in case a rollback becomes necessary. ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#perform-a-full-backup",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#perform-a-full-backup"
  },"766": {
    "doc": "Plan the upgrade",
    "title": "Review upgrade-related documentation",
    "content": "CluedIn publishes upgrade documentation with each new release. Be sure to review this documentation in full before beginning the upgrade process. While many upgrades follow common steps, not all are identical, and some may include specialized procedures. Pay particular attention to any infrastructure-related changes, as these may require additional preparation or configuration. ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#review-upgrade-related-documentation",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#review-upgrade-related-documentation"
  },"767": {
    "doc": "Plan the upgrade",
    "title": "Prepare and test custom packages",
    "content": "If you use custom packages, verify that the following steps are completed before upgrading: . | The package developer reviews the Release notes to identify any breaking changes in the new CluedIn version and adapts the code if necessary. | The packages are compiled and deployed against the new version you plan to upgrade to. | Package developer tests the packages locally with the new version. | The packages are available in the feed accessible by the cluster. If the packages are not deployed towards the standard CluedIn feed, add a custom feed. | . ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade#prepare-and-test-custom-packages",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade#prepare-and-test-custom-packages"
  },"768": {
    "doc": "Plan the upgrade",
    "title": "Plan the upgrade",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/plan-the-upgrade",
    
    "relUrl": "/paas-operations/upgrade/guide/plan-the-upgrade"
  },"769": {
    "doc": "Power Apps pre-configuration guide",
    "title": "On this page",
    "content": ". | Set up a firewall policy | Pre-configuration steps in Microsoft Entra ID . | Register a new application | Create a client secret | . | Pre-configuration steps in Power Apps . | Find your environment ID | Create a new security role | Security role reference table | Create a new application user | Create a Dataverse connection | . | Next steps | . In this guide, you will learn how to prepare for configuring Power Apps integration in CluedIn. The instructions in this guide apply to both public and private CluedIn instances, as Power Apps is hosted in your environment. ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide#on-this-page"
  },"770": {
    "doc": "Power Apps pre-configuration guide",
    "title": "Set up a firewall policy",
    "content": "To enable CluedIn to call Power Apps, you need to add specific rules to your Azure Firewall as described here. Power Apps integration also involves Power Automate, which is used for data ingestion workflow to push data from Dataverse to CluedIn. That’s why you need to add firewall rules both for Power Automate and Power Apps. ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide#set-up-a-firewall-policy",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide#set-up-a-firewall-policy"
  },"771": {
    "doc": "Power Apps pre-configuration guide",
    "title": "Pre-configuration steps in Microsoft Entra ID",
    "content": "To manage all transactions between CluedIn and Dataverse, you need to register a new application in Microsoft Entra ID. When you register a new application, a service principal is automatically created for the app registration. Following the steps in this section will provide you with the Client ID, Tenant ID, and Client Secret required to configure Power Apps integration in CluedIn. For more information, see Register an application in Microsoft Entra ID. Register a new application . | Sign in to the Microsoft Entra admin center as at least an Application Developer role. | If you have access to multiple tenants, switch to the tenant in which you want to register the application. | Go to Identity &gt; Applications &gt; App registrations, and then select New registration. | Enter a Name for your application. | Under Supported account types, specify who can use the application (Accounts in this organizational directory only). | Select Register. On the application’s Overview page, you can find the Application (client) ID and Directory (tenant) ID that you will need to configure Power Apps integration in CluedIn as described in Power Apps configuration guide. | . Create a client secret . | In the Microsoft Entra admin center, in App registrations, select your newly created application. | Go to Certificates &amp; secrets &gt; Client secrets &gt; New client secret. | Add a description for your client secret. Select an expiration for the secret or specify a custom lifetime. Finally, select Add. | Record the Value of the client secret. You will need it to configure Power Apps integration in CluedIn as described in Power Apps configuration guide. This secret value is never displayed again after you leave this page. | . ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide#pre-configuration-steps-in-microsoft-entra-id",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide#pre-configuration-steps-in-microsoft-entra-id"
  },"772": {
    "doc": "Power Apps pre-configuration guide",
    "title": "Pre-configuration steps in Power Apps",
    "content": "In order to manage your master data from CluedIn directly in Power Apps Dataverse tables, you need to prepare a Power Apps environment. This involves creating a new security role and a new application user. These will be used to grant access to the Power Apps environment to the application that you created in Pre-configuration steps in Microsoft Entra ID. Prerequisites . | You need to have an environment in the Power Platform admin center with Dataverse as a data store. | You need to have the System Administrator access to the Power Platform of your organization. | . Find your environment ID . | Sign in to the Power Platform admin center. | On the navigation pane, select Environments, and then select your intended environment. | Find the environment ID in the Details section. The environment ID is needed to configure Power Apps integration in CluedIn as described in Power Apps configuration guide. | . Create a new security role . | Sign in to the Power Platform admin center. | On the navigation pane, select Environments, and then select your intended environment. | Select Settings &gt; Users + permissions &gt; Security roles. | Select + New role. | Enter the role name and then select the business unit. | In Member’s privilege inheritance, select Direct User (Basic) access level and Team privileges. | Select Save. | Grant your app’s table privileges to the newly created security role according to the reference table. To open the edit mode, select the three-dot button next to the table that you want to edit. | Once you’ve updated the security role’s privileges according to the reference table, select Save. | . Security role reference table . | Table | Create | Read | Write | Delete | . | Customization |   |   |   |   | . | Solution | Organization | Organization | Organization | Organization | . | Publisher | Organization | Organization | Organization | Organization | . | Entity | Organization | Organization | Organization | Organization | . | Entity Key | Organization | Organization |   | Organization | . | Attribute | Organization | Organization | Organization | Organization | . | System Form | Organization | Organization | Organization | Organization | . | View | Organization | Organization | Organization | Organization | . | Custom Control Default Config | Organization |   | Organization | Organization | . | Process | Organization | Organization | Organization | Organization | . | Custom Tables |   |   |   |   | . | Connection Reference | Organization | Organization | Organization | Organization | . | Connector | Organization | Organization | Organization | Organization | . | Dataflow | Organization | Organization | Organization | Organization | . | OptionSet | Organization | Organization | Organization | Organization | . Create a new application user . | Sign in to the Power Platform admin center. | On the navigation pane, select Environments, and then select your intended environment. | Select Settings &gt; Users + permissions &gt; Application users. | Select + New app user. | In the App field, select Add an app, and then find and select the application that you created before in Register a new application. | Enter the Business unit. | In the Security roles field, select the pencil icon, and then enter the security role that you created before in Create a new security role. | Select Create. | . Create a Dataverse connection . To enable communication between Dataverse tables and CluedIn and to automate the creation of workflows for ingesting the data from Dataverse to CluedIn, you need to manually create a Dataverse connection and share it with the application user. The workflows in Power Apps integration are different in terms of configuration from the Workflows module in CluedIn. For more information, see Power Automate integration. To create a new connection . | Sign in to make.powerapps.com. | On the left navigation pane, select Connections. | Select + New connection. | Find the Dataverse connection and select the one with the green icon. | Select Create. The new connector appears under Connections. | Open the newly added Dataverse connection, and then select Share. | Find and select the application user that you created in Create a new application user. Grant the following permissions to the application user: Can use or Can edit. Then, select Save. | . You will need the ID of Dataverse connection to configure Power Apps integration in CluedIn as described in Power Apps configuration guide. To find the connection ID, open the connection and look for ID in the URL field. ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide#pre-configuration-steps-in-power-apps",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide#pre-configuration-steps-in-power-apps"
  },"773": {
    "doc": "Power Apps pre-configuration guide",
    "title": "Next steps",
    "content": "Now that you have completed all pre-configuration steps, start the configuration of Power Apps in CluedIn using our Power Apps configuration guide. ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide#next-steps",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide#next-steps"
  },"774": {
    "doc": "Power Apps pre-configuration guide",
    "title": "Power Apps pre-configuration guide",
    "content": " ",
    "url": "/microsoft-integration/powerapps/pre-configuration-guide",
    
    "relUrl": "/microsoft-integration/powerapps/pre-configuration-guide"
  },"775": {
    "doc": "Power Automate pre-configuration guide",
    "title": "On this page",
    "content": ". | Set up a firewall policy | Create a user account | Configure an environment | Create a service application | Next steps | . In this guide, you will learn how to prepare for configuring Power Automate integration for using workflows in CluedIn. The instructions in this guide are applicable to the current version of Power Automate (2502.2) and Dataverse  (9.2.25031.00154). This guide is applicable to both public and private CluedIn instances. ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#on-this-page"
  },"776": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Set up a firewall policy",
    "content": "To enable CluedIn to call Power Automate, you need to add specific rules to your Azure Firewall as described here. Power Automate integration also involves Power Apps, which is used for access to Dataverse where the CluedIn custom connector is stored. That’s why you need to add firewall rules both for Power Automate and Power Apps. ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#set-up-a-firewall-policy",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#set-up-a-firewall-policy"
  },"777": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Create a user account",
    "content": "A user account in Microsoft Entra ID will be used to manage Power Automate workflows. This user account acts like a bot, creating and managing workflows on behalf of the service application. This approach addresses certain Power Automate limitations, such as the requirement that only user accounts can create connector connections. This user account will be the sender of approval requests that users receive in Outlook. Adding a new user might incur costs depending on your Microsoft Entra ID plan. For more details, please consult your IT team. Prerequisites . | To create a new user, you need to have the User Administrator role in Microsoft Entra ID. | . To create and configure a user account . | Create a new user in Microsoft Entra ID following Microsoft documentation. The user in Microsoft Entra ID follows the tenant configuration. The user on the screenshot above is created in CluedIn tenant and therefore it uses cluedin.com as the domain name in User principal name and CluedIn logo as a profile picture. Your user will have the domain name that you specify when creating the user. You can also edit the profile picture after the user account has been created. To do that, select the camera icon in the lower-right corner of the user’s thumbnail. | Add the following licenses to the created user account: . | Microsoft 365 Business Standard – having this license allows sending approval requests to Outlook and Teams. You can use any Microsoft 365 license. | Microsoft Power Apps for Developer – having this license grants access to Dataverse, which is necessary for storing CluedIn custom connector that facilitates communication between CluedIn and Power Automate. | Microsoft Power Automate Free – having this license grants access to Power Automate and the ability to use connectors to create approval flows. | . For more information on how to add a license, see Microsoft documentation. | . ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#create-a-user-account",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#create-a-user-account"
  },"778": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Configure an environment",
    "content": "In order to use workflows, you will need to configure a Power Platform environment with Dataverse as a data store. The purpose of Dataverse is to store the CluedIn custom connector. Without Dataverse, there would be no location to store the CluedIn custom connector, and the workflows would not not function. In this environment, the user account created in the previous step is granted System Administrator access. This access is limited to the specific environment. The System Administrator in the environment is NOT a global Azure role. Prerequisites . | To create a new environment and add a new user to that environment, you need to have the Power Platform Admin role. | . To a configure an environment . | Create a new environment in the Power Platform admin center with Dataverse as a data store. To do this, follow the instruction in Create an environment with a database. | Add the user account that you created in the previous section to the list of users in the environment. To do this, follow the instruction in Add users to an environment that has a Dataverse database. As a result, the user should be available in the list of users for the environment. On the following screenshot, the Username has cluedin.com as the domain name. This is because the user was created in the CluedIn tenant in Microsoft Entra ID. Your user will have the domain name that you specify when creating a user. | Assign the System Administrator security role to the user account that you added in step 2. To do this, follow the instruction in Assign a security role to a user. | . ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#configure-an-environment",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#configure-an-environment"
  },"779": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Create a service application",
    "content": "In order to authenticate the Power Automate widget in CluedIn, you need to register a new application in Microsoft Entra ID and grant it the necessary permissions. This service application is required to create tokens for communication with Power Automate. When you load the workflow in CluedIn, you are actually loading a Power Automate iframe widget. The service application facilitates communication between CluedIn and Power Automate. Therefore, even though you create the workflow in CluedIn, the actual workflow is created in Power Automate. Note that when you register a new application in Microsoft Entra ID, a service principal is automatically created for the app registration. To create and configure an application . | Register an application in Microsoft Entra ID following the instructions here. | Add a redirect URI: . | Under Manage, select Authentication. | Under Platform configurations, select Add a platform. | Under Configure platforms, select Single-page application. | In the Redirect URIs section, enter the reply URI in the following format: https://your-cluedin-domain/admin/workflow/workflow-builder. | In the Implicit grant and hybrid flows section, select both for Access tokens and ID tokens. | Select Configure. | . | In the Advanced settings section, in Allow public client flows, set the toggle to Yes. | Go to Manage &gt; API permissions, and add the following permissions: . | Azure Service Management: . | user_impersonation – Delegated | . | Dynamics CRM / Dataverse: . | user_impersonation – Delegated | . | Microsoft Graph: . | openid – Delegated . | profile – Delegated . | User.ReadBasic.All – Delegated. | . | Power Automate: . | Approvals.Manage.All – Delegated . | Approvals.Read.All – Delegated . | Flows.Manage.All – Delegated . | Flows.Read.All – Delegated . | . | . | . ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#create-a-service-application",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#create-a-service-application"
  },"780": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Next steps",
    "content": "Now that you have completed all pre-configuration steps, start the configuration of workflows in CluedIn using our Power Automate configuration guide. ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide#next-steps",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide#next-steps"
  },"781": {
    "doc": "Power Automate pre-configuration guide",
    "title": "Power Automate pre-configuration guide",
    "content": " ",
    "url": "/microsoft-integration/power-automate/pre-configuration-guide",
    
    "relUrl": "/microsoft-integration/power-automate/pre-configuration-guide"
  },"782": {
    "doc": "Power Automate Integration",
    "title": "Power Automate Integration",
    "content": "Power Automate is a key component of the Power Platform, which is Microsoft’s suite of tools designed for low-code development, automation, and data analysis. Power Automate allows you to create, manage, execute, and automate workflows between various applications and services. Power Automate can be integrated with CluedIn to capture important events in CluedIn and use them as triggers to start workflows. Such events include creating or modifying a vocabulary key, processing new records, inviting a new user, and more. When the workflow is started, it sends approval requests or notifications to the users in Outlook or the Approvals app in Teams. This way you can stay on top of certain processes in CluedIn, whether you are using the platform at the moment or not. The following image is example of approval request in Outlook. The following image shows an approval request in the Approvals app in Teams. Power Automate can be connected with CluedIn with the help of our CluedIn custom connector. This custom connector enables you to create workflows in the Power Automate widget in CluedIn. This means that even though you create workflows in CluedIn, technically they are created in Power Automate. The following screenshot shows an example of a workflow that is created in the Power Automate widget in CluedIn. The first and third actions of the workflow are provided by the CluedIn custom connector. The second action comes from the Approval connector, which is a standard Power Automate connector. Once you create a workflow in CluedIn, you do not have to configure these actions because they are automatically configured for you. The CluedIn custom connector has predefined triggers that correspond to specific events in CluedIn. To install the CluedIn custom connector, you need to complete our Power Automate pre-configuration guide and Power Automate configuration guide. The CluedIn custom connector is an underlying element of the integration between Power Automate and CluedIn. You cannot create workflows without CluedIn custom connector. The CluedIn custom connector and its connector references are stored in a Dataverse table within a dedicated Power Platform environment. Therefore, it is essential to have a dedicated environment with Dataverse as the data store. Without Dataverse, there is no location to store the CluedIn custom connector, and the workflows will not function. To use workflows in CluedIn, start from the Power Automate pre-configuration guide. ",
    "url": "/microsoft-integration/power-automate",
    
    "relUrl": "/microsoft-integration/power-automate"
  },"783": {
    "doc": "Power Apps Integration",
    "title": "Power Apps Integration",
    "content": "Power Apps is a key component of the Power Platform, which is Microsoft’s suite of tools designed for low-code development, automation, and data analysis. Power Apps allows you to build custom applications with minimal coding. In Power Apps, you can connect to Dataverse, which provides a unified and simplified data schema that allows you to integrate data from multiple sources into a single store. Dataverse is a scalable data service and app platform that lets you securely store and manage data used by business applications. Power Apps can be integrated with CluedIn to enable you to manage your master data directly in the Dataverse platform and automatically sync it with CluedIn. Power Apps integration offers the following benefits: . | 2-way synchronization of Dataverse metadata to CluedIn business domains and vocabularies and vice versa: . | CluedIn stream to export golden records from CluedIn to the Dataverse tables. | Data ingestion workflow to push data from Dataverse tables to CluedIn ingestion endpoint. | . | Auto-mapping of columns, keys, and relationships. | . To achieve a 2-way synchronization between Dataverse and CluedIn, the Power Apps integration also involves Power Automate workflows. However, these workflows should not be confused with the approval workflows that are available in the Workflow module in CluedIn. We discuss the approval workflows as part of Power Automate integration. To sync master data between CluedIn and the Dataverse platform, start from the Power Apps pre-configuration guide. ",
    "url": "/microsoft-integration/powerapps",
    
    "relUrl": "/microsoft-integration/powerapps"
  },"784": {
    "doc": "Property rules",
    "title": "On this page",
    "content": ". | Create property rules on your own | Create property rules using AI | Custom CEL expression examples | . Property rules help you improve the quality of mapped records (clues) by normalizing and transforming property values. The difference between property rules and pre-process rules is that property rules are applied only to property values of the record while pre-process rules are applied to the whole record. So, if you want to apply some changes to the property values, create a property rule. Property rules are applied to the clues before pre-process rules and advanced mapping code. You can create property rules on you own or use the CluedIn AI recommendation engine to generate property rules. Prerequisites . To access property rules, go to Administration &gt; Feature Flags and make sure that the following features are enabled: . | Mapping Property Rules . | Data Set Quarantine . | . Create property rules on your own . You can apply the rule to all values of the property or define specific values. The available actions for the rule depend on the type of the property and are divided into several categories: . | Basic – set a new value (for example, you can enter a new value or use the value from another property) or write custom CEL expression. | Normalization – transform data into a consistent form. | Quarantine – send records to the quarantine where you can modify them and decide what to do with them afterwards. | . You can add multiple rules for one property. To create a property rule . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set. | Go to the Map tab, and then select Edit mapping. | On the Map columns to vocabulary key tab, find the property for which you need to create a rule, and then select Add property rule. | In Filter, select whether you want to apply the rule to all values or to specific values. | If you want to apply the rule to specific values: . | Select a condition to determine which values will be affected by the rule. | Select whether you want to apply the rule when the condition is met or when the condition is not met. For example, if you want to detect all invalid email addresses, select Is valid email address, and then select Apply action when condition is not met. | . | Select the Action that will be applied to the values of the property. | In the lower-right corner, select Add rule. The rule is added next to the property. The rule will be applied when you process the records. | . Create property rules using AI . CluedIn AI recommendation engine helps you create property rules in a quick and efficient manner. The engine analyzes properties and runs data type checks to come up with suggested actions. To use AI capabilities to create property rules, first complete all the steps described in Azure Open AI Integration. To create property rules using AI . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set. | Go to the Map tab, and then select Edit mapping. | On the Map columns to vocabulary key tab, select Generate rules. | In the pane that opens, expand each property to review suggested validations and actions. | In the lower-right corner, select Create rules. The rules are added next to the properties. By selecting these rules, you can view, create, or remove rules from the property. The rules will be applied when you process the records. | . ",
    "url": "/integration/additional-operations-on-records/property-rules#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/property-rules#on-this-page"
  },"785": {
    "doc": "Property rules",
    "title": "Custom CEL expression examples",
    "content": "This section contains some examples of CEL expressions in property rules. Remove the first occurrence of characters . The following CEL expression removes the first occurrences of the characters # and $ from property values. You can use this expression if you have a property (for example, postal code) that contains values with one occurrence of # and $. (\"\" + value).replace(\"#\", \"\").replace(\"$\", \"\") . On the screenshot, rows 1 and 3 contain several occurrences of # and $. The property rule removed only the first occurrences of those characters. To remove all occurrences of specific characters, use the expression from the next section. Remove all occurrences of characters . The following CEL expression removes all occurrences of the characters # and $ from property values. You can use this expression if you have a property (for example, postal code) that contains values with multiples occurrences of # and $. (\"\" + value).replace(/[#\\$]/g, \"\") . Turn non-numeric values to blank or null . The following CEL expressions turn the value to blank or null if the value is not a number. You can use any of these expressions if you have a property (for example, revenue) that should contain only numbers, and you want to remove any non-numeric values. (+value == value) ? value : \"\" . (+value == value) ? value : null . If the property contains non-numeric values, applying any of the above expressions will result in golden records without such property. Regardless of the expression you use, the result will be the same. ",
    "url": "/integration/additional-operations-on-records/property-rules#custom-cel-expression-examples",
    
    "relUrl": "/integration/additional-operations-on-records/property-rules#custom-cel-expression-examples"
  },"786": {
    "doc": "Property rules",
    "title": "Property rules",
    "content": " ",
    "url": "/integration/additional-operations-on-records/property-rules",
    
    "relUrl": "/integration/additional-operations-on-records/property-rules"
  },"787": {
    "doc": "Purview pre-configuration guide",
    "title": "On this page",
    "content": ". | Prerequisites | Create resources in Azure . | Register an application and create a service principal | Create a storage account | Create a key vault and register Purview | Create a key vault secret | . | Prepare Microsoft Purview environment . | Create a new collection | Register a new data source | Scan a data source | Assign roles to Purview service principal | . | Next steps | . In this guide, you will learn how to prepare for configuring Purview integration in CluedIn. Generally, you need to prepare the following: . | Purview account . | App registration . | Data sources, such as a storage account, Fabric, SQL Server, Snowflake. | Key vault . | Purview environment: collections, scanned data sources, role assignments . | . ",
    "url": "/microsoft-integration/purview/pre-configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide#on-this-page"
  },"788": {
    "doc": "Purview pre-configuration guide",
    "title": "Prerequisites",
    "content": "Make sure you have a Microsoft Purview account. For information on how to create it, see Create a Microsoft Purview account. ",
    "url": "/microsoft-integration/purview/pre-configuration-guide#prerequisites",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide#prerequisites"
  },"789": {
    "doc": "Purview pre-configuration guide",
    "title": "Create resources in Azure",
    "content": "In this section, you will find instructions for creating and preparing Azure resources needed for Purview integration configuration. Register an application and create a service principal . When you register a new application in Microsoft Entra ID, a service principal is automatically created for the app registration. The service principal is the app’s identity in the Microsoft Entra tenant. For more information, see Register a Microsoft Entra app and create a service principal. To register an application . | Sign in to the Azure portal. | On the Home page of the portal under Azure services, select Microsoft Entra ID. | In the left navigation pane, select App registrations. Then, on the App registrations page, select + New registration. | Provide your application’s registration information: Name and Supported account types. | Select Register. You will need the Application (client) ID and Directory (tenant) ID to configure Purview integration in CluedIn as described in Purview configuration guide. | . Once the application is registered, create a client secret. To create a client secret . | Go to Certificates &amp; secrets &gt; Client secrets &gt; New client secret. | Add a description for your client secret. Select an expiration for the secret or specify a custom lifetime. Finally, select Add. | Record the Value of the client secret. You will need it to configure Purview integration in CluedIn as described in Purview configuration guide. This secret value is never displayed again after you leave this page. | . Create a storage account . | Sign in to the Azure portal. | From the left portal menu, select Storage accounts. | On the Storage accounts page, select Create. | On the Basics tab, provide the essential information for your storage account: . | In the Project details section: . | Select the Subscription for the new storage account. | Create a new Resource group for this storage account or select an existing one. | . | In the Instance details section: . | Enter a unique Storage account name. | Select the appropriate Region for your storage account. | Select the Primary service that provides a unique namespace for your Azure Storage data (Azure Blob Storage or Azure Data Lake Storage Gen 2). | In Performance, leave the default Standard option selected. This type of account is recommended by Microsoft for most scenarios. | In Redundancy, leave the default Geo-redundant storage (GRS) option selected. This way, your data is replicated to a data center in a different region. For read access to data in the secondary region, select Make read access to data available in the event of regional unavailability. | . | Select **Review + create**. | . | When you navigate to the Review + create tab, Azure runs validation on your storage account settings. After the validation is passed, select Create. | . Create a key vault and register Purview . To store encryption keys, secrets, and certificates for communication between Purview, Azure Data Factory, and data stores, create and configure a key vault. To create and configure a key vault . | Sign in to the Azure portal and create a key vault following the instructions from Microsoft. After you create a key vault, you need to grant the Microsoft Purview managed identity access to your Azure Key Vault. | In the navigation pane, go to Settings &gt; Access configuration. | In the Permission model section, select Vault access policy. | Select Apply. | In the navigation pane, go to Access policies. | On the Access policies page, select Create. | On the Permissions tab, in the Secret permissions column, select the checkboxes for Get and List, and then select Next. | On the Principal tab, find and select the Purview account, and then select Next. | On the Application (optional) tab, select Next. | On the Review + create tab, select Create. | . Create a key vault secret . | In the Azure portal, open the key vault that you created before. | In the navigation pane, go to Objects &gt; Secrets. | On the Secrets page, select Generate/Import. | Enter the Name of the secret. | To get the Secret value, do the following: . | Go to the storage account that you created before in Create a storage account. | On the navigation pane, go to Data storage &gt; Access keys. | Copy the value of key1. | Paste the copied value to the Secret value field. | Select Create. | . | . ",
    "url": "/microsoft-integration/purview/pre-configuration-guide#create-resources-in-azure",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide#create-resources-in-azure"
  },"790": {
    "doc": "Purview pre-configuration guide",
    "title": "Prepare Microsoft Purview environment",
    "content": "In this section, you will find instructions for preparing your Purview environment for integration with CluedIn. Create a new collection . You need to create 2 collections: one to store the assess from Azure Data Lake Storage and the other one to store the assess from CluedIn. To create a new collection . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains, and then select your default domain. | On the default domain page, select + New collection. | Provide the details for the collection that will be used to store the assets from CluedIn: . | Enter the Display name and Description of the collection. | Select Create. The new collection is added to the default domain. Pay attention to the collection ID, which can be found in the URL. You will need this collection ID to configure Purview integration in CluedIn as described in in Purview configuration guide. | . | To create a collection that will be used to store the assets from Azure Data Lake Storage, repeat steps 1–3. | . Register a new data source . To enable Purview to discover and catalog metadata from your data sources, register a new data source. To register a new data source . | In the Microsoft Purview portal, navigate to Data Map &gt; Data sources, and then select Register. | Find and select a data source type. This example uses Azure Data Lake Storage Gen2. Select Continue. | Enter the Data source name. | Select the Storage account name. This is the storage account that you created before in Create a storage account. After you select the storage account name, the Azure subscription will be filled out automatically. | Make sure the default Domain is selected. | Select the Collection for storing the assets from Azure Data Lake Storage that you created before in Create a new collection. | Select Register. The new data source is added. | . Scan a data source . To capture technical metadata from your data source in Purview, scan a data source. To create a scan . | In the Microsoft Purview portal, navigate to Data Map &gt; Data sources. | Find the data source that you created in Register a new data source, and then select View details. | On the data source page, select New scan. | Enter the Name for the scan. | Expand the Credential dropdown list, and then select + New. | Enter the Name and Description of the credential. | In Authentication method, leave the default Account key option selected. | In the Account key section, expand the Key Vault connection dropdown, and then select + New. | Enter the Name and Description of the new key vault. | In Key Vault name, select the key vault that you created before in Create a key vault and register Purview. | Select Create. | Expand the Key Vault connection dropdown, and then select the newly created key vault connection. | In Secret name, enter the name of the secret that you created before in Create a key vault secret. | Select Create. | Select Test connection. | Once the connection is successful, select Continue. | Scope your scan to a specific subset of data, and then select Continue. | Keep the default scan rule set, and then select Continue. | Choose your scan trigger. You can set up a schedule or run the scan once. In this example, we run the scan once. Select Continue. | Review your scan, and then select Scan and run. Once the scan is completed, it establishes a connection to the data source and captures technical metadata like names, file size, columns, and so on. As a result of the scan, the collection for storing assets from Azure Data Lake Storage contains a certain number of assets. On the following screenshot, our collection contains 18 assets. | . Assign roles to Purview service principal . To enable Purview to communicate with CluedIn, assign the Data readers and Data source admins roles to the Purview service principal. To assign roles to Purview service principal . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains, and then select your default domain. | On the default domain page, go to the Role assignments tab. | Expand the Edit role assignments dropdown list, and then select Data readers. | Find and select the Purview service principal that you created in Register an application and create a service principal. | Select OK. | Expand the Edit role assignments dropdown list, and then select Data source admins. | Find and select the Purview service principal that you created in Register an application and create a service principal. | Select OK. | . ",
    "url": "/microsoft-integration/purview/pre-configuration-guide#prepare-microsoft-purview-environment",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide#prepare-microsoft-purview-environment"
  },"791": {
    "doc": "Purview pre-configuration guide",
    "title": "Next steps",
    "content": "Now that you have completed all pre-configuration steps, start the configuration of Purview integration in CluedIn using our Purview configuration guide. ",
    "url": "/microsoft-integration/purview/pre-configuration-guide#next-steps",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide#next-steps"
  },"792": {
    "doc": "Purview pre-configuration guide",
    "title": "Purview pre-configuration guide",
    "content": " ",
    "url": "/microsoft-integration/purview/pre-configuration-guide",
    
    "relUrl": "/microsoft-integration/purview/pre-configuration-guide"
  },"793": {
    "doc": "CluedIn Python SDK",
    "title": "On this page",
    "content": ". | Getting started . | Local environment | Cloud environments and notebooks | . | Authentication | Import CluedIn Python SDK module | Creating a Context . | From API token | From an object | From a file | Getting API token from email and password | . | . CluedIn provides powerful REST and GraphQL APIs. To simplify programmatic interaction, we offer the CluedIn Python SDK — a Python package that handles authentication, automation, data import and export, data transformation, and data exploration. You can use CluedIn Python SDK in many environments, from your local laptop to cloud services like: . | Microsoft Fabric | Azure Synapse Analytics | Databricks | Snowflake | Azure Functions | Jupyter Notebook | Google Colab | Airflow | dbt | Kaggle | . The above list is non-exhaustive. You can use CluedIn Python SDK in any environment where you can run Python code. Some of the scenarios where CluedIn Python SDK will be helpful are: . | Querying data from your CluedIn instance. | Exporting and saving the data to CSV, Parquet, Excel, JSON, or many other file formats. | Reading and writing configuration of your CluedIn instance. | Ingesting data in CluedIn. | Automating repetitive actions to reduce manual work in UI. | . ",
    "url": "/playbooks/data-engineering-playbook/python-sdk#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk#on-this-page"
  },"794": {
    "doc": "CluedIn Python SDK",
    "title": "Getting started",
    "content": "CluedIn Python SDK supports Python from version 3.7 and above. Local environment . When you run Python scripts or notebooks on your local machine, we recommend using virtualenv or venv to ensure your local environment has the right version of Python and installed modules. To install CluedIn Python SDK, run the following command in your shell: . pip install cluedin . Cloud environments and notebooks . To install CluedIn Python SDK in a notebook, use %pip instead of pip and run the following command in your notebook’s cell: . %pip install cluedin . Alternatively, when you use notebooks in tools like Microsoft Fabric, Databricks, etc., there is usually a way to configure the notebook’s environment. For example, here’s how to install a module in Microsoft Fabric: https://learn.microsoft.com/en-us/fabric/data-engineering/create-and-use-environment. In all cases, after you install the cluedin package, you can import it in your Python code: . import cluedin . ",
    "url": "/playbooks/data-engineering-playbook/python-sdk#getting-started",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk#getting-started"
  },"795": {
    "doc": "CluedIn Python SDK",
    "title": "Authentication",
    "content": "To connect to your CluedIn instance via API or CluedIn Python SDK, you need two things: URL of your CluedIn instance API (access) token - a JWT token that gives you access to API . CluedIn Python SDK uses the Context object to keep the information it needs to connect to CluedIn API. ",
    "url": "/playbooks/data-engineering-playbook/python-sdk#authentication",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk#authentication"
  },"796": {
    "doc": "CluedIn Python SDK",
    "title": "Import CluedIn Python SDK module",
    "content": "In the code examples below, we assume that you imported CluedIn module in your code: . import cluedin . ",
    "url": "/playbooks/data-engineering-playbook/python-sdk#import-cluedin-python-sdk-module",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk#import-cluedin-python-sdk-module"
  },"797": {
    "doc": "CluedIn Python SDK",
    "title": "Creating a Context",
    "content": "From API token . If you already have the API token, CluedIn Python SDK can build the Context for you: . # assuming your API token is in the CLUEDIN_TOKEN variable ctx = cluedin.Context.from_jwt(CLUEDIN_TOKEN) . Now, you can use the ctx Context variable to call CluedIn APIs. From an object . You can also create a Context from a Python dict: . # assumming you log in to https://foobar.contoso.com # \"foobar\" is org_name, and \"contoso.com\" is the domain context = { \"domain\": \"contoso.com\", \"org_name\": \"foobar\", \"access_token\": CLUEDIN_TOKEN } ctx = cluedin.Context.from_dict(context) . From a file . # cluedin.json # { # \"domain\": \"contoso.com\", # \"org_name\": \"foobar\", # \"access_token\": CLUEDIN_TOKEN # } ctx = cluedin.Context.from_json_file(\"cluedin.json\") . Getting API token from email and password . The API token lets you use some of CluedIn APIs like data ingestion endpoints and search, but to have full access to APIs (assuming you have the necessary permissions), you need to use the token that CluedIn gives you when you log in with your email and password. With the CluedIn Python SDK, you can set your email and password in the Context, and then call the get_token method to get the token: . ctx = cluedin.Context.from_dict({ \"domain\": \"contoso.com\", \"org_name\": \"foobar\", \"user_email\": \"joe@contoso.com\", \"user_password\": \"yourStrong(!)Password\" }) ctx.get_token() # after this line, `ctx.access_token` will be set . ",
    "url": "/playbooks/data-engineering-playbook/python-sdk#creating-a-context",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk#creating-a-context"
  },"798": {
    "doc": "CluedIn Python SDK",
    "title": "CluedIn Python SDK",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/python-sdk",
    
    "relUrl": "/playbooks/data-engineering-playbook/python-sdk"
  },"799": {
    "doc": "Local installation checklist",
    "title": "On this page",
    "content": ". | Get access to CluedIn container registry | Check hardware requirements | Check software requirements | Results | Next steps | . In this article, you will learn about the pre-installation processes that you must perform to ensure successful local installation of CluedIn. Local installation of CluedIn is available to any technical person upon acceptance of our terms and conditions. ",
    "url": "/deployment/local/step-1#on-this-page",
    
    "relUrl": "/deployment/local/step-1#on-this-page"
  },"800": {
    "doc": "Local installation checklist",
    "title": "Get access to CluedIn container registry",
    "content": "The first thing you need to do to run CluedIn locally is to get access to CluedIn Azure Container Registry (ACR). To get access to CluedIn ACR . | Fill in and submit the form. In the form, you need to specify the purposes for which you would like to use CluedIn: Production (deploying CluedIn in a live environment) and/or Testing &amp; development (exploring CluedIn or creating extensions for the product). You must have a valid production license to request production access. After you submit the form, you’ll receive an email with instructions to verify your email address. | In the email, select Verify. A new page opens in your default browser. | Select Confirm ‘Verify’. After you verify your email address, you’ll receive an email with credentials. You’ll need these credentials for authenticating to ACR. | . ",
    "url": "/deployment/local/step-1#get-access-to-cluedin-container-registry",
    
    "relUrl": "/deployment/local/step-1#get-access-to-cluedin-container-registry"
  },"801": {
    "doc": "Local installation checklist",
    "title": "Check hardware requirements",
    "content": "Running clusters locally requires a substantial amount of computational power in terms of both CPU and memory resources. Make sure that your computer meets the following hardware requirements: . | Modern CPU (Intel or AMD). We do not support ARM CPU. | 16 GB of free memory. We recommend having 32 GB of free memory. | . ",
    "url": "/deployment/local/step-1#check-hardware-requirements",
    
    "relUrl": "/deployment/local/step-1#check-hardware-requirements"
  },"802": {
    "doc": "Local installation checklist",
    "title": "Check software requirements",
    "content": "Make sure that you have installed the following software: . | Docker Desktop | Git | PowerShell 7 or later | . In addition, make sure that you have access to CluedIn container registry. ",
    "url": "/deployment/local/step-1#check-software-requirements",
    
    "relUrl": "/deployment/local/step-1#check-software-requirements"
  },"803": {
    "doc": "Local installation checklist",
    "title": "Results",
    "content": ". | You have access to CluedIn container registry. | Your computer meets hardware and software requirements for the local installation of CluedIn. | . ",
    "url": "/deployment/local/step-1#results",
    
    "relUrl": "/deployment/local/step-1#results"
  },"804": {
    "doc": "Local installation checklist",
    "title": "Next steps",
    "content": "Start the local installation of CluedIn as described in our Local installation guide. ",
    "url": "/deployment/local/step-1#next-steps",
    
    "relUrl": "/deployment/local/step-1#next-steps"
  },"805": {
    "doc": "Local installation checklist",
    "title": "Local installation checklist",
    "content": " ",
    "url": "/deployment/local/step-1",
    
    "relUrl": "/deployment/local/step-1"
  },"806": {
    "doc": "Rule types",
    "title": "On this page",
    "content": ". | Data parts rules | Survivorship rules | Golden record rules | Example of using rules | . There are three types of rules: data part rules, survivorship rules, and golden record rules. Each rule serves a distinct purpose in managing and optimizing your golden records. The following diagram shows the order in which the rules should be applied to the records in CluedIn. ",
    "url": "/management/rules/rule-types#on-this-page",
    
    "relUrl": "/management/rules/rule-types#on-this-page"
  },"807": {
    "doc": "Rule types",
    "title": "Data parts rules",
    "content": "The purpose of data parts rules is to modify the values in records that come from different sources. Data part rules are mostly used for normalization and transformation of values on the vocabulary key level. While you have the option to create data part rules manually, they can also be generated automatically from a clean project. The following video explains variuos actions available in the data part rules. ",
    "url": "/management/rules/rule-types#data-parts-rules",
    
    "relUrl": "/management/rules/rule-types#data-parts-rules"
  },"808": {
    "doc": "Rule types",
    "title": "Survivorship rules",
    "content": "The purpose of survivorship rules is to determine which value contributes to the golden record among many potential values. By default, the latest value coming to CluedIn is the value used in the golden record. If you want to use another value for your golden record, you can set up a survivorship rule with the needed action. The following video explains each action available in the survivorship rules. ",
    "url": "/management/rules/rule-types#survivorship-rules",
    
    "relUrl": "/management/rules/rule-types#survivorship-rules"
  },"809": {
    "doc": "Rule types",
    "title": "Golden record rules",
    "content": "The purpose of golden record rules is to facilitate the easy identification and retrieval of golden records within the system. You can use a golden record rule to tag records containing invalid data that should be addressed by a Data Steward. In this case, a responsible person can quickly find tagged records and create a clean project. The following video explains each action available in the golden record rules. ",
    "url": "/management/rules/rule-types#golden-record-rules",
    
    "relUrl": "/management/rules/rule-types#golden-record-rules"
  },"810": {
    "doc": "Rule types",
    "title": "Example of using rules",
    "content": "Let’s take a look at the example of using different rule types in the lifecycle of a golden record in CluedIn. Suppose you are receiving data about a company from various systems such as Salesforce, SAP, and MS SQL Server. Among other properties, the records from these systems include the countryCode property, which is used in the company golden record. However, the countryCode property contains inconsistent values—some country codes are in lower case and others in title case. To normalize these values, you can create a data part rule. After normalizing the values, it turns out that records from different systems contributing to a single golden record contain different countryCode values. To make an informed decision over which value should be used in the golden record, you can create a survivorship rule. For example, the records coming from Salesforce and SAP have the countryCode property set to ‘DK’, while the records coming from MS SQL Server have the countryCode property set to ‘FI’. The survivorship rule’s action can select the winning value based on criteria such as the latest modified value, the record with the most properties, or a preference for a specific provider. In this case, the rule’s action identifies the most frequent value and applies it to the golden record (the chosen value is ‘DK’). To make it easier to find a golden record, you can add tag to it. As an example, let’s add a tag “Nordic” to the golden record with the countryCode property set to ‘DK’. This simplifies the process of locating the tagged record and adding it to the clean project, glossary, or stream. ",
    "url": "/management/rules/rule-types#example-of-using-rules",
    
    "relUrl": "/management/rules/rule-types#example-of-using-rules"
  },"811": {
    "doc": "Rule types",
    "title": "Rule types",
    "content": " ",
    "url": "/management/rules/rule-types",
    
    "relUrl": "/management/rules/rule-types"
  },"812": {
    "doc": "Rules",
    "title": "Rules",
    "content": "Rules enable you to establish business logic for managing your records within CluedIn. In this section, you will learn how to automate data transformation, capture data quality issues, and determine operational values with the help of different types of rules. A rule consists of a filter and one or more associated actions. Filter tells CluedIn what records should be affected by the rule’s action. The following diagram shows the basic steps of creating a rule in CluedIn. This section covers the following areas: . | Rule types – explore different types of rules to learn which one would be best for your specific needs. | Creating a rule – learn how to create a rule and apply it to the records. | Managing rules – learn how to edit, deactivate, and delete a rule, as well how these actions affect your records. | Reference information about rule’s actions – find information about the actions associated with each type of rule. | . ",
    "url": "/management/rules",
    
    "relUrl": "/management/rules"
  },"813": {
    "doc": "Walkthrough: Install CluedIn on Azure",
    "title": "On this page",
    "content": ". | Introduction | Login to Azure | Create a resource group for your Kubernetes Cluster | Create an AKS Cluster to host your CluedIn application | Networking &amp; Security | Install CluedIn using Helm | Next Steps | . Introduction . This installation process is done through command lines using PowerShell 7, Azure CLI, Kubectl and Helm. The purpose of the chart is to install the CluedIn application. This includes the actual CluedIn server, website, and other services required (storage, queues, etc.) . Note: Before proceeding with the installation, you must ensure that all pre-requisites are met. Login to Azure . Open a PowerShell session, and run the following command: . az login . A browser window will open with Microsoft Azure login page: . Choose the account you want to use, enter the appropriate MFA information. Once you are connected, you will get this confirmation message . You can now read, create and access resources in your Azure tenant through Azure CLI. Create a resource group for your Kubernetes Cluster . In connected PowerShell session, run the following command: . $location = \"westeurope\" # location is the deployment region of the resource group. Example: eastus, centralus, westeurope etc... $rgName = \"rg-cluedin-dev\" # rgName is the name of the resource group. Example: rg-cluedin-dev or rg-cluedin-prod $subscription = \"DataOps-Subscription\" # subscription is the name of the parent subscription for the resource group az group create --location $location --name $rgName --subscription $subscription . Create an AKS Cluster to host your CluedIn application . In this step, you will create an AKS cluster with the following nodepool sizing: . | Node Pool | VM SKU Type | Amount | Purpose | . | Core Pool | Standard_DS2_v2 | 1 | Kubernetes agent internal processes | . | Data Pool | Standard_A8_v2 | 2 | Memory Optimized pool for Databases | . | Processing Pool | Standard_F16s_v2 | 1 | CPU Optimized for Processing workloads | . | General Pool | Standard_F4s_v2 | 2 | General Purpose nodepool to house CluedIn Microservices | . NB: Later, you can choose to downscale or upscale your nodepools depending on your needs and your workloads. | Save this ARM Template and ARM Template Parameters files to your C:\\Users\\$env:username\\AzureTools folder. | Open the create-cluster-params.json file and edit the values of the following parameters: . | resourceName: choose a name for your AKS cluster, for example: aks-cluedin-dev, aks-cluedin-test, aks-cluedin-prod etc… | location: choose the deployment region, for example: uksouth, eastus, westeurope etc… | dnsPrefix: DNS prefix for your cluster, for example: aks-cluedin-dev-dns, aks-cluedin-test-dns etc… | kubernetesVersion: 1.20.9 or later | networkPlugin: can be “kubenet” or “azure”. If you choose azure, The provided template and parameters might not be suitable for the remainder of the install, please refer to Azure documentation for more details. For the remainder of the installation, we are using Kubenet. | . | Once you finish editing the create-cluster-params.json, save and close it. | Run the following script to create your AKS cluster with the predefined parameters: $rgName = \"resource-group-name\" # example: \"rg-cluedin-dev\" $deploymentName = \"name-of-deployment\" # example: \"aks-cluedin-dev-deployment\" $armTemplatePath = \"C:\\Users\\$env:username\\AzureTools\\create-cluster-template.json\" # ARM Template that you saved previously. $paramsPath = \"C:\\Users\\$env:username\\AzureTools\\create-cluster-params.json\" # Params file that you saved and edited previously. az deployment group create ` --name $deploymentName ` --resource-group $rgName ` --template-file $armTemplatePath ` --parameters $paramsPath . | Once the cluster is successfully created, run the following command to check cluster’s information: $rgName = \"resource group name\" # for example: rg-cluedin-dev $clusterName = \"cluster name\" # for example: aks-cluedin-dev az aks show --name $clusterName --resource-group $rgName . | . Now, you are ready to install CluedIn on your cluster. Networking &amp; Security . | Merge your AKS context to your created cluster using the following command: . $rgName = \"resource group name\" # for example: rg-cluedin-dev $clusterName = \"cluster name\" # for example: aks-cluedin-dev az aks get-credentials --resource-group $rgName --name $clusterName . | Install HAProxy ingress controller using Helm: . | Navigate to the AzureTools folder and create the namespace that will host the CluedIn components: kubectl create namespace cluedin helm repo add haproxy-ingress https://haproxy-ingress.github.io/charts helm install haproxy-ingress haproxy-ingress/haproxy-ingress --namespace=cluedin . | Run the following command to retrieve the External IP Address assigned to the controller: kubectl get services -n cluedin . If the external IP shows as “pending”, give it a moment before trying again, then save the External IP address from the output. You will need it for your DNS configuration. | . | DNS Routing (If applicable): Through your DNS provider’s management system, make your host point to the public IP of the ingress controller for the following routes: . | app.&lt;hostname&gt;, for example: app.cluedin-dev.companyName.com | &lt;organizationName&gt;.&lt;hostname&gt;, for example: product.cluedin-dev.companyName.com | . | SSL Certificate (If applicable): Create a secret with the SSL certificates for the following routes: . | Option 1: specific subdomains . | app.&lt;hostname&gt;, for example: app.cluedin-dev.cluedin.com | &lt;organizationName&gt;.&lt;hostname&gt;, for example product.cluedin-dev.companyName.com | clean.&lt;hostname&gt;, for example clean.cluedin-dev.companyName.com | Admin URLs: . | grafana.&lt;hostname&gt;, for example grafana.cluedin-dev.companyName.com | promotheus.&lt;hostname&gt;, for example promotheus.cluedin-dev.companyName.com | alertmanager.&lt;hostname&gt;, for example alertmanager.cluedin-dev.companyName.com | seq.&lt;hostname&gt;, for example seq.cluedin-dev.companyName.com | . | . | Option 2: Wildcard certificate . | If possible, the SSL Certificate can be a wildcard one, for example: *.cluedin-dev.companyName.com | . | The certificate secret can be created using the following command: kubectl create secret tls &lt;secret-name&gt;--key &lt;private-key-path&gt; --cert &lt;public-certificate-path&gt; . NB: private-key-path = path to the tls.key file and public-certificate-path = path to tls.crt file. There is an option to run without SSL, although not recommended, especially for production environments. | . | . Install CluedIn using Helm . | Start by registering the Helm chart repository containing the CluedIn chart: helm repo add cluedin https://cluedin-io.github.io/Charts/ helm repo update . | Create a secret with your Docker credentials to access CluedIn images through Docker Hub: Run the following command: kubectl create secret docker-registry docker-registry-key ` --namespace cluedin ` --docker-server='docker.io' ` --docker-username='&lt;your Dockerhub username&gt;' ` --docker-password='&lt;your Dockerhub password&gt;' ` --docker-email='&lt;your Docker Hub email&gt;' . A confirmation message will appear when the secret is created. | Make sure you are in your AzureTools folder, then using the command below, you will fetch the values.yml configuration file to configure your CluedIn instance helm inspect values cluedin/cluedin &gt; values.yml . The file will be downloaded to your AzureTools folder. | . Fill out the values.yml file, specifically the following sections: . Default Organization and application Admin . bootstrap: organization: name: \"orgName\" # Organization Account Name, example: Products, Customers, People, Vendors etc... email: \"admin@companyName.com\" # Admin account's Email username: \"admin@companyName.com \"# Admin account's username (should be the same as above) prefix: \"orgNamePrefix\" # Organization prefix used in the URL to access this organization (tenant), can be the same as organizationName password: \"!!!StrongPassword123!!!\" # Admin account's password emailDomain: \"companyName.com\" # Admin account's Email domain, can be left empty and admin email's domain will be used in this case. Please note that the organization name cannot contain hyphens or dots in it, and that organization prefix can have hyphens, but not dots. DNS Configuration . If your hostname is companyName.com and you choose a prefix to your CluedIn dev application that is cluedin-dev, your DNS section should look like the following: . dns: hostname: \"companyName.com\" # Base host to use for all URLs. Will be combined with the organization name e.g https://&lt;org-name&gt;.&lt;hostname&gt; prefix: \"cluedin-dev\" # Usually a qualifier to the CluedIn instance, but can be left empty subdomains: # The following are the default, each one corresponds to a subdomain application: \"app\" # For app.cluedin-dev.companyName.com openrefine: \"clean\" # For clean.cluedin-dev.companyName.com grafanaAdmin: \"grafana\" # For grafana.cluedin-dev.companyName.com prometheusAdmin: \"prometheus\" # For prometheus.cluedin-dev.companyName.com alertManagerAdmin: \"alertmanager\" # For alertmanager.cluedin-dev.companyName.com seq: \"seq\" # For seq.cluedin-dev.companyName.com . Please note that the above is also equivalent to: . dns: hostname: \"cluedin-dev.companyName.com\" # The prefix is added to the hostname prefix: \"\" # No further prefix defined subdomains: application: \"app\" openrefine: \"clean\" grafanaAdmin: \"grafana\" prometheusAdmin: \"prometheus\" alertManagerAdmin: \"alertmanager\" seq: \"seq\" . Also, all the subdomains values can be overridden by values that you prefer, like in the following example: . dns: hostname: \"companyName.com\" # Main company's hostname is used prefix: \"\" # No prefix used subdomains: application: \"app-cluedin-dev\" openrefine: \"clean-cluedin-dev\" grafanaAdmin: \"grafana-cluedin-dev\" prometheusAdmin: \"prometheus-cluedin-dev\" alertManagerAdmin: \"alertmanager-cluedin-dev\" seq: \"seq-cluedin-dev\" . In this case, the app URL will be: app-cluedin-dev.companyName.com . Finally, if you choose to use the Ingress controller’s external IP with no specific hostname until you get your DNS configuration sorted out, the DNS section would be: . dns: hostname: \"EXTERNAL_IP_VALUE.nip.io\" # For example 20.90.172.127.nip.io prefix: \"\" . SSL and HTTPS (If applicable) . If you are configuring HTTPS too, modify the following section as per your needs: . ingress: forceHttps: True # Set to True if you want to force HTTPS usage annotations: kubernetes.io/ingress.class: haproxy ingress.kubernetes.io/ssl-redirect: \"false\" tls: hosts: [ \"*.cluedin-dev.companyName.com\" ] # enumerate the hosts defined in the certificate, or wildcard URL. secretName: \"SSL-Secret-Name\" # TLS Secret to use for ingress - If no hosts create a wildcard host hasClusterCA: false # If certificates are generated by a local CA (secret has 'ca.crt' section) . Save the values.yml file. Please note that these different settings can be modified later even after the installation of CluedIn. Finally, install Cluedin . Run the following command to install CluedIn: . $releaseName = \"choose a name for the release\" # Example: cluedin-dev $pathToValuesYml = \"Path to values.yml\" # Example: C:\\Users\\$env:username\\AzureTools\\values.yml helm upgrade $releaseName cluedin/cluedin -n cluedin --install --values $pathToValuesYml . Upon running the helm upgrade command, Helm will begin installation of CluedIn platform into your Kubernetes cluster. At the end of the installation process, you will be prompted with the configuration of your install, URLs you can use to access your freshly installed platform. All the workloads may take up to 10 minutes to spin up. You can check your status by running: . kubectl get pods -n cluedin . In a healthy installation scenario, all the pods should be in a Ready state. Additionally, you can check the platform’s health by going to https://app.&lt;hostname&gt;/api/status healthcheck API. You will be able to login to the platform by going to https://app.&lt;hostname&gt;/ (or http://app.&lt;hostname&gt;/ if not using SSL). Next Steps . After logging in to the platform, you can proceed with enabling single sign on for your users to access the platform, as well as start loading data in via Data Sources or installing some crawlers. Below you will find some useful links on achieving the above: . | Enabling Single Sign On | Restricting access to CluedIn Clean via Basic Authentication | Install a crawler/custom component | . Optionally, you can also adjust other settings to cater for more complex scenarios: . | Persistence/Using Managed Disks | Azure SQL Server | Scaling | . ",
    "url": "/deployment/azure/setup/walkthrough#on-this-page",
    
    "relUrl": "/deployment/azure/setup/walkthrough#on-this-page"
  },"814": {
    "doc": "Walkthrough: Install CluedIn on Azure",
    "title": "Walkthrough: Install CluedIn on Azure",
    "content": " ",
    "url": "/deployment/azure/setup/walkthrough",
    
    "relUrl": "/deployment/azure/setup/walkthrough"
  },"815": {
    "doc": "User management",
    "title": "On this page",
    "content": ". | Add user . | Add user by email address | Add user via SSO | . | Deactivate user | User management reference | . In this article, you will learn how to add and deactivate users in your CluedIn organization. This article is intended for users with the OrganizationAdmin role or users with the following claim access level. | Section | Claim | Access level | . | Admin | Users | At least Consulted | . A user is someone who can sign in to CluedIn. The user’s access to CluedIn modules and features is defined by the user’s roles. For more information, see Roles. You can perform the following actions to manage users: . | Add user | Deactivate user | . ",
    "url": "/administration/user-management#on-this-page",
    
    "relUrl": "/administration/user-management#on-this-page"
  },"816": {
    "doc": "User management",
    "title": "Add user",
    "content": "Users can be added to CluedIn in one of the following ways: . | By the user’s email address | Via Azure Active Directory single sign-on (SSO) | . Add user by email address . If you don’t have Azure Active Directory SSO enabled for CluedIn, you can add users to the platform by their email addresses. The following diagram shows the flow of adding users to CluedIn. To add a user by email address . | On the navigation pane, go to Administration &gt; User Management. | On the actions pane, select Users. Then, select Invite User. | On the Invite User pane, enter the user’s email address. Then, in the lower-right corner, select Invite. The user will receive an email with a link to activate their account. After the user follows the link and creates a password, you will see the user on the All users page. By default, the user’s role is OrganizationUser. It means that the user can view all sections and modules in a read-only mode. To give the user access to more features within the platform, add another role to the user. | . Add user via SSO . If you have Azure Active Directory SSO enabled for CluedIn, the users will be able to sign in using SSO. For more information, see Configure SSO. Because the users are part of your SSO domain, they do not need an invite. They can log in directly using their organization credentials. Use invites only for CluedIn non-SSO users. ",
    "url": "/administration/user-management#add-user",
    
    "relUrl": "/administration/user-management#add-user"
  },"817": {
    "doc": "User management",
    "title": "Deactivate user",
    "content": "If you want to prevent the user from signing in to CluedIn, deactivate the user. For SSO users, you can revoke user access in Azure Active Directory. For users added to CluedIn by their email address, you can deactivate them in CluedIn. The following diagram shows the flow of deactivating a user in CluedIn. To deactivate a user in CluedIn . | On the navigation pane, go to Administration &gt; User Management. | On the actions pane, select Users. | On the All users page, find the user whom you want to deactivate. | In the Active column, turn off the status toggle. Alternatively, you can deactivate the user from the user details page. To do that, select the user and on the Settings tab, turn off the status toggle. The status of the user is changed to Blocked. The user won’t be able to sign in to CluedIn. | . ",
    "url": "/administration/user-management#deactivate-user",
    
    "relUrl": "/administration/user-management#deactivate-user"
  },"818": {
    "doc": "User management",
    "title": "User management reference",
    "content": "Whenever a new user is added to CluedIn or changes are made to the user details page, they are recorded and can be found on the Audit Log tab. These actions include the following: . | Create a user | Add a user to a role | Remove a user from a role | Activate a user | Deactivate a user | Delete user invite | Complete registration | . ",
    "url": "/administration/user-management#user-management-reference",
    
    "relUrl": "/administration/user-management#user-management-reference"
  },"819": {
    "doc": "User management",
    "title": "User management",
    "content": " ",
    "url": "/administration/user-management",
    
    "relUrl": "/administration/user-management"
  },"820": {
    "doc": "Add an enricher",
    "title": "On this page",
    "content": ". | Add and configure an enricher | Trigger enrichment | View enrichment results | . In this article, you will learn how to add and configure an enricher. While CluedIn offers a range of predefined enrichers for your use, you also have the option to create custom enrichers tailored to your specific needs. ",
    "url": "/preparation/enricher/add-enricher#on-this-page",
    
    "relUrl": "/preparation/enricher/add-enricher#on-this-page"
  },"821": {
    "doc": "Add an enricher",
    "title": "Add and configure an enricher",
    "content": "You can add an enricher both before and after processing the data. To add and configure an enricher . | On the navigation pane, go to Preparation &gt; Enrich. | Select Add Enricher. | On the Choose Enricher tab, select the needed enricher, and then select Next. | On the Configure tab, enter the authentication details. The list of fields that you need to complete depends on the selected enricher. You can complete the fields now or do it later on the enricher details page. | Select Save. The enricher details page opens, where you can view and change configuration information and other settings. The enricher is active, which means that the enrichment process will begin immediately after you start processing. If you have already processed the data before adding an enricher, you can trigger the enrichment for the existing golden records. | . On the enricher details page, you can manage the enricher: . | Change the display name of the enricher. | Select the source of data coming through the enricher. | Specify the quality of data coming through the enricher. | Inactivate the enricher if you no longer need it. | Manage users who have access to the data coming through the enricher. For more details, see Data access. | . ",
    "url": "/preparation/enricher/add-enricher#add-and-configure-an-enricher",
    
    "relUrl": "/preparation/enricher/add-enricher#add-and-configure-an-enricher"
  },"822": {
    "doc": "Add an enricher",
    "title": "Trigger enrichment",
    "content": "If you process the data and then add an enricher, the enrichment won’t start automatically. To enrich the existing golden records, you can trigger enrichment in one of the following ways: . | Using the GraphQL tool. | Manually trigger enrichment for each golden record. | . To trigger enrichment using the GraphQL tool . | On the navigation pane, go to Consume &gt; GraphQL. | Enter a query to enrich all golden records that belong to a certain business domain. Replace /Organization with the needed name of business domain. { search(query: \"entityType:/Organization\") { entries { actions { enrich } } } } . | Execute the query. You triggered the enrichment for the golden records belonging to the specified business domain. Now, you can view the enrichment results on the golden record details page. | . To trigger enrichment for each golden record manually . | Find and open the needed golden record. | In the upper-right corner of the golden record details page, select More &gt; Trigger external enrichment. Now, you can view the enrichment results on the golden record details page. | . ",
    "url": "/preparation/enricher/add-enricher#trigger-enrichment",
    
    "relUrl": "/preparation/enricher/add-enricher#trigger-enrichment"
  },"823": {
    "doc": "Add an enricher",
    "title": "View enrichment results",
    "content": "The Properties tab of the golden record lists new properties added through enrichment. The source of these properties is specified based on the type of enricher through which they were obtained. According to the data life cycle, properties added through enrichment appear in CluedIn as a clue. This clue is then transferred to the data part, which in turn becomes the constituent element of the golden record. The History tab of the golden record contains all the data parts that make up that golden record, including the data parts that came from the enricher. For more information, see History. ",
    "url": "/preparation/enricher/add-enricher#view-enrichment-results",
    
    "relUrl": "/preparation/enricher/add-enricher#view-enrichment-results"
  },"824": {
    "doc": "Add an enricher",
    "title": "Add an enricher",
    "content": " ",
    "url": "/preparation/enricher/add-enricher",
    
    "relUrl": "/preparation/enricher/add-enricher"
  },"825": {
    "doc": "Add GraphQL entity type resolvers",
    "title": "Add GraphQL entity type resolvers",
    "content": "You can add your own specific resolvers to fetch data given filters such as what business domain a record is. Here is an example of how to return Calendar Events in a different way through the GraphQL endpoints. using System.Linq; using System.Threading.Tasks; using CluedIn.Core.Data; using CluedIn.Core.GraphQL.Builders; using GraphQL.Types; namespace Custom.GraphQL.Types.Specific { /// &lt;summary&gt;The entity graph type.&lt;/summary&gt; /// &lt;seealso cref=\"Entity\" /&gt; /// &lt;seealso cref=\"IEntity\" /&gt; public class CalendarEventEntityGraphType : ObjectGraphType&lt;CalendarEventEntity&gt;, IComplexGraphType&lt;CalendarEventEntity&gt; { public CalendarEventEntityGraphType(ICluedInData data) { this.Name = \"Calendar_Event_Entity\"; this.Field&lt;ListGraphType&lt;EntityInterface&gt;&gt;() .Name(\"attendees\") .Resolve(ctx =&gt; ctx.GetDataLoader(async ids =&gt; { // var ast = ctx.FieldAst; // ast.SelectionSet.Selections; var authors = data.GetEdgesOfType(ids, EntityEdgeType.Attended); var lookup = authors.SelectMany(f =&gt; f.Endpoints.Select(ff =&gt; new { Key = f.ContextEntityId, Endpoint = ff })) .ToLookup(x =&gt; x.Key, x =&gt; TypedEntityConverter.CreateSpecificType(x.Endpoint)); return await Task.FromResult(lookup); }).LoadAsync(ctx.Source.Id)); EntityInterface.ConfigureInterface(this, data); this.ConfigureFields(CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInEvent); this.Interface&lt;EntityInterface&gt;(); } } } . Here is a more complex example: . using System.Linq; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.GraphQL.Builders; using CluedIn.Core.GraphQL.Types.Specific; using GraphQL.Types; namespace Custom.GraphQL.Types.Specific { /// &lt;summary&gt;The entity graph type.&lt;/summary&gt; /// &lt;seealso cref=\"Entity\" /&gt; /// &lt;seealso cref=\"IEntity\" /&gt; public class FilesFileEntityGraphType : ObjectGraphType&lt;FilesFileEntity&gt;, IComplexGraphType&lt;FilesFileEntity&gt; { public FilesFileEntityGraphType(ICluedInData data) { this.Name = \"Files_File_Entity\"; this.Field&lt;ListGraphType&lt;EntityInterface&gt;&gt;() .Name(\"attendees\") .Resolve(ctx =&gt; ctx.GetDataLoader(async ids =&gt; { var ast = ctx.FieldAst; ast.SelectionSet.Selections; var authors = data.GetEdgesOfType(ids, EntityEdgeType.Attended); var lookup = authors.SelectMany(f =&gt; f.Endpoints.Select(ff =&gt; new { Key = f.ContextEntityId, Endpoint = ff })) .ToLookup(x =&gt; x.Key, x =&gt; x.Endpoint); return lookup; }).LoadAsync(ctx.Source.Id)); EntityInterface.ConfigureInterface(this, data); this.ConfigureFields(CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInFile); this.Interface&lt;EntityInterface&gt;(); } } } . ",
    "url": "/consume/graphql/add-graphql-entity-type-resolvers",
    
    "relUrl": "/consume/graphql/add-graphql-entity-type-resolvers"
  },"826": {
    "doc": "Add records in a manual data entry project",
    "title": "On this page",
    "content": ". | Add single record . | Add a record in the manual data entry project | Add a record from the action center | . | Add multiple records | . In this article, you will learn how to add the records manually directly in CluedIn. Once you have created a manual data entry project and added all the form fields you need, you can start adding the records manually. There are two options for adding the records manually: . | Adding single records – use this option if you want to add the records individually one by one. Right after you fill out all the required fields for a record, you can generate the record and publish it to CluedIn. | Adding multiple records – use this option if you want to add the records in one session in a tabular view. You can add as many records as you need in one session, and then generate all records and publish them to CluedIn simultaneously. | . ",
    "url": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#on-this-page",
    
    "relUrl": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#on-this-page"
  },"827": {
    "doc": "Add records in a manual data entry project",
    "title": "Add single record",
    "content": "The process of adding a single record consists of filling out all the required fields and then generating the record. You can add records one at a time. There are two ways to add a record: . | In the manual data entry project | From the action center | . Add a record in the manual data entry project . | On the navigation pane, go to Ingestion &gt; Manual Data Entry. | Find and open the project where you want to add a record. | In the upper-right corner of the project page, select Add &gt; Add single record. | Fill out the fields for a record. | In the upper-right corner of the page, select Generate. After the record is processed, you can find it on the Data tab of the manual data entry project. To add more records, repeat steps 3–5. | . Add a record from the action center . | On the navigation pane, select Create. | Select Enter data manually. The Manual data entry pane opens to the right side of the page. | In the Name dropdown list, find and select the manual data entry project where you want to add a record. After selecting a project, the relevant form fields will appear. | Fill out the fields for a record. | Select Add record. After the record is processed, you can find it on the Data tab of the manual data entry project you selected in step 3. To add more records, repeat steps 2–4. | . ",
    "url": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#add-single-record",
    
    "relUrl": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#add-single-record"
  },"828": {
    "doc": "Add records in a manual data entry project",
    "title": "Add multiple records",
    "content": "The process of adding multiple records requires creating a session, which represents a workspace with a tabular view. There, you can fill out the fields for all records and then generate the records simultaneously. You can add multiple records in the tabular view. Prerequisites . Go to Administration &gt; Feature Flags, and then enable the Manual data entry tabular view feature. To add multiple records . | In the upper-right corner of the manual data entry project page, select Add &gt; Add multiple records. | Enter the name of the session, and the select Create. A manual data entry session opens, where you can enter the data manually in a tabular format. One row represents one record, and the columns represent the form fields. To add more rows, select Add row. | Enter the data in the fields. If you decide that you no longer need the record, you can delete it by selecting the delete icon in the Actions column. If you want to take a break from entering records, you can close the session and return to it later. Any records you have entered will be saved. | After you have entered all records, send them for processing: . | Select Generate. | If you want to remove the session once the records have been processed, select the Cleanup session checkbox. If you do not select this checkbox, the session will remain on the Sessions tab. | Select Confirm. | . Once processed, the records appear on the Data tab of the manual data entry project. | . ",
    "url": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#add-multiple-records",
    
    "relUrl": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project#add-multiple-records"
  },"829": {
    "doc": "Add records in a manual data entry project",
    "title": "Add records in a manual data entry project",
    "content": " ",
    "url": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/add-records-in-a-manual-data-entry-project"
  },"830": {
    "doc": "Azure Data Lake connector",
    "title": "Azure Data Lake connector",
    "content": "This article outlines how to configure the Azure Data Lake connector to publish data from CluedIn to Azure Data Lake Storage Gen2. Prerequisites: Make sure you use a service principal to authenticate and access Azure Data Lake. To configure Azure Data Lake connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Azure Data Lake Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | AccountName – name of the Azure Data Lake storage account where you want to store the data from CluedIn. | AccountKey – access key for authenticating requests to the Azure Data Lake storage account. | FileSystemName – name of a container in Azure Data Lake. | DirectoryName – name of a directory inside the container in Azure Data Lake. | Enable Stream Cache (Sync mode only) – when stream cache is enabled, CluedIn caches the golden records at intervals, and then writes out accumulated data to one file (JSON, Parquet, or CSV). When stream cache is not enabled, CluedIn streams out golden records one by one, each in a separate JSON file. Stream cache is available only for the synchronized stream mode. | Output Format – file format for the exported data. You can choose between JSON, Parquet, and CSV. However, Parquet and CSV formats are available only if you enable stream cache. If stream cache is not enabled, JSON is the default format. | Export Schedule – schedule for sending the files from CluedIn to the export target. The files will be exported based on Coordinated Universal Time (UTC), which has an offset of 00:00. You can choose between the following options: . | Hourly – files will be exported every hour (for example, at 1:00 AM, at 2:00 AM, and so on). | Daily – files will be exported every day at 12:00 AM. | Weekly – files will be exported every Monday at 12:00 AM. | Custom Cron – you can create a specific schedule for exporting files by entering the cron expression in the Custom Cron field. For example, the cron expression 0 18 * * * means that the files will be exported every day at 6:00 PM. | . | (Optional) File Name Pattern – a file name pattern for the export file. For more information, see File name patterns. For example, in the {ContainerName}.{OutputFormat} pattern, {ContainerName} is the Target Name in the stream, and {OutputFormat} is the output format that you select in step 3g. In this case, every time the scheduled export occurs, it will generate the same file name, replacing the previously exported file. If you do not specify the file name pattern, CluedIn will use the default file name pattern: {StreamId:D}_{DataTime:yyyyMMddHHmmss}.{OutputFormat}. | (Optional, for Parquet output format only) Replace Non-Alphanumeric Characters in Column Names – enable this option if you plan to access the output file in Microsoft Purview. When this option is enabled, CluedIn replaces non-alphanumeric characters in the column names (those not in a-z, A-Z, 0-9, and underscore) with the underscore character ( _ ). | (Optional, for Parquet output format only) Write Guid as String – enable this option if you plan to access the output file in Microsoft Purview. When this option is enabled, CluedIn writes GUID values as string instead of a byte array. | . | Test the connection to make sure it works, and then select Add. Now, you can select the Azure Data Lake connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/adl-connector",
    
    "relUrl": "/consume/export-targets/adl-connector"
  },"831": {
    "doc": "Business decisions",
    "title": "On this page",
    "content": ". | Choose a license . | Pay-as-you-go license | Committed-deal license | . | Choose a plan | Get familiar with billing | Review terms and conditions | Choose Azure Administrator | Results | Next steps | . In this article, you will learn about the decisions you need to make before starting the CluedIn PaaS installation process. ",
    "url": "/deployment/azure-marketplace/step-1#on-this-page",
    
    "relUrl": "/deployment/azure-marketplace/step-1#on-this-page"
  },"832": {
    "doc": "Business decisions",
    "title": "Choose a license",
    "content": "We offer two types of licenses: pay-as-you-go and committed-deal. |   | Pay-as-you-go | Committed-deal | . | All CluedIn features | Yes | Yes | . | Billing | Per hour | Upfront | . | Discount on billing | No | Yes | . | CluedIn support | No | Yes | . | Welcome package | No | Yes | . Pay-as-you-go license . If you choose the pay-as-you go license, you can install CluedIn PaaS directly from the Azure Marketplace and start using it without the need to contact anyone at CluedIn. Check out our training materials to get started. With this type of license, you do not get a fixed price for using CluedIn and you will be charged per hour of use. Learn more about the pay-as-you-go pricing model here. Committed-deal license . If you choose the committed-deal license, contact your Account Executive or our sales team to discuss the contract. We’ll make sure you get the best deal to get started with CluedIn. After you sign the contract, we’ll send you the license key by email. The committed-deal license comes with such benefits as: . | Welcome package that includes real-time training where we explain how to run a CluedIn project. | Fixed price for the license. | Enterprise 24/7 support in accordance with our enterprise support agreement and service level agreements (SLAs). | Managed service from CluedIn for the client’s CluedIn application infrastructure. | . In addition, you will automatically get 2 free CluedIn licenses for development and test environments. We recommend using three different environments: . | Development – for your development team or partner customizing your CluedIn PaaS application. | Test – for your business users to validate the changes made by the development team. | Production – for real data and daily operations. | . The license type is required during the installation of CluedIn PaaS, so be sure to tell your Azure Administrator which license type you want. ",
    "url": "/deployment/azure-marketplace/step-1#choose-a-license",
    
    "relUrl": "/deployment/azure-marketplace/step-1#choose-a-license"
  },"833": {
    "doc": "Business decisions",
    "title": "Choose a plan",
    "content": "This section applies only to the committed deal license. We offer three types of plans: Essential, Professional, and Elite. Before choosing a plan, consider how many records you want to process. The more records you want to process, the more CPU cores you will need. Review the details of each plan to choose the one that is right for you. |   | Essential | Professional | Elite | . | All licenses |   |   |   | . | CluedIn features | All | All | All | . | CPU cores | 8 | 16 | 32 | . | Records | Up to 500,000 | Up to 2,000,000 | Up to 5,000,000 | . | Maintenance (for AMA installation) | Yes | Yes | Yes | . | Maintenance (for Vanilla installation) | No | No | No | . | Committed-deal license |   |   |   | . | CluedIn support | Yes | Yes | Yes | . The plan is required during the installation of CluedIn PaaS, so be sure to tell your Azure Administrator which plan you want. Exceeding the maximum number of records? . If you exceed the maximum number of records allowed for your plan, you will be charged an additional fee. We encourage you to contact us directly to check if there is a more efficient plan for you. ",
    "url": "/deployment/azure-marketplace/step-1#choose-a-plan",
    
    "relUrl": "/deployment/azure-marketplace/step-1#choose-a-plan"
  },"834": {
    "doc": "Business decisions",
    "title": "Get familiar with billing",
    "content": "The invoice for using CluedIn PaaS will be based on the following: . | Type of license | Number of licenses | Plan | . With the pay-as-you-go license, you’ll be charged per hour of using CluedIn. With the committed-deal license, you get a fixed price for using CluedIn that you pay upfront. Your CluedIn instance sends us information about your license and the number of records that were processed. We need to verify that you have a valid license and that the number of records is within your plan. If you exceed the number of records, you’ll be charged an additional fee. We send the billing information to the Azure Marketplace. Every month, you’ll get an Azure invoice that will include: . | CluedIn usage – which you discuss directly with CluedIn. | Azure hosting – which covers all your Azure services. | . To pay an invoice, contact your procurement team. ",
    "url": "/deployment/azure-marketplace/step-1#get-familiar-with-billing",
    
    "relUrl": "/deployment/azure-marketplace/step-1#get-familiar-with-billing"
  },"835": {
    "doc": "Business decisions",
    "title": "Review terms and conditions",
    "content": "For CluedIn, we use the Standard Contract from Microsoft. Please note that if you are a Microsoft user, your legal department has probably already approved this Standard Contract. To make sure that the terms and conditions are appropriate for you, contact your legal team. ",
    "url": "/deployment/azure-marketplace/step-1#review-terms-and-conditions",
    
    "relUrl": "/deployment/azure-marketplace/step-1#review-terms-and-conditions"
  },"836": {
    "doc": "Business decisions",
    "title": "Choose Azure Administrator",
    "content": "Now that you have decided on your license and plan and reviewed our billing and legal information, you are ready to start the CluedIn PaaS installation process. The CluedIn PaaS installation process must be performed by an IT professional who is skilled in managing your organization’s Microsoft Azure environment. So, make sure that you choose a dedicated Azure Administrator from your organization who will be responsible for the installation. ",
    "url": "/deployment/azure-marketplace/step-1#choose-azure-administrator",
    
    "relUrl": "/deployment/azure-marketplace/step-1#choose-azure-administrator"
  },"837": {
    "doc": "Business decisions",
    "title": "Results",
    "content": ". | You have decided which license you need. If you have chosen the committed-deal license, then you should have a license key issued to you by CluedIn. | (For committed deal only) You have chosen a plan based on the number of records that you want to process. | You understand how CluedIn will issue invoices. | You are comfortable with our Standard Contract from Microsoft. | You have chosen an Azure Administrator who will be responsible for carrying out the CluedIn installation process. | . ",
    "url": "/deployment/azure-marketplace/step-1#results",
    
    "relUrl": "/deployment/azure-marketplace/step-1#results"
  },"838": {
    "doc": "Business decisions",
    "title": "Next steps",
    "content": "Provide your Azure Administrator with the following information: . | If you have signed a committed deal: . | Email with the license key issued by CluedIn. | Link to our Pre-installation checklist. | . | If you have decided to use the pay-as-you-go approach: . | Link to our Pre-installation checklist. | . | . ",
    "url": "/deployment/azure-marketplace/step-1#next-steps",
    
    "relUrl": "/deployment/azure-marketplace/step-1#next-steps"
  },"839": {
    "doc": "Business decisions",
    "title": "Business decisions",
    "content": " ",
    "url": "/deployment/azure-marketplace/step-1",
    
    "relUrl": "/deployment/azure-marketplace/step-1"
  },"840": {
    "doc": "Backup runbook",
    "title": "On this page",
    "content": ". | Typical persistent disks | Automation account | Snapshots | Input parameters | Process | Permissions | . The backup runbook can be triggered manually or on a set schedule. It is responsible for capturing snapshots of all persistent disks used by CluedIn. This runbook is a PowerShell script, which CluedIn will provide as needed. Prerequisites . | An active CluedIn cluster with a valid license | The runbook script | An automation account | A storage account | Sufficient permissions | . ",
    "url": "/paas-operations/automation/backup-runbook#on-this-page",
    
    "relUrl": "/paas-operations/automation/backup-runbook#on-this-page"
  },"841": {
    "doc": "Backup runbook",
    "title": "Typical persistent disks",
    "content": "CluedIn configurations may vary, but a typical instance includes the nine disks as shown below. ",
    "url": "/paas-operations/automation/backup-runbook#typical-persistent-disks",
    
    "relUrl": "/paas-operations/automation/backup-runbook#typical-persistent-disks"
  },"842": {
    "doc": "Backup runbook",
    "title": "Automation account",
    "content": "An automation account must be provided. The runbook will be installed into the the automation account. Typically, the runbook should be scheduled to run once a day outside of office hours. ",
    "url": "/paas-operations/automation/backup-runbook#automation-account",
    
    "relUrl": "/paas-operations/automation/backup-runbook#automation-account"
  },"843": {
    "doc": "Backup runbook",
    "title": "Snapshots",
    "content": "Upon a successful run, the runbook generates nine snapshots and stores them in a resource group for seven days. ",
    "url": "/paas-operations/automation/backup-runbook#snapshots",
    
    "relUrl": "/paas-operations/automation/backup-runbook#snapshots"
  },"844": {
    "doc": "Backup runbook",
    "title": "Input parameters",
    "content": "| Parameter | Default | Description | . | ClusterName | required | Name of the target AKS cluster | . | CustomerName | required | Name of customer | . | Subscription | required | ID of the azure subscription | . | SnapshotType | Incremental | Incremental or Full | . | BackupResourceGroup | required | Name of resource group where the snapshots will be saved to | . | RetentionCount | 7 | Number of days to retain the snapshots | . | EnableScaling | true | Scale down the cluster during restore | . ",
    "url": "/paas-operations/automation/backup-runbook#input-parameters",
    
    "relUrl": "/paas-operations/automation/backup-runbook#input-parameters"
  },"845": {
    "doc": "Backup runbook",
    "title": "Process",
    "content": ". Scaling up or down deployments is optional. However, it is recommended to prevent data loss while capturing snapshots. ",
    "url": "/paas-operations/automation/backup-runbook#process",
    
    "relUrl": "/paas-operations/automation/backup-runbook#process"
  },"846": {
    "doc": "Backup runbook",
    "title": "Permissions",
    "content": "The runbook must be granted the following permissions: . | Resource | Assigned Role(s) | Why This Role is Needed | . | AKS Resource Group | Reader | Required to read AKS configurations and metadata. | . | AKS Instance | Contributor | Required to scale AKS Instance and send aks command. | . | Snapshot Resource Group | Reader, Disk Snapshot Contributor | Required to read snapshots and creating/managing disk snapshots in the resource group. | . | AKS Node Resource Group | Reader, Disk Snapshot Contributor, VM Restore Contributor | Required to read list of disk, delete old disk, and restore new Disk from snapshot. | . | Storage Account Resource Group | Reader | Required to read Storage account configurations and metadata. | . | Storage Account | Storage Blob Data Contributor, Storage Account Key Operator Service Role | Required to store pod replica configuration during scaling down. | . ",
    "url": "/paas-operations/automation/backup-runbook#permissions",
    
    "relUrl": "/paas-operations/automation/backup-runbook#permissions"
  },"847": {
    "doc": "Backup runbook",
    "title": "Backup runbook",
    "content": " ",
    "url": "/paas-operations/automation/backup-runbook",
    
    "relUrl": "/paas-operations/automation/backup-runbook"
  },"848": {
    "doc": "Clue reference",
    "title": "On this page",
    "content": ". | Clue overview | Clue sample | Clue properties | Ignoring insignificant changes | Advanced operations with clues . | Delta clues and why you may need them | Delta data actions in clues | Remove edge | Remove edge and add edge in same clue (equivelant of an update) | Remove incoming edges using delta clues | Remove properties using delta clues | Remove identifier using delta clues | Remove tag using delta clues | Remove alias using delta clues | Remove description using delta clues | Remove name using delta clues | Remove display name using delta clues | Remove author using delta clues | . | . In this article, you will learn about a clue—an integral part of the data life cycle in CluedIn. Understanding how to interpret clues is important to ensure that they contain all the necessary details for successful processing and creation of golden records. ",
    "url": "/key-terms-and-features/clue-reference#on-this-page",
    
    "relUrl": "/key-terms-and-features/clue-reference#on-this-page"
  },"849": {
    "doc": "Clue reference",
    "title": "Clue overview",
    "content": "Clue is an object model in JSON format that CluedIn generates for your records as a result of mapping. Clue represents a transitional stage between the ingested record and the resulting golden record. Essentially, a clue is what CluedIn can process in order to produce a golden record. After you create the mapping, you can generate clues and take a look at their structure. To generate clues . | Go to the Processing tab of the data set. | Near the upper-right corner of the table, select the vertical ellipsis button, and then select Generate sample clues. A JSON file with all clues created from your data set is downloaded to your computer. You can open the file in any text editor. | . It is a good idea to check the clues before processing to make sure they contain all the necessary details so that identical clues can be merged by identifiers, thus reducing the number of duplicates in the system. You can do the following: . | Check if the identifiers are correct. | Check if the property and pre-process rules have been applied as intended. | Check if the code written in advanced mapping has been applied as intended. | . ",
    "url": "/key-terms-and-features/clue-reference#clue-overview",
    
    "relUrl": "/key-terms-and-features/clue-reference#clue-overview"
  },"850": {
    "doc": "Clue reference",
    "title": "Clue sample",
    "content": "{ \"clue\": { \"attribute-organization\": \"cafab9fd-7185-4f37-a753-20ec9ed71a2c\", \"attribute-origin\": \"/Customer#customers_smallcopycsv:1\", \"attribute-appVersion\": \"2.17.0.0\", \"clueDetails\": { \"data\": { \"attribute-originProviderDefinitionId\": \"03B88B70-D11F-4572-8F3D-AB6260610498\", \"attribute-origin\": \"/Customer#customers_smallcopycsv:1\", \"attribute-appVersion\": \"2.17.0.0\", \"attribute-inputSource\": \"cluedin-annotation\", \"entityData\": { \"name\": \"Grace Acton\", \"description\": \"\", \"entityType\": \"/Customer\", \"attribute-origin\": \"/Customer#customers_smallcopycsv:1\", \"attribute-appVersion\": \"2.17.0.0\", \"attribute-source\": \"cafab9fd-7185-4f37-a753-20ec9ed71a2c\", \"codes\": [ \"/Customer#File Data Source:CluedIn(hash-sha1):f333b1b1269a730fcf756ddbdf776d52edfb9f24\", \"/Customer#cluedin(email):gactony@bravesites.com\" ], \"edges\": { \"outgoing\": [ { \"attribute-from\": \"C:/Customer#customers_smallcopycsv:1\", \"attribute-type\": \"/WorksFor\", \"attribute-to\": \"C:/Company#companiescsv:2\" } ] }, \"properties\": { \"attribute-type\": \"/Metadata/KeyValue\", \"property-customer.id\": \"1\", \"property-customer.firstName\": \"Grace\", \"property-customer.lastName\": \"Acton\", \"property-customer.fullName\": \"Grace Acton\", \"property-customer.email\": \"gactony@bravesites.com\", \"property-customer.country\": \"CN\", \"property-customer.companyId\": \"2\" } } } } } } . ",
    "url": "/key-terms-and-features/clue-reference#clue-sample",
    
    "relUrl": "/key-terms-and-features/clue-reference#clue-sample"
  },"851": {
    "doc": "Clue reference",
    "title": "Clue properties",
    "content": "The following table contains the properties that you can find in the clue. | Property | Description | . | attribute-organization | GUID of the organization. | . | attribute-origin | Primary identifier of the clue as defined in the mapping details. It is the primary identifier that uniquely represents the clue. This property appears in several places in the clue structure. | . | attribute-appVersion | Version of the clue schema. This property appears in several places in the clue structure. | . | attribute-originProviderDefinitionId | GUID of the source that sent the data to CluedIn. The source can be a file, a database, an ingestion endpoint, a manual data entry project, and so on. Each source in CluedIn has a provider definition ID. The provider definition ID is used to restrict or grant user access to a specific data source. | . | attribute-inputSource | System that pushed the clue. Usually, the clue is created as a result of the mapping process, which is represented by the cluedin-annotation service. | . | name | Name of the clue. The name is shown on the search results page and on the golden record details page. | . | description | Description of the clue. The description is shown in the default search results view and on the golden record details page. | . | entityType | A common attribute that defines the business domain that the clue belongs to. The selection of the business domain is part of the mapping process. | . | attributeSource | A service source of the clue. | . | codes | Additional unique identifiers of the clue as defined in the mapping details. | . | edges | An object that represents a specific relation (attribute-type) between the source clue (attribute-from) and the target clue (attribute-to), identified through their primary identifiers. | . | properties | An object (also called a property bag) that contains all the properties (vocabulary keys) of the clue. | . | attribute-type | A format of data in the property bag. | . | aliases | A value used as an alternative or secondary name associated with the clue. | . | tags | A value used as a label to categorize clues across business domains. | . | createdDate | Date when the record was created in the source system. | . | modifiedDate | Date when the record was modified in the source system. | . | quarantine | Metadata information about the rules that were applied to the clue to send it to quarantine. | . ",
    "url": "/key-terms-and-features/clue-reference#clue-properties",
    
    "relUrl": "/key-terms-and-features/clue-reference#clue-properties"
  },"852": {
    "doc": "Clue reference",
    "title": "Ignoring insignificant changes",
    "content": "When CluedIn processes clues, it will generate a Hash value that will take certain values on the clue and create a unique value that represents a hash of those values. By default, all changes and properties in a clue are treated as something that will play a role in the hashing process. Developers can instruct Vocabularies to “Ignore Hashing”, which means that even if these values change on subsequent crawls, it won’t play a role in telling CluedIn that the data has changed. Only properties that have changed or been added that are not marked with “Ignore Hashing” will instruct CluedIn that things have changed. For example, many tools will change a timestamp value when the record has been viewed - not modified, but simply opened or viewed. You might find that this is important to change the clue, but in many occasions you will find that this change is insignificant and you will want to use your Vocabulary mappings to instruct CluedIn to ignore this change and throw away the clue from processing. This is typically to help increase the performance and lack of load placed onto the CluedIn processing servers. ",
    "url": "/key-terms-and-features/clue-reference#ignoring-insignificant-changes",
    
    "relUrl": "/key-terms-and-features/clue-reference#ignoring-insignificant-changes"
  },"853": {
    "doc": "Clue reference",
    "title": "Advanced operations with clues",
    "content": "This section contains information about posting clues via REST POST requests. It is intended for experienced technical users. To post clues to CluedIn, use the following post URL: /public/api/v2/clue?save=true, where baseurl is the address of your CluedIn instance. In the request’s header, add the Authorization key with the value Bearer , where apitoken is your API token that you can find in Administration &gt; API Tokens. Delta clues and why you may need them . By default, CluedIn works in the append mode, where as new data comes in, it will append the data over the top of existing data that has a matching primary identifier or will create new golden records where there is no matching primary identifier. There are situations where you actually don’t want your new data to be in “Append” mode, but rather you want to change the way that CluedIn will process and treat this data. The most common examples include: . | You are sending data to CluedIn with the intention to delete it from CluedIn and/or downstream Export Targets as well. | You are sending data to CluedIn with “Blank Values” and your intention is to ask CluedIn to “forget” that this column or columns ever had a value for this. For example, you had a phone number from a company and you would rather now have a blank value for this as the phone number is no longer active. | You are sending data to CluedIn with an updated value for a column and your intention is to ask CluedIn to REMOVE the old identifier or edge that could have been built off of this, and REPLACE it with the new ones, not just APPEND over the top. | You are sending blank data to CluedIn and you actually want CluedIn to treat the values as Blank and hence if a Golden Record has a value of “Hello” you actually want to turn that value into “”. | . Delta data actions in clues . You can post clues that remove outgoing or incoming edges from golden records. Following is an example of a clue that removes an outgoing edge from a golden record. Remove edge . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"OutgoingEdges\", \"attribute-referenceKey\": \"/EdgeType, Entity §C:/Person#Newman:\" } } } }, \"entityData\": { \"entityType\": \"/Person\", \"codes\": [ \"/Person#Newman:\" ], \"modifiedDate\": \"\", \"edges\": {}, \"edgesSummary\": {}, \"properties\": { \"attribute-type\": \"/Metadata/KeyValue\", \"property-hierarchy.lastUpdated\": \"\" }, \"attribute-id\": \"6\", \"attribute-origin\": \"/Person#Newman:\" }, \"dataActions\": {}, \"processedData\": { \"edges\": {}, \"edgesSummary\": {}, \"properties\": { \"attribute-type\": \"/Metadata/KeyValue\" }, \"attribute-ref\": \"6\", \"attribute-origin\": \"/Person#Newman:\", \"sortDate\": \"\", \"timeToLive\": \"0\", \"isShadowEntity\": \"False\" } }, \"references\": {} } } } . Remove edge and add edge in same clue (equivelant of an update) . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"data\": { \"attribute-persistHash\": \"b5bvwamo85acabdz5xyb1g==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"OutgoingEdges\", \"attribute-referenceKey\": \"/HRCHY_test, Entity §C:/Person#Newman:\" } } } }, \"entityData\": { \"entityType\": \"/Person\", \"codes\": [ \"/Person#Newman:\" ], \"modifiedDate\": \"\", \"edges\": { \"outgoing\": [ { \"attribute-type\": \"/HRCHY_test\", \"attribute-creationOptions\": \"Default\", \"attribute-from\": \"C:/Person#Newman:\", \"attribute-to\": \"C:/Person#Newman:\", \"property-CreatedAt\": \"\" } ] }, \"edgesSummary\": {}, \"attribute-id\": \"6\", \"attribute-origin\": \"/Person#Newman:\" }, \"dataActions\": {}, \"processedData\": { \"edges\": {}, \"edgesSummary\": {}, \"attribute-ref\": \"6\", \"attribute-origin\": \"/Person#Newman:\", \"sortDate\": \"\", \"timeToLive\": \"0\", \"isShadowEntity\": \"False\" } }, \"references\": {} } } } . Remove incoming edges using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"IncomingEdges\", \"attribute-referenceKey\": \"/HRCHY_test, Entity §C:/Person#Newman:\" } } } } } } } } . Remove properties using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"Property\", \"attribute-referenceKey\": \"user.email\" } } } } } } } } . Remove identifier using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"EntityCode\", \"attribute-referenceKey\": \"/Person#Newman:12345\" } } } } } } } } . Remove tag using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"Tag\", \"attribute-referenceKey\": \"Hello\" } } } } } } } } . Remove alias using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"Alias\", \"attribute-referenceKey\": \"Hello\" } } } } } } } } . Remove description using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"Description\" } } } } } } } } . Remove name using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"Name\" } } } } } } } } . Remove display name using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"DisplayName\" } } } } } } } } . Remove author using delta clues . { \"clue\": { \"attribute-organization\": \"\", \"attribute-origin\": \"/Person#Newman:\", \"clueDetails\": { \"deltaData\": { \"attribute-persistHash\": \"hwcrydlp+4jrs1dqu2io9w==\", \"attribute-appVersion\": \"2.17.0.0\", \"processingFlags\": {}, \"deltaDataActions\": { \"action\": { \"attribute-type\": \"/DataAction/Edit\", \"modifications\": { \"remove\": { \"attribute-target\": \"DisplayName\", \"attribute-referenceKey\": \"/Person#Newman:hello@cluedin.com\" } } } } } } } } . The deltaDataActions section contains an action that removes an edge. It consists of the following properties. | Property | Description | . | attribute-type | Type of action that will be executed on a golden record when posting the clue. Use the value as in the sample clue (/DataAction/Edit). | . | attribute-target | Type of edge that will be removed: OutgoingEdges or IncomingEdges. Specify the type of edge that you want to remove. | . | attribute-referenceKey | A specific edge that will be removed. You need to provide the name of the edge and the ID of the target record (to). | . ",
    "url": "/key-terms-and-features/clue-reference#advanced-operations-with-clues",
    
    "relUrl": "/key-terms-and-features/clue-reference#advanced-operations-with-clues"
  },"854": {
    "doc": "Clue reference",
    "title": "Clue reference",
    "content": " ",
    "url": "/key-terms-and-features/clue-reference",
    
    "relUrl": "/key-terms-and-features/clue-reference"
  },"855": {
    "doc": "Connect Microsoft Fabric to CluedIn",
    "title": "On this page",
    "content": ". | Set up Microsoft Fabric | Send data to CluedIn | . In this article, you will learn how to load the data from Microsoft Fabric to CluedIn. As an example, we’ll use a table named imdb_titles in Microsoft Fabric with 601_222 rows. Prerequisites . To connect Microsoft Fabric to CluedIn, you need to have the following: . | An active API token. You can create an API token in CluedIn in Administration &gt; API tokens. | An endpoint in CluedIn. You can find instructions on how to create an endpoint here. After you create an endpoint, you’ll find the POST URL. | . ",
    "url": "/microsoft-integration/fabric/connect-fabric-to-cluedin#on-this-page",
    
    "relUrl": "/microsoft-integration/fabric/connect-fabric-to-cluedin#on-this-page"
  },"856": {
    "doc": "Connect Microsoft Fabric to CluedIn",
    "title": "Set up Microsoft Fabric",
    "content": "Prepare Microsoft Fabric for sending data to CluedIn. To set up Microsoft Fabric . | In Microsoft Fabric, create a Jupyter notebook and install the cluedin library. %pip install cluedin . | Import the cluedin library and create a CluedIn context object. import requests import cluedin ctx = cluedin.Context.from_dict( { \"domain\": \"51.132.187.83.sslip.io\", \"org_name\": \"foobar\", \"access_token\": \"(your token)\", } ) ENDPOINT_URL = \"https://app.51.132.187.83.sslip.io/upload/api/endpoint/9A327661-51FD-4FFC-8DF5-3F80746B996C\" DELAY_SECONDS = 5 BATCH_SIZE = 100_000 . In this example, the URL of the CluedIn instance is https://foobar.51.132.187.83.sslip.io/, so the domain is 51.132.187.83.sslip.io, and the organization name is foobar. The access token is the one specified in prerequisites. | . ",
    "url": "/microsoft-integration/fabric/connect-fabric-to-cluedin#set-up-microsoft-fabric",
    
    "relUrl": "/microsoft-integration/fabric/connect-fabric-to-cluedin#set-up-microsoft-fabric"
  },"857": {
    "doc": "Connect Microsoft Fabric to CluedIn",
    "title": "Send data to CluedIn",
    "content": "After you set up Microsoft Fabric, send the data to CluedIn. This process involves pulling the data from Microsoft Fabric and posting it to CluedIn. To send data to CluedIn . | Select all rows from a table and yield them one by one using the following method. from pyspark.sql import SparkSession def get_rows(): spark = SparkSession.builder.getOrCreate() imdb_names_df = spark.sql(\"SELECT * FROM hive_metastore.cluedin.imdb_titles\") for row in imdb_names_df.collect(): yield row.asDict() . | Create a method that posts a batch of rows to CluedIn. import time from datetime import timedelta def post_batch(ctx, batch): response = requests.post( url=ENDPOINT_URL, json=batch, headers={\"Authorization\": f\"Bearer {ctx.access_token}\"}, timeout=60, ) time.sleep(DELAY_SECONDS) return response def print_response(start, iteration_start, response) -&gt; None: time_from_start = timedelta(seconds=time.time() - start) time_from_iteration_start = timedelta( seconds=time.time() - iteration_start) time_stamp = f'{time_from_start} {time_from_iteration_start}' print(f'{time_stamp}: {response.status_code} {response.json()}\\n') . print_response is a helper method that prints the response status code and the response body. | Iterate over the rows and post them to CluedIn. Note that we are posting the rows in batches of BATCH_SIZE rows. DELAY_SECONDS is the number of seconds to wait between batches. | Post a small batch of rows first to set up mapping on the CluedIn side. We will post ten rows. To do that, add the following lines in the code below. if i &gt;= 10: break . Code that posts the rows to CluedIn. batch = [] batch_count = 0 start = time.time() iteration_start = start for i, row in enumerate(get_rows()): if i &gt;= 10: break batch.append(row) if len(batch) &gt;= BATCH_SIZE: batch_count += 1 print(f'posting batch #{batch_count:_} ({len(batch):_} rows)') response = post_batch(ctx, batch) print_response(start, iteration_start, response) iteration_start = time.time() batch = [] if len(batch) &gt; 0: batch_count += 1 print(f'posting batch #{batch_count:_} ({len(batch):_} rows)') response = post_batch(ctx, batch) print_response(start, iteration_start, response) iteration_start = time.time() print(f'posted {(i + 1):_} rows') . After you run the code, ten rows appear in CluedIn. | In CluedIn, create auto-mapping for the data set following the instructions here. | In CluedIn, edit the mapping for the data set to select the property used as the entity name and the property used for the primary identifier. For more information about mapping details, see Review mapping. | In CluedIn, got to the Process tab of the data set, turn on the Auto submission toggle, and then select Switch to Bridge Mode. | Remove or comment on the following lines in the notebook and rerun it. if i &gt;= 10: break . All table rows will be posted to CluedIn. The output in the notebook will look like this. 0:00:13.398879 0:00:13.398889: 200 {'success': True, 'warning': False, 'error': False} posting batch #2 (100_000 rows) 0:00:22.021498 0:00:08.622231: 200 {'success': True, 'warning': False, 'error': False} posting batch #3 (100_000 rows) 0:00:30.709844 0:00:08.687518: 200 {'success': True, 'warning': False, 'error': False} posting batch #4 (100_000 rows) 0:00:40.026708 0:00:09.316675: 200 {'success': True, 'warning': False, 'error': False} posting batch #5 (100_000 rows) 0:00:48.530380 0:00:08.503460: 200 {'success': True, 'warning': False, 'error': False} posting batch #6 (100_000 rows) 0:00:57.116517 0:00:08.585930: 200 {'success': True, 'warning': False, 'error': False} posting batch #7 (1_222 rows) 0:01:02.769714 0:00:05.652984: 200 {'success': True, 'warning': False, 'error': False} posted 601_222 rows . Give CluedIn some time to process the data, and you should see 601_222 rows in CluedIn. The same approach works with any other data source. You must change the get_rows method to pull data from your source. | . ",
    "url": "/microsoft-integration/fabric/connect-fabric-to-cluedin#send-data-to-cluedin",
    
    "relUrl": "/microsoft-integration/fabric/connect-fabric-to-cluedin#send-data-to-cluedin"
  },"858": {
    "doc": "Connect Microsoft Fabric to CluedIn",
    "title": "Connect Microsoft Fabric to CluedIn",
    "content": " ",
    "url": "/microsoft-integration/fabric/connect-fabric-to-cluedin",
    
    "relUrl": "/microsoft-integration/fabric/connect-fabric-to-cluedin"
  },"859": {
    "doc": "Crawling",
    "title": "Crawling",
    "content": "Crawling servers are stateful but are also transactional in nature i.e. if a complete crawl does not finish, then it will never have a state of “complete”. This also means that if you were to cancel a crawler half way through a job, that you will need to start the crawl from the start again or potentially from the data that had changed state in the source system since the last successful crawl time. Crawlers are responsible for fetching data from a source system. Crawlers are self hosted and when they are run, the CluedIn server is responsible for giving it everything that is necessary to successfully connect and pull the right data from a source system. If you were to cancel a crawl half way through, CluedIn will ingest the data that has been ingested. It is not recommended to run a Crawling server and a Processing Server in this same CluedIn host, due to the change in nature of stateless and stateful environments. There are many crawlers that are provided for you in CluedIn, but if you are building your own, it is important to remember that the crawlers should not contain business logic and should not attempt to fix the data from the source. Crawling data can be complex and error-prone and many things can go wrong such as: . | Time-outs when calling sources | Network errors when calling sources | The source changes structure mid crawl | Authentication can change mid crawl | Source systems don’t have an obvious way to filter the data | . Your crawlers will need to cater for all of the complexities mentioned above and more. Fortunately, the CluedIn crawler framework helps with all of the above mentioned complexities. You have full control over the granularity of error handling that you would like in your crawlers e.g. managing how often to retry or how many times. Here is a list of things you should be covering with any custom crawlers that you extend CluedIn with: . | Supports Webhooks - Does this provider support Webhooks? | Are Webhooks Manual or Automatic - Does the provider support programmatic webhook creation or does the user need to manually set this up? | Test Webhook processing, creation, deletion. - Does the webhook creation process and deletion process work? | Management Endpoint for Webhooks - Is there a link in the User Interface where the user can manage any webhooks that CluedIn creates? . | Test that you are handling for configuration filters e.g. Folders, Labels. - What configuration can the user set to filter what the crawler goes out and gets? | Configuration of Providers -&gt; List, Tree, Setting, Checkbox? | Type of Provider - Is it Cloud? Is it On-Premise? . | Test API Token or Credentials - Does the provider support Developer API tokens. | API Rate Limiting - What rules are in place to limit what the crawler does? | Test 429 Status Code Handling - Does the crawler handle limiting requests? | OAuth2, OAuth, Basic, ApiToken, Custom - What type of authentication can this API support? Remember that you can support many different types of Authentication. | OAuth 2 flow endpoints. - What are the OAuth2 flow endpoints? | Refresh Token - How does a provider refresh an access token? | Expired Token - When does an access token expire? | ReAuth Endpoint - Endpoint in CluedIn to run the ReAuth Process. | Requirements and APP to be installed - Does CluedIn need to install an APP before it can talk to it? | App Install URL - What is the Install URL of an App if it requires an App to be installed first? | Get Permissions - Are there any permissions set in the provider for read/write access? | Requires a Paid Account / Premium Account - Will CluedIn only work if the provider is a paid account? . | Configuration for AutoWatching new things in providers - Will CluedIn automatically sync NEW folders and channels that are added into the provider. | Paging - How to page results in a reasonable amount of data at a time? | Schedule of how often it runs - How often should a crawl run? | Filtering on only getting the latest data (delta/auto crawls) - By default, CluedIn runs a job every 4 hours for a new crawl. This should only pull the latest data since the previous crawl. Please refer to Delta/Auto crawls - Change Data Capture | Has a Dynamic Template - Does the provider support custom and dynamic objects e.g. SharePoint/Podio/HubSpot etc. | Getting the Entity Statistics - Does the provider support a way to figure out how much data of different types they have? | Does the source use a cursor to get latest or a date filter (what format) - If the provider uses a cursor instead of a date filter. | Logo - Does the provider have a good high quality 400x400 image icon of the provider? | Description of Provider - Small description of the provider. | How to Get Account Id and Account Display - What is the unique ID of the provider per user and what is the friendly name to display in the UI. | Id - What is the GUID of the provider? . | Schema for Connections - Does the crawler connect properly with the right edge type, edge direction. | Data formats - Is the data in the format you mapped it to be? | Vocabulary Key mappings - Is all the data mapped to relevant Core Vocabulary Key? | Test that Dates are in the right format. - If we are using dates in the Vocabulary, do they format to ToString(“o”)? | . ",
    "url": "/integration/crawling",
    
    "relUrl": "/integration/crawling"
  },"860": {
    "doc": "Create a hierarchy",
    "title": "On this page",
    "content": ". | Create a hierarchy manually | Create a hierarchy based on relations | . In this article, you will learn about various options for creating a hierarchy. Key terms: . | Node – an individual golden record within a hierarchy. Nodes are the building blocks of hierarchies, and they can be linked together to form parent-child relationships, creating a structured view of data. | Top-level root – the highest node in the hierarchy. This node acts as the primary parent or the starting point from which all other nodes branch out. | . There are several ways to create a hierarchy: . | Create a hierarchy manually – choose this option if you want to build a hierarchy from scratch by dragging golden records to the project canvas. | Create a hierarchy based on relations – choose this option if you want to leverage the existing relations between golden records and visualize them in the hierarchy. | . After you save a hierarchy project, the hierarchy remains static. If any changes are made to the relations or edges outside of the hierarchy project, these changes do not become reflected in the hierarchy. ",
    "url": "/management/hierarchy-builder/create-a-hierarchy#on-this-page",
    
    "relUrl": "/management/hierarchy-builder/create-a-hierarchy#on-this-page"
  },"861": {
    "doc": "Create a hierarchy",
    "title": "Create a hierarchy manually",
    "content": "Creating a hierarchy manually gives you the possibility to build it from scratch, regardless of any existing relations between golden records. To create a hierarchy manually . | On the navigation pane, go to Management &gt; Hierarchy Builder. | In the upper-left corner of the Hierarchy Builder page, select Create Hierarchy. The Create Hierarchy pane opens on the right side of the page. | On the General tab of the Create Hierarchy pane, do the following: . | Enter the name of the hierarchy. To make it easier to identify the hierarchy, consider using concepts that define the relations between the elements in the hierarchy (for example, Manager-Employee). | (Optional) Select the business domain of golden records that will be loaded to the hierarchy project. If you don’t select the business domain, all golden records will be loaded to the hierarchy project. | Select Next. | . | On the Hierarchy configuration tab, do the following: . | In Hierarchy starting point, select Blank. | Select Create. | . | Build the hierarchy by dragging the golden records from the left pane to the project canvas. You can load golden records of other entity types and add them to the hierarchy as well. To learn more, see Work in a hierarchy project. | To save your changes, in the upper-right corner of the page, select Save. Now, when you go the Hierarchies tab of any golden record from the hierarchy, you can view the created hierarchy. After you save the hierarchy project, the hierarchy remains static. If any changes are made to the relations or edges outside of the hierarchy project, these changes do not become reflected in the hierarchy. | To create actual edges between the entities as you have designed in the project, in the upper-right corner of the page, select Publish. Then, confirm that you want to publish the hierarchy. Now, when you go to any golden record from the hierarchy, you can view the following: . | Relations tab – displays the relations between the golden record and other elements of the hierarchy. | History tab – displays records about new relations added to the golden record. | . | . ",
    "url": "/management/hierarchy-builder/create-a-hierarchy#create-a-hierarchy-manually",
    
    "relUrl": "/management/hierarchy-builder/create-a-hierarchy#create-a-hierarchy-manually"
  },"862": {
    "doc": "Create a hierarchy",
    "title": "Create a hierarchy based on relations",
    "content": "Sometimes your data has natural hierarchies that are represented as relations. You can leverage such relations and create hierarchies automatically. To create a hierarchy based on relations . | On the navigation pane, go to Management &gt; Hierarchy Builder. | In the upper-left corner of the Hierarchy Builder page, select Create Hierarchy. The Create Hierarchy pane opens on the right side of the page. | On the General tab of the Create Hierarchy pane, do the following: . | Enter the name of the hierarchy. To make it easier to identify the hierarchy, consider using concepts that define the relations between the elements in the hierarchy (for example, Organization-Project). | Select the business domain of golden records that will be loaded to the hierarchy project. If you don’t select the business domain, all golden records will be loaded to the hierarchy project. | Select Next. | . | On the Hierarchy configuration tab, in Hierarchy starting point, select From existing relations. | In Relation type, select the edge type that represents relations between golden records that will be loaded to the hierarchy project. The edge types available depend on the business domain selected on the General tab. | In Edge direction, review the way in which hierarchy relations between golden records will be generated. The edge direction is selected automatically based on the edge type selected in the previous step. | In Top level configuration, select how you want to define top-level roots: . | Manually – you will need to define the top-level roots manually. | Automatically – top-level roots will be identified automatically based on the existing relations sets between the golden records that will be loaded to the hierarchy project. | . | In Multiple or single top-level hierarchy, select the option for generating the hierarchy in case of multiple top-level roots: . | Single hierarchy project – all hierarchies from top-level roots will be generated in one hierarchy project. | Multiple hierarchy projects – each hierarchy from a top-level root will be generated in a separate hierarchy project. | . | Select Create. The number of hierarchy projects that are created depends on the selected Multiple hierarchy projects option. In case of Single hierarchy project, the hierarchy project opens right away, and you can review and modify it as needed. In case of Multiple hierarchy project, you need to open each project to review and modify it as needed. You can load golden records of other entity types and add them to the hierarchy as well. To learn more, see Work in a hierarchy project. | Modify the created hierarchy as needed. Then, in the upper-right corner of the page, select Save. Now, when you go the Hierarchies tab of any golden record from the hierarchy, you can view the created hierarchy. After you save the hierarchy project, the hierarchy remains static. If any changes are made to the relations or edges outside of the hierarchy project, these changes do not become reflected in the hierarchy. | To create actual edges between the entities as you have designed in the project, select Publish in the upper-right corner of the page. Then, confirm that you want to publish the hierarchy. Now, when you go to any golden record from the hierarchy, you can view the following: . | Relations tab – displays the relations between the golden record and other elements of the hierarchy. | History tab – displays records about new relations added to the golden record. | . | . ",
    "url": "/management/hierarchy-builder/create-a-hierarchy#create-a-hierarchy-based-on-relations",
    
    "relUrl": "/management/hierarchy-builder/create-a-hierarchy#create-a-hierarchy-based-on-relations"
  },"863": {
    "doc": "Create a hierarchy",
    "title": "Create a hierarchy",
    "content": " ",
    "url": "/management/hierarchy-builder/create-a-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/create-a-hierarchy"
  },"864": {
    "doc": "Create a rule",
    "title": "On this page",
    "content": ". | Create a rule | Test a rule on one record | Apply a rule to all records | . In this article, you will learn how to create a rule, validate it on one golden record to ensure it works as expected, and then apply the rule to all records. ",
    "url": "/management/rules/create-rule#on-this-page",
    
    "relUrl": "/management/rules/create-rule#on-this-page"
  },"865": {
    "doc": "Create a rule",
    "title": "Create a rule",
    "content": "Creating a rule involves defining the golden records to which the rule will be applied and specifying the actions to modify these records. The procedure of creating a rule is the same for all types of rules. However, the actions associated with each type of rule are different. To create a rule . | On the navigation pane, go to Management &gt; Rule builder. | Choose the type of rule that you want to create. Then, select Create rule. | Enter the name of the rule, and then select Create. The rule details page opens. | In the Filters section, select Add first filter, and then specify the golden records to which the rule will be applied. For more information on how to set up a filer, see Filters. | In the Actions section, select Add action, and then configure the action that will be applied to the golden records matching the filters: . | Enter the name of the action. | In the Action section, choose the action to be performed by the rule. For more information about the available actions for each rule type, see Rules reference. Depending on the chosen action, you might need to provide additional details. | In the lower-right corner, select Add action. | . | If you want to add more actions to the rule, repeat step 5. | Near upper-right corner, select Save, and then confirm that you want to save the rule. | Activate the rule by turning on the toggle next to the rule status. At this point, the rule has not yet been applied to the golden records. To ensure the rule is configured correctly, we recommend applying it to one golden record to verify the results. | . ",
    "url": "/management/rules/create-rule",
    
    "relUrl": "/management/rules/create-rule"
  },"866": {
    "doc": "Create a rule",
    "title": "Test a rule on one record",
    "content": "After creating a rule, it is a good idea to test it on one golden record to verify that the configuration is correct. This way, you can reprocess just the one golden record instead of all affected golden records. If you find that the rule’s actions have not been applied as intended, you can edit the rule and test it on one golden record again until you achieve the desired configuration. To test a rule on one golden record . | Go to search and add the same filters that you set up in the rule (step 4 in Create a rule). Then, start the search. | Open one golden record from the search results. | Near upper-right corner, select More &gt; Re-process entity. | After you see a notification that the request for reprocessing has been sent, reload the page. | Check if the golden record has been modified according to the rule. | Check what rule has been applied to the golden record: . | Go to the Explain log tab. | Expand the Golden record entry. | Expand Evaluate rules &gt; Summaries. In the tables, you can view the rules and actions that have been applied to the golden record. | . | Depending on the results of your review, do one of the following: . | If the rule has been applied to the golden record as you intended, proceed to apply the rule to all golden records. | If the rule has not been applied to the golden record as intended, edit the rule, and then repeat steps 1–7 until you achieve the desired result. | . | . ",
    "url": "/management/rules/create-rule#test-a-rule-on-one-record",
    
    "relUrl": "/management/rules/create-rule#test-a-rule-on-one-record"
  },"867": {
    "doc": "Create a rule",
    "title": "Apply a rule to all records",
    "content": "After testing a rule on one golden record and verifying that it is applied as intended, you can proceed to apply the rule to all golden records. To apply a rule to all golden records . | On the navigation pane, go to Management &gt; Rule builder. | Choose the needed type of rule, and then open the rule that you created. | Near upper-right corner, select the reprocessing button. | Confirm that you want to reprocess all golden records that match the rule’s filter. The reprocessing option available on the rule details page is applicable only to the records that match the rule’s filter. You can track the reprocessing of records in the status bar. If, during the reprocessing, you decide to add another action to the rule or modify a filter, you can cancel the current reprocessing operation and make the needed changes. After the reprocessing is completed, the records associated with the rule are modified in accordance with the rule’s actions. | . There is an alternative approach to reprocess the records associated with the rule. It involves the GraphQL tool. To reprocess records using GraphQL . | On the navigation pane, go to Consume &gt; GraphQL. | Enter a query to reprocess the records that should be affected by the rule. For example, the following query will reprocess all records that contain the “customer.location” vocabulary key set to “New York”. { search(query: \"customer.location:New York\") { entries { actions { postProcess } } } } . | Execute the query. After the query is executed, the rule’s action will be applied to the records matching the rule’s filter. | . ",
    "url": "/management/rules/create-rule#apply-a-rule-to-all-records",
    
    "relUrl": "/management/rules/create-rule#apply-a-rule-to-all-records"
  },"868": {
    "doc": "Ingest data",
    "title": "On this page",
    "content": ". | Import file | View imported data | Create mapping | Process data | Search for data | Results &amp; next steps | . Ingesting data to CluedIn involves three basic steps: importing, mapping, and processing data. As a result, your records will become searchable and ready to be cleaned, deduplicated, and streamed. In this guide, you will learn how to import a file into CluedIn, create a mapping, process the data, and search for golden records. File for practice: training-data.csv . ",
    "url": "/getting-started/data-ingestion#on-this-page",
    
    "relUrl": "/getting-started/data-ingestion#on-this-page"
  },"869": {
    "doc": "Ingest data",
    "title": "Import file",
    "content": "The easiest way to add data to CluedIn is to import a file. In the following procedure, we will import a CSV file containing 500 records. A CSV (comma-separated values) file format allows data to be saved in a tabular format, making it compatible with most spreadsheet programs such as Excel. So, if you are using Excel for your data analysis or management tasks, importing a CSV file is a convenient option. To import a file . | On the navigation pane, go to Ingestion, and then in the Files section, select Add. | In the Add Files section, add the file. You may drag the file or select the file from the computer. | In the Group section, leave the default Create new group option selected, and then enter the name of the group. A group indicates a department or domain within your company that is associated with the data. Think of a group as a folder where you can store files related to similar types of data. | Select Upload. | . ",
    "url": "/getting-started/data-ingestion#import-file",
    
    "relUrl": "/getting-started/data-ingestion#import-file"
  },"870": {
    "doc": "Ingest data",
    "title": "View imported data",
    "content": "After you uploaded the file, you can view the data from the file as a table with columns and rows. To view imported data . | On the navigation pane, go to Ingestion &gt; Sources. | Find and expand the group that you created in the previous procedure. | Expand the element contained in the group. This element is called a data source; think of a data source as a database. | Select the element contained in the data source. This element is called a data set; think of a data set as a table in the database. The data set details page opens, where you can view imported data. | . The data is parsed and stored in CluedIn, but it is not yet been processed. What does it mean if the data is not processed? . Unprocessed data is not available for any data management tasks. What is more, if you try to search for data, the results won’t return any records. This is because the data is not yet visible for CluedIn. To make the data available for use within CluedIn, you need to create a mapping and process the data. ",
    "url": "/getting-started/data-ingestion#view-imported-data",
    
    "relUrl": "/getting-started/data-ingestion#view-imported-data"
  },"871": {
    "doc": "Ingest data",
    "title": "Create mapping",
    "content": "Mapping is the process of creating a semantic layer for your data so that CluedIn understands it. This process involves the mapping of original fields to standard fields as well as assigning appropriate data types. To create a mapping . | On the data set details page, select the Missing Mapping label. Alternatively, go to the Map tab. | In the middle of the page, select Map Data. | In Mapping Type, leave the default Auto Mapping option selected. | Select Next. | In Business Domain, enter the name of a new business domain and select Create. A business domain is a specific business object within the organization. A well-named business domain is global and should not be changed (for example, Person, Organization, Car) across sources. The Business Domain Identifier is created automatically; it is a string that represents the business domain in code (for example, in clues). | In Icon, select the visual representation of the business domain. | In Vocabulary, enter the name of a new vocabulary and select Create. A vocabulary is a structured framework for creating a unified view of your data. The purpose of vocabulary is to translate your original fields to the language CluedIn understands. The Key prefix is created automatically; it is added before the original field names to establish consistent and well-organized standard fields. | In Primary Identifier, review the field that was selected automatically for generating a unique identifier for each record. | In Preview, review the mapping of original fields to standard fields. You can edit the names of standard fields that come after the key prefix, and you can change the data type if needed. | Select Create Mapping. As a result of the mapping process, you can view how the fields from the file are linked to the standard fields in CluedIn. | . ",
    "url": "/getting-started/data-ingestion#create-mapping",
    
    "relUrl": "/getting-started/data-ingestion#create-mapping"
  },"872": {
    "doc": "Ingest data",
    "title": "Process data",
    "content": "Processing turns your data into golden records that can be cleaned, deduplicated, and streamed. To process data . | On the data set details page, select the Not Processed label. Alternatively, go to the Process tab. | Select Process. | Review information about records that will be processed. Pay attention to the Primary Identifier Status section. If there are duplicates, they will be merged during processing. | Select Confirm. After the records are processed, you can view the resulting golden records on the Data tab. | . ",
    "url": "/getting-started/data-ingestion#process-data",
    
    "relUrl": "/getting-started/data-ingestion#process-data"
  },"873": {
    "doc": "Ingest data",
    "title": "Search for data",
    "content": "After the data has been processed, you can search for any property and view all golden records where it is used. In the following procedure, we will use the email address as an input to search for data. To search for data . | On the Preview tab of the data set, copy any email address. | Paste the email address in the search field, and then select the search icon. | On the search results page, select the name of the record. | On the golden record details page, go to the Properties tab. Here, you can view all the properties of the golden record. | . ",
    "url": "/getting-started/data-ingestion#search-for-data",
    
    "relUrl": "/getting-started/data-ingestion#search-for-data"
  },"874": {
    "doc": "Ingest data",
    "title": "Results &amp; next steps",
    "content": "In this guide, you learned how to import data into CluedIn using a file, create auto-mapping, process the data, and perform data searches. After completing all steps outlined here with our practice file, you now have 500 golden records in the system. With these golden records, you can perform various data management tasks. In the following guides, you’ll find detailed instructions about common data management tasks. If you find errors in your golden records, such as misspelled job titles or emails, you can fix them using the clean functionality. To learn how to do it, go to the next guide, Clean data. ",
    "url": "/getting-started/data-ingestion#results--next-steps",
    
    "relUrl": "/getting-started/data-ingestion#results--next-steps"
  },"875": {
    "doc": "Ingest data",
    "title": "Ingest data",
    "content": " ",
    "url": "/getting-started/data-ingestion",
    
    "relUrl": "/getting-started/data-ingestion"
  },"876": {
    "doc": "Deduplication in practice",
    "title": "Deduplication in practice",
    "content": "In this article, you’ll discover various strategies on how to efficiently deduplicate large data sets in CluedIn. Let’s start with the video, which shows a real-life scenario for deduplicating 100 thousand records. To enhance the efficiency of your deduplication project and to be able to smoothly revert merges, we recommend that you follow these guidelines. When you need to run a deduplication project on a large set of data, we recommend that you start small and limit the size of data. This approach allows you to to verify the accuracy and efficiency of your matching rules before applying them to the entire data set. If the deduplication result doesn’t meet your expectations, you can easily and quickly revert changes and start again. When it comes to matching rules, our recommendation is to create separate projects for strict and fuzzy matching functions, grouped by domain. There are two reasons for this recommendation: . | Quick and efficient merging and unmerging. If all rules are consolidated within a single project and you decide to revert changes, all rules will be reverted, even those that were executed correctly. To prevent this, create separate projects for individual sets of rules. This way, you can selectively revert only those projects where rules didn’t work as intended. | Streamlined management of projects on a day-to-day basis. If you have projects targeting a specific domain, and the data for that domain has been updated, then you can easily re-evaluate those specific projects. For example, suppose you have one project that merges duplicates based on first and last names and another project that merges duplicates based on addresses. Now, you add the libpostal library to enrich addresses. In this scenario, you can easily re-evaluate only the address-based project, knowing that first and last names have remained unchanged. | . To increase the efficiency of your deduplication projects, start by running projects with strict equality matching functions to eliminate obvious duplicates. Once you’ve reached a state with no duplicates based on equality matching, proceed to execute projects with fuzzy matching functions for a more nuanced deduplication process. Since you’ve already merged some records based on equality matching functions, this means that you now have fewer duplicates in the system, so fuzzy matching would run much faster. Finally, when you achieve the desired outcome on a small data set, grow big and proceed to run the deduplication project on the entire data set. ",
    "url": "/management/deduplication/deduplication-in-practice",
    
    "relUrl": "/management/deduplication/deduplication-in-practice"
  },"877": {
    "doc": "Deduplication",
    "title": "Deduplication",
    "content": "The goal of deduplication is to eliminate duplicate records by merging them together into a single, accurate, and consolidated golden record. This process maintains full traceability, allowing you to identify contributing records for the resulting golden record and providing the possibility to revert changes if necessary. You can reduce the number of duplicates in the system proactively even before creating a deduplication project. For this purpose, CluedIn provides the possibility of merging by identifiers: those data parts that have identical primary identifiers or additional identifiers are merged during processing. For more information, see Identifiers. The following diagram shows the basic steps for merging duplicates in CluedIn. This section covers the following areas: . | Concept of deduplication – explore how deduplication works through a simple yet detailed example. | Deduplication in practice – discover the practical application of deduplication guidelines through an example. | Creating a deduplication project – learn how to create a deduplication project and configure matching rules. | Managing a deduplication project – learn how to effectively manage a deduplication project, including generating and discarding results, editing project configuration, and more. | Managing groups of duplicates – learn how to process groups of duplicates, including merging duplicates, verifying merged records, and reverting merges. | Reference information about deduplication projects – find information about matching functions, rule conditions, deduplication project statuses, and group statuses. | . ",
    "url": "/management/deduplication",
    
    "relUrl": "/management/deduplication"
  },"878": {
    "doc": "Delete golden records",
    "title": "Delete one golden record",
    "content": "If you no longer need a specific golden record, you can delete it on the golden record page. To delete a golden record . | Open the golden record that you want to delete. | In the upper-right corner, select Delete. | Review the details of the golden record that will be deleted, and then select Next. | Confirm that you want to delete the golden record by entering DELETE. Then, select Confirm. The entire golden record is permanently deleted and cannot be restored. | . ",
    "url": "/golden-records/delete-golden-records#delete-one-golden-record",
    
    "relUrl": "/golden-records/delete-golden-records#delete-one-golden-record"
  },"879": {
    "doc": "Delete golden records",
    "title": "Delete several golden records",
    "content": "If you no longer need a set of golden records, you can select those golden records on the search results page and delete them. To delete several golden records . | On the search results page, find golden records that you want to delete. You can search for golden records by entering a keyword or applying filters. | In the upper-right corner, select the three-dot button (⋮) &gt; Select for deletion. | Select the checkboxes next to golden records that you want to delete. | In the upper-left corner, select Start deletion. | Review the details of the golden records that will be deleted, and then select Next. | Confirm that you want to delete golden records by entering DELETE. Then, select Confirm. The selected golden records are permanently deleted and cannot be restored. | . ",
    "url": "/golden-records/delete-golden-records#delete-several-golden-records",
    
    "relUrl": "/golden-records/delete-golden-records#delete-several-golden-records"
  },"880": {
    "doc": "Delete golden records",
    "title": "Delete golden records",
    "content": "In this article, you will learn how to delete golden records. Please note that deleting golden records is permanent and irreversible, so carefully consider which golden records you want to delete. You can delete a golden record in two places: . | On the golden record page – use this option if you want to delete one golden record. | On the search results page – use this option if you want to delete several golden records at once. | . Important! Deleting a golden record is different from deleting data parts or removing records from a data source. When you remove records from a data source, all data parts originating from that source are removed from every golden record in which they are used. Deleting data parts from a golden record allows you to delete specific data parts originating from almost any source in a golden record. In contrast, deleting a golden record means that the entire record is permanently removed. ",
    "url": "/golden-records/delete-golden-records",
    
    "relUrl": "/golden-records/delete-golden-records"
  },"881": {
    "doc": "Installation",
    "title": "Installation",
    "content": "CluedIn SaaS Get access to cloud-hosted platform, the easiest way to start with CluedIn CluedIn PaaS Install CluedIn within your company’s Azure infrastructure Local Install CluedIn locally to test its main features and write custom code CluedIn is designed with the Microservices Architecture in mind. That means that CluedIn, as an application, is a set of interconnected services: a web application, GraphQL API, databases, message queues, and so on. Each CluedIn service runs in a separate container, allowing us to test, scale, and monitor each service effectively. While CluedIn is a cloud-native application, you can also run it on your local machine. Docker Compose is the technology that allows us to run a group of containers on our local computer easily. You just run a few commands, and a new CluedIn instance is up and running on your laptop or desktop computer. You can use it for testing and development. Please, follow the Local installation guide for more details. When it comes to production, Kubernetes runs CluedIn services in the cloud and ensures that the containers are healthy and scale as they should. While all modern cloud providers support Kubernetes, we recommend running CluedIn on Microsoft Azure with the help of Azure Kubernetes Service. Read more about it in the PaaS installation section of our documentation. The following video explores the features and differences between the SaaS, PaaS, and local options for deploying CluedIn. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. Azure Managed Application . CluedIn is available as an Azure Managed Application, making deployment and operation effortless for you. Depending on the model you choose—PaaS or SaaS—CluedIn can be deployed within your company’s infrastructure or within our infrastructure. Regardless of the chosen model, this setup allows us to handle almost everything for you, including post-installation, upgrades, backups, and monitoring. This ensures that you do not need to worry about operating and managing a cluster yourself, as long as the CluedIn operations team has full access to your cluster. SaaS vs. PaaS . The following diagram illustrates the difference between SaaS and PaaS in terms of who is responsible for managing what. ",
    "url": "/deployment",
    
    "relUrl": "/deployment"
  },"882": {
    "doc": "DNS",
    "title": "DNS",
    "content": "To access CluedIn via domain name, you need to configure DNS. You would need to create DNS A-records that point the domain name to the AKS public IP address. Say your AKS cluster public IP is 51.116.48.197, the domain you want to associate with your CluedIn instance is cluedin.mycompany.com, and the tenant name is hr. Then you need to set up the following URLs to point to your AKS cluster: . | app.cluedin.mycompany.com | hr.cluedin.mycomany.com | clean.cluedin.mycompany.com | . Note: there could be more URLs to set up if you add more tenants or install additional tools on the cluster. The way you set up DNS depends on your domain registrar, but you usually need to create a set of CNAME and A records in your domain registrar admin panel. ",
    "url": "/deployment/azure/dns",
    
    "relUrl": "/deployment/azure/dns"
  },"883": {
    "doc": "Feature access",
    "title": "On this page",
    "content": ". | Roles | Ownership | Combinations of claim access levels and ownership | . In this article, you will learn about the main settings that define access to CluedIn features: roles and ownership. Feature access does not affect access to data in CluedIn. For example, if you have access to the glossary, you still won’t be able to view golden records from a glossary term without specific data access. For more information, see Data access. Feature access means permissions that allow you to interact with CluedIn modules: Integrations, Governance, Preparation, Engine Room, Management, Consume, and Administration. Every module contains features that enable you to perform specific tasks. For example, in the Preparation module, you can create clean projects and add enrichers for enhancing golden records. Access to those features is managed by CluedIn roles and ownership permissions. ",
    "url": "/administration/user-access/feature-access#on-this-page",
    
    "relUrl": "/administration/user-access/feature-access#on-this-page"
  },"884": {
    "doc": "Feature access",
    "title": "Roles",
    "content": "The main parameter that determines whether the user can access CluedIn features is roles. For more information about roles, see a dedicated article. If you use Microsoft Entra ID and you enable automatic role synchronization, then you don’t need to manage access to features via roles in CluedIn. You can do that using Microsoft Entra ID application roles. If you notice that you don’t have access to a specific feature, you can submit a role request to an Administrator. For a detailed instruction on how to get access to features, see Request access. As an Administrator, you can process such role requests and choose a suitable claim access level and a role for a user. For more information about roles requests, see Process role requests. The user can have multiple roles with different claim access levels. In this case, the higher claim access level will be applied to the user. The claim access levels within roles define the possible actions a user can perform on an element within the feature. To illustrate this statement, let’s take a look at the example of rules. The access to rules is governed by the Rule Builder claim. Depending on the access level for the Rule Builder claim, you can do different actions with the rules: . | If your access level for the Rule Builder claim is None, you don’t have access to the rules at all. | If your access level for the Rule Builder claim is Informed, you can view all rules. | If your access level for the Rule Builder claim is Consulted, you can create and manage your own rules. Also, you can make changes to the rules created by other users and submit such changes for approval to the owners of the rule. | If your access level for the Rule Builder claim is Accountable, you can create and manage your own rules as well as manage all rules created by other users. In other words, this claim access level gives you administrator control over all rules. | . ",
    "url": "/administration/user-access/feature-access#roles",
    
    "relUrl": "/administration/user-access/feature-access#roles"
  },"885": {
    "doc": "Feature access",
    "title": "Ownership",
    "content": "Ownership defines the right to approve or reject change requests submitted by non-owner users as well as make direct changes. Almost every element in CluedIn has the Owners tab with a list of users and/or roles who are considered owners. Such elements include clean projects, enrichers, access control policies, vocabularies, deduplication projects, rules, glossary terms, hierarchies, streams, and export targets. Initially, the user who created the element—for example, a rule—is its owner. However, if you trust other users to process change requests related to the element and to make direct changes to the element, you can add those users to the list of owners. You can also add roles to the list of owners; this way, all users who have that role will get the ownership permissions. To add owners . | Open the element to which you want to add owners, and then go to the Owners tab. | Select Add &gt; Add Users or Add Roles. | Select the checkboxes next to the users or roles who will be granted ownership. | In the lower-right corner, select Add. | . When the users or roles appear on the Owners tab of the element, they are granted permission to approve or reject change requests submitted by other users. Also, if their access level for the relevant claim is at least Consulted, they can make changes directly without submitting them for approval. ",
    "url": "/administration/user-access/feature-access#ownership",
    
    "relUrl": "/administration/user-access/feature-access#ownership"
  },"886": {
    "doc": "Feature access",
    "title": "Combinations of claim access levels and ownership",
    "content": "Since both claim access levels and ownership permissions regulate the activities that can be performed with an element within a CluedIn feature, it is important to understand how they work together. This section does not include the following combinations: . | None claim access level and ownership – the claim access level is not sufficient to open an element, so the ownership permission doesn’t have any practical application in this combination. | Accountable claim access level and ownership – the claim access level gives full control over all elements within a feature, so the ownership permission doesn’t have any practical application in this combination. | . Consulted claim access level with ownership . The Consulted claim access level gives you access to modify elements with a feature. With the ownership permission, you can modify the configuration of an element right away as well as receive and process change requests submitted by other, non-owner users. Informed claim access level with ownership . The Informed claim access level gives you access to view all elements within a feature. However, even if you have the ownership permission, you cannot approve or reject change requests submitted by other, non-owner users. ",
    "url": "/administration/user-access/feature-access#combinations-of-claim-access-levels-and-ownership",
    
    "relUrl": "/administration/user-access/feature-access#combinations-of-claim-access-levels-and-ownership"
  },"887": {
    "doc": "Feature access",
    "title": "Feature access",
    "content": " ",
    "url": "/administration/user-access/feature-access",
    
    "relUrl": "/administration/user-access/feature-access"
  },"888": {
    "doc": "Install CluedIn using PowerShell",
    "title": "On this page",
    "content": ". | Introduction | Download the Installation script and run it | Next Steps | . Introduction . This installation process is done through command lines using PowerShell 7, Azure CLI, Kubectl and Helm. The purpose of the chart is to install the CluedIn application. This includes the actual CluedIn server, website, and other services required (storage, queues, etc.) . Note: Before proceeding with the installation, you must ensure that all pre-requisites are met. Download the Installation script and run it . | Save this Installation Script to a folder of your choice on your computer. In this example, the script is saved to C:\\Users\\$env:UserName . | Open a PowerShell 7 session as administrator on your computer and run the following command, this will enable local scripts to run: Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass . | Run the run-full-install.ps1 script and let it guide you through the whole installation process | . Next Steps . After logging in to the platform, you can proceed with enabling single sign on for your users to access the platform, as well as start loading data in via Data Sources or installing some crawlers. Below you will find some useful links on achieving the above: . | Enabling Single Sign On | Restricting access to CluedIn Clean via Basic Authentication | Install a crawler/custom component | . Optionally, you can also adjust other settings to cater for more complex scenarios: . | Persistence/Using Managed Disks | Azure SQL Server | Scaling | . ",
    "url": "/deployment/azure/setup/powershell#on-this-page",
    
    "relUrl": "/deployment/azure/setup/powershell#on-this-page"
  },"889": {
    "doc": "Install CluedIn using PowerShell",
    "title": "Install CluedIn using PowerShell",
    "content": " ",
    "url": "/deployment/azure/setup/powershell",
    
    "relUrl": "/deployment/azure/setup/powershell"
  },"890": {
    "doc": "Getting access",
    "title": "On this page",
    "content": ". | Getting access for administrators | Getting access for business users . | Sign in by email | Request access | . | . In this article, you will find instructions on how to get access to CluedIn. ",
    "url": "/getting-access#on-this-page",
    
    "relUrl": "/getting-access#on-this-page"
  },"891": {
    "doc": "Getting access",
    "title": "Getting access for administrators",
    "content": "Depending on the type of CluedIn installation that you performed, do one of the following: . | Installation of CluedIn SaaS – sign in to CluedIn using the email address and password that you specified when configuring your CluedIn account. For more information, see CluedIn SaaS installation guide. | Installation of CluedIn PaaS – sign in to CluedIn using the email address and password that you specified on the Instance Setup tab of CluedIn installation. For more information, see CluedIn PaaS installation guide. | Local installation of CluedIn – sign in to CluedIn using the email address and password you created on the last step of local CluedIn installation. For more information, see Local installation guide. | . ",
    "url": "/getting-access#getting-access-for-administrators",
    
    "relUrl": "/getting-access#getting-access-for-administrators"
  },"892": {
    "doc": "Getting access",
    "title": "Getting access for business users",
    "content": "You can sign in to your CluedIn account in one of the following ways: . | Sign in with your email address and password - if your administrator invites you to CluedIn, you will receive an email with a link to sign up for your CluedIn account. The following diagram shows the steps involved in getting access to your CluedIn account by email. | Sign in using Single Sign-On (SSO) – SSO is not enabled by default. If your administrator has set up SSO, you’ll be able to sign in to your CluedIn account by authenticating through your company’s identity provider. If the SSO method is enabled for your organization, you can select the corresponding button on the CluedIn sign-in page. | . Sign in by email . After you created password for your account, you can sign in and start using CluedIn. To sign in to CluedIn by email . | On the CluedIn sign-in page, enter your email and password and then select Sign In. After you sign in for the first time, you may notice that you have read-only access to all sections and modules. To get permissions to specific modules and functions, you need to request access. | . Request access . To get access to more functions within the platform, contact your CluedIn administrator or request access to the needed modules directly in CluedIn. The following diagram shows the flow of requesting access directly in CluedIn. To request access to CluedIn modules . | Sign in to CluedIn. | Find the module that you want to get access to. For example, if you want to view deduplication projects, go to Management &gt; Deduplication. You will see a message similar to the following. | Select Request Access. The request has been sent to your CluedIn administrator. When the request gets approved, you’ll receive a notification. | Sign out and sign in again for the changes to take the effect. Now, you can use CluedIn modules and functions granted by new role. | . ",
    "url": "/getting-access#getting-access-for-business-users",
    
    "relUrl": "/getting-access#getting-access-for-business-users"
  },"893": {
    "doc": "Getting access",
    "title": "Getting access",
    "content": " ",
    "url": "/getting-access",
    
    "relUrl": "/getting-access"
  },"894": {
    "doc": "Local installation guide",
    "title": "On this page",
    "content": ". | Clone CluedIn repository | Authenticate to CluedIn ACR | Start CluedIn | Create your sign-in credentials | Troubleshooting | Results | Next steps | . In this article, you will learn how to install CluedIn locally. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. ",
    "url": "/deployment/local/step-2#on-this-page",
    
    "relUrl": "/deployment/local/step-2#on-this-page"
  },"895": {
    "doc": "Local installation guide",
    "title": "Clone CluedIn repository",
    "content": "The first step for the local installation of CluedIn is to clone the CluedIn public repository named Home https://github.com/CluedIn-io/Home. To clone the CluedIn repository . | At the command prompt, run the following command. git clone https://github.com/CluedIn-io/Home . You will get an output similar to the following. Alternatively, you can clone the repository using your preferred Git client. | . ",
    "url": "/deployment/local/step-2#clone-cluedin-repository",
    
    "relUrl": "/deployment/local/step-2#clone-cluedin-repository"
  },"896": {
    "doc": "Local installation guide",
    "title": "Authenticate to CluedIn ACR",
    "content": "Prerequisites . | For Windows environments: Modify the PowerShell execution policy to enable running local scripts. To do that, use the Set-ExecutionPolicy RemoteSigned command. | For Windows environments: Run your terminal session as an administrator. | For non-Windows environments: Use the sudo command. | . CluedIn uses two ACRs: . | Production – the registry name is cluedinprod. This registry is used to store all official images from CluedIn. | Testing &amp; development – the registry name is cluedindev. This registry is used internally by CluedIn to develop the software. Occasionally, this registry can be used by partners and customers to test new functionality before it is officially released. | . You can find the ACR to which you have been granted access in the email from CluedIn. In the following instructions, we use 2025.05 as the version number. You should always use the latest version number. You can find the list of releases here. To authenticate to CluedIn ACR . | Open to the checkout directory by running the following command: . cd Home . | Sign in to Docker. Depending on the registry to which you have been granted access, do one of the following: . | For Production, run the following command: . docker login cluedinprod.azurecr.io . | For Early Access, run the following command: . docker login cluedindev.azurecr.io . | . | Enter the username and password that you received in the email from CluedIn. | To verify your access, pull images from the ACR by running the following command: . pwsh .\\cluedin.ps1 pull . This process might take some time. You will get an output similar to the following. | Create an environment. In the following command, we use 202505 as the name of the environment and 2025.05 as the version number. To create an environment, run the following command: . pwsh .\\cluedin.ps1 env 202505 -tag 2025.05 . For the environment name, you can use any string of characters. However, periods (.) are not allowed in the environment name.For the tag, always use the latest version as stated on the release page. You can use the version number in one of the following formats: year.month (for example, 2024.07); year.month.update (for example, 2024.07.04); or major.minor (for example, 4.3). You will get an output similar to the following. Environment is used for having scripts that can start CluedIn of different versions. Docker does not support multiple CluedIn versions running in parallel. | . ",
    "url": "/deployment/local/step-2#authenticate-to-cluedin-acr",
    
    "relUrl": "/deployment/local/step-2#authenticate-to-cluedin-acr"
  },"897": {
    "doc": "Local installation guide",
    "title": "Start CluedIn",
    "content": "To start CluedIn, run the following command: . pwsh .\\cluedin.ps1 up 202401 . where 202401 is the name of the environment. You will get an output similar to the following. ",
    "url": "/deployment/local/step-2#start-cluedin",
    
    "relUrl": "/deployment/local/step-2#start-cluedin"
  },"898": {
    "doc": "Local installation guide",
    "title": "Create your sign-in credentials",
    "content": "The last step of the local installation of CluedIn is to create an organization and credentials for signing in to CluedIn. Prerequisites . Make sure that the server has started correctly. To do that, in Docker Desktop, select the server and look for a similar section in logs. In the following procedure, we’ll use 202401 as an environment, example as an organization name, and Example123! as a password for signing in to CluedIn. To create organization and sign-in credentials . | Run the following command: . pwsh .\\cluedin.ps1 createorg 202401 -Name example -Pass Example123! . You’ll get the credentials for signing in to CluedIn. | Open the CluedIn sign-in page by running the following command: . pwsh .\\cluedin.ps1 open 202401 -Org example . As a result, CluedIn opens in your default browser. By default, CluedIn uses the following address: http://app.127.0.0.1.nip.io:9080. If the nip.io domain does not work, refer to the Troubleshooting section. | Sign in to CluedIn using the credentials that you received in step 1. | . ",
    "url": "/deployment/local/step-2#create-your-sign-in-credentials",
    
    "relUrl": "/deployment/local/step-2#create-your-sign-in-credentials"
  },"899": {
    "doc": "Local installation guide",
    "title": "Troubleshooting",
    "content": "When the nip.io domain does not work, do the following: . | In the hosts file (C:\\Windows\\System32\\drivers\\etc\\hosts), add the following lines: . 127.0.0.1 cluedin.local 127.0.0.1 app.cluedin.local 127.0.0.1 foobar.cluedin.local . | In the .env file, find the following line: . CLUEDIN_DOMAIN=127.0.0.1.nip.io . | Replace the line that you found in step 2 with the following line: . CLUEDIN_DOMAIN=cluedin.local . | . ",
    "url": "/deployment/local/step-2#troubleshooting",
    
    "relUrl": "/deployment/local/step-2#troubleshooting"
  },"900": {
    "doc": "Local installation guide",
    "title": "Results",
    "content": "You have installed CluedIn locally. ",
    "url": "/deployment/local/step-2#results",
    
    "relUrl": "/deployment/local/step-2#results"
  },"901": {
    "doc": "Local installation guide",
    "title": "Next steps",
    "content": "Add more features to CluedIn with the help of extension packages. Learn how to do that in Extension packages. ",
    "url": "/deployment/local/step-2#next-steps",
    
    "relUrl": "/deployment/local/step-2#next-steps"
  },"902": {
    "doc": "Local installation guide",
    "title": "Local installation guide",
    "content": " ",
    "url": "/deployment/local/step-2",
    
    "relUrl": "/deployment/local/step-2"
  },"903": {
    "doc": "Manage access control policies",
    "title": "On this page",
    "content": ". | Edit a policy rule | Activate and deactivate a policy | Delete a policy | . In this article, you will learn how to manage an access control policy and modify it when changes are required. ",
    "url": "/management/access-control/manage-access-control-policies#on-this-page",
    
    "relUrl": "/management/access-control/manage-access-control-policies#on-this-page"
  },"904": {
    "doc": "Manage access control policies",
    "title": "Edit a policy rule",
    "content": "If you need to change something in the access control policy—for example, add more users to the policy rule or add more vocabulary keys—you can easily do that by editing the policy rule. To edit an access control policy rule . | In the navigation pane, go to Management &gt; Access Control. | Find and open the access control policy that you want to edit. | Make the needed changes and then save them. The changes in the access control policy will be applied right away. | . ",
    "url": "/management/access-control/manage-access-control-policies#edit-a-policy-rule",
    
    "relUrl": "/management/access-control/manage-access-control-policies#edit-a-policy-rule"
  },"905": {
    "doc": "Manage access control policies",
    "title": "Activate and deactivate a policy",
    "content": "When active, the policy grants access to its members to specific golden records and vocabulary keys. If you no longer want to apply the policy, you can deactivate it. After the policy is deactivated, users and/or roles who were granted access to golden records and vocabulary keys by the policy will lose that access. Yet, if you change your mind, you can easily activate the policy. There are two ways to activate and deactivate a policy: . | From the list of policies – this option allows you to activate or deactivate multiple policies at once. | From the policy details page. | . To activate or deactivate the policy from the list of policies . | Select the checkbox next to the policy that you want to activate or deactivate. | Near the upper-right corner, select the needed action. | . To activate or deactivate the policy from the policy details page . | Near the upper-right corner, turn on or off the status toggle. | . ",
    "url": "/management/access-control/manage-access-control-policies#activate-and-deactivate-a-policy",
    
    "relUrl": "/management/access-control/manage-access-control-policies#activate-and-deactivate-a-policy"
  },"906": {
    "doc": "Manage access control policies",
    "title": "Delete a policy",
    "content": "You can delete an access control policy if you no longer need it. After the policy is deleted, users and/or roles who were granted access to golden records and vocabulary keys by the policy will lose that access. There are two ways to delete a policy: . | From the list of policies – this option allows you to delete multiple policies at once. | From the policy details page. | . To delete a policy from the list of policies . | Select the checkbox next to the policy that you want to delete. | Select Delete, and then confirm your choice. | . To delete a policy from the policy details page . | Select the delete icon. Then, confirm your choice. | . ",
    "url": "/management/access-control/manage-access-control-policies#delete-a-policy",
    
    "relUrl": "/management/access-control/manage-access-control-policies#delete-a-policy"
  },"907": {
    "doc": "Manage access control policies",
    "title": "Manage access control policies",
    "content": " ",
    "url": "/management/access-control/manage-access-control-policies",
    
    "relUrl": "/management/access-control/manage-access-control-policies"
  },"908": {
    "doc": "Manage a clean project",
    "title": "On this page",
    "content": ". | Generate results | Clean data | Process cleaned data | Revert changes | Regenerate results | Edit a clean project | Archive a clean project | Duplicate a clean project | . After creating a clean project, you can begin the cleaning process to get a more ready-to-use set of data. In this article, you will learn how to load data for cleaning, perform data cleaning operations, and publish the cleaned data back to CluedIn. The following diagram illustrates the main actions available within the clean project. Each of these actions is described in detail in the following sections of this article. In addition to these actions, you can always edit or archive the clean project if necessary. ",
    "url": "/preparation/clean/manage-clean-project#on-this-page",
    
    "relUrl": "/preparation/clean/manage-clean-project#on-this-page"
  },"909": {
    "doc": "Manage a clean project",
    "title": "Generate results",
    "content": "Generating results retrieves the values that match the criteria from the clean project filter and loads them into the clean application. To generate the results . | In the upper-right corner of the clean project, select Generate Results, and then confirm your choice. You can track the result generation status in the progress bar. When the results are generated, the status of the clean project becomes Ready for clean, which means that you can start to clean the data. | . Clean project and real-time data . When you generate the results for the clean project, CluedIn captures a snapshot of data for you to clean. It’s important to note that you’re working with this snapshot, not real-time data. When new data with the same issues appears in CluedIn, you’ll need to clean it as well. You can regenerate the results in the same clean project or rely on rules generated by CluedIn based on your actions in the clean project. You’ll find more details about generating rules from the clean projects in Process cleaned data. ",
    "url": "/preparation/clean/manage-clean-project#generate-results",
    
    "relUrl": "/preparation/clean/manage-clean-project#generate-results"
  },"910": {
    "doc": "Manage a clean project",
    "title": "Clean data",
    "content": "After generating the results, you can open the clean application to start fixing data issues. To open the clean application . | In the upper-right corner of the clean project, select Clean. The clean application containing records that need to be cleaned opens in the same page. | . To work with the clean application . | Transform the contents of the cells in a column: . | In the column heading, expand the menu, and then select Edit cells &gt; Common transforms. Then, choose the transformation option (for example, collapse consecutive whitespace or transform the text to title case). For more information, see Clean application reference. | . | Edit the contents of a column using a text facet: . | In the column heading, expand the menu, and then select Facet &gt; Text facet. A text facet collects the total contents of cells in a column and matches them up. To edit an entry in the facet display, hover over the facet and select Edit. You can then enter a new value that will be applied to all identical cells in the column, allowing for efficient mass editing. This is a great option for fixing typos, whitespace, and other issues. | . | Cluster and edit the contents of a column: . | In the column heading, expand the menu, and then select Edit cells &gt; Cluster and edit. This feature helps you find groups of different cell values that might be alternative representations of the same thing. You can explore different methods and key functions to see how many clusters can be found in your data. The following screenshot shows that all values in a cluster will be changed to the new value. | . | Edit one cell at a time: . | Hover over that cell and select Edit. A pop-up window appears where you can edit the contents of the cell. You can apply your changes to all identical cells in the same column. Use this option sparingly. The most efficient way to clean your data is through automated and bulk operations. | . | . ",
    "url": "/preparation/clean/manage-clean-project#clean-data",
    
    "relUrl": "/preparation/clean/manage-clean-project#clean-data"
  },"911": {
    "doc": "Manage a clean project",
    "title": "Process cleaned data",
    "content": "When you have cleaned the data, send it back to CluedIn. Processing cleaned data replaces incorrect values in golden records with the corrected ones. To process cleaned data . | In the upper-right corner of the clean project, select Process. | In the Stale data strategy section, choose an option for dealing with stale data if it is identified during processing: . | Skip stale data – if the records from the clean project contain more recent data in CluedIn, then such records won’t be processed. | Write stale data – if the records from the clean project contain more recent data in CluedIn, they will be processed. | . | If you want to automatically fix the same data issues that might appear in future, leave the Enable rules auto generation checkbox selected. | Confirm that you want to process the data. You can track the processing status in the progress bar. When the cleaned data is processed, the status of the clean project becomes Processed. It means that the corrected values have been submitted to the golden records in CluedIn. | . Automatic rules generation . Once you fix a data issue, it’s likely you’ll need to fix the same issue again in the future. To save time and ensure data accuracy, you can generate rules based on your actions in the clean project. There are two ways of generating rules in the clean project: . | By selecting the Enable rules auto generation checkbox in the processing confirmation dialog. | Be selecting Generate rules on the Rules tab of the clean project. | . In both cases, rules will be generated only if you made significant changes in the clean application (for example, changed all values in the column to upper case). If you edited individual cells, rules won’t be generated. You can find generated rules in the appropriate tab in the clean project. The rules from the clean projects are also stored in Management &gt; Rule Builder &gt; Data Part Rules. Note that the rule is initially inactive. To apply the rule to the records that will be loaded to CluedIn in future, activate the rule. This way, the values that correspond to the configuration parameters from the rule will be automatically fixed when the records are processed. Clean project and streams . If the data from the clean project is streamed to the export target, the stream will automatically receive the cleaned data after processing. This ensures that the data in the export target is automatically updated with the cleaned values. ",
    "url": "/preparation/clean/manage-clean-project#process-cleaned-data",
    
    "relUrl": "/preparation/clean/manage-clean-project#process-cleaned-data"
  },"912": {
    "doc": "Manage a clean project",
    "title": "Revert changes",
    "content": "You can undo changes made to values in your golden records after processing the cleaned data. Reverting these changes restores the values to their pre-cleaning state. To revert changes . | In the upper-right corner of the clean project, select Revert Changes, and then confirm your choice. As a result, the changed values are removed, returning the values to their pre-cleaning state. The status of the clean project becomes Ready to process, which means that your previous changes are saved in the clean application. Reverting is not tracked anywhere. | . Consider the following actions that you can do after reverting changes in the clean project: . | If you want to continue working with the same set of data, go to the clean application and make the needed changes. | If you want to get the latest data, regenerate the results, and start the data cleaning process. Keep in mind that regenerating the results in this case removes your progress in the clean application. | . ",
    "url": "/preparation/clean/manage-clean-project#revert-changes",
    
    "relUrl": "/preparation/clean/manage-clean-project#revert-changes"
  },"913": {
    "doc": "Manage a clean project",
    "title": "Regenerate results",
    "content": "Generating results for the clean project creates a snapshot of the data. If new data is loaded into CluedIn, it won’t be automatically added to the clean project. To add the latest data to the clean project, you should regenerate the results. Regenerating the results will cause you to lose all previous progress in the clean project. So, if you have already cleaned the data and want to publish your changes, process the cleaned data first, and then regenerate the results. To regenerate the results . | In the upper-right corner of the clean project, select Regenerate, and then confirm your choice. As a result, the data that matches the filter from the clean project is loaded to the clean application. Next, you can start the data cleaning process. | . ",
    "url": "/preparation/clean/manage-clean-project#regenerate-results",
    
    "relUrl": "/preparation/clean/manage-clean-project#regenerate-results"
  },"914": {
    "doc": "Manage a clean project",
    "title": "Edit a clean project",
    "content": "You can edit a clean project to make necessary changes in project name, description, filters, and properties that you need to clean. If you change filters or properties, all previous cleaning progress will be lost and the project will be regenerated. To edit a clean project . | In the upper-right corner of the clean project, select Edit. | Make the needed changes. | Select Save. If you changed filters or properties, confirm that you want to save changes. If you changed the name or description, the status of the clean project remains the same. Otherwise, it becomes Ready for clean. | . ",
    "url": "/preparation/clean/manage-clean-project#edit-a-clean-project",
    
    "relUrl": "/preparation/clean/manage-clean-project#edit-a-clean-project"
  },"915": {
    "doc": "Manage a clean project",
    "title": "Archive a clean project",
    "content": "You can archive a clean project if you no longer need it or if you created it by mistake. You can also archive a clean project if you’re confident that you won’t need to run it again in the future or if you’ve already generated rules to address the same data quality issues. Archiving does not affect the cleaned data that has been submitted to CluedIn. After the clean project is archived, it cannot be unarchived. Archived projects are available only for viewing. To archive a clean project . | In the upper-right corner of the clean project, select Edit &gt; Archive. Then, confirm that you want to archive the project. The status of the clean project becomes Archived, and you can no longer work with the project. | . To view an archived clean project . | In the upper-right corner of the Clean Project page, select View Archived. | From the list of archived clean projects, select the one that you want to view. | . ",
    "url": "/preparation/clean/manage-clean-project#archive-a-clean-project",
    
    "relUrl": "/preparation/clean/manage-clean-project#archive-a-clean-project"
  },"916": {
    "doc": "Manage a clean project",
    "title": "Duplicate a clean project",
    "content": "Duplicating a clean project means creating a new clean project with the configuration of the existing project. This configuration includes filters and properties for cleaning but does not include the cleaning activities performed in the project and data part rules generated from cleaning activities. You can duplicate a clean project if you want to perform different cleaning activities on the same set of golden records. Duplication is a beta feature. To access it, go to Administration &gt; Feature Flags, and enable the Duplicate Actions feature. To duplicate a clean project . | In the list of clean projects, find a project that you want to duplicate. Then, open the three-dot menu for the project, and select Duplicate. | In Project Name, review the default name of the new clean project and modify it if needed. The default name is created by adding _duplicate to the name of the project that you’re duplicating. | In Filters, review the filters that will be duplicated for the new clean project. | In Selected Properties, review the information about the properties that will be duplicated for the new clean project. To view the list of properties, select View All Properties. | Select Duplicate. The new clean project is created, and it has the New status. Now, you can modify the clean project configuration if needed. When you reach the desired configuration, save your changes, generate results, and then start the cleaning activities. | . ",
    "url": "/preparation/clean/manage-clean-project#duplicate-a-clean-project",
    
    "relUrl": "/preparation/clean/manage-clean-project#duplicate-a-clean-project"
  },"917": {
    "doc": "Manage a clean project",
    "title": "Manage a clean project",
    "content": " ",
    "url": "/preparation/clean/manage-clean-project",
    
    "relUrl": "/preparation/clean/manage-clean-project"
  },"918": {
    "doc": "Manage streams",
    "title": "On this page",
    "content": ". | Start, pause, and stop a stream | Edit a stream | View stream details | Duplicate a stream | . In this article, you will learn how to manage streams to keep the entire process of delivering data to external systems efficient, well-organized, and aligned with your data management objectives. ",
    "url": "/consume/streams/manage-streams#on-this-page",
    
    "relUrl": "/consume/streams/manage-streams#on-this-page"
  },"919": {
    "doc": "Manage streams",
    "title": "Start, pause, and stop a stream",
    "content": "Stream controls allows you to manage the process of sending records to the export target. | Start – the stream will start accumulating records in the queue and sending them to the export target. | Pause – the stream will stop sending records to the export target, but it will still continue accumulating records in the queue. | Stop – the stream will stop sending records to the export target and accumulating records in the queue. | . Think of these stream controls as similar to the controls on a video player. When you select Pause, the stream halts temporarily, remembering your playback position and storing records in the queue. This way, when you resume the stream, it continues from where you left off, maintaining your progress. On the other hand, Stop leads to a complete termination of the streaming process and clearing of the queue. If you start the stream after it had been stopped, it will start sending records to the export target from the beginning, not from the point at which you stopped the stream. ",
    "url": "/consume/streams/manage-streams#start-pause-and-stop-a-stream",
    
    "relUrl": "/consume/streams/manage-streams#start-pause-and-stop-a-stream"
  },"920": {
    "doc": "Manage streams",
    "title": "Edit a stream",
    "content": "You can edit the stream configuration and the export target configuration regardless of the stream status. If you change filters or actions in the stream configuration or if you make any changes in the export target configuration, saving these changes will trigger stream reprocessing. It means that all records existing in the export target will be deleted, and the stream will start sending records to the export target again. To edit the stream configuration . | On the Configuration tab, edit the needed items. You can edit any field or section. Then, select Save and confirm your choice. | . To edit the export target configuration . | On the Export Target Configuration tab, select Edit Export Configuration, and then confirm your choice. | Make the needed changes, select Save, and then confirm your choice. | . ",
    "url": "/consume/streams/manage-streams#edit-a-stream",
    
    "relUrl": "/consume/streams/manage-streams#edit-a-stream"
  },"921": {
    "doc": "Manage streams",
    "title": "View stream details",
    "content": "On the stream details page, there are several tabs where you can view stream-related information: . | Preview Condition – you can view the records that match the filters from the Configuration tab. | Data – you can view the records that will be sent to the export target. These records match the filters from the Configuration tab and contain the properties you selected on the Export Target Configuration tab. | Monitoring – you can view real-time data on ingestion, processing, and publishing of records, as well as any exceptions. | . ",
    "url": "/consume/streams/manage-streams#view-stream-details",
    
    "relUrl": "/consume/streams/manage-streams#view-stream-details"
  },"922": {
    "doc": "Manage streams",
    "title": "Duplicate a stream",
    "content": "Duplicating a stream means creating a new stream with the configuration of the existing stream. This configuration includes filters and actions but does not include the export target configuration. This means that you need to select and configure the export target and choose the properties for export from scratch for the duplicated stream. You can duplicate a stream if you want to send the same selection of golden records to another export target. Duplication is a beta feature. To access it, go to Administration &gt; Feature Flags, and enable the Duplicate Actions feature. To duplicate a stream . | In the list of streams, find a stream that you want to duplicate. Then, open the three-dot menu for the stream, and select Duplicate. | In Name, review the default name of the new stream and modify it if needed. The default name is created by adding _duplicate to the name of the stream that you’re duplicating. | In Conditions, review the filters that will be duplicated for the new stream. | In Actions, review the list of actions that will be duplicated for the new stream. To view the details of a specific action, select View Action Details. | Select Duplicate. The new stream is created. By default, the export target for the stream is not configured. Now, you can modify the stream configuration if needed and configure the export target. When you reach the desired configuration, start the stream. | . ",
    "url": "/consume/streams/manage-streams#duplicate-a-stream",
    
    "relUrl": "/consume/streams/manage-streams#duplicate-a-stream"
  },"923": {
    "doc": "Manage streams",
    "title": "Manage streams",
    "content": " ",
    "url": "/consume/streams/manage-streams",
    
    "relUrl": "/consume/streams/manage-streams"
  },"924": {
    "doc": "Power Apps configuration guide",
    "title": "On this page",
    "content": ". | Basic Power Apps configuration | Features of Power Apps integration | Next steps | . In this guide, you will learn how to configure Power Apps integration in CluedIn. Make sure that you have completed all of the actions described in Power Apps pre-configuration guide. ",
    "url": "/microsoft-integration/powerapps/configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/powerapps/configuration-guide#on-this-page"
  },"925": {
    "doc": "Power Apps configuration guide",
    "title": "Basic Power Apps configuration",
    "content": "Basic Power Apps configuration is required to establish connection between your CluedIn instance and your Power Apps environment. To configure Power Apps in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Power Apps section. | In Base URL, enter the root address used to access resources within your Power Apps environment. You can find this value in your Power Apps environment: in the upper-right corner of the Power Apps page, select the settings icon, and then select Developer Resources. Copy the value in Web API endpoint and paste it to the URL field in CluedIn. You do not need to copy the version of the API (/api/data/v9.2). | In Tenant Id, enter a unique identifier for your Microsoft Entra ID tenant in which your application is registered. This is the application that you created during the pre-configuration stage in Register a new application. You can find this value in Directory (tenant) ID. | In Client Id, enter a unique identifier assigned to an application when you registered it in Microsoft Entra ID. This is the application that you created during the pre-configuration stage in Register a new application. You can find this value in Application (client) ID. | In Client Secret, enter a string value that your application uses to prove its identity when requesting a token. This is the client secret that you created during the pre-configuration stage in Create a client secret . | In Environment Id, enter a unique identifier assigned to you Power Apps environment. This is the environment ID that you found during the pre-configuration stage in Find your environment ID . | In the upper-right corner, select Save. Proceed to the next section to configure Power Apps integration features that you want to use. | . ",
    "url": "/microsoft-integration/powerapps/configuration-guide#basic-power-apps-configuration",
    
    "relUrl": "/microsoft-integration/powerapps/configuration-guide#basic-power-apps-configuration"
  },"926": {
    "doc": "Power Apps configuration guide",
    "title": "Features of Power Apps integration",
    "content": "Power Apps integration offers a variety of features for syncing data between CluedIn and Dataverse. To configure specific features of Power Apps integration . | If you want to sync multiple CluedIn business domains to Dataverse tables, specify the Parallel Execution Count. This is the number of business domains that can be simultaneously synced with Dataverse. Be default, this number is 5. | If you want to sync CluedIn business domains to Dataverse tables: . | Turn on the toggle next to Sync CluedIn Business Domains to Dataverse Table. | Enter the business domains that you want to sync. If you want to sync multiple business domains, separate them with a comma (for example, /Type1,/Type2,/Type3). Each business domain will be synced into a separate Dataverse table. For more information, see Sync business domains to Dataverse tables. | . | If you want to sync Dataverse tables and columns to CluedIn business domains and vocabulary keys: . | Turn on the toggle next to Sync Dataverse Table/Columns to CluedIn Business Domains and Vocabulary. | Enter the name of the Dataverse table that you want to sync. This should be the logical name of the Dataverse table. If you want to sync multiple tables, separate them with a comma (for example, logical_name1,logical_name2,logical_name3). Each table will be synced into a separate CluedIn business domain and vocabulary associated with that business domain. The columns from the table will be synced into the vocabulary keys of the vocabulary associated with the business domain. For more information, see Sync Dataverse tables to CluedIn business domains and vocabularies. | . | If you want to automate the ingestion of data from Dataverse to CluedIn: . | Turn on the toggle next to Create workflow to Ingest Data to CluedIn. | If you want to allow specific users to view the workflow in Power Apps, add the email addresses of those users in Workflow Access Users List. | Enter the Dataverse Connection ID. This is the ID that you found during the pre-configuration stage in Create a connection. For more information, see Create ingestion endpoint workflow. | . | If you want to create a stream to export golden records from CluedIn to a Dataverse table: . | Make sure you have the Dataverse export target installed in your CluedIn instance. It should be available in the list of export targets (Consume &gt; Export Targets &gt; Add Export Target). You do not need to configure the Dataverse export target because it will be configured automatically. | Make sure you have enabled Sync CluedIn Business Domains to Dataverse Table for business domains of golden records that you want to export to a Dataverse table. | Turn on the toggle next to Create CluedIn Stream. For more information, see Create streams. | . | In the upper-right corner, select Save. | . ",
    "url": "/microsoft-integration/powerapps/configuration-guide#features-of-power-apps-integration",
    
    "relUrl": "/microsoft-integration/powerapps/configuration-guide#features-of-power-apps-integration"
  },"927": {
    "doc": "Power Apps configuration guide",
    "title": "Next steps",
    "content": "Explore different features that are available as part of Power Apps integration in CluedIn. ",
    "url": "/microsoft-integration/powerapps/configuration-guide#next-steps",
    
    "relUrl": "/microsoft-integration/powerapps/configuration-guide#next-steps"
  },"928": {
    "doc": "Power Apps configuration guide",
    "title": "Power Apps configuration guide",
    "content": " ",
    "url": "/microsoft-integration/powerapps/configuration-guide",
    
    "relUrl": "/microsoft-integration/powerapps/configuration-guide"
  },"929": {
    "doc": "Power Automate configuration guide",
    "title": "Power Automate configuration guide",
    "content": "In this guide, you will learn how to configure and register a custom connector in CluedIn. This guide is applicable to both public and private CluedIn instances. Make sure that you have completed all of the actions described in Power Automate pre-configuration guide. To configure workflows in CluedIn . | Go to Administration &gt; Feature Flags, and then enable the Workflow Builder feature. As a result, the Workflows module appears on the navigation pane. | Go to Administration &gt; Settings. Scroll down to the Workflows section and complete the following fields: . | Client Username – an email of a Microsoft Entra ID user for handling workflows. This is the user that you created during the pre-configuration stage in Create a user account. You can find the needed value in the User principal name field. | Client Password – a password of a Microsoft Entra ID user for handling workflows. This is the user that you created during the pre-configuration stage in Create a user account. | Client ID – a unique identifier assigned to an application when you registered it in Microsoft Entra ID. This is the application that you created during the pre-configuration stage in Create a service application. You can find this value in Application (client) ID. | Azure Tenant ID – an ID of the Azure tenant that contains the service principal that handles the Power Automate widget. This is the service principal that was created automatically when you registered a new application in Create a service application. For guidance on how to find the ID, see Find your Microsoft Entra tenant. | Power Automate Environment ID – an ID of the Power Automate environment. This is the environment that you prepared during the pre-configuration stage in Configure an environment. You can find the ID on the home page of the environment. | CluedIn Connector – a name of the custom connector that allows communication between CluedIn and Power Automate. By default, it is CluedIn, and you do not need to change it. | Approvals for creating items – enabling this control means that when a user creates an element in CluedIn (for example, a vocabulary, a vocabulary key, or a rule), an approval request is sent to other users with the same or higher claim access level to the feature. For example, if a Data Governance Administrator creates a vocabulary, the approval request is sent to other users with the same role. If you don’t enable this control, the approval requests will be sent only in case of modifications of the existing elements. | Approvals for activating/deactivating rules – enabling this control means that when a user activates or deactivates a rule, an approval request is sent to all rule owners. | Enterprise Flow Cache Duration – a time period for which data is stored in the cache for enterprise flows. This duration can impact the performance and efficiency of your workflows. In the context of Power Automate, the cache duration helps manage the flow’s performance by temporarily storing data to reduce the need for repeated data retrievals. | Business Domain Cache Duration – a time period for which the records that belong to the business domains with the Batch approval workflow option enabled are stored in the cache. | . | Select Register Custom Connector. A custom connector is going to be created in Power Automate. Next, proceed to Power Automate post-configuration guide to verify that the connector has been created successfully. | . ",
    "url": "/microsoft-integration/power-automate/configuration-guide",
    
    "relUrl": "/microsoft-integration/power-automate/configuration-guide"
  },"930": {
    "doc": "Prepare for the upgrade",
    "title": "On this page",
    "content": ". | Get access to CluedIn application | Connect Helm and kubectl to the CluedIn AKS cluster | Configure kubectl | Configure Helm | Connect Lens or Freelens to your CluedIn cluster | . Before upgrading CluedIn, it’s important to make sure your environment is ready. Proper preparation helps reduce downtime, avoid data loss, and ensures a smooth transition to the new version. This page covers stage 2 of the CluedIn upgrade process. It walks you through the key steps to check and complete before starting the upgrade process. ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#on-this-page"
  },"931": {
    "doc": "Prepare for the upgrade",
    "title": "Get access to CluedIn application",
    "content": "Before starting an upgrade, it is best practice to open the CluedIn UI and confirm that all services are running as expected. If you don’t have direct access, make sure that someone who does is available to assist throughout the upgrade process. For instructions, see Check CluedIn UI. ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#get-access-to-cluedin-application",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#get-access-to-cluedin-application"
  },"932": {
    "doc": "Prepare for the upgrade",
    "title": "Connect Helm and kubectl to the CluedIn AKS cluster",
    "content": "To connect Helm and kubectl to the CluedIn Azure Kubernetes Service (AKS) cluster, you will need a valid kubeconfig file. | Contact your Azure administrator and ask them to provide the kubeconfig file to you. Treat this file as sensitive information – it contains access credentials and cluster details. | Store the file in a secure location. Do not commit the file to a source control tool. | Once you have the kubeconfig file, configure your environment to use it. For example, in PowerShell, run the following: . $env:KUBECONFIG=\"path-to-file\" . This ensures that both kubectl and Helm commands will use the correct cluster context. | . ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#connect-helm-and-kubectl-to-the-cluedin-aks-cluster",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#connect-helm-and-kubectl-to-the-cluedin-aks-cluster"
  },"933": {
    "doc": "Prepare for the upgrade",
    "title": "Configure kubectl",
    "content": "Before performing an upgrade, you need to ensure that kubectl is both installed and properly connected to your Kubernetes cluster. The steps below help you verify the installation and confirm that kubectl can communicate with the cluedin namespace. | To verify that kubectl is installed correctly, run the following command in PowerShell: . kubectl version --client . | If kubectl is installed, you’ll see the client version details (for example, Client Version: v1.30.0). | If not, you’ll get a “command not found” error. In this case, contact your system administrator to install and configure kubectl properly. | . | To confirm that kubectl is correctly connected to your cluster, run the following command in PowerShell: . kubectl get pods --namespace cluedin . | You should see a list of pods as a result. | If not, your kubeconfig or network access may not be configured correctly. Contact your administrator. | . | . ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#configure-kubectl",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#configure-kubectl"
  },"934": {
    "doc": "Prepare for the upgrade",
    "title": "Configure Helm",
    "content": "Before upgrading CluedIn, you need to make sure Helm is installed, connected to the correct Kubernetes cluster, and configured with the CluedIn Helm repository. The steps below walk you through verifying your Helm setup and updating to the latest charts. | To verify that Helm is installed correctly, run the following command in PowerShell: . helm version . | If Helm is installed, you’ll see the client version details (e.g., v3.x.x). | If you get a “command not found” error, this means that Helm is not installed or is not in your PATH. In this case, contact your system administrator to install and configure it properly. | . | To verify that you are connected to the cluster, run the following command in PowerShell: . helm config current-context . | You should see the cluster name (for example, aks-cluedin-eastus). | If you get an error similar to “current-context is not set”, contact your system administrator to ensure that the kubeconfig is configured correctly. | . | CluedIn publishes its latest Helm charts to a dedicated Helm repository. Verify that the repository is configured by running the following command: . helm repo list . | If CluedIn appears in the list, you are ready to use the repository. | If not, add the repository by running the following command: . Helm repo add https://cluedin-io.github.io/Charts Helm repo list . | . | Before performing any upgrade, always fetch the latest CluedIn charts: . Helm repo update . This ensures that you are deploying the most up-to-date configurations and fixes. | . ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#configure-helm",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#configure-helm"
  },"935": {
    "doc": "Prepare for the upgrade",
    "title": "Connect Lens or Freelens to your CluedIn cluster",
    "content": "This step is optional. It does not depend on the previous steps, you can perform it whenever appropriate. Lens or Freelens (depending on what you selected) connects to Kubernetes using your kubeconfig file. You can add clusters in two ways: . | Drop-in method – Place your kubeconfig into the system’s .kube folder (commonly located at ~/.kube/config). | UI method – Import or configure your cluster directly through the Lens (or Freelens) graphical interface. | . ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#connect-lens-or-freelens-to-your-cluedin-cluster",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade#connect-lens-or-freelens-to-your-cluedin-cluster"
  },"936": {
    "doc": "Prepare for the upgrade",
    "title": "Prepare for the upgrade",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/prepare-for-the-upgrade",
    
    "relUrl": "/paas-operations/upgrade/guide/prepare-for-the-upgrade"
  },"937": {
    "doc": "Pre-process rules",
    "title": "Pre-process rules",
    "content": "Pre-process rules are applied to the records that are already mapped (clues). You can create pre-process rules when you want to add tags or aliases to the records or send the records to quarantine. The difference between property rules and pre-process rules is that property rules are applied only to property values of the record while pre-process rules are applied to the whole record. So, if you want to apply some changes to multiple records, create a pre-process rule. Pre-process rules are applied to the clues after the property rules have been already applied. All pre-process rules are listed on the Pre-process rules tab of the data set. Prerequisites . To access pre-process rules, go to Administration &gt; Feature Flags and make sure that the following features are enabled: . | Data Set Pre-Process Rules . | Data Set Quarantine . | . To create a pre-process rule . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set. | Go to the Pre-process rules tab, and then select Add pre-process rule. | Enter the Display name of the rule. | In the Filter section, select whether you want to apply the rule to all records or to specific records. | If you want to apply the rule to specific records: . | Select a condition to determine which records will be affected by the rule. | Depending on what you selected in the previous step, specify the entity property or the vocabulary key that will be affected by the rule. | Select whether you want to apply the rule when the condition is met or when the condition is not met. The following screenshot demonstrates that all records that do not contain the job title will be sent to the quarantine. You can add only one filter to the rule. For example, if you want to quarantine all records that, in addition to the job title, do not contain the email address and the phone number, you need to create separate rules for each vocabulary key. | Select the Action that will be applied to the records. | . | In the lower-right corner, select Add rule. The rule is added to the table on the Pre-process rules tab. You can edit or delete the rule, if needed. The rule will be applied after you process the records. | . ",
    "url": "/integration/additional-operations-on-records/preprocess-rules",
    
    "relUrl": "/integration/additional-operations-on-records/preprocess-rules"
  },"938": {
    "doc": "Purview configuration guide",
    "title": "On this page",
    "content": ". | Basic Purview configuration | Purview integration features configuration | . In this guide, you will learn how to configure Purview integration in CluedIn. Make sure that you have completed all of the actions described in Purview pre-configuration guide. ",
    "url": "/microsoft-integration/purview/configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/configuration-guide#on-this-page"
  },"939": {
    "doc": "Purview configuration guide",
    "title": "Basic Purview configuration",
    "content": "Basic Purview configuration is required to establish connection between your CluedIn instance and your Microsoft Purview portal. To configure Purview in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | In Base URL, enter the base URL for HTTP calls to Purview API. To find the base URL, go to your Microsoft Purview account, select JSON View, and then copy the value of catalog. The base URL value can be in one of following formats: . | https://xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx-api.purview-service.microsoft.com/ – for tenant-level Purview account. | https://[purview-account-name].purview.azure.com/) – for a separate Purview account. | . | In Client ID, enter a unique identifier assigned to an application when you registered it in Microsoft Entra ID. This is the application that you created during the pre-configuration stage in Register an application and create a service principal. You can find this value on the application overview page, in Application (client) ID. | In Tenant ID, enter a unique identifier for your Microsoft Entra ID tenant in which your application is registered. This is the application that you created during the pre-configuration stage in Register an application and create a service principal. You can find this value on the application overview page, in Directory (tenant) ID. | In Client Secret, enter a string value that your application uses to prove its identity when requesting a token. This is the client secret that you created during the pre-configuration stage in Register an application and create a service principal. | In Collection Name, enter the ID of the collection in Purview for storing golden records from CluedIn. You can find the collection ID in the URL as described in Create a new collection. | Select Save. | . ",
    "url": "/microsoft-integration/purview/configuration-guide#basic-purview-configuration",
    
    "relUrl": "/microsoft-integration/purview/configuration-guide#basic-purview-configuration"
  },"940": {
    "doc": "Purview configuration guide",
    "title": "Purview integration features configuration",
    "content": "Once you have completed basic Purview configuration, you can enable and configure various Purview integration features: . | Sync data sources . | Azure Data Factory (ADF) pipeline automation . | Sync manual data entry to Purview . | Sync processing rules to Purview . | Sync clean projects to Purview . | Sync deduplication projects to Purview . | Sync streams to Purview . | Sync Purview glossaries to CluedIn glossaries . | Sync Purview glossaries to CluedIn vocabularies . | Sync data products . | . ",
    "url": "/microsoft-integration/purview/configuration-guide#purview-integration-features-configuration",
    
    "relUrl": "/microsoft-integration/purview/configuration-guide#purview-integration-features-configuration"
  },"941": {
    "doc": "Purview configuration guide",
    "title": "Purview configuration guide",
    "content": " ",
    "url": "/microsoft-integration/purview/configuration-guide",
    
    "relUrl": "/microsoft-integration/purview/configuration-guide"
  },"942": {
    "doc": "Microsoft Purview Integration",
    "title": "Microsoft Purview Integration",
    "content": "CluedIn integrates with your Microsoft Purview instance to synchronize your assets, glossaries, and lineage of your data journey in CluedIn. We have the following assumptions about the customers’ Microsoft Azure setup: . | Customers own only one Purview resource. | When ingesting data from ADF, the ADF pipeline should only do a one-to-one mapping from targeted file on Azure to API endpoint. | . The following table lists the available Purview packages and versions. | Package name | Package version | Source code | . | Azure-Extensions | 4.4.3 | Contact CluedIn for details | . | Azure-Extensions | 4.4.2 | Contact CluedIn for details | . | Azure-Extensions | 4.4.1 | Contact CluedIn for details | . | Azure-Extensions | 4.4.0 | Contact CluedIn for details | . | Azure-Extensions | 4.3.0 | Contact CluedIn for details | . | Azure-Extensions | 4.2.2 | Contact CluedIn for details | . | Azure-Extensions | 4.2.1 | Contact CluedIn for details | . | Azure-Extensions | 4.2.0 | Contact CluedIn for details | . | Azure-Extensions | 4.1.0 | Contact CluedIn for details | . | Azure-Extensions | 4.0.0 | Contact CluedIn for details | . ",
    "url": "/microsoft-integration/purview",
    
    "relUrl": "/microsoft-integration/purview"
  },"943": {
    "doc": "Roles",
    "title": "Roles",
    "content": "In this article, you will learn about claims and access levels, which are the main concepts that define roles. Roles are used to grant permissions to users to perform specific actions in CluedIn. These permissions are only granted to the modules within the platform, not to the data. For information on how to restrict access to data, see Data access. All roles are listed in Administration &gt; Roles. The following table provides a list of default CluedIn roles. In the Access status column, you can download a file with details of the default role’s access to the features in CluedIn. However, you can change the configuration of the role and extend its access to features according to your needs. | Role name | Description | Access status | . | DataArchitect | Responsible for designing an organization’s enterprise data strategy. | Download | . | DataCompliance | Responsible for daily operations around data compliance. | Download | . | DataComplianceAdministrator | Responsible for approving changes made by users with the DataCompliance role. | Download | . | DataGovernance | Responsible for monitoring and maintaining data quality. | Download | . | DataGovernanceAdministrator | Responsible for approving changes made by the users with the DataGovernance role. | Download | . | DataSteward | Responsible for cleaning data using the Clean and Prepare modules. | Download | . | DataStewardAdministrator | Responsible for approving changes made by the users with the DataSteward role. | Download | . | DeduplicationAdministrator | Responsible for creating and maintaining deduplication projects and merging the results back into the system. | Download | . | DeduplicationReviewer | Responsible for reviewing deduplication project results and approving groupings. | Download | . | Guest | User with minimal, read-only permissions. | Download | . | OrganizationAdmin | Administrator within the organization. | Download | . | OrganizationUser | User within the organization who can view all modules as read-only. | Download | . | ReportManager | User who can generate reports for compliance matters such as breach, subject request, and retention. | Download | . | User | User who can view all modules as read-only. | Download | . A role is a container for claims and access levels. A claim is the name of a specific feature or operation that can be performed in CluedIn. Most of the time, the name of the claim is the same as the name of the module in CluedIn. Learn more about claims in a dedicated article. An access level indicates the type of activity that can be performed with the claim. To get acquainted with the required claims and access levels for all actions in CluedIn, download this file. To view the role’s claims and access levels, select the role. In the first column, you can find the name of the section in CluedIn (a) and claims within that section (b). In the second column, you can find access levels (c) to each claim. Each role contains the same list of claims, but different access levels. We recommend that you familiarize yourself with the default CluedIn roles, so that you know which role to assign to users in your organization. If the default configuration is not suitable for you, you can change the access levels in the default roles or create your own roles. In CluedIn, the following access levels are used: . | None – no access to the claim. | Informed – read-only access to the claim. The user will be able to view all information within the claim, but will not be able to add, edit, or delete items within the claim. | Consulted – read and write access to the claim. The user will be able to add, edit, or delete items within the claim. | . The Responsible and Accountable access levels are reserved for upcoming versions. Currently, the activities represented by these access levels are the same as in the Consulted access level. All authorized users have a list of claims and access levels applied to them. If a user has multiple roles with different claim access levels, then the higher access level will be applied to the user. ",
    "url": "/administration/roles",
    
    "relUrl": "/administration/roles"
  },"944": {
    "doc": "SaaS installation guide",
    "title": "On this page",
    "content": ". | Submit a request and receive a license key | Subscribe to CluedIn SaaS in Azure Marketplace | Configure an account with CluedIn | Next steps | . In this article, you will learn how to subscribe to CluedIn SaaS in the Azure portal and how to configure your CluedIn account. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. ",
    "url": "/deployment/saas/installation-guide#on-this-page",
    
    "relUrl": "/deployment/saas/installation-guide#on-this-page"
  },"945": {
    "doc": "SaaS installation guide",
    "title": "Submit a request and receive a license key",
    "content": "CluedIn SaaS is currently available by invitation only. You can request an invitation here. After you submit an application, you’ll receive an email with the license key needed to complete the SaaS installation process. The CluedIn SaaS installation process consists of 2 parts: . | Subscribing to CluedIn SaaS in the Azure Marketplace. All details provided in this step are required by Microsoft. | Configuring an account with CluedIn. All details provided in this step are required by CluedIn. | . ",
    "url": "/deployment/saas/installation-guide#submit-a-request-and-receive-a-license-key",
    
    "relUrl": "/deployment/saas/installation-guide#submit-a-request-and-receive-a-license-key"
  },"946": {
    "doc": "SaaS installation guide",
    "title": "Subscribe to CluedIn SaaS in Azure Marketplace",
    "content": "Before you start the CluedIn SaaS installation process, make sure you meet all Azure requirements. Make sure that you have enabled marketplace purchases and configured the required user permissions (at least Contributor) for the subscription where you want to store the CluedIn SaaS application. For more information, see Enable marketplace purchases in Azure. To subscribe to CluedIn SaaS . | In the Azure Marketplace, find CluedIn Master Data Management – MDM (SaaS). | On the CluedIn Master Data Management SaaS page, select Get It Now. | On the page that opens, review basic information about CluedIn SaaS. Then, select Subscribe. | On the Basics tab, in the Project details section, do the following: . | Select a Subscription where you want to store CluedIn SaaS application in your Azure account. If you encounter an error stating that the subscription requires permission to make purchases, it means that you don’t have the required permission for the subscription. Contact your IT team to get the required permissions. For more information, see Enable marketplace purchases in Azure. | Select or create a Resource group where the SaaS subscription will be stored. If you create a new resource group, you need to choose the location for that resource group. The location of the resource group is not the same as the Azure region where CluedIn will be installed, it is just the location where subscription will be stored. You’ll select the Azure region when configuring your CluedIn account as described in the next section. | . | In the SaaS details section, enter a Name for the SaaS application. | (Optional) Select Next: Tags On the Tags tab, you can define tags to help you find your SaaS resource later. | Select Review + subscribe. | On the Review + subscribe tab, review the terms of use, contact information, and billing details. Billing is handled exclusively by Microsoft. | Select Subscribe. You’ll see the following message: Your SaaS subscription is in progress. This process takes a few minutes. Don’t close the window until it’s finished. After the subscription is completed, the Configure account now button becomes active, and you’ll receive an email from Microsoft requesting you to activate the new subscription. | Either in the email or in the Azure Marketplace, select the corresponding button to configure the account. If you selected the button in the Azure Marketplace, you can ignore the email. You’ll be redirected to the CluedIn account configuration page. | . ",
    "url": "/deployment/saas/installation-guide#subscribe-to-cluedin-saas-in-azure-marketplace",
    
    "relUrl": "/deployment/saas/installation-guide#subscribe-to-cluedin-saas-in-azure-marketplace"
  },"947": {
    "doc": "SaaS installation guide",
    "title": "Configure an account with CluedIn",
    "content": "To configure an account with CluedIn, you need a valid license key. You can find a license key in an email from CluedIn if you have previously requested SaaS application. If you don’t have a license key, request it here. To configure a CluedIn account . | On the CluedIn account configuration page, enter a license key, and then select Validate your license key. | Provide basic information for setting up your CluedIn account: . | Enter your Email. | Enter an Organization Name to create a link to your CluedIn instance. | Enter a Password for signing in to your CluedIn instance. | Select an Azure Region where you want CluedIn to be installed. | Select Complete configuration. | . After the configuration is completed, you’ll receive an email from Microsoft notifying you that the configuration was successful. Also, you can view your SaaS subscription in the Azure portal. | . Our automatic installer will start preparing your isolated environment. Once it is ready, you’ll receive an email from CluedIn with instructions on how to get started. It can take up to 30 minutes to receive an email. ",
    "url": "/deployment/saas/installation-guide#configure-an-account-with-cluedin",
    
    "relUrl": "/deployment/saas/installation-guide#configure-an-account-with-cluedin"
  },"948": {
    "doc": "SaaS installation guide",
    "title": "Next steps",
    "content": "After configuring an account, you will receive an email with a link to your CluedIn instance. To sign in, enter the email and password that you provided when configuring your CluedIn account. To enable SSO (Entra ID) in SaaS, please refer to our SSO configuration guide. Useful links: . | Getting access – learn how to sign in to CluedIn. | Geeting started – get acquainted with the main CluedIn features. | Pricing – learn about our pricing options (pay-as-you-go and committed deal). | . ",
    "url": "/deployment/saas/installation-guide#next-steps",
    
    "relUrl": "/deployment/saas/installation-guide#next-steps"
  },"949": {
    "doc": "SaaS installation guide",
    "title": "SaaS installation guide",
    "content": " ",
    "url": "/deployment/saas/installation-guide",
    
    "relUrl": "/deployment/saas/installation-guide"
  },"950": {
    "doc": "Query data using GraphQL and Python SDK",
    "title": "On this page",
    "content": ". | GraphQL Search API . | Simple GraphQL search query | GraphQL search query with variables and cursor | . | CluedIn Python SDK . | Installation and initialization | Running a GraphQL query | Getting the next page | Using a generator | Using automatic pagination | Using the search method | Using the search method with a subset of data | . | GraphQL Actions | . ",
    "url": "/playbooks/data-engineering-playbook/search#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/search#on-this-page"
  },"951": {
    "doc": "Query data using GraphQL and Python SDK",
    "title": "GraphQL Search API",
    "content": "CluedIn provides a powerful GraphQL API that is very helpful in almost every interaction with CluedIn (one of a few exceptions is Ingestion Endpoints that are not GraphQL). You can read about CluedIn GraphQL in a separate article: documentation.cluedin.net/consume/graphql. When you open the Consume section in your CluedIn instance, you will find a GraphQL playground where you can run GraphQL queries. In our example, we have some /Duck entities (from the DuckTales). To find them, I can run a query like this: . Simple GraphQL search query . { search(query:\"+entityType:/Duck\") { entries { id name entityType } } } . The query will return me the top 20 of the /Duck entities. The query parameter tells the API to filter the response by a given business domain (previously entity type). You can also specify the entity properties you want to get in the payload: id, name, and entityType. GraphQL search query with variables and cursor . We can sophisticate the query a little so that it will take parameters from the GraphQL variables: . query ($query: String, $pageSize: Int) { search( query: $query pageSize: $pageSize sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id name entityType } } } . Variables: . { \"query\": \"+entityType:/Duck\", \"pageSize\": 10000 } . Here are a few things to notice in this query: . | query ($query: String, $pageSize: Int) - we defined a query with parameters. We can also give this query a name: query searchEntities($query: String, $pageSize: Int) | sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} - it’s important to sort by a unique field to get predictable results when you page data. | cursor - we ask CluedIn to return a special value that we can pass to our next query to get the next page of results. | \"pageSize\": 10000 - By default, the page size is 20, so if you have millions of entities, you will get only the first 20. Setting the page size to its maximum value (10000) will decrease the number of requests you send to CluedIn API, but there are situations when you only want to get a few entities from the top, and a query with a smaller page size will be faster. | . ",
    "url": "/playbooks/data-engineering-playbook/search#graphql-search-api",
    
    "relUrl": "/playbooks/data-engineering-playbook/search#graphql-search-api"
  },"952": {
    "doc": "Query data using GraphQL and Python SDK",
    "title": "CluedIn Python SDK",
    "content": "Installation and initialization . You can use any programming language to send a GraphQL request to CluedIn and get the data back. Let’s explore how you can do it in Python. First of all, you will need to install the latest version of CluedIn Python SDK: . %pip install cluedin . Let’s import it then together with Pandas (we will use Pandas to load data in DataFrames): . import pandas as pd import cluedin . You need an API token; you can copy or create a new one by going to “API Tokens” under “Administration” in CluedIn. Now, in our example, CluedIn is installed at https://foobar.contoso.com/, so we need to initialize a Context for our CluedIn instance by providing org_name (foobar), domain (contoso.com), and the access_token (the one you copied from CluedIn UI): . ctx = cluedin.Context.from_dict({ 'domain': 'contoso.com', 'org_name': 'foobar', 'access_token': '{paste_your_token_here}' }) . Running a GraphQL query . We can now run GraphQL queries from Python code: . query = \"\"\" query searchEntities($query: String, $pageSize: Int) { search( query: $query pageSize: $pageSize sort: FIELDS, sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id name entityType } } } \"\"\" variables = { 'query': '+entityType:/Duck', 'pageSize': 3 } cluedin.gql.gql(ctx, query=query, variables=variables) . The result is top three entities (because we use the page size = 3 for demo purpose), and we also get a cursor that we can use to get the next page: . {'data': {'search': {'cursor': 'ewAiAFAAYQBnAGUAIgA6ADEALAAiAFAAYQBnAGUAUwBpAHoAZQAiADoAMwAsACIAQwBvAG0AcABvAHMAaQB0AGUAQQBmAHQAZQByACIAOgB7AH0ALAAiAFMAZQBhAHIAYwBoAEEAZgB0AGUAcgAiADoAWwAiADYAMwA1ADMAOAAzAGEAOQAtADkAYwA3ADUALQA1AGQANgAxAC0AOABmADIAYgAtAGYAZQA0ADkANgBmAGQAOAAyAGIAZQA3ACIALAAiADYAMwA1ADMAOAAzAGEAOQAtADkAYwA3ADUALQA1AGQANgAxAC0AOABmADIAYgAtAGYAZQA0ADkANgBmAGQAOAAyAGIAZQA3ACIAXQB9AA==', 'entries': [{'id': '145afb55-4e78-5dad-b208-633b5b6d19cf', 'name': 'Donald Duck', 'entityType': '/Duck'}, {'id': '17bad60e-6782-5ae5-84bf-7efe05e78e58', 'name': 'Jake McDuck', 'entityType': '/Duck'}, {'id': '635383a9-9c75-5d61-8f2b-fe496fd82be7', 'name': 'Dewey Duck', 'entityType': '/Duck'}]}}} . Getting the next page . We can change our code to pass the cursor as a parameter: . query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query cursor: $cursor pageSize: $pageSize sort: FIELDS, sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id name entityType } } } \"\"\" variables = { 'query': '+entityType:/Duck', 'pageSize': 3 'cursor': 'ewAiAFAAYQBnAGUAIgA6ADEALAAiAFAAYQBnAGUAUwBpAHoAZQAiADoAMwAsACIAQwBvAG0AcABvAHMAaQB0AGUAQQBmAHQAZQByACIAOgB7AH0ALAAiAFMAZQBhAHIAYwBoAEEAZgB0AGUAcgAiADoAWwAiADYAMwA1ADMAOAAzAGEAOQAtADkAYwA3ADUALQA1AGQANgAxAC0AOABmADIAYgAtAGYAZQA0ADkANgBmAGQAOAAyAGIAZQA3ACIALAAiADYAMwA1ADMAOAAzAGEAOQAtADkAYwA3ADUALQA1AGQANgAxAC0AOABmADIAYgAtAGYAZQA0ADkANgBmAGQAOAAyAGIAZQA3ACIAXQB9AA==' } cluedin.gql.gql(ctx, query=query, variables=variables) . The result is the next three entities: . {'data': {'search': {'cursor': 'ewAiAFAAYQBnAGUAIgA6ADIALAAiAFAAYQBnAGUAUwBpAHoAZQAiADoAMwAsACIAQwBvAG0AcABvAHMAaQB0AGUAQQBmAHQAZQByACIAOgB7AH0ALAAiAFMAZQBhAHIAYwBoAEEAZgB0AGUAcgAiADoAWwAiADkAMwBiADkAMgA4ADMANQAtADkANgBmADIALQA1ADYAYQA5AC0AOQA4AGMAMAAtAGMAOAA0ADgAMgAzADYANQAyADEAYQA5ACIALAAiADkAMwBiADkAMgA4ADMANQAtADkANgBmADIALQA1ADYAYQA5AC0AOQA4AGMAMAAtAGMAOAA0ADgAMgAzADYANQAyADEAYQA5ACIAXQB9AA==', 'entries': [{'id': '6ae43a44-81b4-5fd7-9c7b-47cb24d407ea', 'name': 'Angus McDuck', 'entityType': '/Duck'}, {'id': '9353b703-13d8-59a1-886c-f40b95283c06', 'name': 'Hortense McDuck', 'entityType': '/Duck'}, {'id': '93b92835-96f2-56a9-98c0-c848236521a9', 'name': 'Matilda McDuck', 'entityType': '/Duck'}]}}} . Using a generator . But what if you want to avoid manually passing a new cursor to every new call? You can just use the cluedin.gql.entries method, and it will return you a Generator that you can convert to a list or just iterate as you wish: ... # this is where you need a smaller page size # if you don't want to iterate to the end variables = { 'query': '+entityType:/Duck', 'pageSize': 2 } generator = cluedin.gql.entries(ctx, query=query, variables=variables) print(next(generator)) print(next(generator)) . Result: . {'id': '145afb55-4e78-5dad-b208-633b5b6d19cf', 'name': 'Donald Duck', 'entityType': '/Duck'} {'id': '17bad60e-6782-5ae5-84bf-7efe05e78e58', 'name': 'Jake McDuck', 'entityType': '/Duck'} . Using automatic pagination . You can also load all entities in a DataFrame, in this case, it makes sense using the maximum page size (10000) to reduce the number of calls to the server: . query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query cursor: $cursor pageSize: $pageSize sort: FIELDS, sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id name entityType } } } \"\"\" variables = { 'query': '+entityType:/Duck', 'pageSize': 10_000 } print(pd.DataFrame(cluedin.gql.entries(ctx, query=query, variables=variables))) . Result: . id name entityType 0 145afb55-4e78-5dad-b208-633b5b6d19cf Donald Duck /Duck 1 17bad60e-6782-5ae5-84bf-7efe05e78e58 Jake McDuck /Duck 2 635383a9-9c75-5d61-8f2b-fe496fd82be7 Dewey Duck /Duck 3 6ae43a44-81b4-5fd7-9c7b-47cb24d407ea Angus McDuck /Duck 4 9353b703-13d8-59a1-886c-f40b95283c06 Hortense McDuck /Duck 5 93b92835-96f2-56a9-98c0-c848236521a9 Matilda McDuck /Duck 6 a388a77d-7d43-51d1-87b2-efb4f854b5ad Fergus McDuck /Duck 7 b2fb05cb-e806-5088-955b-2ff3f9261236 Scrooge McDuck /Duck 8 b8fc5baf-b679-5e26-abb5-50ca77467992 Huey Duck /Duck 9 cd8fe1dd-5637-5037-931e-f8bf1a15c0b4 Della Duck /Duck 10 f5bf5d66-5698-515a-800e-9d778d916dcd Louie Duck /Duck . Using the search method . Starting from CluedIn Python SDK 2.5.0, you can shrink the code above to one line and get almost the same result. The difference is that it will also return you all codes and properties of entities, and you don’t have to copy and paste the same GraphQL query every time you want to query some data: . # this will return all the queried entities with all properties and codes print(pd.DataFrame(cluedin.gql.search(ctx, '+entityType:/Duck'))) . Using the search method with a subset of data . Finally, if you only want a subset of data, you can use the itertools.islice, but then remember to set a smaller page_size to not query more data than you need: . from itertools import islice # gets a generaror that queries entities from the server by three gen = cluedin.gql.search(ctx, '+entityType:/Duck', page_size=3) # wrap in an iterator that stops after three iterations iter = islice(gen, 3) # convert to DataFrame df = pd.DataFrame(iter) print(df) . Or simply: . pd.DataFrame(itertools.islice(cluedin.gql.search(ctx, '+entityType:/Duck', 3), 3)) . ",
    "url": "/playbooks/data-engineering-playbook/search#cluedin-python-sdk",
    
    "relUrl": "/playbooks/data-engineering-playbook/search#cluedin-python-sdk"
  },"953": {
    "doc": "Query data using GraphQL and Python SDK",
    "title": "GraphQL Actions",
    "content": "You can add GraphQL Actions when running GraphQL queries in the Python SDK. Actions are a way to run commands in bulk, such as processing, enriching, or deleting entities. For example, if you take the previous GraphQL query and add an actions field to it, you can delete, enrich or process entities entities in bulk. Here is an example of bulk enrichment: . query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query cursor: $cursor pageSize: $pageSize sort: FIELDS, sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id name entityType actions { enrich } } } } . And here is another example of how to use Actions in the Python SDK to delete entities (please note that this is a destructive operation and should be used with caution): delete_entities.py. Here is a list of available actions you can use: . | deleteEntity - deletes an entity. | postProcess - reprocesses an entity. | enrichEntity - enriches an entity. | . ",
    "url": "/playbooks/data-engineering-playbook/search#graphql-actions",
    
    "relUrl": "/playbooks/data-engineering-playbook/search#graphql-actions"
  },"954": {
    "doc": "Query data using GraphQL and Python SDK",
    "title": "Query data using GraphQL and Python SDK",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/search",
    
    "relUrl": "/playbooks/data-engineering-playbook/search"
  },"955": {
    "doc": "SSO",
    "title": "On this page",
    "content": ". | Overview of SSO for CluedIn | Register an application in the Azure portal . | Create a client secret | Update authentication | Granting external users access via single sign-on | Add API permissions for Microsoft Graph | Expose an API | Map Microsoft Entra application roles to CluedIn roles | CluedIn roles | . | Create Kubernetes secret and enable SSO via Helm | . In this article, you will learn how to configure single sign-on (SSO) for CluedIn using Microsoft Entra group-managed role membership. The steps described here apply to both PaaS and SaaS. However, for SaaS users, the final step must be completed by our support team. ",
    "url": "/deployment/infra-how-tos/configure-sso#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-sso#on-this-page"
  },"956": {
    "doc": "SSO",
    "title": "Overview of SSO for CluedIn",
    "content": "SSO for CluedIn can be enabled in one of the following modes: . | SSO with local CluedIn role membership management – all users from the directory can sign in to the CluedIn application. After the user signs in for the first time, CluedIn roles can be assigned in the usual way in the CluedIn UI. | SSO with Microsoft Entra group-managed role membership – Microsoft Entra application roles will need to be created within your Azure application registration so they can be mapped to your Microsoft Entra groups or users. This mode requires Automatic Role Synchronization to be enabled in the Administration Settings page in CluedIn. | . Configuring SSO for CluedIn using Microsoft Entra involves two main steps: . | Register an application in the Azure portal . | Create Kubernetes secret and enable SSO via Helm . | . Important! Before configuring SSO, make sure that you have configured DNS and TLS. ",
    "url": "/deployment/infra-how-tos/configure-sso#overview-of-sso-for-cluedin",
    
    "relUrl": "/deployment/infra-how-tos/configure-sso#overview-of-sso-for-cluedin"
  },"957": {
    "doc": "SSO",
    "title": "Register an application in the Azure portal",
    "content": "Registering your application establishes a trust relationship between your application and the Microsoft identity platform. The trust is unidirectional: your application trusts the Microsoft identity platform, and not the other way around. After you create the application, it cannot be moved between different tenants. Throughout the documentation, we’ll be working with ficticious domain yourdomain.com and the two subdomains: . | app – references the back-end application. By default, it is just app, but can be changed in the values file. | cluedin – references the main CluedIn URL you would access on a daily basis. | . To register an application in the Azure portal . | In the Azure portal, go to the tenant in which you want to register the application. | Search for and select Microsoft Entra ID. | Under Manage, select App registrations &gt; New registration. | Enter a display Name for your application. | Select the Supported account types that can use the application. | Update Redirect URI to Web and set the URL to the app subdomain of your instance with the path of /auth/signin-oidc. e.g. https://app.domain.com/auth/signin-oidc . | Select Register. When the registration finishes, the Azure portal displays the Overview pane of the application registration. Here you can see the Application (client) ID. This value uniquely identifies your application. Make note of this ID as you’ll need it to enable SSO via Helm. | . After you register the application, complete the following steps: . | Create a client secret . | Update authentication . | Add API permissions . | Expose an API . | Grant external users access via single sign-on . | Map Microsoft Entra application roles to CluedIn roles . | . Create a client secret . A client secret is used to configure CluedIn to communicate with Microsoft Entra. To create client secret . | In the Azure portal, in App registrations, select your application. | Select Certificates &amp; secrets &gt; Client secrets &gt; New client secret. | Add a description for your client secret and an expiration date. | Select Add. | Copy and save the secret value because this will be used later in your CluedIn Helm configuration. | . For more information about the client secret, see Microsoft documentation. Update authentication . | Under Manage select Authentication . | Under Front-channel logout URL, add a logout URL for your application. e.g. https://app.yourdomain.com/logout . | In the Implicit grant and hybrid flows section, select the ID tokens checkbox. | At the bottom of the page, select Save. | . For more information about redirect URIs, see Microsoft documentation. Granting external users access via single sign-on . When enabling single sign-on to your CluedIn application, it is possible to also allow external guests (domains) to sign in and be granted access to the application. This will also be dependant on your network setup. To grant SSO access to external users . | Select Authentication. | Scroll down to Supported account types and change the mode to Accounts in any organizational directory (Any Microsoft Entra ID tenant - Multitenant). | Select Save. | . This will allow guests invited to your Microsoft Entra ID to be able to use this application for single sign-on. The process remains the same whereby the user signs in like you normally would. Add API permissions for Microsoft Graph . When you register an application in the Azure portal, the Microsoft Graph API with the User.Read permission is added automatically. You need to add additional permissions for Microsoft Graph. To add API permissions for Microsoft Graph . | Select API permissions. | In the Configured permissions section, click on the existing Microsoft Graph entry. | In the right pane, select the following permissions: email, offline_access, openid, and profile. At the bottom of the pane, select Update permissions. The API permissions for Microsoft Graph are updated. | . For more information about API permissions, see Microsoft documentation. Expose an API . You need to register a web API with the Microsoft identity platform and expose it to the client app by adding a scope. By registering your web API and exposing it through scope, you can provide permissions-based access to authorized users that access your API. To expose the API . | Select Expose an API. | In the Scopes defined by this API section, select Add a scope. | For the first-time setup, you will need to specify the Application ID URI. For this, leave it as the default selection which will be a randomly generated GUID. | Select Save and continue. | Specify the following scope attributes: . | Scope name: user_impersonation . | Who can consent: Admins and Users . | Admin consent display name: CluedIn SSO . | Admin consent description: CluedIn SSO . | . For detailed instructions on how to configure an app to expose web API, see Microsoft documentation. | Select Add scope when done. | . Map Microsoft Entra application roles to CluedIn roles . After you have created your application registration and attached it to your CluedIn instance, you can create the application roles on the Microsoft Entra side. These roles will be translated into the CluedIn platform roles and assigned to the users as they sign in to the application when the Automatic Role Synchronization option is enabled in the CluedIn settings. If you change the role of the user after they sign in, they will need to sign out and sign back in for the new role to take affect. To map Microsoft Entra application roles to CluedIn roles . | Select App roles. | On the menu, select Create app role. | Enter the details of the role. See CluedIn roles for recommended values. | Select Apply to save your changes. The role is added to the App roles list. | Repeat steps 3-5 to add all roles listed below. | . In the CluedIn application, you can find all CluedIn roles by navigating to Administration &gt; Roles. Any changes made in the application registration will be saved in your Azure subscription. We do not impose strict requirements on how app roles are set up, so you can follow your organization’s internal requirements. By default, any user will be able to sign into CluedIn but won’t have a role assigned. If you would prefer to reject the user from signing in, you can set this under the Enterprise application of the same name of the app registration. Within properties is a toggle switch called Assignment required? which can be used for this purpose. You can then add groups and role assignments on the left-hand blade labelled Users and groups. CluedIn roles . The following table provides a list of the CluedIn application roles and recommended values to use when creating your Microsoft Entra application roles with your application registration. | Display name | Value | Description | . | Data Governance Administrator | DataGovernanceAdministrator | Role responsible for approving changes made by Data Governance users | . | Data Compliance | DataCompliance | Role responsible for daily operations around data compliance | . | Data Steward | DataSteward | Role dedicated to cleaning data using Clean and Prepare modules | . | Data Compliance Administrator | DataComplianceAdministrator | Role responsible for approving changes made by Data Compliance users | . | Guest | Guest | Guest User with minimal, read-only permissions | . | User | User | User who can view all modules as read-only | . | Data Architect | DataArchitect | Role responsible for designing an Organizations enterprise data strategy | . | Deduplication Reviewer | DeduplicationReviewer | Role responsible for reviewing Deduplication Project results and approving the groupings | . | Organization User | OrganizationUser | User within an Organization who can view all modules as read-only | . | Data Governance | DataGovernance | Role responsible for monitoring and maintaining data quality | . | Report Manager | ReportManager | User who can generate reports for compliance matters such as breach, subject request, and retention | . | Organization Admin | OrganizationAdmin | Administrator within the Organization | . | Deduplication Administrator | DeduplicationAdministrator | Role responsible for creating and maintaining Deduplication Projects and merging the results back into the system | . | Data Steward Administrator | DataStewardAdministrator | Role responsible for approving changes made by Data Stewards | . ",
    "url": "/deployment/infra-how-tos/configure-sso#register-an-application-in-the-azure-portal",
    
    "relUrl": "/deployment/infra-how-tos/configure-sso#register-an-application-in-the-azure-portal"
  },"958": {
    "doc": "SSO",
    "title": "Create Kubernetes secret and enable SSO via Helm",
    "content": "(For SaaS users) Once you have completed all the previous steps, please reach out to CluedIn support at support@cluedin.com to complete the final step. After you complete the Azure application registration and app roles configuration, this will then need to be enabled on the CluedIn platform. As this touches the inner workings of Kubernetes, if you prefer, a member of CluedIn will be able to facilitate these steps for you. Prerequisites . | You should be comfortable working in either PowerShell or bash terminal via Azure Cloud Shell. | You should be connected to your AKS cluster. See Connect to CluedIn cluster for detailed instructions. | Your Helm repository is set up. | . If you have any questions, you can request CluedIn support by sending an email to support@cluedin.com (or reach out to your delivery manager if you have a committed deal). Once you have connected to your cluster and you are able to issue commands using kubectl and Helm, complete the following procedure to enable SSO for CluedIn. To create Kubernetes secret and enable SSO via Helm . | Create a new Kubernetes secret with your Azure app registration secret by running the following command: . kubectl create secret generic \"myorg-sso-cs\" -n cluedin --from-literal=clientSecret=\"Xcy8Q~.....\" . In the command, replace Xcy8Q~….. with your Azure app registration secret value. | In Azure Cloud Shell, run the following command to create a new empty file: . nano Cluedin-SSO-Config.yaml . | In the file, paste the following configuration: . apiVersion: api.cluedin.com/v1 kind: Feature metadata: name: myorg-sso spec: enableSso: clientId: \"0cXXXX-XXXX-XXXX-XXX-XXXXXX575\" organizationName: \"myorg\" clientSecretName: \"myorg-sso-cs\" clientSecretKey: \"clientSecret\" . | Change the name and organizationName values to match your CluedIn organization/tenant name. | Change the clientId value to the client ID from your Azure app registration. | Save the file and apply your SSO configuration by running the following command in Azure Cloud Shell: . kubectl apply -n cluedin -f Cluedin-SSO-Config.yaml . | Verify that the SSO feature has been enabled successfully by running the following command in Azure Cloud Shell: . kubectl get features -n cluedin . | . If your SSO feature has been successfully applied, you should see something similar to the screenshot below. If the Phase is not in the Active state, wait for 5 minutes and run the command again. If nothing changes, reach out to CluedIn support at support@cluedin.com for help in enabling your SSO. ",
    "url": "/deployment/infra-how-tos/configure-sso#create-kubernetes-secret-and-enable-sso-via-helm",
    
    "relUrl": "/deployment/infra-how-tos/configure-sso#create-kubernetes-secret-and-enable-sso-via-helm"
  },"959": {
    "doc": "SSO",
    "title": "SSO",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-sso",
    
    "relUrl": "/deployment/infra-how-tos/configure-sso"
  },"960": {
    "doc": "How to tag records with data quality issues",
    "title": "How to tag records with data quality issues",
    "content": "In this article, you will learn how to tag records with data quality issues using golden record rules and CluedIn Copilot. We will use invalid email address as an example of a data quality issue. To begin with, we have ingested, mapped, and processed a file containing contact data. Some records include invalid email addresses. Note that the email addresses in rows 1–3 violate common email address formatting rules. To tag records with such data quality issues, create a data part rule and add an action to tag records if the email value does not match the acceptable patten of a common regular expression (for example, ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}$). You can use the Conditions section of the action to specify the acceptable pattern for the vocabulary key value. Alternatively, you can use CluedIn Copilot to create a data part rule. First, ask Copilot to generate a common regular expression that would check for valid email format. For example, you can use the following prompt. Write a common regular expression that would check for valid email format and shortly describe it. Then, instruct Copilot to create a data part rule using the prompt similar to the following. Create a golden record rule named \"User email format validation\" for the TrainingContact entity type. This rule should use CluedIn AI action on the trainingcontact.email vocabulary key to tag with \"Invalid email format\" any email with a pattern that doesn't match the above regex. Next, activate and re-process the rule. To verify that the rule has been applied, go to search and use the Tags filter. As a result, you will see all records where the email value is in the invalid format. Records with invalid email addresses contain the corresponding tag. When the records with data quality issues are tagged, you can then create a clean project to fix such issues. To do it, in the upper-right corner of the search results page, open the three-dot menu, and select Clean. ",
    "url": "/kb/how-to-tag-records-with-data-quality-issues",
    
    "relUrl": "/kb/how-to-tag-records-with-data-quality-issues"
  },"961": {
    "doc": "Vocabulary",
    "title": "On this page",
    "content": ". | Vocabulary overview | Vocabulary details page | Create a vocabulary | Manage a vocabulary | . In this article, you will learn how to create and manage vocabularies to enhance the efficiency and organization of metadata in CluedIn. ",
    "url": "/management/data-catalog/vocabulary#on-this-page",
    
    "relUrl": "/management/data-catalog/vocabulary#on-this-page"
  },"962": {
    "doc": "Vocabulary",
    "title": "Vocabulary overview",
    "content": "A vocabulary is a framework that defines how metadata is stored and organized within the system. The primary purpose of vocabulary is to hold vocabulary keys. A well-defined vocabulary is essential for maintaining data consistency, accuracy, and usability across an organization. By providing a standardized framework, a vocabulary contributes to effective data integration, improved decision making, and streamlined operations. It ensures that all stakeholders are working with consistent and reliable master data definitions and structures. In CluedIn, all vocabularies are stored in Management &gt; Data Catalog &gt; View All Vocabularies. ",
    "url": "/management/data-catalog/vocabulary#vocabulary-overview",
    
    "relUrl": "/management/data-catalog/vocabulary#vocabulary-overview"
  },"963": {
    "doc": "Vocabulary",
    "title": "Vocabulary details page",
    "content": "On the vocabulary details page, you can view relevant information about a vocabulary and take other actions to manage a vocabulary. Configuration . This tab contains general information about the vocabulary, including: . | Vocabulary name – a user-friendly identifier of the vocabulary. | Primary business domain – a business domain linked with the vocabulary. | Source – a source system that indicates the origin of data within the vocabulary. | Key prefix – a namespace that is added before vocabulary key names for consistent naming, efficient searching, and data filtering. | Description – a summary that explains the purpose of the vocabulary. You can add rich formatting to the description, such as bolding, underlining, or italicizing text. | . You can edit any aspect of the vocabulary configuration, with the exception of the key prefix. Usage . This tab provides the global view of vocabulary usage in the system: number of golden records, streams, glossary terms, and rules where the vocabulary is used. By selecting the respective button, you can access a list of streams, glossary terms, and rules, along with links to the corresponding elements in the system. Owners . This tab contains a list of users who can manage the vocabulary (Vocabulary Owners). You can add or remove vocabulary owners if necessary. Vocabulary keys . This tab contains groups of vocabulary keys associated with the vocabulary. You can search for specific keys or use data type and classification filters to explore the available metadata within the vocabulary. For information on how to manage vocabulary keys, see Vocabulary keys. Pending changes . This tab contains tasks for reviewing changes to the vocabulary submitted by users who are not Vocabulary Owners. Audit log . This tab contains a detailed history of changes to the vocabulary, such as: . | Create a vocabulary | Added user to owners | Create a vocabulary key | Delete a vocabulary key | . ",
    "url": "/management/data-catalog/vocabulary#vocabulary-details-page",
    
    "relUrl": "/management/data-catalog/vocabulary#vocabulary-details-page"
  },"964": {
    "doc": "Vocabulary",
    "title": "Create a vocabulary",
    "content": "Depending on the selected data modeling approach, you can create a vocabulary in two ways: . | Automatically – this option is part of the data-first approach. When creating a mapping for a data set, you have the option to enter the name of a new vocabulary. CluedIn will then automatically suggest the key prefix and generate the vocabulary. Once the mapping is created, you can then open the vocabulary and make any necessary adjustments. | Manually – this option is part of the model-first approach, which assumes that you need to create a vocabulary before using it in the mapping for a data set. The following procedure outlines the steps to manually create a vocabulary. | . To create a vocabulary . | On the navigation pane, go to Management &gt; Data Catalog. Then, select View All Vocabularies. | Select Create Vocabulary. | Enter the name of the vocabulary. | Find and select the primary business domain that will most likely use the vocabulary. | (Optional) Find and select the source of the vocabulary to indicate where the data in the vocabulary comes from. | Enter the key prefix of the vocabulary that will be added before vocabulary key names. The key prefix cannot start with a number, but it can include numbers in other positions. Additionally, the key prefix must not contain special characters or spaces. Note that it is not possible to edit the key prefix once the vocabulary has been created. | (Optional) Enter the description of the vocabulary. | Select Create. The vocabulary page opens, where you can view and manage vocabulary details. | . ",
    "url": "/management/data-catalog/vocabulary#create-a-vocabulary",
    
    "relUrl": "/management/data-catalog/vocabulary#create-a-vocabulary"
  },"965": {
    "doc": "Vocabulary",
    "title": "Manage a vocabulary",
    "content": "Once the vocabulary is created, you can edit its configuration based on your requirements to ensure the maintenance of organized and consistent metadata. Only Vocabulary Owners and Administrators can edit the vocabulary configuration. When you’re editing a vocabulary configuration, you can change almost all of its aspects: name, primary business domain, source, and description. To edit vocabulary configuration . | In the upper-right corner of the vocabulary, select Edit. | Make the needed changes, and then select Save and confirm your choice. | . ",
    "url": "/management/data-catalog/vocabulary#manage-a-vocabulary",
    
    "relUrl": "/management/data-catalog/vocabulary#manage-a-vocabulary"
  },"966": {
    "doc": "Vocabulary",
    "title": "Vocabulary",
    "content": " ",
    "url": "/management/data-catalog/vocabulary",
    
    "relUrl": "/management/data-catalog/vocabulary"
  },"967": {
    "doc": "Work with CluedIn Copilot",
    "title": "On this page",
    "content": ". | Copilot overview | Copilot skills . | Data set skills | Rule-related skills | Data catalog skills (vocabulary and vocabulary key) | Deduplication-related skills | Stream and export target skills | Clean-related skills | Hierarchy skills | Glossary skills | Other skills | . | . In this article, you will learn about Copilot skills that are available in CluedIn and find examples of prompts for each skill. ",
    "url": "/microsoft-integration/copilot-integration/work-with-copilot#on-this-page",
    
    "relUrl": "/microsoft-integration/copilot-integration/work-with-copilot#on-this-page"
  },"968": {
    "doc": "Work with CluedIn Copilot",
    "title": "Copilot overview",
    "content": "CluedIn Copilot is an AI assistant designed to help you with day-to-day data management tasks through conversational prompts in any language. You can open the Copilot chat from any place in CluedIn by clicking on the message icon in the lower-right corner of the page. The Copilot pane contains all chats marked with color circles. If you want to rename or delete a chat, select Copilot next to the search bar. CluedIn Copilot currently has 40+ specific skills (also called functions). To get a list of all these skills, use prompts similar to the following: . | What functions can you call? . | What are all the functions that I can do? . | . You’ll find a description of each skill in Copilot skills. To enhance your efficiency in interacting with CluedIn Copilot, you can chain different skills together. Instead of creating separate prompts for actions like creating, activating, and reprocessing a rule, you can combine these tasks into a single prompt. Create a rule that tags all records with customer.country set to 'Norway' as 'Nordic', then activate and reprocess the rule. Another example of chaining multiple skills together is using the anomaly detection skill followed by creating rules to mitigate it. Instead of entering two separate prompts, you can simply enter one combined prompt. Find anomalies in training.country vocabulary keys. For each anomaly, create a rule (one per anomaly), which would best mitigate the anomaly. All rules are related only to /TrainingContact entity type. CluedIn Copilot is multilingual, so you can activate all skills in any language you want, it doesn’t have to be English. In most cases, CluedIn Copilot knows your current location in the platform. You can write prompts referring to what is currently displayed on the page, such as this data set, this vocabulary key, and so on. You can combine general knowledge with CluedIn skills in one prompt. Can you create a golden record rule that tags Company records where the company.state is not one of the Australian states. Note that CluedIn Copilot does not offer a preview feature for its actions; it directly executes them. ",
    "url": "/microsoft-integration/copilot-integration/work-with-copilot#copilot-overview",
    
    "relUrl": "/microsoft-integration/copilot-integration/work-with-copilot#copilot-overview"
  },"969": {
    "doc": "Work with CluedIn Copilot",
    "title": "Copilot skills",
    "content": "This section contains all CluedIn Copilot skills grouped by categories or modules. Each skill contains one or several prompt examples to give you an understanding of how to use its functionality effectively. Sometimes you may encounter issues with CluedIn Copilot where it does not understand your requests as expected. In such cases, as with any generative AI solution, please start over and create a new chat. Data set skills . CluedIn Copilot can analyze a data set to provide general overview, suggest possible mapping options, and create a mapping for the data set. | Copilot function | Description | Prompt example | . | DescribeDataSet | Provides general information about a data set: column description, possible validation checks, data quality issues, and so on.If you are on the data set page, you can just tell the Copilot to describe this data set. Otherwise, you can refer to the data set by its ID, which you can find in the URL of the page. | Tell me a bit about this data set.Describe this data set.Describe the data set with ID 443259BB-1D17-4078-A069-7ECAD418BA19. | . | SuggestDatasetMapping | Provides suggestions on how to map a data set to an existing entity type and vocabulary. The suggested mapping can be used to define how the data set columns should be transformed and linked to the specified vocabulary.If you are on the data set page, you can just tell the Copilot to suggest mapping for this data set. Otherwise, you can refer to the data set by its ID, which you can find in the URL of the page. | Can you suggest a mapping from this data set to the Employee vocabulary?Can you suggest how to map this data set to the Company vocabulary? | . | CreateDatasetMapping | Create a mapping from a data set to an existing business domain and vocabulary. Note that you’ll need to set up the primary identifier to complete the mapping.If you are on the data set page, you can just tell the Copilot to create mapping for this data set. Otherwise, you can refer to the data set by its ID, which you can find in the URL of the page. | Can you create a mapping from this data set to the Employee vocabulary? | . | ListDataSets | Provides a list of all available data sets. Note that it is not possible to list data sets by creation date or other properties, you can only get a list of all data sets. | Can you list all data sets? | . | EntitySearchByDataSetColumnSample | Allows you to check if the values you have chosen as an entity code can already be found in the system. This can be helpful when you want to ensure that the chosen entity codes are unique and do not already exist in the system. | Can you check if values in the customerId column already have values in the system? | . Rule-related skills . CluedIn Copilot can create all types of rules—data part rules, survivorship rules, and golden records rules—and apply any rule action. | Copilot function | Description | Prompt example | . | ActivateRule | Activates a rule. For more efficiency in your data management tasks, you can use this skill in one prompt along with creating a rule. | Can you create a rule to transform all values of the contact.country vocabulary key to upper-case, and then activate this rule? | . | CloneRule | Creates a copy of a rule. Once the copy is created, you’ll get a link to view and manage the rule. | Can you create a copy of this rule? | . | CreateGoldenRecordRule | Creates a new golden record rule. You can use this skill in one prompt along with activating and reprocessing a rule. Once the copy is created, you’ll get a link to view and manage the rule. | Create a golden record rule that tags all Company records where the company.state is not in one of the valid Australian states, and tag it with “Invalid Australian State”.Can you create a rule that detects if the doctor.npi matches the pattern of a valid NPI number and tag any record that does not with “Invalid NPI Number”? | . | CreateProcessingRule | Creates a new data part rule. You can use this skill in one prompt along with activating and reprocessing a rule. Once the rule is created, you’ll get a link to view and manage the rule. | Can you create a rule to transform all values of the contact.country vocabulary key to upper-case? | . | CreateSurvivorshipRule | Creates a new survivorship rule. You can use this skill in one prompt along with activating and reprocessing a rule. Once the rule is created, you’ll get a link to view and manage the rule. | Can you create a survivorship rule to define the winning value for organization.countryCode based on the most frequently used value? | . | DeactivateRule | Deactivates a rule. | Can you deactivate all golden record rules? | . | ReprocessRule | Reprocesses a rule to apply the rule’s actions to records that match the rule’s filter. This skill can be used in one prompt along with creating and activating a rule for more efficient data management tasks. | Create a rule that tags all records with customer.country set to ‘Norway’ as ‘Nordic’, then activate and reprocess the rule. | . | SuggestVocabularyKeyRules | Provides suggestions for validation or business rules for a particular vocabulary key. You can review these suggestions and then decide which rules to create.If you are on the vocabulary key page, you can just tell the Copilot to suggest rules for this vocabulary key. | Can you suggest rules for this vocabulary key? | . Data catalog skills (vocabulary and vocabulary key) . CluedIn Copilot can create vocabularies and vocabulary keys as well detect anomalies in vocabulary key values. | Copilot function | Description | Prompt example | . | CreateVocabulary | Creates a new vocabulary with the specified name. Once a new vocabulary is created, you’ll get a link to view and manage the vocabulary details.If you are on the page of the business domain you want to associate the vocabulary with, you can just tell the Copilot to create the vocabulary for this business domain. If you don’t specify the business domain, it will be provided automatically, but you can change it later. | Can you create a new vocabulary called Company for the Company business domain?Can you create a new vocabulary called Company for this business domain? | . | CreateVocabularyKey | Creates a new vocabulary key with the specified name. If you previously created a vocabulary in the chat, the new vocabulary keys will be added to that vocabulary.You can create individual vocabulary keys one at a time by entering a separate prompt for each vocabulary key. However, if you need to create multiple vocabulary keys, you can instruct Copilot to perform the task in a single prompt. | Can you create 10 vocabulary keys including Name, Age, Gender, JobTitle, ContactNumber, Email, ManagedBy, Salary, Tenure and NickName? | . | ProfileVocabularyKey | Creates profiling for a vocabulary key. | Can you profile this vocabulary key? | . | StandardizeData | Provides suggestions on how to standardize or normalize values within a vocabulary key. You can review the suggestions and then instruct Copilot to create rules or do it on your own. | Can you standardize values of this vocabulary key? | . | DetectAnomaly | Detects anomalies in the top 10000 values of a vocabulary key.While viewing a vocabulary key, you can ask CluedIn Copilot to suggest some standardization of the data and it will recommend (but not change) values that are similar and that should be set up as rules. | Can you find any anomalies in this vocabulary key? If so, can you create a golden record rule per anomaly that tags it with “Invalid City“? | . | ListVocabularies | Lists all vocabularies. | Can you list all vocabularies that are currently used in the system? | . | ListVocabularyKeys | Lists vocabulary keys associated with a vocabulary. | Can you list all vocabulary keys associated with the Contact vocabulary? | . | ChangeVocabularyKeyToGlossaryTermLookupKey | Changes a vocabulary key into a lookup value. | Can you change the company.country vocabulary key to a lookup key? | . Deduplication-related skills . CluedIn Copilot can create deduplication projects and display information about those projects and groups of duplicates. However, it currently cannot create the matching rules. | Copilot function | Description | Prompt example | . | CreateDeduplicationProject | Creates a deduplication project. Once the project is created, you’ll get a link to view and configure the project. | Can you create a deduplication project for Customer records? | . | ExplainDeduplicationGroup | Provides general information about a deduplication group based on the comparison of the fields. You need to provide the group ID, which you can find in the URL of the page. | Explain the group of duplicates with this ID 71926613-deed-4f2e-b300-311359b76869. | . | GenerateResultsDeduplicationProject | Generates results of a deduplication project.If you are on the project page, you can just tell the Copilot to generate results of this project. Otherwise, you can refer to the project by its ID, which you can find in the URL of the page. | Can you generate the results of this deduplication project? | . | ListDeduplicationProject | Provides a list of all available deduplication projects, including project ID, project name, ID of the user who created the project, creation date, and whether the project is archived or not. | Can you list all deduplication projects? | . Stream and export target skills . CluedIn Copilot can create and start streams to share data with other systems. Note that it cannot yet pause or stop streams. | Copilot function | Description | Prompt example | . | CreateStream | Creates a stream. Since stream configuration requires many details, consider asking the Copilot to provide a list of information needed to create a stream. You can then enter the required information step by step.Typically, you need to provide the stream name, condition, actions (if necessary), name for the content in the external system, export target ID, streaming mode, edges (if necessary), and properties to export. | I need to create a stream. What details do you need? | . | CloneStream | Creates a copy of the stream configuration. Note that you’ll have to provide the export target configuration in the copied stream. | Can you create a copy of this stream? | . | StartStream | Starts a stream. | Can you start this stream? | . | ListStreams | Provides a list of all available streams, including stream ID, name, and status. | Can you list all streams? | . | ListExportTargets | Provides a list of all available export targets, including export target ID, type, and whether it is enabled or not. | Can you list all export targets? | . Clean-related skills . CluedIn Copilot can create clean projects according to your requirements and display information about the existing clean projects. However, it currently cannot generate the results of the clean project. | Copilot function | Description | Prompt example | . | CreateCleanProject | Creates a clean project. You’ll get a brief project description, including top 10 records that match the project’s filters. You can click the link to go to the clean project and validate if Copilot did the right thing. Then you’ll be able to generate the project results on your own. | Can you create a clean project to fix contact.jobTitle values in records of the Contact business domain? | . | ListCleaningProjects | Provides a list of all available clean projects, including project ID and project name. | Can you list all clean projects?What clean project are currently available in the platform? | . Hierarchy skills . CluedIn Copilot can create hierarchies to visualize relations between golden records. | Copilot function | Description | Prompt example | . | CreateHierarchy | Creates a new hierarchy. Before creating the hierarchy, make sure that the records have the appropriate edge type defined. Once the hierarchy is created, you’ll get a link to view the details. | Can you create a hierarchy called Org Chart for all records of the Contact business domain? | . | ListHierarchies | Provides a list of all available hierarchies, including status, number of nodes, creation and modification dates. | Can you list all hierarchies that are available in the system? | . Glossary skills . CluedIn Copilot can create glossary terms within specific category and display information about the existing glossary categories. | Copilot function | Description | Prompt example | . | CreateGlossaryTermSkill | Creates a glossary term within the specified category. You might be asked to provide the ID of the category, which you can find in the URL of the page. Once the term is created, you’ll get a link to view the details. | Can you create a glossary term called North America that would include Customer records whose contact.businessRegion vocabulary key is set to North America? | . | ListGlossaryCategories | Provides a list of all available glossary categories. | Can you list all glossary categories?What glossary categories are available in the platform? | . Other skills . CluedIn Copilot can search for golden records according to your requirements as well as perform actions related to business domains (create, describe, list). | Copilot function | Description | Prompt example | . | DataQualityMetrics | Provides current global data quality metrics. | Can you show me the global quality metrics? | . | Search | Finds records according to your input.CluedIn will return the top 10 results in a table and a link to launch into the full search query as well. | Can you find the Person records where the user.country is in the Nordics? | . | ListEntityTypes | Provides a list of all available business domains. Note that business domains that have no associated data will not appear in the list. | Can you list all entity types? | . | CreateEntityType | Creates a new business domain with the specified name. Once a new business domaine is created, you’ll get a link to view details. | Can you create a new entity type named Company? | . | DescribeEntity | Provides general information about a golden record: business domain, name, codes, properties, vocabularies, and so on.If you are on the golden record page, you can just tell the Copilot to describe this golden record. | Can you describe this golden record? | . ",
    "url": "/microsoft-integration/copilot-integration/work-with-copilot#copilot-skills",
    
    "relUrl": "/microsoft-integration/copilot-integration/work-with-copilot#copilot-skills"
  },"970": {
    "doc": "Work with CluedIn Copilot",
    "title": "Work with CluedIn Copilot",
    "content": " ",
    "url": "/microsoft-integration/copilot-integration/work-with-copilot",
    
    "relUrl": "/microsoft-integration/copilot-integration/work-with-copilot"
  },"971": {
    "doc": "Access control reference",
    "title": "On this page",
    "content": ". | Access control policy structure | Access control policy rule actions . | View | Mask | Add/edit | . | . In this article, you will find reference information about the structure of an access control policy and the actions available in the access control policy rule. ",
    "url": "/management/access-control/access-control-reference#on-this-page",
    
    "relUrl": "/management/access-control/access-control-reference#on-this-page"
  },"972": {
    "doc": "Access control reference",
    "title": "Access control policy structure",
    "content": "Access control policy is a mechanism that allows you to precisely define access to golden records and their properties. The access control policy consists of several building blocks. | Filters – conditions to define which golden records the access control policy applies to. | Policy rule – an object that grants specific users and/or roles a particular type of access to golden records that match the rule’s filters and conditions. The policy rule contains the following settings: . | Conditions – additional criteria on top of the filters to define which golden records are affected by the specific policy rule. | Action – operation to allow a specific type of access to golden records that match the rule’s filters and conditions. | Members – users and/or roles to whom the policy rule’s action is applied. | All Vocabulary Keys – a checkbox that, if selected, means that the policy rule’s action is applied to all vocabulary keys in golden records that match the rule’s filters and conditions. | Vocabulary Keys – a field where you can select specific vocabulary keys to which the policy rule’s action will be applied. | Vocabularies – a field where you can select specific vocabularies to which the policy rule’s action will be applied. You need to select specific vocabulary keys or vocabularies to which the policy rule will be applied. If no vocabulary keys or vocabularies are selected, the members of the policy rule won’t be able to view golden records at all. | . | . ",
    "url": "/management/access-control/access-control-reference#access-control-policy-structure",
    
    "relUrl": "/management/access-control/access-control-reference#access-control-policy-structure"
  },"973": {
    "doc": "Access control reference",
    "title": "Access control policy rule actions",
    "content": "Access control policy rule actions define the type of access to the properties in golden records that match the rule’s filters and conditions. View . This action gives access to view the values of specific vocabulary keys in golden records that match the rule’s filters and conditions. This action does not give access to edit the existing properties of a golden record or add new properties. That is why the members of the policy rule with the view action will see the icon indicating that the value cannot be edited. If the users are not granted access to specific vocabularies or vocabulary keys that are used in golden records, they will see the No value text on the search page for certain properties. This means that either the property does not exist in a specific golden record or that the users do not have access to the property according to access control. If you want to indicate that a golden record has a specific property without displaying this property to users, create a policy rule with the mask action. Mask . This action hides the values of specific vocabulary keys in golden records that match the rule’s filters and conditions. The members of the policy rule with the mask action will see the icon indicating that the value is masked due to access control policy. Masked value cannot be retrieved in any way, and it cannot be edited. The mask value action in data part and golden record rules will be deprecated in future releases. Therefore, use the mask action in access control policy rules. Add/edit . This action allows the members of the policy rule to add new properties to the golden record and/or edit the existing properties in the golden record. The properties that the members can add/edit depend on the vocabularies and/or vocabulary keys selected in the policy rule: . | If the policy rule is applied to all vocabulary keys, then the members can add or edit all vocabulary keys that are used in golden records that match the rule’s filters and conditions. | If policy rule is applied to specific vocabulary keys, then the members can add or edit only the selected vocabulary keys that are used in golden records that match the rule’s filters and conditions. | If the policy rule is applied to specific vocabularies, then the members can add or edit any vocabulary keys belonging to the specified vocabularies that are used in golden records that match the rule’s filters and conditions. | . The add/edit action takes precedence over the view action. If a user is a member of both a policy rule with the view action and a policy rule with the add/edit action, and both rules apply to the same vocabulary keys, it means that the user will be able to add or edit those vocabulary keys. Add/edit action and RACI permissions . When adding a property to the golden record, you might want to create a new value. This is possible only if you have the needed RACI permissions: either you are the owner of the vocabulary or you have the Consulted access to the management.datacatalog claim. When you are trying to add or edit a property in a golden record, CluedIn first checks if you have the required RACI permissions and then it checks if you are allowed to make changes to that specific golden record according to the access control policy. If your access level is less than Consulted, you will not be able to add or edit the property in a golden record. ",
    "url": "/management/access-control/access-control-reference#access-control-policy-rule-actions",
    
    "relUrl": "/management/access-control/access-control-reference#access-control-policy-rule-actions"
  },"974": {
    "doc": "Access control reference",
    "title": "Access control reference",
    "content": " ",
    "url": "/management/access-control/access-control-reference",
    
    "relUrl": "/management/access-control/access-control-reference"
  },"975": {
    "doc": "Administration",
    "title": "Administration",
    "content": "In this article, you will learn about the main administrative tasks in CluedIn. Select one of the following items to get started: . User management Add users and deactivate users in your CluedIn organization Roles Learn about claims and access levels, manage user roles User access Learn about main aspects that define user access in CluedIn ",
    "url": "/administration",
    
    "relUrl": "/administration"
  },"976": {
    "doc": "Advanced mapping code",
    "title": "On this page",
    "content": ". | Write advanced mapping code | Available methods | CluedIn Expression Language (C.E.L.) . | Variables | Operators | String . | Case | Manipulation | Chain | Chop | . | Math . | Construction | Arithmetic | Statistics | Trigonometry functions | Constants | Utils | . | . | . In this article, you will learn about the possibility of introducing changes to your clues using JavaScript glue code. You can perform similar actions as with property and pre-process rules but you gain greater flexibility to set up complex conditions. The advanced mapping code is applied to the clues after property and pre-process rules. Prerequisites . To access advanced mapping, go to Administration &gt; Feature Flags, and then turn on the Advanced Mapping feature. ",
    "url": "/integration/additional-operations-on-records/advanced-mapping-code#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/advanced-mapping-code#on-this-page"
  },"977": {
    "doc": "Advanced mapping code",
    "title": "Write advanced mapping code",
    "content": "Sometimes, the conditions in property and pre-process rules might appear cumbersome when you need to execute complex logic on your records. In such cases, you can use the advanced mapping capabilities that allow you to modify clues by executing the code. From a security perspective, advanced mapping runs on a virtual machine that does not have access to your network. The advanced mapping code can be executed on the following levels: . | Code mapping before sending clues – default level that allows you to modify clues using built-in methods. Our article focuses on this level because it is the most commonly used. | Code mapping before creating clues – additional level that only allows you to modify record values using the following code: . value['customer.companyName'] = 'Star'; [value] . Where 'customer.companyName' is the name of the property, 'Star' is the value that will be set to the property in each record, and [value] is an element that indicates that the code is executed for each record. | . Example . Suppose you have ingested the revenue data, and you want to add tags to facilitate the retrieval of golden records in CluedIn. You want to add a tag “Golden” to those records where the revenue is greater than or equal to 1000000; and another tag “Silver” to those records where the revenue is greater than or equal to 500000 . You can achieve that by writing the advanced mapping code similar to the following example. if (getVocabularyKeyValue('customer.revenue') &gt;= 1000000) { addTag('Golden'); } else if (getVocabularyKeyValue('customer.revenue') &gt;= 500000) { addTag('Silver'); } [value] . [value] is an important element that indicates that the code is executed for each clue. To modify clues by writing the advanced mapping code . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set. | Go to the Map tab, and then select Advanced. | In the upper-right corner, make sure that Code mapping before sending clues is selected. | Select Run to load all clues that were created from the data set. The clues appear on the right side of the page. If you have created property or pre-processed rules, note that they have already been applied to the clues. | On the left side of the page, write the code to modify the clues as needed. You can write any JavaScript code. To check if the code is applied as intended, select Run. | If you are satisfied with the result, select Save. Your changes to the clues are saved. The advanced mapping code is executed right before you send the data into processing pipeline. | . ",
    "url": "/integration/additional-operations-on-records/advanced-mapping-code#write-advanced-mapping-code",
    
    "relUrl": "/integration/additional-operations-on-records/advanced-mapping-code#write-advanced-mapping-code"
  },"978": {
    "doc": "Advanced mapping code",
    "title": "Available methods",
    "content": "This is a reference section that lists all available methods that you can use to write the advanced mapping code. Please refer to CluedIn Expression Language (C.E.L.) for all string and math methods. This section is focused on built-in CluedIn-specific methods. getVocabularyKeyValue . Retrieves the clues that have a specific vocabulary key in order to apply further actions on the values or clues. In the following example, the code retrieves specific values of 'customer.industry' and adds a tag “Golden Oil &amp; Gas” to those clues where 'customer.revenue' is greater than 750000. if ( (getVocabularyKeyValue('customer.industry') === 'Natural Gas Distribution' || getVocabularyKeyValue('customer.industry') === 'Oil &amp; Gas Production') &amp;&amp; getVocabularyKeyValue('customer.revenue') &gt; 750000 ) { addTag('Golden Oil &amp; Gas'); } [value] . setVocabularyKeyValue . Changes the vocabulary key values. In the following example, the code retrieves the current value of 'customer.industry' using getVocabularyKeyValue, converts it to lowercase using toLowerCase(), and then sets the updated lowercase value using setVocabularyKeyValue. setVocabularyKeyValue('customer.industry', getVocabularyKeyValue('customer.industry').toLowerCase()); [value] . getEntityProperty . Retrieves the clues that have a specific entity property in order to apply further actions on the values or clues. In the following example, the code takes the 'name' property from the clue, converts it to uppercase using toUpperCase(), and then sets the result as the value of the 'description' property for the clue. const name = getEntityProperty('name'); setEntityProperty('description', name.toUpperCase()); [value] . setEntityProperty . Adds or changes the value of the entity property, such as name, description, entityType, date created, and so on. In the following example, the code checks if the 'customer.industry' vocabulary key is set to 'Precious Metals', and if it is, the code sets the 'description' property of the clue to the specified string. if (getVocabularyKeyValue('customer.industry') === 'Precious Metals') { setEntityProperty('description', 'This record comes from corporate CRM'); } [value] . removeVocabularyKey . Removes vocabulary key from the clues. In the following example, the code removes the 'customer.revenue' vocabulary key from all clues. removeVocabularyKey('customer.revenue') [value] . quarantine . Send the clues that do not meet certain conditions to quarantine. In the following example, the code checks if the value of 'customer.revenue' is absent or equal to 0. If it is, the code send such clues to quarantine. const customerRevenue = getVocabularyKeyValue('customer.revenue'); if (customerRevenue === undefined || customerRevenue === null || customerRevenue === 0) { quarantine(); } [value] . addAlias . Adds aliases to the clues. In the following example, the code adds an alias taken from the 'customer.company' vocabulary key to each clue. addAlias(getVocabularyKeyValue('customer.company')); [value] . addTag . Adds tags to the clues. In the following example, the code checks if the value of 'customer.industry' is equal to “Oil &amp; Gas”. If the condition is true, a tag “Oil &amp; Gas” is added to the corresponding clues. if (getVocabularyKeyValue('customer.industry') === 'Oil &amp; Gas') { addTag('Oil &amp; Gas'); } [value] . addCode . Adds an identifier (previously known as code) to the clue. The identifier usually consists of a business domain (previously known as entity type), an origin, and a specific value. You can specify the required origin and value to generate the code. In the following example, the first parameter is the origin and the second parameter is the value. The resulting code would be \"/Customer#myOrigin:myCode\". addCode(\"myCode\",\"myOrigin\") [value] . removeTags . Removes all tags from the clue. removeTags() [value] . removeCodes . Removes all codes from the Codes section of the clue. removeCodes() [value] . removeAliases . Removes all aliases from the clue. removeAliases() [value] . ",
    "url": "/integration/additional-operations-on-records/advanced-mapping-code#available-methods",
    
    "relUrl": "/integration/additional-operations-on-records/advanced-mapping-code#available-methods"
  },"979": {
    "doc": "Advanced mapping code",
    "title": "CluedIn Expression Language (C.E.L.)",
    "content": "CluedIn Expression Language (C.E.L.) is a language used to work with and manipulate your data in CluedIn. Variables . | cell – the current cell. | value – the current cell’s value; this is a shortcut for ‘cell.value’. | row – the current row; it has the index and cells. | cells – the cells of the current row. This is a shortcut for ‘row.cells’. A particular cell can be retrieved with cells.&lt;columnName&gt; if the &lt;column name&gt; is a single word or with cells[\"&lt;column name&gt;] otherwise. | column – access to properties of the column (for example, type, name). | . Operators . C.E.L. supports the following logical operators: . | and (&amp;&amp;) . return a &amp;&amp; b; . | or (||) . return a || b; . | not (!) . return !a; . | . String . Case . camelCase . Converts the value to camel case. camelCase('bird flight'); // =&gt; 'birdFlight' . camelCase('BirdFlight'); // =&gt; 'birdFlight' . camelCase('-BIRD-FLIGHT-'); // =&gt; 'birdFlight' . capitalize . Converts the first character of the value to uppercase. If restToLower is true, convert the rest of value to lowercase. capitalize('apple'); // =&gt; 'Apple' . capitalize('aPPle', true); // =&gt; 'Apple' . decapitalize . Converts the first character of value to lowercase. decapitalize('Sun'); // =&gt; 'sun' . decapitalize('moon'); // =&gt; 'moon' . kebabCase . Converts the value to kebab case, also called spinal case or lisp case, where each word is in lowercase and separated with a dash character (-). kebabCase('goodbye blue sky'); // =&gt; 'goodbye-blue-sky' . kebabCase('GoodbyeBlueSky'); // =&gt; 'goodbye-blue-sky' . kebabCase('-Goodbye-Blue-Sky-'); // =&gt; 'goodbye-blue-sky' . lowerCase . Converts the value to lowercase. lowerCase('Green'); // =&gt; 'green' . lowerCase('BLUE'); // =&gt; 'blue' . snakeCase . Converts the value to snake case, where each word is in lowercase and separated with an underscore character (_). snakeCase('learning to fly'); // =&gt; 'learning_to_fly' . snakeCase('LearningToFly'); // =&gt; 'learning_to_fly' . snakeCase('-Learning-To-Fly-'); // =&gt; 'learning_to_fly' . swapCase . Converts the uppercase alpha characters of a value to lowercase and lowercase characters to uppercase. swapCase('League of Shadows'); // =&gt; 'lEAGUE OF sHADOWS' . swapCase('2 Bees'); // =&gt; '2 bEES' . upperCase . Converts the value to uppercase. upperCase('school'); // =&gt; 'SCHOOL' . slugify . Slugifies the value. Cleans the value by replacing diacritics with corresponding Latin characters. slugify('Italian cappuccino drink'); // =&gt; 'italian-cappuccino-drink' . slugify('caffé latté'); // =&gt; 'caffe-latte' . slugify('хорошая погода'); // =&gt; 'horoshaya-pogoda' . Manipulation . insert . Inserts a string toInsert into value at specified position. insert('ct', 'a', 1); // =&gt; 'cat' . insert('sunny', ' day', 5); // =&gt; 'sunny day' . trim . Removes whitespaces from left and right sides of the value. trim(' Mother nature '); // =&gt; 'Mother nature' . trim('--Earth--', '-'); // =&gt; 'Earth' . trimLeft . Removes whitespaces from the left side of the value. trimLeft(' Starship Troopers'); // =&gt; 'Starship Troopers' . trimLeft('***Mobile Infantry', '*'); // =&gt; 'Mobile Infantry' . trimRight . Removes whitespaces from the right side of the value. trimRight('the fire rises '); // =&gt; 'the fire rises' . trimRight('do you feel in charge?!!!', '!'); // =&gt; 'do you feel in charge?' . latinise . Latinises the value by removing diacritic characters. latinise('cafe\\u0301'); // or 'café' // =&gt; 'cafe' . latinise('août décembre'); // =&gt; 'aout decembre' . latinise('как прекрасен этот мир'); // =&gt; 'kak prekrasen etot mir' . pad . Pads value to a new length. pad('dog', 5); // =&gt; ' dog ' . pad('bird', 6, '-'); // =&gt; '-bird-' . pad('cat', 6, '-='); // =&gt; '-cat-=' . padLeft . Pads value from left to a new length. padLeft('dog', 5); // =&gt; ' dog' . padLeft('bird', 6, '-'); // =&gt; '--bird' . padLeft('cat', 6, '-='); // =&gt; '-=-cat' . padRight . Pads value from right to a new length. padRight('dog', 5); // =&gt; 'dog ' . padRight('bird', 6, '-'); // =&gt; 'bird--' . padRight('cat', 6, '-='); // =&gt; 'cat-=-' . repeat . Repeats the value a number of times. repeat('w', 3); // =&gt; 'www' . repeat('world', 0); // =&gt; '' . replace . Replaces the matches of pattern with replacement. replace('swan', 'wa', 'u'); // =&gt; 'sun' . replace('domestic duck', /domestic\\s/, ''); // =&gt; 'duck' . replace('nice duck', /(nice)(duck)/, (match, nice, duck) =&gt; ( 'the ' + duck + ' is ' + nice; )); // =&gt; 'the duck is nice' . replaceAll . Replaces all matches of pattern with replacement. replaceAll('good morning', 'o', '*'); // =&gt; 'g**d m*rning' . replaceAll('evening', /n/, 's'); // =&gt; 'evesisg' . reverse . Reverses the value. reverse('winter'); // =&gt; 'retniw' . splice . Changes value by deleting deleteCount of characters starting at the position start. Places a new string toAdd instead of deleted characters. splice('new year', 0, 4); // =&gt; 'year' . splice('new year', 0, 3, 'happy'); // =&gt; 'happy year' . splice('new year', -4, 4, 'day'); // =&gt; 'new day' . Chain . stringChain . Creates a stringChain object that wraps value, enabling explicit chain sequences. Use value() to unwrap the result. stringChain('Back to School') .lowerCase() .words() .value() // =&gt; ['back', 'to', 'school'] . Chop . charAt . Access a character from a value at specified position. charAt('helicopter', 0); // =&gt; 'h' . charAt('helicopter', 1); // =&gt; 'e' . first . Extracts the first length characters from a value. first('helicopter'); // =&gt; 'h' . first('vehicle', 2); // =&gt; 've' . first('car', 5); // =&gt; 'car' . last . Extracts the last length characters from a value. last('helicopter'); // =&gt; 'r' . last('vehicle', 2); // =&gt; 'le' . last('car', 5); // =&gt; 'car' . prune . Truncates a value to a new length and does not break the words. Guarantees that the truncated string is no longer than the specified length. prune('Once upon a time', 7); // =&gt; 'Once...' . prune('Good day, Little Red Riding Hood', 16, ' (more)'); // =&gt; 'Good day (more)' . prune('Once upon', 10); // =&gt; 'Once upon' . slice . Extracts a string from value from start position up to end position. The character at the end position is not included. slice('miami', 1); // =&gt; 'iami' . slice('florida', -4); // =&gt; 'rida' . slice('florida', 1, 4); // =&gt; \"lor\" . substr . Extracts a string from a value from start position a number of length characters. substr('infinite loop', 9); // =&gt; 'loop' . substr('dreams', 2, 2); // =&gt; 'ea' . truncate . Truncates a value to a new length. truncate('Once upon a time', 7); // =&gt; 'Once...' . truncate('Good day, Little Red Riding Hood', 14, ' (...)'); // =&gt; 'Good day (...)' . truncate('Once upon', 10); // =&gt; 'Once upon' . count . Counts the characters in a value. count('rain'); // =&gt; 4 . split . split('rage against the dying of the light', ' '); // =&gt; ['rage', 'against', 'the', 'dying', 'of', 'the', 'light'] . split('the dying of the light', /\\s/, 3); // =&gt; ['the', 'dying', 'of'] . Math . Construction . mathChain . Wrap any value in a chain, allowing to perform chained operations on the value. mathChain(3) .add(4) .subtract(2) .done() // 5 . Arithmetic . add . Add two or more values, x + y. add(2, 3) // returns number 5 add(2, 3, 4) // returns number 9 . round . Round a value towards the nearest integer. round(3.2) // returns number 3 round(3.8) // returns number 4 round(-4.2) // returns number -4 round(-4.7) // returns number -5 round(pi, 3) // returns number 3.142 round(123.45678, 2) // returns number 123.46 . log . Calculate the logarithm of a value. log(3.5) // returns 1.252762968495368 exp(log(2.4)) // returns 2.4 pow(10, 4) // returns 10000 log(10000, 10) // returns 4 log(10000) / log(10) // returns 4 log(1024, 2) // returns 10 pow(2, 10) // returns 1024 . multiply . Multiply two or more values, x * y. multiply(4, 5.2) // returns number 20.8 multiply(2, 3, 4) // returns number 24 . subtract . Subtract two values, x - y. For matrices, the function is evaluated element-wise. subtract(5.3, 2) // returns number 3.3 . divide . Divide two values, x / y. To divide matrices, x is multiplied with the inverse of y: x * inv(y). divide(2, 3) // returns number 0.6666666666666666 . abs . Calculate the absolute value of a number. abs(3.5) // returns number 3.5 abs(-4.2) // returns number 4.2 abs([3, -5, -1, 0, 2]) // returns Array [3, 5, 1, 0, 2] . sqrt . Calculate the square root of a value. sqrt(25) // returns 5 square(5) // returns 25 sqrt(-4) // returns Complex 2i . square . Compute the square of a value, x * x. square(2) // returns number 4 square(3) // returns number 9 pow(3, 2) // returns number 9 multiply(3, 3) // returns number 9 square([1, 2, 3, 4]) // returns Array [1, 4, 9, 16] . pow . Calculates the power of x to y, x ^ y. pow(2, 3) // returns number 8 const b = [[1, 2], [4, 3]] pow(b, 2) // returns Array [[9, 8], [16, 17]] . ceil . Round a value towards plus infinity. If x is complex, both real and imaginary parts are rounded towards plus infinity. ceil(3.2) // returns number 4 ceil(3.8) // returns number 4 ceil(-4.2) // returns number -4 ceil(-4.7) // returns number -4 ceil([3.2, 3.8, -4.7]) // returns Array [4, 4, -4] . factorial . Factorial only supports an integer value as an argument. For matrices, the function is evaluated element-wise. factorial(5) // returns 120 factorial(3) // returns 6 . gcd . Calculate the greatest common divisor for two or more values or arrays. gcd(8, 12) // returns 4 gcd(-4, 6) // returns 2 gcd(25, 15, -10) // returns 5 gcd([8, -4], [12, 6]) // returns [4, 2] . Statistics . min . Compute the minimum value of a matrix or a list of values. max(2, 1, 4, 3) // returns 4 max([2, 1, 4, 3]) // returns 4 // maximum over a specified dimension (zero-based) max([[2, 5], [4, 3], [1, 7]], 0) // returns [4, 7] max([[2, 5], [4, 3]], [1, 7], 1) // returns [5, 4, 7] max(2.7, 7.1, -4.5, 2.0, 4.1) // returns 7.1 min(2.7, 7.1, -4.5, 2.0, 4.1) // returns -4.5 . max . Compute the maximum value of a matrix or a list with values. In case of a multi-dimensional array, the maximum of the flattened array will be calculated. When dim is provided, the maximum over the selected dimension will be calculated. Parameter dim is zero-based. max(2, 1, 4, 3) // returns 4 max([2, 1, 4, 3]) // returns 4 // maximum over a specified dimension (zero-based) max([[2, 5], [4, 3], [1, 7]], 0) // returns [4, 7] max([[2, 5], [4, 3]], [1, 7], 1) // returns [5, 4, 7] max(2.7, 7.1, -4.5, 2.0, 4.1) // returns 7.1 min(2.7, 7.1, -4.5, 2.0, 4.1) // returns -4.5 . Trigonometry functions . cos . Calculate the cosine of a value. cos(2) // returns number -0.4161468365471422 cos(pi / 4) // returns number 0.7071067811865475 cos(unit(180, 'deg')) // returns number -1 cos(unit(60, 'deg')) // returns number 0.5 const angle = 0.2 pow(sin(angle), 2) + pow(cos(angle), 2) // returns number ~1 . cosh . Calculate the hyperbolic cosine of a value, defined as cosh(x) = 1/2 * (exp(x) + exp(-x)). cosh(0.5) // returns number 1.1276259652063807 . sin . Calculate the inverse sine of a value. asin(0.5) // returns number 0.5235987755982989 asin(sin(1.5)) // returns number ~1.5 asin(2) // returns Complex 1.5707963267948966 -1.3169578969248166 i . sinh . Calculate the hyperbolic sine of a value, defined as sinh(x) = 1/2 * (exp(x) - exp(-x)). sinh(0.5) // returns number 0.5210953054937474 . tan . Calculate the tangent of a value. tan(0.5) // returns number 0.5463024898437905 sin(0.5) / cos(0.5) // returns number 0.5463024898437905 tan(pi / 4) // returns number 1 tan(unit(45, 'deg')) // returns number 1 . tanh . Calculate the hyperbolic tangent of a value, defined as tanh(x) = (exp(2 * x) - 1) / (exp(2 * x) + 1). // tanh(x) = sinh(x) / cosh(x) = 1 / coth(x) tanh(0.5) // returns 0.46211715726000974 sinh(0.5) / cosh(0.5) // returns 0.46211715726000974 1 / coth(0.5) // returns 0.46211715726000974 . Constants . e . Euler’s number, the base of the natural logarithm. e . pi . The number pi is a mathematical constant that is the ratio of a circle’s circumference to its diameter. pi . phi . Phi is the golden ratio. Two quantities are in the golden ratio if their ratio is the same as the ratio of their sum to the larger of the two quantities. Phi is defined as (1 + sqrt(5)) / 2. phi . Utils . isInteger . Test whether a value is an integer number. isInteger(2) // returns true isInteger(0) // returns true isInteger(0.5) // returns false isInteger(bignumber(500)) // returns true isInteger(fraction(4)) // returns true isInteger('3') // returns true isInteger([3, 0.5, -2]) // returns [true, false, true] . isNaN . Test whether a value is NaN (not a number). The function supports such types as number, BigNumber, Fraction, Unit, and Complex. isNaN(3) // returns false isNaN(NaN) // returns true isNaN(0) // returns false isNaN(bignumber(NaN)) // returns true isNaN(bignumber(0)) // returns false isNaN(fraction(-2, 5)) // returns false isNaN('-2') // returns false isNaN([2, 0, -3, NaN]') // returns [false, false, false, true] . isNegative . Test whether a value is negative: smaller than zero. The function supports such types as number, BigNumber, Fraction, and Unit. isNegative(3) // returns false isNegative(-2) // returns true isNegative(0) // returns false isNegative(-0) // returns false isNegative(bignumber(2)) // returns false isNegative(fraction(-2, 5)) // returns true isNegative('-2') // returns true isNegative([2, 0, -3]') // returns [false, false, true] . isNumeric . Test whether a value is a numeric value. isNumeric(2) // returns true isNumeric('2') // returns true hasNumericValue('2') // returns true isNumeric(0) // returns true isNumeric(bignumber(500)) // returns true isNumeric(fraction(4)) // returns true isNumeric([2.3, 'foo', false]) // returns [true, false, true] . isPositive . Test whether a value is positive: larger than zero. The function supports such types as number, BigNumber, Fraction, and Unit. isPositive(3) // returns true isPositive(-2) // returns false isPositive(0) // returns false isPositive(-0) // returns false isPositive(0.5) // returns true isPositive(bignumber(2)) // returns true isPositive(fraction(-2, 5)) // returns false isPositive(fraction(1,3)) // returns false isPositive('2') // returns true isPositive([2, 0, -3]) // returns [true, false, false] . isPrime . Test whether a value is prime: has no divisors other than itself and one. isPrime(3) // returns true isPrime(-2) // returns false isPrime(0) // returns false isPrime(-0) // returns false isPrime(0.5) // returns false isPrime('2') // returns true isPrime([2, 17, 100]) // returns [true, true, false] . isZero . Test whether a value is zero. isZero(0) // returns true isZero(2) // returns false isZero(0.5) // returns false isZero(bignumber(0)) // returns true isZero(fraction(0)) // returns true isZero(fraction(1,3)) // returns false isZero('0') // returns true isZero('2') // returns false isZero([2, 0, -3]') // returns [false, true, false] . ",
    "url": "/integration/additional-operations-on-records/advanced-mapping-code#cluedin-expression-language-cel",
    
    "relUrl": "/integration/additional-operations-on-records/advanced-mapping-code#cluedin-expression-language-cel"
  },"980": {
    "doc": "Advanced mapping code",
    "title": "Advanced mapping code",
    "content": " ",
    "url": "/integration/additional-operations-on-records/advanced-mapping-code",
    
    "relUrl": "/integration/additional-operations-on-records/advanced-mapping-code"
  },"981": {
    "doc": "Advanced network configuration",
    "title": "On this page",
    "content": ". | Default network configuration example | Advanced network configuration example | Advanced network configuration options . | Using an existing vNet | Private cluster - BYO vNet | . | Internal load balancer | Integration options . | Virtual network peering | Private link service with private endpoint | Next steps | . | Host name resolution | . In this article, you will learn about advanced network configuration options that are available to you during CluedIn installation. ",
    "url": "/deployment/infra-how-tos/advanced-network#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#on-this-page"
  },"982": {
    "doc": "Advanced network configuration",
    "title": "Default network configuration example",
    "content": "The following diagram shows the default network configuration of CluedIn after installation. ",
    "url": "/deployment/infra-how-tos/advanced-network#default-network-configuration-example",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#default-network-configuration-example"
  },"983": {
    "doc": "Advanced network configuration",
    "title": "Advanced network configuration example",
    "content": "The following diagram shows the advanced network configuration of CluedIn. . HubVNet This virtual network is hosted by the client. All IP addresses and address ranges in this design are suggestions only and must be validated and finalized by the client according to their internal IP planning policies. AzureFirewallSubnet The Azure Firewall service should be provisioned and managed by the client. | IP address ranges shown in this design are placeholders and may be adjusted to align with the client’s networking strategy. | The subnet must meet minimum size requirements (at least /26) to support firewall functionality and scaling. | . AzureBastionSubnet Azure Bastion is optional and can be deployed by the client for secure, browser-based RDP/SSH access. | The subnet name must be AzureBastionSubnet, and the IP range must be /26 or larger, per Azure requirements. | If CluedIn provisions Bastion, it and the jumpbox will be deployed inside the AKS VNet rather than the Hub. | . AksVNet This virtual network is dedicated to hosting Azure Kubernetes Service (AKS) resources. | NICs (Network Interface Cards) and Private Endpoints within this VNet will be assigned IPs dynamically from the allocated subnet pools. | Subnets must be sized appropriately to support expected service scaling, including node pools and private endpoints. | When integrating Bastion or other management services, ensure sufficient IP allocation and configure NSGs (Network Security Groups) to allow secure access. | . Advanced network configuration requires that you have read and configured your firewall as per the firewall policy. For more information, see Configure firewall. During the installation, you will have the option to use the recommended CluedIn defaults, or to use your own vNet and subnet to comply with internal policies. CluedIn supports two main options, which are detailed in the following table. | Supported CNI Modes | Description | Recommended class | Reference | . | Azure CNI Overlay | *Default*CluedIn recommends this option as it only requires 1 IP per node from your subnet.This means if you have 5 nodes, a total of 5 IP addresses will need to be used upfront. | /2730 available IP addresses | Azure CNI Overlay planning | . | Azure CNI | Assigns 1 IP per pod, per node based on the maxPods property, which defaults to 50 during installation.This means that if you have 5 nodes, each with a max of 50 pods, a total of 250 IP addresses will need to be used upfront. | /23510 available IP addresses | CNI network planning | . It is best to always accomodate for expansion of your cluster, especially during AKS upgrades as additional nodes will be spun up at upgrade time. The above is CluedIn’s recommendation. ",
    "url": "/deployment/infra-how-tos/advanced-network#advanced-network-configuration-example",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#advanced-network-configuration-example"
  },"984": {
    "doc": "Advanced network configuration",
    "title": "Advanced network configuration options",
    "content": "During the installation of CluedIn from the Azure Marketplace, you can set up advanced network configuration on the Networking and Monitoring tab. The default option is to create a new vNet and subnet with recommended defaults, but this can be changed to use an existing vNet. If you’re using an existing vNet as part of the deployment, you will need to ensure that the AKS cluster identity has appropriate permissions to manage the subnet. This is because AKS nodes will join and leave the subnet at times. Out of the box, CluedIn will use a public ingress and egress load balancer to serve traffic initially regardless of vNet choice. In advanced networking setups, we recommend changing this so ingress is only accessible via a private vNet. The following sections will cover how to do this, and as part of the CluedIn managed service, we will work with you to ensure it’s done in a supported way. Using an existing vNet . If you are using an existing virtual network as part of your CluedIn deployment, you need to consider the following security aspects: . | The cluster identity used by the AKS cluster must have at least Network Contributor permissions on the subnet within your virtual network. Note: On a standard CluedIn instance, this will be the User Assigned Identity rather than Machine Assigned. This is located in the Managed Resource Group. | If you want to define a custom role instead of using the built-in Network Contributor role, the following permissions are required: . | Microsoft.Network/virtualNetworks/subnets/join/action | Microsoft.Network/virtualNetworks/subnets/read | Microsoft.Authorization/roleAssignments/write | . | . For details on creating custom roles, see Azure custom roles. Private cluster - BYO vNet . When using your own network (BYON) for private cluster installation, the Network Contributor permissions are required for the vNet created outside the managed resource group. To meet this requirement, you can create a user-managed identity and assign it the Network Contributor role on the specified vNet. During installation, ensure this user-assigned managed identity is specified in the Network and Monitoring section. ",
    "url": "/deployment/infra-how-tos/advanced-network#advanced-network-configuration-options",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#advanced-network-configuration-options"
  },"985": {
    "doc": "Advanced network configuration",
    "title": "Internal load balancer",
    "content": "This section contains a procedure for configuring CluedIn to use your internal load balancer and IP address. Prerequisites . | You should be comfortable working in either PowerShell or bash terminal via Azure Cloud Shell. | You should be connected to your AKS cluster. See Connect to CluedIn cluster for detailed instructions. | Your Helm repository is set up. | . If you have any questions, you can request CluedIn support by sending an email to support@cluedin.com (or reach out to your delivery manager if you have a committed deal). To configure CluedIn to use your load balancer and internal IP address . | Download the current cluster configuration file by running the following command: helm get values cluedin-platform -n cluedin -o yaml &gt; Cluster-Current-values.yaml . | Open the file in nano editor by running the following command: nano Cluster-Current-values.yaml . | In the file, find a section that looks like the example below. infrastructure: haproxy-ingress: controller: service: annotations: service.beta.kubernetes.io/azure-load-balancer-resource-group: mrg-azurecluedin loadBalancerIP: {PublicIP} # This will match the allocated Azure Resource . This section controls the load balancer configuration and associated IP address. The example shows external load balancer with external IP address. | Replace the section that you found with the following section: infrastructure: haproxy-ingress: controller: service: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" loadBalancerIP: {PrivateIP} # This must be valid in the vNet subnet range . | Reconfigure a new load balancer to use your internal IP address. To do this, replace {PrivateIP} with a valid IP address from your AKS subnet range. The IP address can be any IP from the range that is not in use by the nodes, pods, or other resources. To verify that the IP address is not in use, look at connected devices in the vNet resource page in the Azure portal. | Save the file and post the new configuration to the cluster by running the following command: helm upgrade -i cluedin-platform cluedin/cluedin-platform -n cluedin --values Cluster-Current-values.yaml . After a short time, a confirmation appears in the console. It means that CluedIn is now configured to use your new load balancer and internal IP address. In Azure, you should then see a new Load Balancer resource called kubernetes-internal, which will be used for ingress. The original kubernetes then simply becomes egress only. | . ",
    "url": "/deployment/infra-how-tos/advanced-network#internal-load-balancer",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#internal-load-balancer"
  },"986": {
    "doc": "Advanced network configuration",
    "title": "Integration options",
    "content": "With Kubernetes now configured to use the internal load balancer for ingress, you will need to decide how you want to access this front-end load balancer from your internal network. The two options supported are Virtual network peering and Private link service with private endpoint. Note: If your subnets overlap in any way, the only option will be Private link service with private endpoint. Virtual network peering . To configure virtual network peering, refer to Microsoft documentation. As all resources have already been created, you only need to follow the Create virtual network peer step in the tutorial. Private link service with private endpoint . In the event there is overlapping of subnets, this is not a problem as private endpoints also work here. This is slightly more complicated than virtual network peering and we’ll offer a bit of guidance here. Microsoft has a document explaining how to achieve this. As AKS maintains the load balancers, and the vnet is created at deployment time (or an existing one is used), you only need to follow the steps Create a private link service and Create a private endpoint in the above document. For the Private Link Service, ensure that the Load balancer is set to kubernetes-internal and uses the frontend IP address from the private range. The Source NAT vnet and subnet should default correctly. Ensure that Source NAT subnet is set to No and it is best to assign a Static Private IP address. For the Private endpoint, ensure you select the virtual network that will be used to access CluedIn along with the subnet. It is recommended to statically allocate an IP address here as this will be used as the entry point for CluedIn and will be required as part of your DNS at a later stage. Next steps . The next step is to test access from the private network to CluedIn to ensure that traffic can reach the endpoint as expected. By default, ICMP packets do not respond, but HTTPS (443/TCP) will. It may cause an error due to certificate, but this can be corrected. When you’ve confirmed that traffic is flowing, the final steps are to set up host name/DNS and TLS. The following documents will help configure DNS and TLS: . | Configure DNS | Configure certificates | . ",
    "url": "/deployment/infra-how-tos/advanced-network#integration-options",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#integration-options"
  },"987": {
    "doc": "Advanced network configuration",
    "title": "Host name resolution",
    "content": "When an internal load balancer is used, it may no longer be possible for the cluedin-server pod to resolve the hostname. This will prevent the application from functioning correctly unless some additional steps are taken to allow resolution. One way to achieve this is to use an Azure Private DNS zone linked to the kubernetes vnet. The other is to add the hostAlias section to your CluedIn values file which will get passed down to the servers host file. application: cluedin: hostAliases: - hostnames: - app.cluedin.com ip: {privateIP} . If you need assistance with this, please reach out to CluedIn support. ",
    "url": "/deployment/infra-how-tos/advanced-network#host-name-resolution",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network#host-name-resolution"
  },"988": {
    "doc": "Advanced network configuration",
    "title": "Advanced network configuration",
    "content": " ",
    "url": "/deployment/infra-how-tos/advanced-network",
    
    "relUrl": "/deployment/infra-how-tos/advanced-network"
  },"989": {
    "doc": "Pre-installation checklist",
    "title": "On this page",
    "content": ". | Check qualification . | Navigate to Billing Policies in Azure Portal | . | Check quota | Register resource providers . | Manual registration of resource providers | Automatic registration of resource providers | Verification script | . | Configure firewall settings | Configure network settings . | Define VNet | Analyze CluedIn network configuration | . | Analyze Azure policies and tagging | Results | Next steps | . In this article, you will learn about the pre-installation processes that you must perform to ensure successful installation of CluedIn PaaS. ",
    "url": "/deployment/azure-marketplace/step-2#on-this-page",
    
    "relUrl": "/deployment/azure-marketplace/step-2#on-this-page"
  },"990": {
    "doc": "Pre-installation checklist",
    "title": "Check qualification",
    "content": "As a Microsoft Azure Administrator, you should have expertise in implementing, managing, and monitoring your organization’s Microsoft Azure environment. Because CluedIn will be installed in your Azure resource group, you need to be the Owner of that resource group or at least the Contributor to that resource group. In addition, you should have permission to purchase paid applications from the Azure Marketplace. Set the permission to Free + Paid to complete the purchase registration in the Azure Marketplace. Navigate to Billing Policies in Azure Portal . To access Policies for a specific billing tenant in the Azure Portal: . | Open the Azure Portal. | In the left-hand navigation menu, go to Cost Management. | Click on Billing Profiles. | Select the appropriate Tenant from the list. | In the sidebar, choose Settings. | Under Settings, click on Policies. | . Note: Ensure you have the appropriate permissions to view or modify billing policies. If you don’t see these options, verify your directory and tenant context in the portal. To learn how to allow purchases, see Azure Marketplace purchasing. ",
    "url": "/deployment/azure-marketplace/step-2#check-qualification",
    
    "relUrl": "/deployment/azure-marketplace/step-2#check-qualification"
  },"991": {
    "doc": "Pre-installation checklist",
    "title": "Check quota",
    "content": "You should perform a quota check to make sure that your subscription has enough quotas for creating VMs required for CluedIn clusters. Each cluster will contain multiple nodes which are made up of virtual machines (VM). There are 3 common VM family types that CluedIn will create as part of the AKS node pools. Your subscription must have enough spare vCPU quota to provision different nodes. For details on the quota for each VM, see the table below. | VM | Quota | . | System Pool (1 x Standard_DS2_v5) | At least 2 vCPUs in the Standard DSv5 Family vCPUs quota | . | General Pool (2 x Standard_D8s_v4) | At least 16 vCPUs in the Standard DSv4 Family vCPUs quota | . | Data Pool (3 x Standard D8as_v5) | At least 24 vCPUs in the Standard DASv5 Family vCPUs quota | . In addition, check if you have enough quota for your plan. | Plan | Quota | . | Essential &amp; PAYG | At least 8 vCPUs in the StandardDASv5Family quota | . | Professional | At least 16 vCPUs in the StandardDASv5Family quota | . | Elite | At least 32 vCPUs in the StandardDASv5Family quota | . And finally, ensure that the region you’re deploying to also has enough quota. This is called ‘Total Regional vCPUs’ in Azure and must be equal to or greater than the sum of all vCPUs being deployed. You can check if you have enough quota by running a verification script. If the quota is not available, then the installation will fail. ",
    "url": "/deployment/azure-marketplace/step-2#check-quota",
    
    "relUrl": "/deployment/azure-marketplace/step-2#check-quota"
  },"992": {
    "doc": "Pre-installation checklist",
    "title": "Register resource providers",
    "content": "Before using a resource provider, register your Azure subscription for that specific resource provider. Make sure that the following resource providers are registered: . | Microsoft.Cache | Microsoft.Capacity | Microsoft.Compute | Microsoft.ContainerService | Microsoft.EventHub | Microsoft.KeyVault | Microsoft.ManagedIdentity | Microsoft.Network | Microsoft.OperationalInsights | Microsoft.OperationsManagement | Microsoft.Resources | Microsoft.Storage | Microsoft.Sql | . You can register the resource providers in two ways: manually in the Azure portal and automatically by running a script. After you register the resource providers, run the verification script to make sure that all resource providers are registered. Manual registration of resource providers . This section contains the step-by-step procedure for manual registration of resource providers in the Azure portal. To register resource providers . | In the Azure portal, find and select the subscription where you need to install CluedIn. For more information about the subscription, see Get subscription and tenant IDs in the Azure portal. | On the left menu, under Settings, select Resource providers. | One by one, find and select the resource providers that you want to register, and then select Register. Wait until the status of the resource provider is changed from Registering to Registered. | Run the verification script to make sure that all resource providers are registered. | . For more information about registering resource providers, see Azure documentation. Automatic registration of resource providers . If you want a faster way to register the needed resource providers, run the script for automatic registration of resource providers. Prerequisites . | PowerShell 7. We recommend installing PowerShell via Winget: winget search Microsoft.PowerShell. For more details, see Installing PowerShell on Windows. | Azure CLI. We recommend installing Azure CLI via Winget: winget install -e --id Microsoft.AzureCLI. For more details, see Install Azure CLI on Windows. | Your Azure subscription ID. For more details, see Find your Azure subscription. | . To run the script for registering resource providers . | Download this script. | Open your PowerShell terminal and run the following: .\\pre-checks.ps1 &lt;subscrioptionId&gt; -Register # e.g.\\pre-checks.ps1 abc68d12-8e99-4890-b3a0-ca25816b1c26 -Register . You will get an autput similar to the following. | . Verification script . The verification script checks if you have enough quota and if all required resource providers are registered. Prerequisites . | PowerShell 7. We recommend installing PowerShell via Winget: winget search Microsoft.PowerShell. For more details, see Installing PowerShell on Windows. | Azure CLI. We recommend installing Azure CLI via Winget: winget install -e --id Microsoft.AzureCLI. For more details, see Install Azure CLI on Windows. | Your Azure subscription ID. For more details, see Find your Azure subscription. | Azure region that you will select during installation. | . To run the verification script . | Download this verification script. | Open your PowerShell terminal and run the following: .\\check.ps1 -SubscriptionId &lt;subscrioptionId&gt; -Location &lt;location&gt; # e.g.\\check.ps1 -SubscriptionId abc68d12-8e99-4890-b3a0-ca25816b1c26 -Location uksouth . As a result, you will get a file named results_location.json that will be stored in the same directory. The file will contain the details about the quota and the status of resource providers. | . Example output of quota check . \"Location\": \"westeurope\", \"Quotas\": [ { \"Name\": \"Standard Dv4 Family vCPUs\", \"Plan\": \"All\", \"Required\": 40, \"Available\": 50, \"IncreaseRequired\": false } . Example output of resource providers check . \"Providers\": [ { \"Name\": \"Microsoft.Authorization\", \"Registered\": \"Registered\" }, { \"Name\": \"Microsoft.Cache\", \"Registered\": \"Registered\" } . ",
    "url": "/deployment/azure-marketplace/step-2#register-resource-providers",
    
    "relUrl": "/deployment/azure-marketplace/step-2#register-resource-providers"
  },"993": {
    "doc": "Pre-installation checklist",
    "title": "Configure firewall settings",
    "content": "Your Azure Firewall should cover the following: . | Default AKS functionality – logs and pods should be able to see Kubernetes API Server (as recommended in Outbound network and FQDN rules for AKS clusters). | CluedIn resource access – resources needed for the CluedIn installation. | . For the list of firewall rules that should be added to your Azure Firewall, see Configure firewall. If the rules have not been added, the installation will fail. ",
    "url": "/deployment/azure-marketplace/step-2#configure-firewall-settings",
    
    "relUrl": "/deployment/azure-marketplace/step-2#configure-firewall-settings"
  },"994": {
    "doc": "Pre-installation checklist",
    "title": "Configure network settings",
    "content": "CluedIn is very flexible in terms of network configuration and comes out of the box with a new vnet with a predefined address space and subnet. CluedIn is isolated out of the box, and this should work for most setups, but we do allow granular control over the network to support zero-trust and integrations. If you prefer to use an existing vnet or would like to do some advanced configuration, please contact one of our infrastructure experts before installation to ensure it’s properly implemented as it can be challenging to change post-deployment. Define VNet . During the installation process, you will have the option to update the vnet from the default address space of 10.0.0.0/8 and a subnet 10.0.0.0/16 for the Azure Kubernetes Service. If you have an existing VNet and you want to reuse it for CluedIn installation, contact one of our infrastructure experts for assistance. You can specify the existing VNet in the Network and monitoring step of CluedIn installation. Analyze CluedIn network configuration . The following diagram shows default CluedIn network configuration after installation. The deployed Azure Kubernetes Service is deployed with a single Azure Load Balancer that is used for both ingress and egress traffic. It is possible to update this post-deployment so that access is only possible via internal vnet integration, but it does require a number of steps. If you would like support on this, please reach out to one of CluedIn’s infrastructure engineers to assist you with this change. More information regarding networking can be found in Advanced network configuration. ",
    "url": "/deployment/azure-marketplace/step-2#configure-network-settings",
    
    "relUrl": "/deployment/azure-marketplace/step-2#configure-network-settings"
  },"995": {
    "doc": "Pre-installation checklist",
    "title": "Analyze Azure policies and tagging",
    "content": "The installation wizard supplies consumers to add resource tags during runtime. Please be aware of how tagging works before going through the installation as additional steps may be required. Tags supplied at runtime will apply to the deployed CluedIn resources. This includes the AKS cluster, key vault, virtual network, log analytics, and a few other resources. It does not apply to the managed resource group, the AKS node resource group, or the managed application resource itself. This is a limitation of how the Marketplace Applications works. If your tenant has comprehensive tagging, it is recommended to add a temporary exemption for the subscription at install time, and then add tags post-deployment. ",
    "url": "/deployment/azure-marketplace/step-2#analyze-azure-policies-and-tagging",
    
    "relUrl": "/deployment/azure-marketplace/step-2#analyze-azure-policies-and-tagging"
  },"996": {
    "doc": "Pre-installation checklist",
    "title": "Results",
    "content": ". | You are qualified to perform the CluedIn installation process, and you have all the required permissions. | Your Azure subscription can sustain the required quota. | You have registered the required resource providers. | Your firewall is ready and configured to support the installation of CluedIn. | You have planned your network architecture. | Your security team is comfortable with CluedIn having co-ownership of the deployed resources. | . ",
    "url": "/deployment/azure-marketplace/step-2#results",
    
    "relUrl": "/deployment/azure-marketplace/step-2#results"
  },"997": {
    "doc": "Pre-installation checklist",
    "title": "Next steps",
    "content": "Start the CluedIn installation process as described in our Installation guide. ",
    "url": "/deployment/azure-marketplace/step-2#next-steps",
    
    "relUrl": "/deployment/azure-marketplace/step-2#next-steps"
  },"998": {
    "doc": "Pre-installation checklist",
    "title": "Pre-installation checklist",
    "content": " ",
    "url": "/deployment/azure-marketplace/step-2",
    
    "relUrl": "/deployment/azure-marketplace/step-2"
  },"999": {
    "doc": "Assign or remove roles",
    "title": "On this page",
    "content": ". | Assign roles . | Assign roles in Users | Assign roles in Roles | . | Remove roles | . In this article, you will learn how to assign and remove roles for users to manage their access to features within CluedIn. ",
    "url": "/administration/roles/assign-roles#on-this-page",
    
    "relUrl": "/administration/roles/assign-roles#on-this-page"
  },"1000": {
    "doc": "Assign or remove roles",
    "title": "Assign roles",
    "content": "After you add a user to CluedIn, you can give the user access to more features within the platform by adding other roles to the user. You can assign roles to the users in two places within the platform: . | Administration &gt; User Management &gt; Users – this option is useful when you want to assign multiple roles to one user. | Administration &gt; Roles – this option is useful when you want to assign one role to multiple users. | . The following diagram shows the flow of assigning roles to users. Assign roles in Users . You can review the roles that are already assigned to the user and assign other roles in the Users section. To assign roles to a user in the Users section . | On the navigation pane, go to Administration &gt; User Management. Then, select Users. | On the All users page, select the user to whom you want to assign the role. Then, go to the Roles tab. | Select Add role to user. | On the Add role to user pane, select the roles that you want to assign to the user. In the upper-right corner, select Add Roles, and then confirm that you want to assign the role to the user. A new role is assigned to the user and it is displayed on the Roles tab of the User Details page. The user will receive an email about the role changes. For the changes to take effect, the user has to sign out and sign in again. | . Assign roles in Roles . You can review the users who have been already added to the role and add other users in the Roles section. To assign roles to a user in the Roles section . | On the navigation pane, go to Administration &gt; Roles. | Select the role that you want to assign to the user. Then, go to the Users tab. Here, you can view the users who have been added to the role. | Select Add users. | On the Select users to add pane, find and select the users that you want to add to the role. In the upper-right corner, select Add users, and then confirm that you want to add the users to the role. The users are added to the role and they are displayed on the Users tab of the role. The users will receive an email about the role changes. For the changes to take effect, the users have to sign out and sign in again. | . ",
    "url": "/administration/roles/assign-roles#assign-roles",
    
    "relUrl": "/administration/roles/assign-roles#assign-roles"
  },"1001": {
    "doc": "Assign or remove roles",
    "title": "Remove roles",
    "content": "The following diagram shows the flow of removing roles from users. To remove roles from a user . | On the navigation pane, go to Administration &gt; Roles. Then, select the needed role. Alternatively, you can go to Administration &gt; User Management &gt; Users. Select a user, go to the Roles tab, and then select the role that you want to remove from the user. | Go to the Users tab. | Select the checkbox next to the user whom you want to remove from the role. | Select Remove from role, and then confirm your choice. The role is removed from the user. The user will receive an email about the role changes. For the changes to take the effect, the users have to sign out and sign in again. | . ",
    "url": "/administration/roles/assign-roles#remove-roles",
    
    "relUrl": "/administration/roles/assign-roles#remove-roles"
  },"1002": {
    "doc": "Assign or remove roles",
    "title": "Assign or remove roles",
    "content": " ",
    "url": "/administration/roles/assign-roles",
    
    "relUrl": "/administration/roles/assign-roles"
  },"1003": {
    "doc": "Using Python SDK for Automation",
    "title": "On this page",
    "content": ". | Two GraphQL API endpoints | Authentication | Exploring GraphQL API | Python SDK | . CluedIn, as a Master Data Management system, encourages users to work with data using a UI and low-code approach. You can do a full data cycle from ingesting data into CluedIn to data modeling, transformation, cleaning, enriching, deduplication, and export without writing a single line of code. However, there are situations when automation and basic scripting can save the day. Moreover, in the same way as we can free business users from dealing with IT and code, we can free them from repetitive actions in UI when, for example, you implement a CI/CD pipeline between your development, staging, and production instances. In this article, we will implement basic automation without knowing too much about it beforehand. Let’s start with the fact that CluedIn UI communicates with the server via GraphQL API. ",
    "url": "/playbooks/data-engineering-playbook/automation#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation#on-this-page"
  },"1004": {
    "doc": "Using Python SDK for Automation",
    "title": "Two GraphQL API endpoints",
    "content": "CluedIn provides two GraphQL API endpoints: . | /api/api/graphql - the endpoint used to query data. | /graphql - the endpoint for UI interactions. | . ",
    "url": "/playbooks/data-engineering-playbook/automation#two-graphql-api-endpoints",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation#two-graphql-api-endpoints"
  },"1005": {
    "doc": "Using Python SDK for Automation",
    "title": "Authentication",
    "content": "CluedIn uses JWT token-based authorization. There are two kinds of tokens in CluedIn: API Tokens and Access Tokens. API Tokens are used to extract or ingest data from and into CluedIn. You can create or copy an API Token from Administration -&gt; API Tokens. The Access Tokens are generated when you log in to CluedIn as a user. You need an API Token to use the /api/api/graphql endpoint. The /graphql endpoint only accepts Access Tokens, so you must log in with your email and password to obtain one. ",
    "url": "/playbooks/data-engineering-playbook/automation#authentication",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation#authentication"
  },"1006": {
    "doc": "Using Python SDK for Automation",
    "title": "Exploring GraphQL API",
    "content": "In this article, we explore a use case of automating the actions you usually do in UI with the help of CluedIn GraphQL API and CluedIn Python SDK. Let’s use a simple example: You want to automate Vocabulary creation. You can do it from the CluedIn UI, but you would prefer an automated approach to synchronize Vocabularies from another place. The first thing you can do is create a test Vocabulary in a browser and see how the API calls look. You can do it with the Network tab or with the help of GraphQL Network Inspector (better). Let’s try both approaches. First, I go to UI and create a test CluedIn Vocabulary while I have my Network tab opened: . Now, when I inspect GraphQL queries, I can see the createVocabulary mutation call: . Finding the right call is even easier with GraphQL Network Inspector: . No matter how we found it, now we can copy the GraphQL query: . mutation createVocabulary($vocabulary: InputVocabulary) { management { id createVocabulary(vocabulary: $vocabulary) { ...Vocabulary __typename } __typename } } fragment Vocabulary on Vocabulary { vocabularyId vocabularyName keyPrefix isCluedInCore entityTypeConfiguration { icon entityType displayName __typename } isDynamic isProvider isActive grouping createdAt providerId description connector { id name about icon __typename } __typename } . And the variables: . { \"vocabulary\": { \"vocabularyName\": \"Test\", \"entityTypeConfiguration\": { \"new\": false, \"icon\": \"Profile\", \"entityType\": \"/IMDb/Name\", \"displayName\": \"IMDb Name\" }, \"providerId\": \"\", \"keyPrefix\": \"test\", \"description\": \"\" } } . ",
    "url": "/playbooks/data-engineering-playbook/automation#exploring-graphql-api",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation#exploring-graphql-api"
  },"1007": {
    "doc": "Using Python SDK for Automation",
    "title": "Python SDK",
    "content": "Now, we can use the CluedIn Python SDK to create a Vocabulary. First of all, I create a file with my CluedIn credentials. You can also provide them via environment variables: . { \"domain\": \"172.167.52.102.sslip.io\", \"org_name\": \"foobar\", \"user_email\": \"admin@foobar.com\", \"user_password\": \"mysecretpassword\" } . Then, I install the CluedIn Python SDK: . %pip install cluedin . Next, I need to “log in” to CluedIn to get the Access Token: . import cluedin ctx = cluedin.Context.from_json_file('cluedin.json') ctx.get_token() . The ctx object now contains the Access Token, and I can use it to create a Vocabulary: . def create_vocabulary(ctx, name, prefix): query = \"\"\" mutation createVocabulary($vocabulary: InputVocabulary) { management { id createVocabulary(vocabulary: $vocabulary) { ...Vocabulary } } } fragment Vocabulary on Vocabulary { vocabularyId vocabularyName keyPrefix entityTypeConfiguration { icon entityType displayName } providerId description } \"\"\" variables = { \"vocabulary\": { \"vocabularyName\": name, \"entityTypeConfiguration\": { \"new\": False, \"icon\": \"Profile\", \"entityType\": \"/IMDb/Name\", \"displayName\": \"IMDb Name\" }, \"providerId\": \"\", \"keyPrefix\": prefix, \"description\": \"\" } } return cluedin.gql.org_gql(ctx, query, variables) . Let’s test it: . print(create_vocabulary(ctx, 'Foo Bar', 'foo.bar')) . Output: . { \"data\": { \"management\": { \"id\": \"management\", \"createVocabulary\": { \"vocabularyId\": \"e4528e92-f4ad-406a-aeab-756acc20fd01\", \"vocabularyName\": \"Foo Bar\", \"keyPrefix\": \"foo.bar\", \"entityTypeConfiguration\": { \"icon\": \"Profile\", \"entityType\": \"/IMDb/Name\", \"displayName\": \"IMDb Name\" }, \"providerId\": null, \"description\": \"\" } } } } . Check the UI: . Here are a couple more of examples used on real projects: . | Create missing Vocabularies and keys for a given Entity: ensure_vocab_keys.py | Explore a CluedIn Entity’s Data Parts to troubleshoot overmerging: scrutinize_entity.py | . ",
    "url": "/playbooks/data-engineering-playbook/automation#python-sdk",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation#python-sdk"
  },"1008": {
    "doc": "Using Python SDK for Automation",
    "title": "Using Python SDK for Automation",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/automation",
    
    "relUrl": "/playbooks/data-engineering-playbook/automation"
  },"1009": {
    "doc": "TLS Certificate",
    "title": "TLS Certificate",
    "content": "When you have a cluster with a public IP and DNS links your domain name to that cluster, it’s about time to think about the security and prepare a TLS-certificate that you that the CluedIn application will use when users access it via HTTPS. You can buy a certificate or get a free one via a service like Let’s Encrypt. It has to be either a wildcard certificate for all the subdomains of your primary domain or a multi-domain certificate that will cover the URLs listed in the DNS. ",
    "url": "/deployment/azure/certificate",
    
    "relUrl": "/deployment/azure/certificate"
  },"1010": {
    "doc": "Clean project reference",
    "title": "On this page",
    "content": ". | Clean project statuses | Clean project status workflow | Clean project audit log actions | . In this article, you will find reference information to help you understand the statuses of clean projects and the process of working with them. ",
    "url": "/preparation/clean/clean-reference#on-this-page",
    
    "relUrl": "/preparation/clean/clean-reference#on-this-page"
  },"1011": {
    "doc": "Clean project reference",
    "title": "Clean project statuses",
    "content": "The following table provides the description of the clean project statuses. | Status | Description | . | New | The clean project has been created, no results have been generated yet. | . | Generating | CluedIn is generating the results of the clean project. The records that match the filters and selected properties from the configuration are loaded into the clean application. This status also appears when you are regenerating the results to load the latest data to the clean application. | . | Generation aborting | The process of generating or regenerating the results of the clean project is being cancelled. The status will shortly change to New. | . | Ready for clean | CluedIn has generated the results of the clean project and you are now ready to perform cleaning activities in the clean application. | . | Ready to process | The clean project contains some changes of values that were made in the clean application. | . | Processing | CluedIn is publishing fixed values to the golden records. | . | Processing aborting | The process of publishing fixed values to the golden records is being cancelled. The status will shortly change to Ready to process. | . | Processed | CluedIn has published the fixed values to the golden records. | . | Reverting | CluedIn is reverting the results of the clean project, returning the values in golden records to their state before cleaning. | . | Revert aborting | The process of reverting changes is being cancelled. The status will shortly change to Ready to process. | . | Archived | The clean project is no longer active. Once the project is archived, it cannot be unarchived. | . ",
    "url": "/preparation/clean/clean-reference#clean-project-statuses",
    
    "relUrl": "/preparation/clean/clean-reference#clean-project-statuses"
  },"1012": {
    "doc": "Clean project reference",
    "title": "Clean project status workflow",
    "content": "The following diagram shows the clean project workflow along with its statuses and main activities. The Archived status is not shown in the diagram, but you can archive the clean project when it is in any status except Generation aborting, Processing aborting, and Revert aborting. ",
    "url": "/preparation/clean/clean-reference#clean-project-status-workflow",
    
    "relUrl": "/preparation/clean/clean-reference#clean-project-status-workflow"
  },"1013": {
    "doc": "Clean project reference",
    "title": "Clean project audit log actions",
    "content": "Whenever some changes or actions are made in the clean project, they are recorded and can be found on the Audit Log tab. These actions include the following: . | Create a clean project | Add users to owners | Update a clean project | Generate results | Generate rules | Commit a project | Regenerate results | Revert (undo) changes | Cancel committing a project | Cancel generation of results | Cancel reverting (undoing) changes | Archive a project | . ",
    "url": "/preparation/clean/clean-reference#clean-project-audit-log-actions",
    
    "relUrl": "/preparation/clean/clean-reference#clean-project-audit-log-actions"
  },"1014": {
    "doc": "Clean project reference",
    "title": "Clean project reference",
    "content": " ",
    "url": "/preparation/clean/clean-reference",
    
    "relUrl": "/preparation/clean/clean-reference"
  },"1015": {
    "doc": "Copilot Integration",
    "title": "Copilot Integration",
    "content": "CluedIn Copilot is an AI assistant designed to streamline your interactions within the CluedIn platform using natural language commands. With the AI assistant, tasks such as adding tags to records based on specific criteria become effortless—simply prompt the assistant, and it automatically generates rules for you. The CluedIn Copilot experience can be personalized according to your language preference. Note that CluedIn Copilot is not enabled by default. For more information, see Get access to CluedIn Copilot. The following diagram shows how CluedIn AI assistant can help you with your day-to-days tasks in CluedIn. Sometimes you may encounter issues with CluedIn Copilot where it does not understand your requests as expected. In such cases, as with any generative AI solution, please start over and create a new chat. Notes on CluedIn’s integration with Azure OpenAI . CluedIn integrates with Azure OpenAI services solely to enable you to make requests and store responses through our platform. To interact with Azure OpenAI services via CluedIn’s Rule Engine, AI mapping, or Copilot, you must decide where to host Azure OpenAI, including which tenants and locations to use. CluedIn does not provide a default Azure OpenAI token; you must configure CluedIn with your own Azure OpenAI token. Selection of Azure OpenAI services and models within your environment is entirely at your discretion. CluedIn transmits your prompts and the data contained within them to Azure OpenAI, giving you full control over the models and deployments used within your environment. Your prompts and results are also stored in CluedIn’s SQL server to preserve your prompt history. Please note that CluedIn cannot comment on how Azure OpenAI manages or processes data internally. Data handling by Azure OpenAI is governed solely by the agreement established directly between you and Microsoft Azure. ",
    "url": "/microsoft-integration/copilot-integration",
    
    "relUrl": "/microsoft-integration/copilot-integration"
  },"1016": {
    "doc": "Copy snapshots runbook",
    "title": "On this page",
    "content": ". | Typical snapshots | Automation account | Transfer of snapshots | Input parameters | Process | . The copy snapshots runbook can be triggered manually or on a set schedule. It copies snapshots from one Azure location to another. However, if your source and target CluedIn instances are the same or reside in the same Azure location, this runbook is not required. This runbook is a PowerShell script that CluedIn will provide as needed. Prerequisites . | At least one active CluedIn cluster with a valid license | The runbook script | An automation account | A storage account | Sufficient permissions | . ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#on-this-page",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#on-this-page"
  },"1017": {
    "doc": "Copy snapshots runbook",
    "title": "Typical snapshots",
    "content": "Typically, Cluedin requires nine snapshots, which will be attached to the disks during a restore process. The snapshot type is incremental. ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#typical-snapshots",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#typical-snapshots"
  },"1018": {
    "doc": "Copy snapshots runbook",
    "title": "Automation account",
    "content": "An automation account must be provided, the runbook will be installed into the the automation account. Typically, the runbook should be triggered once the snapshots have been created successfully. ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#automation-account",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#automation-account"
  },"1019": {
    "doc": "Copy snapshots runbook",
    "title": "Transfer of snapshots",
    "content": "Snapshot transfers occur over the Microsoft backbone and can take anywhere from 20 minutes for small datasets to several hours for larger ones. Additionally, Azure inter-region transfer costs will apply. ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#transfer-of-snapshots",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#transfer-of-snapshots"
  },"1020": {
    "doc": "Copy snapshots runbook",
    "title": "Input parameters",
    "content": "| Parameter | Default | Description | . | LicenseKey | required | License key tag on snapshot | . | Timestamp | required | Timestamp on snapshot | . | SourceResourceGroup | required | Name of source resource group | . | TargetSubscriptionId | required | ID of target Azure subscription | . | TargetResourceGroup | required | Name of target resource group | . | TargetRegion | required | Target Azure location | . ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#input-parameters",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#input-parameters"
  },"1021": {
    "doc": "Copy snapshots runbook",
    "title": "Process",
    "content": ". When the source and target resource groups are in the same location, the snapshot copy is not neccessary. ",
    "url": "/paas-operations/automation/copy-snapshots-runbook#process",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook#process"
  },"1022": {
    "doc": "Copy snapshots runbook",
    "title": "Copy snapshots runbook",
    "content": " ",
    "url": "/paas-operations/automation/copy-snapshots-runbook",
    
    "relUrl": "/paas-operations/automation/copy-snapshots-runbook"
  },"1023": {
    "doc": "Create and manage workflows",
    "title": "On this page",
    "content": ". | Workflow templates | Create a workflow | View workflow details | . To use the Workflow module in CluedIn, you need to configure Power Automate integration. In this article, you will learn how to create workflows for automating certain approval processes. With workflows, you can easily streamline and track such processes as modifying vocabularies, inviting users, creating rules, and more. ",
    "url": "/workflow/create-and-manage-workflows#on-this-page",
    
    "relUrl": "/workflow/create-and-manage-workflows#on-this-page"
  },"1024": {
    "doc": "Create and manage workflows",
    "title": "Workflow templates",
    "content": "Currently, you can automate the approval process for certain actions in CluedIn. The following table provides the description of such actions along with the name of the workflow templates. | Action in CluedIn | Workflow template | Description | . | A stream being idle | Stream Idle Event | If the stream has not streamed golden records for 30 seconds, the idle stream event is triggered. | . | Vocabulary modification | Vocabulary Change Approval | If somebody tries to make changes to the vocabulary, an approval request is sent to the owners of the vocabulary. | . | Vocabulary key modification | Vocabulary Key Change Approval | If somebody tries to make changes to the vocabulary key, an approval request is sent to the owners of the vocabulary key. | . | Processing new records* | Batched Clues Approval | If somebody tries to process new records, an approval request is sent to the users with the same or higher claim access level. | . | Editing a property in the record* | Batched Clues Approval | If somebody tries to edit a property in the record, an approval request is sent to the users with the same or higher claim access level. | . | Rule modification | Processing Rule Change Approval | If somebody tries to make changes to the rule, an approval request is sent to the owners of the rule. | . | Receiving internal CluedIn notifications | Notification | If somebody receives an internal notification in CluedIn, the same notification is sent to the external systems such as Outlook or the Approval app in Teams. | . * The approval requests for this action are sent only if the business domain of the records has the Batch approval workflow option enabled. One action can be used only in one workflow. For example, if you created a workflow with the Vocabulary Change Approval action, you can’t select this action in another workflow. If you don’t create a workflow to automate the approval process for a specific action, no external approval requests will be sent to the users. However, the internal approval process is still in place, so the responsible users will receive change requests in CluedIn. ",
    "url": "/workflow/create-and-manage-workflows#workflow-templates",
    
    "relUrl": "/workflow/create-and-manage-workflows#workflow-templates"
  },"1025": {
    "doc": "Create and manage workflows",
    "title": "Create a workflow",
    "content": "If you want to automate the approval process for actions mentioned in the previous section, create a corresponding workflow. Only SSO users can create workflows in CluedIn. If you see the following message when trying to create a workflow, it means you are not signed in as an SSO user and will not be able to set up a workflow. To create a workflow . | On the navigation pane, go to Workflow &gt; Workflow Builder. | Select Create Workflow. | Enter the name of the workflow. | Select the template for a workflow. The name of the template corresponds to the action that triggers the approval flow. | Select Create. Creating a workflow might take some time because it is being sent to Power Automate. After the workflow is created, you can view it in the Power Automate widget in CluedIn. | Review the workflow steps. In all workflows, except for Notification and Stream Idle Event, each step is configured automatically. However, if needed, you can modify the Send for approval step, which contains approval request title and details. We do not recommend modifying the Send approval response step. In the Notification and Stream Idle Event workflows, you need to add and configure an action to send a notification to the desired external system. | Turn on the toggle next to the workflow status to enable it. Now, when the action that triggers the workflow is executed, the approval request is sent to the responsible users in Outlook or Approvals app in Teams. | . ",
    "url": "/workflow/create-and-manage-workflows#create-a-workflow",
    
    "relUrl": "/workflow/create-and-manage-workflows#create-a-workflow"
  },"1026": {
    "doc": "Create and manage workflows",
    "title": "View workflow details",
    "content": "After the workflow is created, it is displayed in the Power Automate widget in CluedIn. Each workflow contains two tabs: . | Configuration – here, you can view the flow steps: the action that triggers the flow, the approval action that sends the approval request, and the approval response action that sends the response back to CluedIn. | Properties – here, you can view the flow details in the following sections: . | Details – find the name, status, and creation and modification dates. You can also add or edit the workflow description by selecting Edit. | 28-day run history – find the flow run history from the last 28 days. To view the details of a specific flow run, select it from the Start column. You can view the specific action that triggered the flow and the response that came back to CluedIn. | . | . ",
    "url": "/workflow/create-and-manage-workflows#view-workflow-details",
    
    "relUrl": "/workflow/create-and-manage-workflows#view-workflow-details"
  },"1027": {
    "doc": "Create and manage workflows",
    "title": "Create and manage workflows",
    "content": " ",
    "url": "/workflow/create-and-manage-workflows",
    
    "relUrl": "/workflow/create-and-manage-workflows"
  },"1028": {
    "doc": "Create a deduplication project",
    "title": "On this page",
    "content": ". | Create a deduplication project | Add a matching rule | . In this article, you will learn how to create a deduplication project and configure matching rules for detecting duplicates. The approach described here ensures that you can easily revert merges when needed. ",
    "url": "/management/deduplication/create-a-deduplication-project#on-this-page",
    
    "relUrl": "/management/deduplication/create-a-deduplication-project#on-this-page"
  },"1029": {
    "doc": "Create a deduplication project",
    "title": "Create a deduplication project",
    "content": "Before creating a deduplication project, take the following aspects into account: . | Number of golden records that you want to check for duplicates. For an extensive set of data with hundreds of thousands or millions of records, use advanced filters to narrow down the number of records in the project to test your configuration. When you are satisfied with the configuration, you can then modify the filters and run the project on the entire set of data. For example, if you want to deduplicate all golden records of the Company business domain, start by narrowing down the companies to a specific country. To do this, apply two filter rules: one to identify all golden records of the Company business domain, and another to find all golden records that match the specific country. This approach allows you to refine your matching rules on a targeted subset of data. When you are satisfied with the configuration, simply remove the filter rule for the specific country and run the project for all golden records of the Company business domain. | Matching functions that you want to use for detecting duplicates. To ensure a faster deduplication process and make it easier to revert merges, create separate projects for equality matching functions and fuzzy matching functions. You can create multiple deduplication projects. | . To create a deduplication project . | On the navigation pane, go to Management &gt; Deduplication. | Select Create Deduplication Project. | Enter the name of the deduplication project. | In the Choose project type section, select an option for identifying the golden records that you want to deduplicate: . | By business domain – select the business domain; all golden records belonging to the selected business domain will be checked for duplicates. You can add multiple business domain. This is useful when you want to run a deduplication project across similar business domain. | Using advanced filters – add filter rules; all golden records that meet the filter criteria will be checked for duplicates. You can add multiple filter rules. Read more about filters here. This option is useful when you are working with a large set of data. You can narrow down the number of golden records and run a deduplication project on a sample set of data to make sure your matching rules work correctly. When you are confident in the effectiveness of your configuration with the sample set, you can then modify the filters and run the project on a larger set of data. | . | Select Create. After you create a deduplication project, add matching rules for detecting duplicates among golden records that correspond to the selected business domain or filter criteria. | . ",
    "url": "/management/deduplication/create-a-deduplication-project",
    
    "relUrl": "/management/deduplication/create-a-deduplication-project"
  },"1030": {
    "doc": "Create a deduplication project",
    "title": "Add a matching rule",
    "content": "Deterministic and probabilistic matching rules play a crucial role in detecting duplicates. You can add multiple matching rules into a single deduplication project. However, the fewer rules you have in a project, the faster it runs and the easier it is to revert merges if necessary. In the project, matching rules are combined using the OR logical operator, while matching criteria within a rule are combined using the AND logical operator. To add a matching rule . | On the Matching Rules tab, select Add Matching Rule. | Enter the name of the matching rule, and select Next. | On the Matching Criteria tab, do the following: . | Enter the name of the matching criteria. | Select the property type—vocabulary key or property—that will be checked for duplicate values. If the property or vocabulary key contains empty values, they will be ignored when generating the deduplication project results. | Choose the matching function for detecting duplicates. | (Optional) Select the normalization rules to apply during duplicate detection. These rules are temporarily applied solely for the purpose of identifying duplicates. For example, selecting To lower-case means that the system will convert values to lower case before comparing them to identify duplicates. | Select Next. | . | On the Preview tab, do one of the following: . | If you want to add another matching criteria, select Add Matching Criteria, and then repeat step 3. Multiple matching criteria within one rule are combined using the AND logical operator. It means that a golden record is identified as a duplicate only if it meets all the matching criteria specified in the rule. However, if you want to add another matching criteria combined using the OR logical operator, then add another rule. | If you are satisfied with the matching rule configuration, select Add Rule. | . After you add matching rules, generate matches to detect duplicates among golden records that correspond to the selected business domain or filter criteria. | . ",
    "url": "/management/deduplication/create-a-deduplication-project#add-a-matching-rule",
    
    "relUrl": "/management/deduplication/create-a-deduplication-project#add-a-matching-rule"
  },"1031": {
    "doc": "Data catalog",
    "title": "Data catalog",
    "content": "A data catalog is a collection of metadata that encompasses essential information about the organization, structure, usage, and lineage of data within the system. It comprises vocabularies and vocabulary keys, acting as fundamental building blocks for constructing a comprehensive data model. The following diagram shows the basic steps of defining a data model in CluedIn. This section covers the following areas: . | Modeling approaches – explore the options available for building data models and learn how to use vocabulary key mapping to unify your model across various sources. | Vocabularies – learn how to create and manage vocabularies for storing and organizing groups of vocabulary keys. | Vocabulary keys – learn how to create and manage vocabulary keys for defining characteristics of golden records. | Search the data catalog – learn how to make the process of finding metadata faster and more efficient by using filters. | Data types – find reference information about data types that can be used in vocabulary keys. | . ",
    "url": "/management/data-catalog",
    
    "relUrl": "/management/data-catalog"
  },"1032": {
    "doc": "Azure Dedicated SQL Pool connector",
    "title": "On this page",
    "content": ". | Configure Azure Dedicated SQL Pool connector | Limitations of Azure Synapse Analytics | . This article outlines how to configure the Azure Dedicated SQL Pool connector to push data from CluedIn to Microsoft’s Dedicated SQL Pool, which is a part of Azure Synapse Analytics platform. ",
    "url": "/consume/export-targets/azure-dedicated-sql-pool-connector#on-this-page",
    
    "relUrl": "/consume/export-targets/azure-dedicated-sql-pool-connector#on-this-page"
  },"1033": {
    "doc": "Azure Dedicated SQL Pool connector",
    "title": "Configure Azure Dedicated SQL Pool connector",
    "content": "Prerequisites: Make sure you have a Dedicated SQL Pool resource created under Azure Synapse Studio or Azure Synapse Analytics. For more information on how to create a dedicated SQL pool using Synapse Studio, see Microsoft documentation. To configure Azure Dedicated SQL Pool connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Azure Dedicated SQL Pool Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | Host – dedicated SQL endpoint in the workspace. To find this value, sign in to Azure Synapse Analytics, and then select the needed workspace. On the Overview page, copy the value from Dedicated SQL endpoint. | Database Name – name of the SQL pool where you want to store the data from CluedIn. | Username – SQL admin username in the workspace. | Password – SQL admin password, which was provided at the creation of a workspace. If you forget the password, you can reset it by selecting Reset SQL admin password as displayed on the screenshot. | Port Number – this is an optional field. You may provide the port number if it is set up in the database. | . | Test the connection to make sure it works, and then select Add. Now, you can select the Azure Dedicated SQL Pool connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/azure-dedicated-sql-pool-connector#configure-azure-dedicated-sql-pool-connector",
    
    "relUrl": "/consume/export-targets/azure-dedicated-sql-pool-connector#configure-azure-dedicated-sql-pool-connector"
  },"1034": {
    "doc": "Azure Dedicated SQL Pool connector",
    "title": "Limitations of Azure Synapse Analytics",
    "content": "Deciding whether to use the Azure Dedicated SQL Pool connector to stream golden records from CluedIn to Azure Synapse Analytics requires careful consideration. This section outlines several limitations of Azure Synapse Analytics, explaining why it may not be the best choice for streaming large sets of data directly from CluedIn. As an alternative, we recommend using One Lake or Azure Data Lake. Not optimized for frequent small updates . Azure Synapse Analytics is a massively parallel processing (MPP) data warehouse designed for handling large-scale, batch-oriented analytical workloads. It is not optimized for frequent, small data updates or transactional workloads. Frequent small updates might lead to inefficient use of resources, data imbalances that impact query performance, and overhead of write operations. High data ingestion costs . Frequent data pushes require the use of data pipelines, storage transactions, and potentially recomputing indexes and materialized views in Synapse. Constant data ingestion using Synapse pipelines can result in high operational costs. In addition, frequent ingestion competes with analytical workloads for resources, reducing overall performance. Latency in Synapse processes . Synapse works best when given time to process large volumes of data, but frequent updates introduce latency at every stage. Loading data into Synapse requires preprocessing, uploading to staging layers, and running COPY commands, which are not instant. In addition, frequent changes to metadata (for example, partitions, indexes) slow down the ingestion process. ",
    "url": "/consume/export-targets/azure-dedicated-sql-pool-connector#limitations-of-azure-synapse-analytics",
    
    "relUrl": "/consume/export-targets/azure-dedicated-sql-pool-connector#limitations-of-azure-synapse-analytics"
  },"1035": {
    "doc": "Azure Dedicated SQL Pool connector",
    "title": "Azure Dedicated SQL Pool connector",
    "content": " ",
    "url": "/consume/export-targets/azure-dedicated-sql-pool-connector",
    
    "relUrl": "/consume/export-targets/azure-dedicated-sql-pool-connector"
  },"1036": {
    "doc": "Delete individual data parts",
    "title": "Delete data parts in History",
    "content": "On the History tab of a golden record, you can find all data parts that form a golden record. If you don’t need a specific data part in a golden record, you can delete it. To delete data parts on the History tab . | Find the data part that you want to delete. | On the right side of the row, select the three-dot button (⋮) &gt; Delete. Alternatively, select the record ID. In the data part details pane that opens, select Delete. | Review the data part that will be deleted, and then select Next. | Confirm that you want to delete the data part by entering DELETE. Then, select Confirm. The data part is removed from the History tab. Consequently, all changes that the data part contributed to the golden record are also removed. | . ",
    "url": "/golden-records/delete-data-parts-from-golden-records#delete-data-parts-in-history",
    
    "relUrl": "/golden-records/delete-data-parts-from-golden-records#delete-data-parts-in-history"
  },"1037": {
    "doc": "Delete individual data parts",
    "title": "Delete data parts in Topology",
    "content": "On the Topology tab of a golden record, you view how the data parts forming a golden record are linked. If you don’t need a specific data part in a golden record, you can delete it. To delete data parts on the Topology pane . | Find and select the data part that you want to delete. | In the data part details pane that opens, select Delete. | Review the data part that will be deleted, and then select Next. | Confirm that you want to delete the data part by entering DELETE. Then, select Confirm. The data part is removed from the Topology tab. Consequently, all changes that the data part contributed to the golden record are also removed. | . ",
    "url": "/golden-records/delete-data-parts-from-golden-records#delete-data-parts-in-topology",
    
    "relUrl": "/golden-records/delete-data-parts-from-golden-records#delete-data-parts-in-topology"
  },"1038": {
    "doc": "Delete individual data parts",
    "title": "Delete individual data parts",
    "content": "In this article, you will learn how to delete specific data parts (also referred to as records) from a golden record. For more information about records, data parts, and golden records, see Data life cycle. If you no longer need specific data parts that form a golden record, you can easily delete them. You can delete data parts from a golden record in two places: . | On the History tab – use this option if you want to have a detailed and chronological view of all data parts forming a golden record. For more details, see History. | On the Topology tab – use this option if you want to have a visual representation of how a golden record is formed. | . Deleting data parts from a golden record is different from removing records from a data source. When you remove records from a data source, all data parts originating from that source are removed from every golden record in which they are used. On the contrary, deleting data parts from a golden record allows you to delete specific data parts originating from almost any source. In this case, the data parts are deleted only from the specific golden record. Deleting data parts from either the History tab or the Topology tab produces the same result: the data part is removed from the golden record. However, there are several cases when you cannot delete data parts from a golden record. A data part cannot be deleted from a golden record under the following conditions: . | If it is the only data part that contains the origin code of the golden record. | If it is a manual merge data part. You can undo the manual merge on the Topology tab. | If it is a deduplication project merge data part. You can undo the deduplication project merge on the Topology tab. | If it is one of two data parts that contribute to the merge data part. | . Even though you can delete a data part from an enricher, keep in mind that it may appear again. To prevent this, deactivate the related enricher before deleting a data part. ",
    "url": "/golden-records/delete-data-parts-from-golden-records",
    
    "relUrl": "/golden-records/delete-data-parts-from-golden-records"
  },"1039": {
    "doc": "Enricher reference",
    "title": "On this page",
    "content": ". | Azure OpenAI | Brreg | BvD | Clearbit | Companies House | CVR | DuckDuckGo | Duns &amp; Bradstreet | Gleif | Google Maps | Knowledge Graph | Libpostal | Open Corporates | PermId | Vatlayer | Web | . In this article, you will find reference information about the built-in enrichers available in CluedIn. Please note that the enrichers are not included in the CluedIn license. Each enricher is an open-source package provided by the CluedIn team for free to help you enrich your golden records with information from external sources. If you need a custom enricher, use the REST API enricher or contact us, and we will build one for you. ",
    "url": "/preparation/enricher/enricher-reference#on-this-page",
    
    "relUrl": "/preparation/enricher/enricher-reference#on-this-page"
  },"1040": {
    "doc": "Enricher reference",
    "title": "Azure OpenAI",
    "content": "The Azure OpenAI enricher allows you to enhance data quality by providing more complete, current, and detailed information for your golden records. It supports the following endpoints: . | {baseUrl}/openai/deployments/{deploymentName}/completions?api-version=2022-12-01 . | {baseUrl}/openai/deployments/deploymentName}/chat/completions?api-version=2024-06-01 . | . | Package name | Package version | Source code | . | CluedIn.Enricher.AzureOpenAI | 4.4.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#azure-openai",
    
    "relUrl": "/preparation/enricher/enricher-reference#azure-openai"
  },"1041": {
    "doc": "Enricher reference",
    "title": "Brreg",
    "content": "The Brreg enricher retrieves a wide range of information about Norwegian and foreign businesses operating in Norway. It supports the following endpoints: . | http://data.brreg.no/enhetsregisteret/api/enheter/{id}, where {id} is the Brreg code. | http://data.brreg.no/enhetsregisteret/api/enheter?page=0&amp;size=30&amp;navn={name}, where {name} is the company name. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.Bregg | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.Bregg | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.Bregg | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#brreg",
    
    "relUrl": "/preparation/enricher/enricher-reference#brreg"
  },"1042": {
    "doc": "Enricher reference",
    "title": "BvD",
    "content": "The BvD enricher retrieves a wide range of information about companies. | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.BvD | 4.4.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#bvd",
    
    "relUrl": "/preparation/enricher/enricher-reference#bvd"
  },"1043": {
    "doc": "Enricher reference",
    "title": "Clearbit",
    "content": "The Clearbit enricher retrieves company logo and domain information. It supports the following endpoint: . | https://autocomplete.clearbit.com/v1/companies/suggest?query= | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.ClearBit | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.ClearBit | 4.1.1 | Source code | . | CluedIn.Provider.ExternalSearch.ClearBit | 4.1.0 | Source code | . | CluedIn.Provider.ExternalSearch.ClearBit | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#clearbit",
    
    "relUrl": "/preparation/enricher/enricher-reference#clearbit"
  },"1044": {
    "doc": "Enricher reference",
    "title": "Companies House",
    "content": "The Companies House enricher retrieves information about UK companies. This enricher uses the company name to return public information including registered office address, filing history, and so on. The Companies House enricher supports the following endpoints: . | https://api.companieshouse.gov.uk/search/company/{companyNumber}, where {companyNumber} is the Companies House number – this endpoint is called when the Companies House number is provided. | https://api.companieshouse.gov.uk/search/companies?q={name}, where {name} is the company name – this endpoint is called when the Companies House number is not provided. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.CompanyHouse | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.CompanyHouse | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.CompanyHouse | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#companies-house",
    
    "relUrl": "/preparation/enricher/enricher-reference#companies-house"
  },"1045": {
    "doc": "Enricher reference",
    "title": "CVR",
    "content": "The CVR enricher retrieves a wide range of information about companies registered in Denmark. It supports the following endpoint: . | http://{username}:{password}@distribution.virk.dk/cvr-permanent/_search, where {username} and {password} are the valid credentials for your CVR account. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.1.1 | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.1.0 | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.0.2 | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.CVR | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#cvr",
    
    "relUrl": "/preparation/enricher/enricher-reference#cvr"
  },"1046": {
    "doc": "Enricher reference",
    "title": "DuckDuckGo",
    "content": "The DuckDuckGo enricher retrieves general information about organizations from the DuckDuckGo search engine. It supports the following endpoint: . | https://api.duckduckgo.com | . | Package name | Package version | Source code | . | CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider | 4.4.0 | Source code | . | CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider | 4.3.0 | Source code | . | CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider | 4.0.1 | Source code | . | CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#duckduckgo",
    
    "relUrl": "/preparation/enricher/enricher-reference#duckduckgo"
  },"1047": {
    "doc": "Enricher reference",
    "title": "Duns &amp; Bradstreet",
    "content": "The Duns &amp; Bradstreet enricher retrieves information about organizations. It supports the following endpoints: . | {hostUrl}/data/duns/{dunsNumber} – this endpoint is called when the D&amp;B number is provided. | {hostUrl}/v1/match/extendedMatch – this endpoint is called when the D&amp;B number is not provided. | . | Package name | Package version | Source code | . | CluedIn.ExternalSearch.Providers.DNB | - | Contact CluedIn for details | . ",
    "url": "/preparation/enricher/enricher-reference#duns--bradstreet",
    
    "relUrl": "/preparation/enricher/enricher-reference#duns--bradstreet"
  },"1048": {
    "doc": "Enricher reference",
    "title": "Gleif",
    "content": "The Gleif enricher retrieves information about organizations using the Legal Entity Identifier (LEI). It supports the following endpoint: . | https://api.gleif.org/api/v1/lei-records?page[size]=1&amp;page[number]=1&amp;filter[lei]={leicode}, where {leicode} is the LEI code of the company. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.Gleif | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.Gleif | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.Gleif | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#gleif",
    
    "relUrl": "/preparation/enricher/enricher-reference#gleif"
  },"1049": {
    "doc": "Enricher reference",
    "title": "Google Maps",
    "content": "The Google Maps enricher cleans, standardizes, and enriches international postal addresses with geocoding information. This enricher returns correct information about company address. The Google Maps enricher supports the following endpoints: . | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationName}{organizationAddress}{organizationCity}{organizationZip}{organizationState}{organizationCountry} – if organization name, address, city, zip, state, and country are provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationName} – if organization name is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationAddress} – if organization address is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={location} – if one of the addresses other than organization address is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={location}&amp;location={latitude}{longitude} – if one of the addresses other than organization address and latitude and longitude are provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query=location={latitude}{longitude} – if latitude and longitude are provided. | https://maps.googleapis.com/maps/api/place/details/json?placeid={id} – the ID is taken from the previous text search API (the result of https://maps.googleapis.com/maps/api/place/textsearch/json). | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.1.1 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.1.0 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.0.5 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.0.4 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.0.3 | Source code | . | CluedIn.Provider.ExternalSearch.GoogleMaps | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#google-maps",
    
    "relUrl": "/preparation/enricher/enricher-reference#google-maps"
  },"1050": {
    "doc": "Enricher reference",
    "title": "Knowledge Graph",
    "content": "The Knowledge Graph enricher retrieves descriptions of organizations using the Google Knowledge Graph API. It supports the following endpoint: . | https://kgsearch.googleapis.com?query={nameOrUri}&amp;key={apiKey}&amp;limit=10&amp;indent=true, where {nameOrUri} is the name or website of the organization and {apiKey} is your API key for accessing Google’s Knowledge Graph database. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.KnowledgeGraph | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.KnowledgeGraph | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.KnowledgeGraph | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#knowledge-graph",
    
    "relUrl": "/preparation/enricher/enricher-reference#knowledge-graph"
  },"1051": {
    "doc": "Enricher reference",
    "title": "Libpostal",
    "content": "The Libpostal enricher parses and normalizes street addresses using statistical NLP and open data. This enricher returns international street address. The Libpostal enricher supports the following endpoint: . | http://&lt;host&gt;:&lt;port&gt;/parser body = {query: {address}} | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.Libpostal | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.Libpostal | 4.1.2 | Source code | . | CluedIn.Provider.ExternalSearch.Libpostal | 4.1.1 | Source code | . | CluedIn.Provider.ExternalSearch.Libpostal | 4.1.0 | Source code | . | CluedIn.Provider.ExternalSearch.Libpostal | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#libpostal",
    
    "relUrl": "/preparation/enricher/enricher-reference#libpostal"
  },"1052": {
    "doc": "Enricher reference",
    "title": "Open Corporates",
    "content": "The Open Corporates enricher retrieves information on all companies worldwide. This enricher returns VAT/TAX number. The Open Corporates enricher supports the following endpoints: . | https://api.opencorporates.com/v0.4/companies/search?q={nameLookup}, where {nameLookup} is the name of company – this endpoint is called when no company codes can be found in golden record’s organization.codes.cvr, organization.codes.brreg, organization.codes.companyHouse, or organization.codes.cik . | https://api.opencorporates.com/v0.4companies/{jurisdictionCodeLookup.Jurisdiction}/{jurisdictionCodeLookup.Value}?format=json – this endpoint is called when there are company codes, so the jurisdiction of golden record will be retrieved and used in the API. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.OpenCorporates | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.OpenCorporates | 4.1.2 | Source code | . | CluedIn.Provider.ExternalSearch.OpenCorporates | 4.1.1 | Source code | . | CluedIn.Provider.ExternalSearch.OpenCorporates | 4.1.0 | Source code | . | CluedIn.Provider.ExternalSearch.OpenCorporates | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#open-corporates",
    
    "relUrl": "/preparation/enricher/enricher-reference#open-corporates"
  },"1053": {
    "doc": "Enricher reference",
    "title": "PermId",
    "content": "The PermId enricher retrieves information about organizations from the PermID database. It supports the following endpoints: . | https://api-eit.refinitiv.com/permid – this API is called first. | https://permid.org/api/mdaas/getEntityById/ – this API is called second to get the social data. | . | Package name | Package version | Source code | . | CluedIn.ExternalSearch.Providers.PermId.Provider | 4.4.0 | Source code | . | CluedIn.ExternalSearch.Providers.PermId.Provider | 4.0.1 | Source code | . | CluedIn.ExternalSearch.Providers.PermId.Provider | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#permid",
    
    "relUrl": "/preparation/enricher/enricher-reference#permid"
  },"1054": {
    "doc": "Enricher reference",
    "title": "Vatlayer",
    "content": "The Vatlayer enricher validates and cleans EU VAT numbers. It supports the following endpoint: . | http://www.apilayer.net/api/validate?access_key={apiToken}&amp;vat_number={vat}&amp;format=1, where {apiToken} is your API key for retrieving information from the Vatlayer website and {vat} is the VAT number of the company. | . | Package name | Package version | Source code | . | CluedIn.Provider.ExternalSearch.Providers.VatLayer | 4.4.0 | Source code | . | CluedIn.Provider.ExternalSearch.Providers.VatLayer | 4.0.1 | Source code | . | CluedIn.Provider.ExternalSearch.Providers.VatLayer | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#vatlayer",
    
    "relUrl": "/preparation/enricher/enricher-reference#vatlayer"
  },"1055": {
    "doc": "Enricher reference",
    "title": "Web",
    "content": "The Web enricher retrieves information about organizations through their websites. The Web enricher supports any website provided in the Website Vocab Key field of the enricher configuration. | Package name | Package version | Source code | . | CluedIn.ExternalSearch.Providers.Web | 4.4.0 | Source code | . | CluedIn.ExternalSearch.Providers.Web | 4.1.2 | Source code | . | CluedIn.ExternalSearch.Providers.Web | 4.1.0 | Source code | . | CluedIn.ExternalSearch.Providers.Web | 4.0.0 | Source code | . ",
    "url": "/preparation/enricher/enricher-reference#web",
    
    "relUrl": "/preparation/enricher/enricher-reference#web"
  },"1056": {
    "doc": "Enricher reference",
    "title": "Enricher reference",
    "content": " ",
    "url": "/preparation/enricher/enricher-reference",
    
    "relUrl": "/preparation/enricher/enricher-reference"
  },"1057": {
    "doc": "Extension packages",
    "title": "On this page",
    "content": ". | Overview of extension packages | Add SQL Server Connector to CluedIn | Results | Next steps | . In this article, you will learn how to add the SQL Server Connector extension package to CluedIn. This is our most commonly used package, and it allows you to stream data towards database instances. ",
    "url": "/deployment/local/step-3#on-this-page",
    
    "relUrl": "/deployment/local/step-3#on-this-page"
  },"1058": {
    "doc": "Extension packages",
    "title": "Overview of extension packages",
    "content": "To extend CluedIn with additional features—such as enrichers, export targets, and vocabularies—you can restore extension packages for the environment. Packages are made available through NuGet feeds. NuGet feeds are a convenient way to distribute and manage packages for your local development environment. These feeds can be hosted publicly, privately, or even on your local machine. To facilitate local development, a preconfigured environment includes a specific folder called packages/local NuGet. This folder serves as a designated location where you can place locally built NuGet packages that need to be included in the CluedIn environment. The easiest way to integrate your locally built packages into CluedIn is to build your extension packages directly to this folder. CluedIn has many built-in extension packages. To learn more, reach out to your CluedIn contact. ",
    "url": "/deployment/local/step-3#overview-of-extension-packages",
    
    "relUrl": "/deployment/local/step-3#overview-of-extension-packages"
  },"1059": {
    "doc": "Extension packages",
    "title": "Add SQL Server Connector to CluedIn",
    "content": "The following image presents an overview of the steps involved in adding SQL Server Connector to CluedIn. In the procedure, we’ll use the following input variables: . | 202401 – name of the environment . | CluedIn.Connector.SqlServer – name of the extension package. | . To add SQL Server Connector to CluedIn . | Add a reference to the package by running the following command: . pwsh .\\cluedin.ps1 packages 202401 -Add CluedIn.Connector.SqlServer . You will get an output similar to the following. You can also specify a version for your package (-version) and use floating versions (for example, 1.0.0-*) for the latest pre-release. | Restore the package by running the following command: . pwsh .\\cluedin.ps1 packages 202401 -Restore . You will get an output similar to the following. | Stop the CluedIn server by running the following command: . pwsh .\\cluedin.ps1 stop 202401 . You will get an output similar to the following. | Start the CluedIn server by running the following command: . pwsh .\\cluedin.ps1 up 202401 . You will get an output similar to the following. Starting the CluedIn server takes some time. When CluedIn starts up, it takes all extension assets from the disk and copies them into the container. | After you start CluedIn, make sure the package was included. In Docker Desktop, select the CluedIn server and look for the similar section in logs. | In browser, open the CluedIn application and check if the SQL Connector is there. To do that, go to Consume &gt; Export Targets &gt; Add Export Target. The SQL Server Connector may take a few minutes to appear in the application. | . ",
    "url": "/deployment/local/step-3#add-sql-server-connector-to-cluedin",
    
    "relUrl": "/deployment/local/step-3#add-sql-server-connector-to-cluedin"
  },"1060": {
    "doc": "Extension packages",
    "title": "Results",
    "content": "You have added the SQL Server Connector extension package. Now, you are ready to stream data to Microsoft SQL Server databases. ",
    "url": "/deployment/local/step-3#results",
    
    "relUrl": "/deployment/local/step-3#results"
  },"1061": {
    "doc": "Extension packages",
    "title": "Next steps",
    "content": ". | Learn more about CluedIn functionality in our Getting started guide. | Learn how to install CluedIn from the Azure Marketplace. | . ",
    "url": "/deployment/local/step-3#next-steps",
    
    "relUrl": "/deployment/local/step-3#next-steps"
  },"1062": {
    "doc": "Extension packages",
    "title": "Extension packages",
    "content": " ",
    "url": "/deployment/local/step-3",
    
    "relUrl": "/deployment/local/step-3"
  },"1063": {
    "doc": "Features",
    "title": "On this page",
    "content": ". | Sync business domains to Dataverse tables | Sync Dataverse table to Cluedin business domains/vocabularies | Create ingestion endpoint workflow | Create a batch approval workflow process | Create streams | . ",
    "url": "/microsoft-integration/powerapps/features#on-this-page",
    
    "relUrl": "/microsoft-integration/powerapps/features#on-this-page"
  },"1064": {
    "doc": "Features",
    "title": "Sync business domains to Dataverse tables",
    "content": "This feature allows you to sync CluedIn business domains, vocabularies, and vocabulary keys with Dataverse table and columns. To sync CluedIn business domains with Dataverse table . | On the navigation pane, go to Administration &gt; Settings, and then find the PowerApps section. | In Sync CluedIn Business Domains to Dataverse Table, turn on the toggle, and then enter the business domain that you want to sync. If you want to sync multiple business domains, separate them with a comma (for example, /_Type1,/Type2,/Type3). Another way to enable this feature is to navigate to Management &gt; Business Domains and select the business domain you want to sync. Then, select Edit and turn on the toggle for Sync CluedIn Business Domains to Dataverse Table. Finally, save changes. All the vocabulary keys below will be created as columns in the Dataverse table. Once the synchronization has been completed, you’ll receive two notifications: Dataverse Table Created and Dataverse Column Created/Updated. | Verify the table and columns created in Dataverse. | . ",
    "url": "/microsoft-integration/powerapps/features#sync-business-domains-to-dataverse-tables",
    
    "relUrl": "/microsoft-integration/powerapps/features#sync-business-domains-to-dataverse-tables"
  },"1065": {
    "doc": "Features",
    "title": "Sync Dataverse table to Cluedin business domains/vocabularies",
    "content": "This feature allows you to sync Dataverse table and columns into CluedIn business domains, vocabulary, and vocabulary keys. Prerequisites . You’ll need to provide the logical name of the Dataverse table. There are the following ways to identify or get the logical name of the table: . | Go to table Properties, and then copy the value from the Logical name field. | Go to Tools, and then select Copy logical name. | In the table list view, the logical name is right after the table name. | . To sync Dataverse table and columns into CluedIn business domains and vocabulary . | On the navigation pane, go to Administration &gt; Settings, and then find the PowerApps section. | In Sync Dataverse Table/Columns to CluedIn Business Domains and Vocabulary, turn on the toggle, and then enter the Dataverse table name. The value should be the logical name of the table. If you want to sync multiple tables, separate them with a comma (for example, logical_name1,logical_name2,logical_name3_). Once the synchronization has been successfully completed, you’ll receive three notifications: Entity Type Created, Vocabulary Created, and Vocabulary Keys Created. | Verify the business domain, vocabulary, and vocabulary keys created in CluedIn. | . ",
    "url": "/microsoft-integration/powerapps/features#sync-dataverse-table-to-cluedin-business-domainsvocabularies",
    
    "relUrl": "/microsoft-integration/powerapps/features#sync-dataverse-table-to-cluedin-business-domainsvocabularies"
  },"1066": {
    "doc": "Features",
    "title": "Create ingestion endpoint workflow",
    "content": "This feature allows you to automate the creation of workflow that will send the data from Dataverse to CluedIn via ingestion endpoint. Prerequisites . | Dataverse connection. | . To automate the workflow creation . | In CluedIn, on the navigation pane, go to Administration &gt; Settings, and then find the PowerApps section. | In Create workflow to Ingest Data to CluedIn, turn on the toggle. | . Ingestion endpoint . As part of workflow automation, the ingestion endpoint will be created as well. From our sample above, you can expect two ingestion endpoints to be created, one for each of the cluedin_dog and crc12_customer tables. Workflow . The creation of workflow will depend on the values of Sync Business Domains and Sync Dataverse Tables. Once the execution of the job is done, from the sample values above, you can expect two workflows to be created, one for each of the cluedin_dog and crc12_customer tables. You can expect to see a notification when the creation is successful. The content of the workflow will be composed of a Dataverse event named When a row is added, modified or delete (but mainly focused on Added and Modified) and an HTTP event that pushes the data into CluedIn ingestion endpoint. On the following screenshot, the token has been edited to show the full content of HTTP event. Auto mapping and processing . As we already know the structure of the table/vocabulary that we are working on, the system will automate the data mapping and processing. By navigating to the data set page, you can notice that the Map, Prepare, and Process tabs are now available as we already automated the creation of the data mapping into our vocabularies. On the Map tab, you can find the the full view of all columns mapped to our vocabulary, including edges (relationships) and identifiers, if there are any. On the Preview tab, you can find the data received from Dataverse. Once the data is received, you can expect to see it processed because we have also enabled the Auto submission property of the ingestion endpoint. ",
    "url": "/microsoft-integration/powerapps/features#create-ingestion-endpoint-workflow",
    
    "relUrl": "/microsoft-integration/powerapps/features#create-ingestion-endpoint-workflow"
  },"1067": {
    "doc": "Features",
    "title": "Create a batch approval workflow process",
    "content": "This feature enables you to automate the creation of the workflow for the batch approval process. If you process the data (regardless of the source) and the system identifies that the business domain used has been tagged for the approval process, the data will be halted, and the approval process will start and wait for the user’s approval to continue the data processing. Prerequisites . | Dataverse connection | Approval connection | . To enable the batch approval workflow . | In CluedIn, on the navigation pane, go to Management &gt; Business Domains, and then select the business domain that you want to sync. | Select Edit and then turn on the toggle for Enable for Batch Approval Workflow. | Select Save. After enabling this feature, a new table (Approval Queue Table) will be created in Dataverse. | . Approval Queue table in Dataverse . This table will serve as a storage of the CluedIn data or information on the data waiting for approval. The CluedIn Approval Queue ID is the ID of the data that we are trying to approve in this process. Workflow . The content of the approval workflow will be composed of events such as condition, approval, variables, and HTTP. A 60-second cycle will occur to check if there is data in the Approval Queue table. Once we receive a response in the Approval Process, we send the Approval details together with the CluedIn Approval Queue IDs to the CluedIn API via an HTTP event. Notifications . Once the automation has been done, you can expect a notification for creating the Approval Queue Table/Columns and the creation of the Batch Approval Workflow. ",
    "url": "/microsoft-integration/powerapps/features#create-a-batch-approval-workflow-process",
    
    "relUrl": "/microsoft-integration/powerapps/features#create-a-batch-approval-workflow-process"
  },"1068": {
    "doc": "Features",
    "title": "Create streams",
    "content": "This feature allows you to automate the creation of export targets and streams. To automate the creation of export targets and streams . | On the navigation pane, go to Administration &gt; Settings, and then find the PowerApps section. | In Create CluedIn Stream, turn on the toggle. | . Export targets . Export target will be created automatically using the same credentials from Organization Settings. Streams . The creation of a stream will depend on the values of Sync Business Domains and Sync Dataverse Tables. Once the execution of the job is done, from the sample values above, two streams should have been created, one for each of the cluedin_dog and crc12_customer tables. Each stream will have a certain configuration filtered by business domain. It will automatically assign the same export target that was created from the Dataverse connector. Incoming and outgoing edges are set to be exported. All the properties associated with it have been automatically added too. Notifications . Two notifications can be expected in this job: Stream created and Stream mapping updated. ",
    "url": "/microsoft-integration/powerapps/features#create-streams",
    
    "relUrl": "/microsoft-integration/powerapps/features#create-streams"
  },"1069": {
    "doc": "Features",
    "title": "Features",
    "content": " ",
    "url": "/microsoft-integration/powerapps/features",
    
    "relUrl": "/microsoft-integration/powerapps/features"
  },"1070": {
    "doc": "Key terms and features",
    "title": "Key terms and features",
    "content": "CluedIn has many features in it. In this section, you will find an overview of essential terminology and functionalities that are important for most users. Get acquainted with these terms to confidently navigate and explore the system. ",
    "url": "/key-terms-and-features",
    
    "relUrl": "/key-terms-and-features"
  },"1071": {
    "doc": "Manage a manual data entry project",
    "title": "On this page",
    "content": ". | Edit manual data entry project configuration | Edit form fields | Manage sessions | Manage access to manual data entry project | Manage access to data from manual data entry project | . In this article you will learn how to manage a manual data entry project: edit the project configuration and form fields and manage access to the project and data from the project. ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#on-this-page",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#on-this-page"
  },"1072": {
    "doc": "Manage a manual data entry project",
    "title": "Edit manual data entry project configuration",
    "content": "Once you have created a manual data entry project, you can change its configuration if needed. You can edit the following configuration details of a manual data entry project: project name, vocabulary, record approval option, and description. You cannot change the business domain. To edit manual data entry project configuration . | On the navigation pane, go to Ingestion &gt; Manual Data Entry. | Find and open the project that you want to edit. | In the upper-right corner, select Edit. | Make the needed changes. | Select Save. The manual data entry project configuration is updated. | . ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#edit-manual-data-entry-project-configuration",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#edit-manual-data-entry-project-configuration"
  },"1073": {
    "doc": "Manage a manual data entry project",
    "title": "Edit form fields",
    "content": "Once you have created the form fields, you can modify them if needed. You can edit all the details of a form field. Additionally, if you no longer want to use the form field to create manual records, you can archive such form field. Note that the changes you make in the form fields do not affect the records that have been already generated and processed. To edit a form field . | In the manual data entry project, go to the Form Fields tab. | Select the form field that you want to edit. | Make the needed changes. | Select Save. The form field configuration is updated. | . To archive a form field . | In the manual data entry project, go to the Form Fields tab. | Select the form field that you want to archive. | Select Archive, and then select Confirm. The status of the form field becomes Archived. This form field is no longer available when adding new manual records. | . ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#edit-form-fields",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#edit-form-fields"
  },"1074": {
    "doc": "Manage a manual data entry project",
    "title": "Manage sessions",
    "content": "If you used an option to add multiple records simultaneously, then your manual data entry project contains sessions that store such records. These sessions are available on the Sessions tab. The sessions can have one of the following statuses: . | Not generated – this status means that either the session does not contain any records, or it contains some records but they have not yet been generated and processed. You can open such session and add some records. When you select the session, it opens in a new tab. | Generated – this status means that the records from the session have been generated and processed. You can open such session and add more records if needed. When you generate the records, previously generated records will remain unchanged, while newly added records will be generated and processed. | . If you no longer need the session, you can delete it. This action is irreversible, and you will not be able to recover the deleted session. The records generated from such session will remain intact. To delete a session . | In the manual data entry project, go to the Sessions tab. | Find the session that you want to delete. | On the right side of the session row, open the three-dot menu, and then select Delete. | Confirm that you want to delete the session. The session is no longer listed on the Sessions tab. | . ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-sessions",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-sessions"
  },"1075": {
    "doc": "Manage a manual data entry project",
    "title": "Manage access to manual data entry project",
    "content": "The user who created the manual data entry project is the owner of the project. This user can make direct changes to the project, approve or reject changes submitted by non-owner users, process the records that require approval, as well as add other users or roles to the list of owners. You can find the list of users and/or roles who can manage the manual data entry project on the Owners tab of the manual data entry project. For more information about ownership, see Feature access. ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-access-to-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-access-to-manual-data-entry-project"
  },"1076": {
    "doc": "Manage a manual data entry project",
    "title": "Manage access to data from manual data entry project",
    "content": "The user who created the manual data entry project has access to all records generated within the project. This user can grant permission to other users and/or roles to access the records generated within the project. On the Permissions tab of the manual data entry project, you can find the list of users and/or roles who have access to all records generated within the project. For more information about permissions, see Data access. If the user is not listed on the Permissions tab of the project, they will not be able to view the generated records on the Data tab. ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-access-to-data-from-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project#manage-access-to-data-from-manual-data-entry-project"
  },"1077": {
    "doc": "Manage a manual data entry project",
    "title": "Manage a manual data entry project",
    "content": " ",
    "url": "/integration/manual-data-entry/manage-a-manual-data-entry-project",
    
    "relUrl": "/integration/manual-data-entry/manage-a-manual-data-entry-project"
  },"1078": {
    "doc": "Manage rules",
    "title": "On this page",
    "content": ". | Edit a rule | Delete a rule | Activate and deactivate a rule | Edit rule processing order | Process pending changes | Duplicate a rule | . In this article, you will learn how to manage rules to keep them organized, efficient, and aligned with your data management goals. ",
    "url": "/management/rules/manage-rules#on-this-page",
    
    "relUrl": "/management/rules/manage-rules#on-this-page"
  },"1079": {
    "doc": "Manage rules",
    "title": "Edit a rule",
    "content": "You can edit a rule to make necessary changes in rule name, description, filters, and actions. To edit a rule . | On the navigation pane, go to Management &gt; Rule builder. | Choose the needed type of rule, and then open the rule that you want to edit. | In the rule details page, make the needed changes. | If you are the owner of the rule, near the upper-right corner of the rule details page, select Save. If you modified filters or actions, select the checkbox to reprocess the records affected by the previous and current rule configuration. Selecting the checkbox will revert the records affected by the previous rule configuration to their initial state before the rule was applied. Simultaneously, the records affected by the current rule configuration will be adjusted according to the rule’s action. If you don’t select the checkbox, the rule will be updated but the records won’t be reprocessed. After you confirm that you want to update the rule, your changes will be applied immediately. | If you are not the owner of the rule, near the upper-right corner of the rule details page, select Submit for approval. The owner of the rule will receive a notification about your changes and can then approve or reject them. | . ",
    "url": "/management/rules/manage-rules#edit-a-rule",
    
    "relUrl": "/management/rules/manage-rules#edit-a-rule"
  },"1080": {
    "doc": "Manage rules",
    "title": "Delete a rule",
    "content": "You can delete a rule in any status if you no longer need it. You can delete a rule only if you are the owner of the rule; you cannot delete rules created by other users. After the rule is deleted, the records affected by the rule will still contain changes according to the rule’s action. However, when you reprocess such records, they will return to their initial state before the rule was applied. There are two ways to delete a rule: . | From the list of rules – this option allows you to delete multiple rules at once. | From the rule details page. | . To delete a rule from the list of rules . | Select the checkbox next to the rule that you want to delete. | Select Delete, and then confirm your choice. | . To delete a rule from the rule details page . | Select the delete icon. Then, confirm your choice. | . ",
    "url": "/management/rules/manage-rules#delete-a-rule",
    
    "relUrl": "/management/rules/manage-rules#delete-a-rule"
  },"1081": {
    "doc": "Manage rules",
    "title": "Activate and deactivate a rule",
    "content": "When active, the rule is applied to records that match its filter during processing or reprocessing. If you no longer want to apply the rule, you can deactivate it. If you change your mind, you can easily activate the rule. After the rule is deactivated, the records affected by the rule will still contain changes according to the rule’s action. However, when you reprocess such records, they will return to their initial state before the rule was applied. There are two ways to activate and deactivate a rule: . | From the list of rules – this option allows you to activate or deactivate multiple rules at once. | From the rule details page. | . To activate or deactivate the rule from the list of rules . | Select the checkbox next to the rule that you want to activate or deactivate. | Near the upper-right corner of the list of rules, select the needed action. | . To activate or deactivate the rule from the rule details page . | Near the upper-right corner, turn on or off the status toggle. | . ",
    "url": "/management/rules/manage-rules#activate-and-deactivate-a-rule",
    
    "relUrl": "/management/rules/manage-rules#activate-and-deactivate-a-rule"
  },"1082": {
    "doc": "Manage rules",
    "title": "Edit rule processing order",
    "content": "The default rule processing order is the order in which the rules were created. However, you have the flexibility to adjust the rule processing sequence. This allows you to prioritize the execution of specific rules over others. For example, if you have two rules with the same filters but different actions, you can determine which rule should be applied to the records first. To edit the rule processing order . | In the list of rules, near the upper-right corner, select Edit rule processing order. | Change the rule processing order by doing one of the following: . | Select the rule and drag it to the needed position. | Choose the number of the rule to define the order of processing. | Move the rule up or down by using the arrows in the right side of the rule. | . | Select Save. | . ",
    "url": "/management/rules/manage-rules#edit-rule-processing-order",
    
    "relUrl": "/management/rules/manage-rules#edit-rule-processing-order"
  },"1083": {
    "doc": "Manage rules",
    "title": "Process pending changes",
    "content": "If you are the owner of the rule and somebody else makes changes to the rule, you’ll receive a notification about a request for approval. All requests are listed on the Pending changes tab of the rule. To process pending changes . | Review the details of the change request by selecting View changes. A new pane opens, where you can view the changes to the rule. | Depending on whether you agree with the suggested changes, in the Actions tab, select Approve or Reject. If you approved the change request, the rule is updated accordingly. If you rejected the change request, no changes are made to the rule. | . ",
    "url": "/management/rules/manage-rules#process-pending-changes",
    
    "relUrl": "/management/rules/manage-rules#process-pending-changes"
  },"1084": {
    "doc": "Manage rules",
    "title": "Duplicate a rule",
    "content": "Duplicating a rule means creating a new rule with the configuration of the existing rule. This configuration includes filters and rule actions but does not include the activities performed by the rule. By activities, we mean the changes applied to data parts or golden records according to the rule’s actions. For example, if you duplicate a golden record rule with the action to add tags to all golden records, the new rule will not automatically add the same tags to golden records. You can duplicate a rule if you want to: . | Use the same actions for a different set of golden records. In this case, you only need to modify the filters in the duplicated rule. | Use different rule actions for the same set of golden records. In this case, you only need to modify the actions in the duplicated rule. | . Duplication is a beta feature. To access it, go to Administration &gt; Feature Flags, and enable the Duplicate Actions feature. To duplicate a rule . | In the list of rules, find a rule that you want to duplicate. Then, open the three-dot menu for the rule, and select Duplicate. | In Name, review the default name of the new rule and modify it if needed. The default name is created by adding _duplicate to the name of the rule that you’re duplicating. | In Conditions, review the filters that will be duplicated for the new rule. | In Actions, review the list of rule actions that will be duplicated for the new rule. To view the details of a specific rule action, select View Action Details. | Select Duplicate. The new rule is created. By default, it is inactive. Now, you can modify the rule configuration as needed. When you reach the desired configuration, save your changes, activate the rule, and then reprocess the affected records. | . ",
    "url": "/management/rules/manage-rules#duplicate-a-rule",
    
    "relUrl": "/management/rules/manage-rules#duplicate-a-rule"
  },"1085": {
    "doc": "Manage rules",
    "title": "Manage rules",
    "content": " ",
    "url": "/management/rules/manage-rules",
    
    "relUrl": "/management/rules/manage-rules"
  },"1086": {
    "doc": "Clean data",
    "title": "On this page",
    "content": ". | Find data | Create a clean project | Modify data in the clean application | Results &amp; next steps | . Cleaning the data in CluedIn involves finding the data that needs to be cleaned, creating a clean project, and modifying the data in the clean application. In this guide, you will learn how to manually clean the data that you have ingested into CluedIn. Before you start: Make sure you have completed all steps in the Ingest data guide. Context: This guide focuses on resolving a specific issue—a misspelled job title (Acountant). Here, you’ll find step-by-step instructions on how to correct this error. If you come across errors in the record properties or discover records with missing data, you can perform manual data cleaning in CluedIn. This process allows you to ensure the accuracy and completeness of your data set. CluedIn will automatically identify the changes and update the stream with the cleaned records. Useful links: Search, Filters . ",
    "url": "/getting-started/manual-data-cleaning#on-this-page",
    
    "relUrl": "/getting-started/manual-data-cleaning#on-this-page"
  },"1087": {
    "doc": "Clean data",
    "title": "Find data",
    "content": "Finding the data that needs to be cleaned involves defining search filters and specific properties with incorrect values. To find data . | In the search field, select the search icon. Then, select Filter. | In the Business Domains dropdown list, select the business domain to filter the records. As a result, all records with the selected business domain are displayed on the page. By default, the search results are shown in the following columns: Name, Business Domain, and Description. | To find the specific values that you want to fix, add the corresponding column to the list of search results: . | In the upper-right corner, select Column Options. | Select Add columns &gt; Vocabulary. | In the Vocabulary Keys section, expand the vocabulary that contains the needed vocabulary key, and then select the checkbox next to it. | Move the vocabulary key to the Selected Vocabulary Keys section using the arrow pointing to the right. | Select Add Vocabulary Columns. | . | Turn on the advanced filter mode. | Add a filter rule to display the records containing values that need to be cleaned. The fields for configuring a filter rule appear one by one. After you complete the previous field, the next field appears. | Select Search. The records that match the filter criteria are displayed on the search results page. After finding the records, save the search. This way you can quickly verify if the values have been cleaned. | . ",
    "url": "/getting-started/manual-data-cleaning#find-data",
    
    "relUrl": "/getting-started/manual-data-cleaning#find-data"
  },"1088": {
    "doc": "Clean data",
    "title": "Create a clean project",
    "content": "After you have found the data that needs to be cleaned, create a clean project. To create a clean project . | In the upper-right corner of the search results page, open the three-dot menu, and then select Clean. | Enter the Project Name and then select Next. | Select the checkboxes next to the properties that do not require fixing, and then select Remove Property. | Select Create. The clean project is created. | Select Generate Results and then confirm your choice. When the results are generated, the status of the clean project changes to Ready for clean. Now, you can proceed to correct the misspelled values. | . ",
    "url": "/getting-started/manual-data-cleaning#create-a-clean-project",
    
    "relUrl": "/getting-started/manual-data-cleaning#create-a-clean-project"
  },"1089": {
    "doc": "Clean data",
    "title": "Modify data in the clean application",
    "content": ". | In the upper-right corner of the clean project, select Clean. The clean application opens, where you can view the values that should be modified. | Point to the value than needs to be modified, and then select Edit. | Enter the correct value, and then select Apply to all identical cells. | In the upper-right corner, select Process. In the confirmation dialog box, select Skip and leave the Enable automatic generation of data part rules checkbox cleared. Then, confirm that you want to process the data. CluedIn automatically identifies the changes and updates the records. To verify that your changes have been applied, retrieve the saved search. | . All changes to the records in CluedIn are tracked. You can search for the needed record and on the History pane, you can view all actions associated with the record. For more information, see History. ",
    "url": "/getting-started/manual-data-cleaning#modify-data-in-the-clean-application",
    
    "relUrl": "/getting-started/manual-data-cleaning#modify-data-in-the-clean-application"
  },"1090": {
    "doc": "Clean data",
    "title": "Results &amp; next steps",
    "content": "After you manually cleaned the data, the misspelled values were corrected. By following the steps outlined in this guide, you can address various errors and inconsistencies in your data. The next item on the list of common data management tasks is deduplication. Learn how to identify and merge duplicates in the Deduplicate data guide. ",
    "url": "/getting-started/manual-data-cleaning#results--next-steps",
    
    "relUrl": "/getting-started/manual-data-cleaning#results--next-steps"
  },"1091": {
    "doc": "Clean data",
    "title": "Clean data",
    "content": " ",
    "url": "/getting-started/manual-data-cleaning",
    
    "relUrl": "/getting-started/manual-data-cleaning"
  },"1092": {
    "doc": "Perform the upgrade",
    "title": "On this page",
    "content": ". | Get current Helm user values | Prepare new Helm user values | Perform system pre-checks | Perform Helm upgrade . | Basic Helm upgrade | Helm upgrade with data upgrade | . | Verify the upgrade | Notify about upgrade completion | . This page covers stage 3 of the CluedIn upgrade process. It provides step-by-step instructions to apply the new version of CluedIn safely and verify that the upgrade completes successfully. Before you begin, make sure that you have completed all actions outlined in Plan the upgrade and Prepare for the upgrade. Keep in mind that upgrade steps may vary depending on the release. The following sequence outlines a typical upgrade process: . | Get current Helm user values . | Prepare new Helm user values . | Perform system pre-checks . | Perform Helm upgrade . | Verify the upgrade . | Notify about upgrade completion . | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#on-this-page"
  },"1093": {
    "doc": "Perform the upgrade",
    "title": "Get current Helm user values",
    "content": "The Helm user values file is a YAML file that defines the configuration values to be applied when upgrading a Kubernetes cluster. The file will look similar to the following: . global: image: tag: \"2024.12.02\" strategy: type: Recreate application: cluedin: image: tag: \"2024.12.02\" components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.0.1\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.4.0\" - name: \"CluedIn.Connector.Http\" version: \"4.0.0\" . We recommend the following format for naming your user values files: values-&lt;environment&gt;-&lt;release-version&gt;.yml, where: . | &lt;environment&gt; – The target environment (for example, dev, staging, or prod). | &lt;release-version&gt; – The release identifier, written as a hyphen-separated date or version number. | . For example, for release 2024.12.02 on the dev environment, the file should be named values-dev-2024-12-02.yml. | To get your Helm user values, run the following command in PowerShell (replace &lt;environment&gt; and &lt;release-version&gt; with your values): . helm get values cluedin-platform -n cluedin -o yaml &gt; ./ values-&lt;environment&gt;-&lt;release-version&gt;.yml . | YAML file is created in your working directory. Open the file in your preferred IDE (we typically use Visual Studio Code). | Check the contents of the file. It should look similar to the example above. If it appears empty, this usually means you are not connected to the cluster correctly. In that case, revisit the earlier steps to verify your connection. | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#get-current-helm-user-values",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#get-current-helm-user-values"
  },"1094": {
    "doc": "Perform the upgrade",
    "title": "Prepare new Helm user values",
    "content": ". | Make sure that your packages are ready to be installed in the new version. For details, see Prepare and test custom packages. | The required values for the target release are published in CluedIn documentation. Before proceeding, carefully verify that your current release is compatible with the target release. Once verified, duplicate the values file created in the previous step and rename it to match the target release. For example, to go to release 2025.05.00 on the dev environment, the file should be named values-dev-2025-05-00.yml. | Carefully compare your existing values file with the new release’s required values. Update any outdated entries and add newly introduced values: . | In most cases, you will update container image tags and package versions, but additional configuration keys may also be required. | Platform version (specified in tag) must align with the specified package versions. | To learn which package version must be used, see the Release notes section. The Release notes column points to the release details. Package versions are listed in the Packages section of the release details page. | An IDE such as Visual Studio Code will highlight any formatting or indentation issues as you edit the YAML file. YAML is whitespace-sensitive—use spaces (not tabs) and ensure correct indentation, or the deployment will fail. | . | Once everything looks correct, save the file. Based on the earlier example, your YAML file should now look like this: . global: image: tag: \"2025.05.00\" strategy: type: Recreate application: cluedin: components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.5.0\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.5.0\" - name: \"CluedIn.Connector.Http\" version: \"4.5.0\" . | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#prepare-new-helm-user-values",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#prepare-new-helm-user-values"
  },"1095": {
    "doc": "Perform the upgrade",
    "title": "Perform system pre-checks",
    "content": ". | Complete the following checks: . | Verify that CluedIn UI is running correctly. | Confirm that all pods are in a healthy (green) state. | Review the server logs and ensure they are free of errors. | . If any issues are detected, it is recommended to resolve them before proceeding with the upgrade. In some cases, the upgrade itself may address certain problems, but if you’re uncertain, it’s best to consult CluedIn support team for guidance. | Check CluedIn workload. Determine whether CluedIn is currently processing a high volume of data. If the system is under heavy load, it is best to allow all data to finish processing before performing the upgrade. Although any data still in the queues should remain forward-compatible, reducing the workload helps minimize risk during the upgrade. | Check internal CluedIn queues. | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#perform-system-pre-checks",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#perform-system-pre-checks"
  },"1096": {
    "doc": "Perform the upgrade",
    "title": "Perform Helm upgrade",
    "content": "There are several methods to upgrade Helm: . | Basic Helm upgrade – This is the default method. Use it unless you are explicitly instructed otherwise. | Helm upgrade with data upgrade – Use this method only when specifically instructed to do so. | . Basic Helm upgrade . During a Helm upgrade, the UI will be temporarily unavailable. Make sure to notify all users in advance so that they are aware of the downtime. A standard CluedIn upgrade typically results in 20–30 minutes of downtime. If the upgrade includes data migrations or additional updates, the outage may take longer. | Check the current CluedIn Helm chart version. To do this, run the following command to list all Helm releases in the cluedin namespace: . helm list -a -n cluedin . The output will display the currently installed chart version. For example: . cluedin-platform-2.5.1 . This sample output indicates that the current Helm chart version in use is 2.5.1. | Prepare the Helm command. The exact Helm command for the current update will be provided in the target documentation. In general, the command follows this structure: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version {CluedInChartVersion} \\ --values ${CustomValues} \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true \\ --wait \\ --timeout 10m0s . Parameter details: . | {CluedInChartVersion} – The target chart version specified in the documentation. | ${CustomValues} – The full path to the user values file you created in the previous step. | runDatabaseJobsOnUpgrade=true – Ensures that the database is refreshed during the upgrade. | runNugetFullRestore=true – Ensures that all packages are fully restored. | --timeout 10m0s – A best practice is to allow a 10-minute timeout. If the upgrade fails due to a timeout, follow the documented mitigation steps. | . | When you are ready, execute the Helm command in PowerShell. | The PowerShell prompt will remain active for several minutes—this is expected. Please be patient while the process runs. | During the upgrade, the Helm release status will progress through the following states: Active → Pending Upgrade → Upgrading. | . | Allow the process to complete without interruption. | After about 10 minutes, the Helm command in PowerShell should complete and display a confirmation message. This indicates that the Helm command executed successfully. | However, while the Helm upgrade itself will be finished, some pods may still be starting up. It can take an additional 10–15 minutes for all pods to become fully healthy (green). | . | Verify that CluedIn Helm chart version matches the target version. Run the following command: . helm list -a -n cluedin . The output should display the newly installed chart version. For example: . cluedin-platform-2.5.2 . | . Helm upgrade with data upgrade . In some cases, an upgrade also requires changes to the underlying data. When this occurs, CluedIn must be placed into the Upgrade Mode during installation. Upgrade Mode ensures that no data is ingested or processed while the upgrade is in progress, preventing inconsistencies and maintaining data integrity. During a Helm upgrade, the UI will be temporarily unavailable. Make sure to notify all users in advance so that they are aware of the downtime. A standard CluedIn upgrade typically results in 20–30 minutes of downtime. If the upgrade includes data migrations or additional updates, the outage may take longer. Use this method of upgrading Helm only if you are explicitly instructed to do so. It replaces the steps provided in Basic Helm upgrade. | Verify the current CluedIn Helm chart version. Run the following command to list all Helm releases in the cluedin namespace: . helm list -a -n cluedin . The output will display the currently installed chart version. For example: . cluedin-platform-2.5.1 . This sample output indicates that the current Helm chart version in use is 2.5.1. | Prepare the Helm command: # ${CustomValues} refers to the values file you have amended above. Enter the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version {CluedInChartVersion} \\ --values ${CustomValues} \\ --set application.system.upgradeMode=true \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true \\ --wait \\ --timeout 10m0s . Parameter details: . | {CluedInChartVersion} – The target chart version specified in the documentation. | ${CustomValues} – The full path to the user values file created in the previous step. | upgradeMode=true – Ensures no data is ingested or processed during the upgrade. | runDatabaseJobsOnUpgrade=true – Ensures the database is refreshed as part of the upgrade. | runNugetFullRestore=true – Ensures all packages are fully restored. | --timeout 10m0s – Allows a 10-minute timeout (recommended). If the upgrade fails due to a timeout, follow the documented mitigation steps. | . | Run the command. After a few minutes, it should successfully complete. | Wait until the deployment finishes. Make sure that all pods are healthy and all jobs are completed. Do not proceed further if any of the pods have health issues. | Run the data upgrade. The data upgrade requires a trigger which is applied by running the following command: . kubectl apply -f - &lt;&lt;EOF apiVersion: api.cluedin.com/v1 kind: DataUpgrade metadata: annotations: meta.helm.sh/release-name: cluedin-platform meta.helm.sh/release-namespace: cluedin labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.5.1 release: cluedin-platform name: data-upgrade-450 namespace: cluedin spec: toVersion: Version450 EOF . | Check the CluedIn server pod logs. The logs should display the following (the Version450 label will vary): . [#098 05:57:02 INF] Performing Upgrade Scenario Apply DataSource and Manual Data Entry Project Owners Upgrade for version Version450 [#090 05:57:03 INF] Completed Upgrade Scenario Apply DataSource and Manual Data Entry Project Owners Upgrade for version Version450 [#090 05:57:03 INF] Performing Upgrade Scenario Elastic search upgrade mapping for version Version450 [#095 05:57:04 INF] Completed Upgrade Scenario Elastic search upgrade mapping for version Version450 [#095 05:57:04 INF] HTTP POST /api/upgradeto/Version450 responded 200 in 1857.3003 ms . The responded 200 part indicates a successful upgrade. | Finalise the upgrade. CluedIn must now be taken back out of the Upgrade Mode. To bring CluedIn back online, run the following command: . # ${CustomValues} refers to the values file you have amended above. Enter the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version {CluedInChartVersion} \\ --values ${CustomValues} \\ --set application.system.upgradeMode=false \\ --set application.system.runDatabaseJobsOnUpgrade=false \\ --set application.system.runNugetFullRestore=false . After a few minutes, the command should complete. | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#perform-helm-upgrade",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#perform-helm-upgrade"
  },"1097": {
    "doc": "Perform the upgrade",
    "title": "Verify the upgrade",
    "content": ". | To monitor the upgrade progress, check the following: . | CluedIn pods | CluedIn server logs . When checking the server logs, look for the following message: . Application started . This indicates a successful startup. | . | Check CluedIn UI and ensure everything is running smoothly. | . ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#verify-the-upgrade",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#verify-the-upgrade"
  },"1098": {
    "doc": "Perform the upgrade",
    "title": "Notify about upgrade completion",
    "content": "Once the upgrade completes and the cluster is back online, send a notification to all relevant teams and stakeholders to inform them that the cluster is ready for use. ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade#notify-about-upgrade-completion",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade#notify-about-upgrade-completion"
  },"1099": {
    "doc": "Perform the upgrade",
    "title": "Perform the upgrade",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/perform-the-upgrade",
    
    "relUrl": "/paas-operations/upgrade/guide/perform-the-upgrade"
  },"1100": {
    "doc": "Power Automate post-configuration guide",
    "title": "On this page",
    "content": ". | Verify a custom connector | Troubleshooting . | No custom connector | Failed triggers after registering a connector | Failed triggers after enabling a workflow | Sign-in error: ID token not enabled | Sign-in error: no reply address | . | Next steps | . In this guide, you will learn how to verify that the CluedIn custom connector has been created successfully. This guide is applicable to both public and private CluedIn instances. Make sure that you have completed all of the actions described in Power Automate configuration guide. Prerequisites . | You need to have the System Administrator security role in the Power Automate environment. | . ",
    "url": "/microsoft-integration/power-automate/post-configuration-guide#on-this-page",
    
    "relUrl": "/microsoft-integration/power-automate/post-configuration-guide#on-this-page"
  },"1101": {
    "doc": "Power Automate post-configuration guide",
    "title": "Verify a custom connector",
    "content": ". | Open the Power Automate environment that you prepared during pre-configuration stage in Configure an environment, and then select Custom connectors. Alternatively, you can use the following link, but make sure you provide your environment ID: https://make.powerautomate.com/environments/&lt;env-id&gt;/connections/custom. You should see a custom connector with CluedIn logo. | Open the custom connector and select Edit. Alternatively, hover over the custom connector and select Edit. | Go to the Definition tab. | Check the Triggers section. All items there should be marked with green checkboxes. If you see green checkboxes for all items, it means that the custom connector has been configured successfully. Now, you can start creating workflows in the Workflow module in CluedIn. | . ",
    "url": "/microsoft-integration/power-automate/post-configuration-guide#verify-a-custom-connector",
    
    "relUrl": "/microsoft-integration/power-automate/post-configuration-guide#verify-a-custom-connector"
  },"1102": {
    "doc": "Power Automate post-configuration guide",
    "title": "Troubleshooting",
    "content": "This section includes the description of errors that might appear after configuring the custom connector as well as remediation steps. No custom connector . Place of error: Power Automate . If you do not see a custom CluedIn connector in your Power Automate environment, make sure you have the System Administrator security role in the Power Automate environment. If you have the required role and still see a blank page, do the following: . | In CluedIn, go to Administration &gt; Settings, and then scroll down to the Workflows section. | Select Delete Custom Connector. | Select Register Custom Connector. | Repeat the steps from the section above to verify that the connector has been created successfully. | If the issue has not been resolved, reach out to CluedIn support at support@cluedin.com. | . Failed triggers after registering a connector . Place of error: Power Automate . If some triggers on the Definitions tab do not have the green checkmark, register the connector again as described in the previous section. Failed triggers after enabling a workflow . Place of error: CluedIn . After you create a workflow in CluedIn, you need to enable it, and then check its Properties tab. If you see the following error, it means that the workflow is not running. To fix this error, register the connector again as described in the previous section. Sign-in error: ID token not enabled . Place of error: CluedIn . In CluedIn, when you go to Workflow &gt; Workflow Builder or Workflow &gt; Approvals you may see an error similar to the following. This error indicates that the service application that is used to authenticate the Power Automate widget in CluedIn does not have the platform configurated. To fix this, add a platform and a redirect URI as described in Create a service application. Sign-in error: no reply address . Place of error: CluedIn . In CluedIn, when you go to Workflow &gt; Workflow Builder or Workflow &gt; Approvals you may see an error similar to the following. This error indicates that the redirect URI in the service application that is used to authenticate the Power Automate widget in CluedIn is incorrect. To fix this, make sure the domain in the redirect URI matches your CluedIn application. For more details, see Create a service application. ",
    "url": "/microsoft-integration/power-automate/post-configuration-guide#troubleshooting",
    
    "relUrl": "/microsoft-integration/power-automate/post-configuration-guide#troubleshooting"
  },"1103": {
    "doc": "Power Automate post-configuration guide",
    "title": "Next steps",
    "content": "Once you have verified that the CluedIn custom connector has been successfully configured, you can start creating workflows in the Workflow module in CluedIn. ",
    "url": "/microsoft-integration/power-automate/post-configuration-guide#next-steps",
    
    "relUrl": "/microsoft-integration/power-automate/post-configuration-guide#next-steps"
  },"1104": {
    "doc": "Power Automate post-configuration guide",
    "title": "Power Automate post-configuration guide",
    "content": " ",
    "url": "/microsoft-integration/power-automate/post-configuration-guide",
    
    "relUrl": "/microsoft-integration/power-automate/post-configuration-guide"
  },"1105": {
    "doc": "Help with building robust integrations",
    "title": "Help with building robust integrations",
    "content": "With the CluedIn team building just over 220 integrations to date, we have learnt quite a lot of tips and tricks in guiding developers to build good and robust integrations. Here is an exhaustive list of advice from the team to help you also build robust integrations. | Make sure you attend the 3 day developer training given by the CluedIn team. You will be given first hand advice and labs on building solid integrations. | Your integrations are responsible for fetching data, not cleaning it, not access control, not joining or blending data. In an ETL world, CluedIn wants your integrations to be more like ELT i.e. just extract and load. The CluedIn server will do the “T” or the Transform. | Build your crawlers with a mindset that the structure can and will change on a regular basis (even if it does not). | Many source systems will contain “Discovery Endpoints” in which you can “sniff” what types of data is possible for you to fetch. These are great to use, because they make your integration more dynamic than static. In this way you can build integrations that are very flexible and can use the source system to discover what objects or data is available and then “iterate” over all objects to fetch all the rows or instances of those objects. It is quite typical that these types of sources will also describe the data in forms of “type”, “constraints” and more. | Use the inbuilt Crawler Validation Framework within the Crawler Templates to help validate if you are building your crawler with best practices. | Your crawlers may be responsible for fetching billions of rows of data and hence this should be designed into your crawler as well. This means that practices like paging, filtering and more is recommended. | You might find that many CluedIn integrations will use “Service Accounts” to access data. This means that CluedIn will need to “iterate” all the individual accounts that this “Service Account” has access to. | There is a default timeout on crawlers in that if they have not fetched data within a 10 minute window, the CluedIn server assumes that something has gone wrong and will terminate the job. You can sometimes cause this accidentally or it might be that sometimes you will need to increase this timeout. For example, if your integration was built in a way to sort a table of data before ingesting it (because of a performance reason) then you might find that your table takes longer than 10 minutes to sort before it can start iterating and ingesting data from the platform. Increasing the timeout is fine and supported. You can change this in your container.config file. | . The best way to learn how to build a new integration is typically to view the over 220 prebuilt integrations as to see what the final solution looks like. You will notice that CluedIn is using the advice above in its integrations as to also abide by best practices. You can probably imagine that an integration that we build and deploy today, may change requirements tomorrow. Due to the nature of the integrations being very simple in nature and just tasked with fetching data, you will often find that the common failures are due to the underlying system changing in some way. The typical issues are: . | The location of the source has changed e.g. a connection string or physical location. | The data itself has changed structure and can no longer be properly reserialized into the JSON or XML format that CluedIn is asking for. | . There are some of these changes that are more drastic than others and CluedIn integrations can be setup in a way to “survive” some changes without the need for new deployments and changes to integration code. For example, if your underlying data has simply added a new column or property, CluedIn can “survive” this change, but eventually you should map that new property into its rightful Entity Code, Edge, Vocabulary Key or other. At least data will continue to flow from source to CluedIn. If the object structure changes dramatically, you will notice that CluedIn will throw notifications and alerts that we detect that a change is dramatic enough to stop the flow of data suffering from this issue until it is addressed. Making the changes in your code and redeploying will address this issue and data will continue to flow. You might also find that endpoints (REST) change, but we do find that most providers are quite good at versioning their endpoints so that data will continue to flow. However, in this case you might find that changing to the new endpoints will require to to run a complete re-crawl as you might be getting more rich data from the new endpoints. Therefor some sources might be sophisticated enough that we don’t require to fetch all data, but only the new fields that we are interested in. This complicates the crawler too much and it should potentially be avoided. It really depends on the source itself and if you find that you are charged a lot of money for pulling data. There are many integration sources that can be like this and it means that you may have to control this in your integration code. The CluedIn Crawler framework ships with a validation framework in which this can be extended as well. To add new Rules, all you need to do is implement the IClueValidationRule interface and then compile and drop this into your CluedIn directory to reboot. The Framework currently watches for: . DATA_001_File_MustBeIndexed ENTITYTYPE_001_Person_MustNotBeUsedDirectly ENTITYTYPE_002_Document_MustNotBeUsedDirectly METADATA_001_Name_MustBeSet METADATA_002_Uri_MustBeSet METADATA_003_Author_Name_MustBeSet METADATA_004_Invalid_EntityType METADATA_005_PreviewImage_RawData_MustBeSet METADATA_006_Created_Modified_Date_InFuture METADATA_007_Created_Modified_Date_InPast METADATA_008_Created_Modified_Date_UnixEpoch METADATA_009_Created_Modified_Date_MinDate METADATA_010_Created_Modified_Date_MaxDate EDGES_001_Outgoing_Edge_MustExist EDGES_002_Incoming_Edge_ShouldNotExist PROPERTIES_001_MustExist PROPERTIES_002_Unknown_VocabularyKey_Used PROPERTIES_003_Value_ShouldNotBeQuoted . ",
    "url": "/integration/robust-ntegrations",
    
    "relUrl": "/integration/robust-ntegrations"
  },"1106": {
    "doc": "Search",
    "title": "On this page",
    "content": ". | Search box | Search results page . | Add columns | Manage columns | Sort search results | Change page view | . | Saved searches | Export search results | . In this article, you will learn how to use the search capabilities in CluedIn to efficiently find the golden records you need. You can search through all golden records in CluedIn—just enter a keyword in the search box, and CluedIn will return all relevant results. Use filters to precisely define the golden records you’re looking for based on various criteria. ",
    "url": "/key-terms-and-features/search#on-this-page",
    
    "relUrl": "/key-terms-and-features/search#on-this-page"
  },"1107": {
    "doc": "Search",
    "title": "Search box",
    "content": "The search box is the starting point of your search for golden records. This is where you can enter a key word to start the search. Additionally, this is where you can quickly retrieve your recent searches and saved searches by clicking in the search box. The Recent Searches section displays up to 5 of your previous searches. To run a recent search, select it from the list. The Saved Searches section displays up to 7 saved searches. It contains a toggle to switch between your private saved searches and shared saved searches. In the screenshot above, the toggle is turned on to show private saved searches (My Saved Searches). Turning the toggle off will show public saved searches (Shared Saved Searches). To run a saved search, select it from the list. If you cannot find the saved search, select View all saved searches, and then look for the needed search in the Saved Searches pane. You can find more information in the article in Saved searches. In the search box, you can also select a business domain in which you want to search for golden records. To do this, expand the dropdown list and select the needed business domain. The Business Domains dropdown list displays up to 8 business domains that contain the biggest number of golden records. You can also view the number of golden records per business domain. After you select a business domain, enter a keyword, and start to search, CluedIn will display only those golden records that match the keyword and belong to the selected business domain. ",
    "url": "/key-terms-and-features/search#search-box",
    
    "relUrl": "/key-terms-and-features/search#search-box"
  },"1108": {
    "doc": "Search",
    "title": "Search results page",
    "content": "By default, the search results page displays golden records in the tabular view in the following columns: Name, Business Domain, and Description. You can customize the search results page to focus on the information that is important to you. In this section, you will find different customization options. Add columns . If you want to view other golden record properties or vocabulary keys, you can add the corresponding columns to the search results page. To add columns to the search results page . | On the search results page, select Column Options. The Column Options pane opens where you can view the columns that are currently displayed on the search results page. | Select Add columns, and then choose the type of column that you want to add to the search results page: . | Entity Property – to select the following golden record properties: Date Created, Discovery Date, or Date Modified. | Vocabulary – to find and select any vocabulary keys. | . | Depending on the type of column that you selected, do one of the following: . | For Entity Property, select the checkboxes next to the needed properties, and then select Save Selection. | For Vocabulary, do the following: . | In the Vocabulary Keys section, find the vocabulary that contains the vocabulary keys that you want to add to the search result page. By default, this section displays the vocabularies that are used in golden records. To limit the search results, use filters. You can choose to view the vocabularies that are associated with a specific business domain or the vocabularies from a specific integration. If you want to add all vocabulary keys from a specific vocabulary, select the checkbox next to the vocabulary name. If you want to add specific vocabulary keys, expand the vocabulary and select the checkboxes next to the needed vocabulary keys. | Move the vocabulary keys to the Selected Vocabulary Keys section using the arrow pointing to the right. If you decide that you do not want to add a specific vocabulary or vocabulary key, you can move it back to the Vocabulary Keys section. To do this, select the checkbox next to the needed element in the Selected Vocabulary Keys section, and then use the arrow pointing to the left. | After you move all the needed vocabulary keys to the Selected Vocabulary Keys section, select Add Vocabulary Columns. | . | . The columns are added to the search results page. You can close the Column Options pane. | . Manage columns . If you want to improve the organization of information on the search results page, you can change the order of columns or remove columns. To manage columns on the search results page . | On the search results page, select Column Options. The Column Options pane opens where you can view the columns that are currently displayed on the search results page. | To reorder columns, select the row and drag it to a new position in the list. | To remove a column, select the delete icon in the corresponding row. Your changes are immediately applied to the search results page. After you reorder or delete the columns, close the Column Options pane. | . Sort search results . By default, the golden records on the search results page are sorted by relevance. This means that CluedIn prioritizes golden records that are most likely to match your search query closely. Sorting by relevance ensures that the most pertinent results are displayed at the top of the page. In addition to sorting by relevance, CluedIn provides two alternative sorting options: . | Sorting by latest – to arrange golden records in descending order of their creation or modification date, with the most recently added or updated records appearing at the top. | Sorting by old – to arrange golden records in ascending order of their creation or modification date, with the oldest records appearing at the top. | . To sort the search results . | In the upper-right corner of the search results page, expand the sorting dropdown menu, and then select the needed sorting option. The new sorting is applied to the search results. | . Change page view . CluedIn provides two page view options: . | Tile view – presents golden records in a visual grid-like format. In this view, golden records are arranged in rectangular tiles, each representing a specific golden record. | Tabular view – presents golden records in a structured table format. In this view, golden records are arranged in rows and columns, resembling a spreadsheet or database table. | . To change the page view . | In the upper-right corner of the search results page, select the needed page view option: Tile view (a) or Tabular view (b). The new view is applied to the search results page. | . ",
    "url": "/key-terms-and-features/search#search-results-page",
    
    "relUrl": "/key-terms-and-features/search#search-results-page"
  },"1109": {
    "doc": "Search",
    "title": "Saved searches",
    "content": "Saved searches help you quickly retrieve a set of golden records that meet specific filter criteria. Once you define the filters and add the needed vocabulary keys to the search page, you can save the current search configuration for future use. Next time when you need to review a specific set of golden records and their vocabulary keys, you can quickly open a saved search instead of configuring the search from scratch. You can share the saved search with everybody else in the organization or just keep it to yourself. To save a search . | In the upper-right corner of the search results page, select the save icon. | Enter the name of the search. | If you want to make this search available to everybody in your organization, turn on the toggle next to Shared. | Select Save. The search is saved in CluedIn. Now, you can use it when you need to quickly find a specific set of golden records or when you want to clean those golden records. | . To retrieve a saved search . | Do one of the following: . | Click anywhere in the search box, and then use the toggle to define which saved searches you want to access: your private saved searches (My Saved Searches) or public saved searches (Shared Saved Searches). If you cannot find the needed saved search, select View all saved searches. | In the upper-right corner of the search results page, open the three-dot menu, and then select Saved Searches. The Saved Searches pane opens, containing your private saved searches and shared saved searches. | . | Find and select the needed saved search. The golden records matching the saved search filters are displayed on the search results page. | . ",
    "url": "/key-terms-and-features/search#saved-searches",
    
    "relUrl": "/key-terms-and-features/search#saved-searches"
  },"1110": {
    "doc": "Search",
    "title": "Export search results",
    "content": "After performing a search, you can export your results in one of the following formats: JSON, CSV, XLSX, or Parquet. Prerequisites . | Go to Administration &gt; Feature flags, and then enable the Exporting golden records feature. | . To export search results . | In the upper-right corner of the search results page, open the three-dot menu, and then select Export. | In Export Name, enter the name of the export file. | In Exporting Format, select the file format you want to use for exporting search results. | In Filters, review the search filters that define which golden records will be exported. | In Columns, review the columns that will be exported. These columns are currently displayed on the search results page. If you want to change the columns, you can add, remove, or reorder the columns as described in Manage columns. | Select Export. After the file for export is prepared, you will receive a notification. | In the notification, select View. The Exported Files page opens, where you can find the files available for download. To view the search filters that define golden records in a file, hover over the value in the Filter Type column. | To download the file, select the download button next to the file name or the file name itself. The exported file is downloaded to your computer. | . ",
    "url": "/key-terms-and-features/search#export-search-results",
    
    "relUrl": "/key-terms-and-features/search#export-search-results"
  },"1111": {
    "doc": "Search",
    "title": "Search",
    "content": " ",
    "url": "/key-terms-and-features/search",
    
    "relUrl": "/key-terms-and-features/search"
  },"1112": {
    "doc": "How to standardize dates",
    "title": "How to standardize dates",
    "content": "In this article, you will learn how to standardize dates using two different methods: by applying data part rules and by configuring data type during mapping. To begin with, we have ingested, mapped, and processed a file containing contact data. Note that the records include dates in various short date and long date patterns typical to the en-US culture. Next, we’ll demonstrate how to apply two standardization methods, both leading to the same result. Standardizing dates using data part rules . To convert dates into ISO 8601 format (YYYY-MM-DDT00:00:00+00:00), create a data part rule and add an action to normalize dates. In the action, specify the culture of the input dates to help CluedIn interpret the values. Additionally, you can specify the input format for the same purpose. Both culture and input are optional parameters. If you don’t provide the culture or input format, CluedIn will analyze the values and determine their input date format before converting them to ISO 8601 format. Next, save, activate and re-process the rule. To verify that the rule has been applied, go to the Data tab of the data set that initially contained non-standardized dates. As a result, all dates are standardized according to ISO 8601 format. This method is preferred for standardizing dates as it avoids changing the data type of the original field during mapping to the vocabulary key. Keeping the default data type (Text) helps reduce the likelihood of format validation errors during initial processing. Standardizing dates using mapping . This method requires enabling the Date Time setting in Administration &gt; Settings &gt; Processing Property Data Type Normalization. By default, this setting is disabled because date normalization using rules is the preferred method. When creating mapping for the data set, change the data type for the field containing dates to DateTime. After processing the data set, the dates will be converted to the ISO 8601 format. ",
    "url": "/kb/how-to-standardize-dates",
    
    "relUrl": "/kb/how-to-standardize-dates"
  },"1113": {
    "doc": "Stream reference",
    "title": "On this page",
    "content": ". | Stream statuses | Stream indicators | Message queues | . In this article, you will find reference information to help you understand stream statuses and other stream indicators. ",
    "url": "/consume/streams/stream-reference#on-this-page",
    
    "relUrl": "/consume/streams/stream-reference#on-this-page"
  },"1114": {
    "doc": "Stream reference",
    "title": "Stream statuses",
    "content": "| Status | Description | . | New | Stream has not been started yet. | . | Streaming | Stream is accumulating or sending data to the export target. | . | Paused | Data is being accumulated in the queue. | . | Stopped | No data is being accumulated or sent to the export target. | . ",
    "url": "/consume/streams/stream-reference#stream-statuses",
    
    "relUrl": "/consume/streams/stream-reference#stream-statuses"
  },"1115": {
    "doc": "Stream reference",
    "title": "Stream indicators",
    "content": "Stream indicators assist in understanding the state of the stream, highlighting missing elements, and indicating areas that still require configuration. | Indicator | Description | . | No export target | When no export target is defined, the golden records matching the stream’s filters will be accumulated in the queue. If you want to send the records to an external system, you need to add the export target to the stream. | . | No filters | When no filters are defined, all golden records will be accumulated or sent to the export target. | . | No properties selected | When no properties are selected, default properties of the golden records will be accumulated or sent to the export target. The list of default properties vary depending on the export target. | . | Streaming | The records are being sent to the export target. | . | Exporting | The export target is receiving records. | . | Not exporting | The export target is not receiving records. To fix this, check the export target configuration. | . ",
    "url": "/consume/streams/stream-reference#stream-indicators",
    
    "relUrl": "/consume/streams/stream-reference#stream-indicators"
  },"1116": {
    "doc": "Stream reference",
    "title": "Message queues",
    "content": "When starting the stream, records in CluedIn undergo a three-part process before being sent to the export target: ingestion, processing, and publishing. At each stage, the records are temporarily stored in queues named accordingly. | Queue | Description | . | Ingestion | This is where all of the potential records that could be streamed are stored. | . | Processing | This is where the records that meet the stream’s filters are stored and where the stream’s actions are executed. | . | Publishing | This is where the records are sent to the export target. | . ",
    "url": "/consume/streams/stream-reference#message-queues",
    
    "relUrl": "/consume/streams/stream-reference#message-queues"
  },"1117": {
    "doc": "Stream reference",
    "title": "Stream reference",
    "content": " ",
    "url": "/consume/streams/stream-reference",
    
    "relUrl": "/consume/streams/stream-reference"
  },"1118": {
    "doc": "Sync data sources",
    "title": "On this page",
    "content": ". | Preparation . | Preparation in Purview | Preparation in CluedIn | . | Feature demonstration . | Sync data sources from Purview to CluedIn | Sync data sources from CluedIn to Purview | . | . In this article, you will learn how to sync data sources between Purview and CluedIn. When this feature is enabled, CluedIn fetches all assets from Purview and creates data source groups and their respective data sources in CluedIn. Note that this feature only syncs asset metadata, not the data itself. To sync the data, use the Azure Data Factory pipeline feature. Additionally, this feature enables syncing data sources from CluedIn to Purview. ",
    "url": "/microsoft-integration/purview/sync-data-sources#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-data-sources#on-this-page"
  },"1119": {
    "doc": "Sync data sources",
    "title": "Preparation",
    "content": "To sync data sources between Purview and CluedIn, complete 2 preparation steps: . | Prepare data sources in Purview – create a glossary and a glossary term, and then associate this term with the data sources that you want to sync. | Configure settings in CluedIn – enable the sync data sources feature in Purview settings and provide the glossary term to identify the data sources for syncing. | . Preparation in Purview . | In the Microsoft Purview portal, navigate to Unified Catalog &gt; Catalog management &gt; Classic types. | Select New glossary. | Enter the Name of the glossary. | Select the default Domain that contains the collections you created in Create a new collection. | Find and select the Data Steward and the Expert who will manage the glossary. | Select Create. The new glossary is created. Next, add a term to the glossary. | To add a term to the glossary: . | On the terms card, select View terms. | Select New term. | Select the System default term template and select Continue. | Enter the Name of the term and select Create. | . The new term is added to the glossary. Next, add the term to the asset that you want to sync with CluedIn. | To add the term to the asset that you want to sync with CluedIn: . | Navigate to Data Map &gt; Domains. In your default domain, select the collection that stores the assets from Azure Data Lake Storage. | Select the assets card. | Find and select the asset that you want to sync with CluedIn. | On the asset details page, select Edit. | In Glossary terms, find and select the term you created in step 7. | Select Save. | . The term is added to the asset that you want to sync with CluedIn. On the term details page, you can find the assets associated with the term. Once you have prepared the data sources that you want to sync, configure the appropriate settings in CluedIn. | . Preparation in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync CluedIn Data Sources. | In Sync CluedIn Data Sources Keywords, enter the name of the term that is associated with assets you want to sync. If you want to sync all assets, enter an asterisk (*). | Select Save. Once you save the changes, synchronization begins. | . ",
    "url": "/microsoft-integration/purview/sync-data-sources#preparation",
    
    "relUrl": "/microsoft-integration/purview/sync-data-sources#preparation"
  },"1120": {
    "doc": "Sync data sources",
    "title": "Feature demonstration",
    "content": "This section describes the two methods of data source synchronization: from Purview to CluedIn and from CluedIn to Purview. Sync data sources from Purview to CluedIn . When the synchronization is completed, you will receive a notification. After synchronization, you can view the data source groups in Integrations &gt; Data Sources. Each Purview asset is created in CluedIn as a separate data source group. As a result of synchronization, asset metadata is now available in CluedIn. Synchronization runs every minute, so new assets will be automatically synced with CluedIn. Sync data sources from CluedIn to Purview . Once you create the mapping and process the data set from the data source synced from Purview, you will receive a notification when the data set is synced back to Purview. To find the asset in Purview . | In the Microsoft Purview portal , navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the data set in CluedIn. | On the asset details page, go to Lineage. Here, you can view a visual representation of how a data set moves in the CluedIn processing pipeline. The following screenshot shows the initial data set that has been ingested into CluedIn. The data set is then mapped to standard fields, resulting in clues. Finally, the mapped records (clues) are sent to the processing pipeline. The following screenshot shows the continuation of the lineage from the previous screenshot. As a result of processing, the data set produced golden records of the Account entity type. | . ",
    "url": "/microsoft-integration/purview/sync-data-sources#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-data-sources#feature-demonstration"
  },"1121": {
    "doc": "Sync data sources",
    "title": "Sync data sources",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-data-sources",
    
    "relUrl": "/microsoft-integration/purview/sync-data-sources"
  },"1122": {
    "doc": "Use CluedIn rules in Microsoft Fabric",
    "title": "Use CluedIn rules in Microsoft Fabric",
    "content": "CluedIn provides a powerful GraphQL API for getting data and metadata and executing actions. In this article, you will learn how to read CluedIn rules’ metadata in Microsoft Fabric and use it to work with data in OneLake. To use CluedIn rule in Microsoft Fabric . Create a notebook in Microsoft Fabric, install dependencies, and get a CluedIn access token. !pip install cluedin !pip install jqqb_evaluator import cluedin ctx = cluedin.Context.from_dict({ \"domain\": \"cluedin.demo\", \"org_name\": \"foobar\", \"user_email\": \"admin@cluedin.demo\", \"user_password\": \"yourStrong(!)Password\" }) ctx.get_token() . Get all the rules from CluedIn. # get all data part rules rules = cluedin.rules.get_rules(ctx) # output rule names list(map(lambda x: x['name'], rules['data']['management']['rules']['data'])) . If you want to get a particular rule by ID, use the following method. cluedin.rules.get_rule(ctx, rule_id) . From a rule’s conditions, you can create an Evaluator that helps to evaluate if a given object matches with the rule’s conditions. In the following example, we take all data part rules from CluedIn, create a list of evaluators, and then test if a test object matches to at least one evaluator in the list: . # get all data part rules ids rule_ids = list(map(lambda x: x['id'], cluedin.rules.get_rules(ctx)['data']['management']['rules']['data'])) # get full rule data rules = list(map(lambda rule_id: cluedin.rules.get_rule(ctx, rule_id), rule_ids)) # get a list of evaluators for all rules evaluators = list(map(lambda rule: cluedin.rules.Evaluator(rule['data']['management']['rule']['condition']), rules)) # test object obj = { 'employee.job': 'Akkounting' } # returns True if the object matches to at least one evaluator in the list any(map(lambda evaluator: evaluator.object_matches_rules(obj), evaluators)) . The evaluator’s explain() method helps to understand the current evaluator’s conditions. It outputs code in terms of pandas’ DataFrame.query method: . # explain all evaluators list(map(lambda evaluator: evaluator.explain(), evaluators)) . Output: . [ 'df.query(\\'`employee.job` == \"Ackounting\" | `employee.job` == \"Acounting\" | `employee.job` == \"Akkounting\" | `employee.job` == \"aCoUnTiNg\" | `employee.job` == \"account ing\" | `employee.job` == \"accounting\"\\')', 'df.query(\\'`employee.job` == \"Software Dev\"\\')', 'df.query(\\'`employee.job` == \"Softwear Dev\"\\')' ] . With the help of a few methods, you can transform your data using CluedIn rules: . def set_value_action(obj, field, val): \"\"\" Set Value action: takes an object (obj), and sets its property (field) to a value (val). \"\"\" obj[field] = val return obj def get_action(action_json): \"\"\" Takes a Rule Action JSON, and returns a lambda \"\"\" if action_json['type'] == 'CluedIn.Rules.Actions.SetValue, CluedIn.Rules': field = None val = None for prop in action_json['properties']: if prop['name'] == 'FieldName': field = prop['value'] elif prop['name'] == 'Value': val = prop['value'] return lambda obj: set_value_action(obj, field, val) print(f'Action \"{action_json[\"type\"]}\" is not supported. Object:', obj) return lambda obj: obj def get_actions_with_evaluators(rule): \"\"\" For a given rule, returns an iterable of objects containing an action and a corresponding evaluator: { 'action': lambda x: ..., 'evaluator': ... } \"\"\" for r in rule['data']['management']['rule']['rules']: for a in r['actions']: yield { 'evaluator': cluedin.rules.Evaluator(rule['data']['management']['rule']['condition']), 'action': get_action(a) } # test action (not evaluator) result = list(get_actions_with_evaluators(rules[0])) result[0]['action']({ 'employee.job': 'CEO' }) . def apply_actions(actions_with_evaluators, obj): \"\"\" Given a list of actions with evaluators pairs and an object (obj), apply action to the object if it passes the corresponding evaluator. \"\"\" for action_with_evaluator in actions_with_evaluators: if action_with_evaluator['evaluator'].object_matches_rules(obj): obj = action_with_evaluator['action'](obj) return obj # test actions_with_evaluators = [action_with_evaluator for rule in rules for action_with_evaluator in get_actions_with_evaluators(rule)] apply_actions(actions_with_evaluators, { 'employee.job': 'Akkounting' }) . Now, we can load CluedIn data in a DataFrame: . import pandas as pd query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query sort: FIELDS cursor: $cursor pageSize: $pageSize sortFields: {field: \"id\", direction: ASCENDING} ) { cursor entries { id entityType properties } } } \"\"\" df = pd.DataFrame(cluedin.gql.entries(ctx, query, { 'query': 'entityType:/Employee', 'pageSize': 10_000 }, flat=True)) df.head(20) . And apply Rule Actions to matched records: . # apply rule actions to a data frame df.apply(lambda row: apply_actions(actions_with_evaluators, row), axis=1) . Or you can filter the DataFrame with the evaluators: . def evaluate(row): return any(map(lambda evaluator: evaluator.object_matches_rules(row), evaluators)) df_filtered = df[df.apply(evaluate, axis=1)] display(df_filtered) . ",
    "url": "/microsoft-integration/fabric/use-cluedin-rules-in-fabric",
    
    "relUrl": "/microsoft-integration/fabric/use-cluedin-rules-in-fabric"
  },"1123": {
    "doc": "Vocabulary keys",
    "title": "On this page",
    "content": ". | Vocabulary key overview | Vocabulary key details page . | Configuration | Usage | All values | Lineage | Profiling | Pending changes and Audit log | . | . In this article, you will learn about vocabulary keys and the importance of specifying the correct data type for a vocabulary key to unlock the full potential of filters. ",
    "url": "/management/data-catalog/vocabulary-keys#on-this-page",
    
    "relUrl": "/management/data-catalog/vocabulary-keys#on-this-page"
  },"1124": {
    "doc": "Vocabulary keys",
    "title": "Vocabulary key overview",
    "content": "A vocabulary key helps CluedIn understand the data you send. It is an attribute that helps describe a golden record. Such attributes provide information about the golden record’s features, connections, and characteristics that are important for understanding and using the data effectively. In CluedIn, you can browse all vocabulary keys by going to Management &gt; Data Catalog &gt; View All Vocabulary Keys. In addition, you can browse all vocabulary keys belonging to a specific vocabulary on the vocabulary details page. ",
    "url": "/management/data-catalog/vocabulary-keys#vocabulary-key-overview",
    
    "relUrl": "/management/data-catalog/vocabulary-keys#vocabulary-key-overview"
  },"1125": {
    "doc": "Vocabulary keys",
    "title": "Vocabulary key details page",
    "content": "On the vocabulary key details page, you can view relevant information about a vocabulary key and take other actions to manage a vocabulary key. Configuration . This tab contains general information about the vocabulary key, including: . | Display name – a user-friendly identifier of the vocabulary key. This name is displayed at the top of the vocabulary details page. | Name – an identifier of the vocabulary key that, together with the key prefix, forms a complete vocabulary key. | Group – a collection that brings together related vocabulary keys, facilitating easier navigation among all keys within the vocabulary. | Visibility settings – a configuration that determines if the vocabulary key is visible in the system. When the toggle is turned off, it means that the vocabulary key is not available in filters, column options in search results page, or golden record properties. While all vocabulary key values are retained in the system, they can only be viewed on the vocabulary key details page. | Data type – a type of data that the vocabulary key can hold. For more information, see Data types. The data type is closely connected with the list of available operators in filters. Depending on the data type assigned to the vocabulary key, the list of operators may differ. For example, the list of operators for a vocabulary key with the Number data type is different from that for a vocabulary key with the Text data type. | Storage type – a type of field—keyword, typed, untyped—where the vocabulary key value is stored. Keyword is generally used for strings. Typed is the default choice for numeric and date-related data types, enabling the use of specific operators in filters (for example, Between, Equals, Greater or Equal). If you change the storage to Untyped, you won’t have access to the specific operators designed for numeric or date-related data types. Nevertheless, you can still use operators available for strings. | Classification – a logical category that makes the vocabulary key easy to search. | Description – a summary that explains the purpose of the vocabulary key. You can add rich formatting to the description, such as bolding, underlining, or italicizing text. | Mapping to another vocabulary key – a section that shows if the current vocabulary key is mapped to another vocabulary key. The purpose of such mapping is to avoid having duplicate values in the system. | . Usage . This tab contains the global view of vocabulary key usage in the system: number of golden records, streams, glossary terms, rules, data set mappings, clean projects, deduplication projects, and saved searches where the vocabulary key is used. You can select the View all button to view more details and find links to the corresponding elements in the system. All values . This tab displays all values associated with the key, along with the total number of golden records in which each value is used. When you select a specific value, a new tab containing the search results will open. This tab lists golden records where the selected vocabulary key value is used. Note that the vocabulary key column is not displayed by default, so you may need to add columns to the search results page. Lineage . This tab shows the transformations of the vocabulary key. | Integrations – data source from which the vocabulary key originates. | Source vocabulary keys – vocabulary key that originates from a specific data source (integration). | Core vocabulary keys – vocabulary key to which the source vocabulary key is mapped. | Streams – streams where the vocabulary key is used. | . Profiling . This tab contains a breakdown and distribution of all vocabulary key values in a graphic format. It can help you identify issues and anomalies in data quality. Profiling is a beta feature. To access profiling, go to Administration &gt; Feature Flags, and enable the Profiling dashboards feature. Profiling is type-specific, so the dashboards for text and number vocabulary keys are different. The following image shows an example of profiling for a text vocabulary key. The profiling for a text vocabulary key contains the following dashboards: . | Total values – the total number of values that the vocabulary key has. | Values over time – a time-series visualization displaying the count of values that appeared in CluedIn over time. The x axis represents the time, and the y axis represents the count of values. For a more granular view, you can select a specific time period using the mouse. | Distribution of values (bar gauge) – the distribution of vocabulary key values based on the number of records where each value is used. Each bar represents a distinct value, with the color gradient indicating the frequency of that value’s occurrence in the records: green indicates a lower number of records, while red indicates a higher number of records. This gradient provides a quick visual cue for identifying the most and least common values. | Distribution of values (pie chart) – the distribution of vocabulary key values based on the number of records where each value is used. Each slice of the pie represents a distinct key value, with the area of the slice indicating the frequency of that value’s occurrence in the records. The larger the slice, the higher the number of records that contain the value. This type of visualization helps to quickly understand the relative proportions of different vocabulary key values. | . The following image shows an example of profiling for a number vocabulary key. The profiling for a number vocabulary key contains the following dashboards: . | Minimum value – the minimum value among all values of a vocabulary key. | Standard deviation – the amount of variation or dispersion in a set of values. It indicates how much individual values deviate from the average value. A low standard deviation indicates that the values tend to be close to the average, while a high standard deviation indicates that the values are spread out over a wider range of values. | Maximum value – the maximum value among all values of a vocabulary key. | Average value – the average value, calculated by dividing the sum of all values by the number of values. This visualization helps to quickly understand the central tendency among the vocabulary key values. | Number of values – the total number of values that the vocabulary key has. | Sum value – the sum of all vocabulary key values. | Values over time – a time-series visualization displaying the count of values that appeared in CluedIn over time. The x axis represents the time, and the y axis represents the count of values. | . Pending changes and Audit log . The Pending changes tab contains tasks for reviewing changes to the vocabulary key submitted by users who are not Vocabulary Owners. The Audit log tab contains a detailed history of changes to the vocabulary key. ",
    "url": "/management/data-catalog/vocabulary-keys#vocabulary-key-details-page",
    
    "relUrl": "/management/data-catalog/vocabulary-keys#vocabulary-key-details-page"
  },"1126": {
    "doc": "Vocabulary keys",
    "title": "Vocabulary keys",
    "content": " ",
    "url": "/management/data-catalog/vocabulary-keys",
    
    "relUrl": "/management/data-catalog/vocabulary-keys"
  },"1127": {
    "doc": "Work in a hierarchy project",
    "title": "On this page",
    "content": ". | Load golden records | Modify a hierarchy | Use hierarchy tools | . In this article, you will learn how to do the following: . | Load golden records connected by different types of relations into a hierarchy project. | Modify the hierarchy. | Use various tools to make working with the hierarchy more convenient. | . ",
    "url": "/management/hierarchy-builder/work-in-a-hierarchy-project#on-this-page",
    
    "relUrl": "/management/hierarchy-builder/work-in-a-hierarchy-project#on-this-page"
  },"1128": {
    "doc": "Work in a hierarchy project",
    "title": "Load golden records",
    "content": "After you create a hierarchy, you can load golden records that are connected by various types of relations so that these relations are automatically visualized in the hierarchy. Note that the types of relations available for selection depend on the business domain used in the hierarchy project. If you selected a business domain when creating the hierarchy, only the types of relations associated with the golden records of that business domain are available for selection. If you want to explore all types of relations, you can modify the hierarchy properties and remove the business domain. This way all golden records will be loaded to the project, and you can select any type of relation that you want. To load golden records . | In the hierarchy project, select Load Entities. The Load entities pane opens on the right hand of the screen. | In Relation type, select the edge type that represents relations between golden records that will be loaded to the hierarchy project. | In Edge direction, select the way in which hierarchy relations between golden records will be generated. The edge direction is selected automatically based on the edge type used when creating the hierarchy. | In the Preview entities section, review the golden records that will be loaded to the hierarchy project. | Select Load Entities. The golden records are added to the hierarchy project canvas in a way that shows their relationships to other golden records. You can load as many golden records as you want. | Review the generated hierarchy and modify it if needed. | To save your changes, select Save in the upper-right corner of the page. | To create the actual relations between golden records as you have designed in the hierarchy project, select Publish in the upper-right corner of the page. Then, confirm that you want to publish the hierarchy. As a result, these relations will be available on the Relations tab of golden records. | . ",
    "url": "/management/hierarchy-builder/work-in-a-hierarchy-project#load-golden-records",
    
    "relUrl": "/management/hierarchy-builder/work-in-a-hierarchy-project#load-golden-records"
  },"1129": {
    "doc": "Work in a hierarchy project",
    "title": "Modify a hierarchy",
    "content": "While working in a hierarchy project, you can replace nodes or delete a node or a subtree as needed. To replace nodes . | In the lower-right corner of the page, select the button to turn on the Replace on drop mode. | In the left pane, find a replacement node and then drag it to the place of the node that you want to replace. | After replacing all nodes, turn off the Replace on drop mode by clicking the corresponding button. The node that you replaced is still available in the left pane, so you can drag it to the canvas again if needed. | . To delete a node or subtree . | Hover over a node, open the three-dot menu, and then select the needed option: . | Delete Node – the node as well as all of its child nodes will be deleted. | Delete Subtree – all child nodes will be deleted, but the parent node will remain intact. The nodes are deleted from the hierarchy. However, they are still available in the left pane, so you can drag them to the canvas again if needed. | . | . If you accidentally replaced or deleted a node, you can revert the changes using the Undo action. To bring the changes back, use the Redo action. ",
    "url": "/management/hierarchy-builder/work-in-a-hierarchy-project#modify-a-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/work-in-a-hierarchy-project#modify-a-hierarchy"
  },"1130": {
    "doc": "Work in a hierarchy project",
    "title": "Use hierarchy tools",
    "content": "While working in a hierarchy project, you can adjust the hierarchy view using various tools. To adjust the hierarchy view . | To collapse all levels of the hierarchy, in the lower-right corner, select the Collapse All button. All subtrees will be collapsed to the top-level parent node. | To expand all levels of the hierarchy, in the lower-right corner, select the Expand All button. All subtrees will be expanded. | To collapse a subtree of a node, hover over the node, and then select Collapse. | To expand a subtree of a node, hover over the node, and then select Expand. | To navigate the hierarchy canvas, use the Zoom In, Zoom Out, Focus, and Full Screen buttons. | . ",
    "url": "/management/hierarchy-builder/work-in-a-hierarchy-project#use-hierarchy-tools",
    
    "relUrl": "/management/hierarchy-builder/work-in-a-hierarchy-project#use-hierarchy-tools"
  },"1131": {
    "doc": "Work in a hierarchy project",
    "title": "Work in a hierarchy project",
    "content": " ",
    "url": "/management/hierarchy-builder/work-in-a-hierarchy-project",
    
    "relUrl": "/management/hierarchy-builder/work-in-a-hierarchy-project"
  },"1132": {
    "doc": "Installation guide",
    "title": "On this page",
    "content": ". | Prerequisites | Find CluedIn | Complete the Basics tab | Complete the Instance Setup tab | Review the Network and Monitoring tab | Review the Azure Kubernetes tab | Review the Advanced tab . | Bring Your Own Private DNS Zones (Optional) | . | Installation Configuration . | Jumpbox (Optional) | Bastion (Optional) | . | Review the Tags tab | Complete the Review + Create tab | Results | Next steps | . In this article, you will learn how to install CluedIn PaaS from the Azure Marketplace. CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. ",
    "url": "/deployment/azure-marketplace/step-3#on-this-page",
    
    "relUrl": "/deployment/azure-marketplace/step-3#on-this-page"
  },"1133": {
    "doc": "Installation guide",
    "title": "Prerequisites",
    "content": ". | Make sure that you have completed all of the actions described in Pre-installation checklist. | . ",
    "url": "/deployment/azure-marketplace/step-3#prerequisites",
    
    "relUrl": "/deployment/azure-marketplace/step-3#prerequisites"
  },"1134": {
    "doc": "Installation guide",
    "title": "Find CluedIn",
    "content": "Start the installation process by finding CluedIn PaaS in the Azure Marketplace. To find CluedIn . | In the Azure Marketplace, go to our PaaS offering: CluedIn Master Data Management. | On the CluedIn Master Data Management page, select Get It Now. | In Software plan, select the plan of your choice: . | CluedIn MDM PaaS - Existing License – select this option if you possess a CluedIn PaaS license key acquired through a committed deal. | CluedIn MDM PaaS - Pay As You Go – select this option if you want to use CluedIn on a pay-as-you-go basis. For more information, see Pricing. | . | Review your contact information, and then select Continue. | . ",
    "url": "/deployment/azure-marketplace/step-3#find-cluedin",
    
    "relUrl": "/deployment/azure-marketplace/step-3#find-cluedin"
  },"1135": {
    "doc": "Installation guide",
    "title": "Complete the Basics tab",
    "content": "On the Basics tab, you can select the Azure resource group where CluedIn will be installed and provide license or company details depending on the chosen software plan. To complete the Basics tab . | Fill in the Project details section: . | In Subscription, select your Azure subscription where you want to install CluedIn. | In Resource group, create a new resource group. You can select the existing resource group, but it must be empty. If the resource group that you select contains other Azure resources, you will get an error. | . | Fill in the Instance details section: . | In Region, specify the Azure region where different resources will be created. The region will probably match your resource group location. | Depending on the selected software plan, do one of the following: . | If you selected CluedIn MDM PaaS - Existing License, enter the license key that you should have received in an email from CluedIn. | If you selected CluedIn MDM PaaS - Pay As You Go, enter the Company name and Contact Email Address. | . | . | Make sure that the Managed Application Details section is filled out correctly. This section is usually filled out by default, but you can make changes if needed. | . ",
    "url": "/deployment/azure-marketplace/step-3#complete-the-basics-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#complete-the-basics-tab"
  },"1136": {
    "doc": "Installation guide",
    "title": "Complete the Instance Setup tab",
    "content": "On the Instance Setup tab, you can create the organization used within the CluedIn application along with an administrator account for your CluedIn instance as well as optionally specifying SMTP details. To complete the Instance Setup tab . | Make sure that the Installation Name field is filled out correctly. This field is usually filled out by default, but you can make changes if needed. This is the name of the Managed Application resource that gets created within the resource group specified on the Basics tab. | Specify organization details: . | Specify Organization Name. The organization name will be used as part of the URL. Ideally, it should be one word with no dashes or hyphens. | Specify Administrator Email Address. | Specify CluedIn Administrator Password, and then Confirm password. The password must be more than 12 characters long. | . | Specify SMTP details: . | Specify SMTP - Server. | In SMTP - Port, specify SSL port. | Add details about the email that will be used for sending invitations to other users to join CluedIn. You can change SMTP details after installation by submitting a ticket to CluedIn support. | . | . ",
    "url": "/deployment/azure-marketplace/step-3#complete-the-instance-setup-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#complete-the-instance-setup-tab"
  },"1137": {
    "doc": "Installation guide",
    "title": "Review the Network and Monitoring tab",
    "content": "On the Network and Monitoring tab, you can review vNet default configuration settings and make changes if needed. ",
    "url": "/deployment/azure-marketplace/step-3#review-the-network-and-monitoring-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#review-the-network-and-monitoring-tab"
  },"1138": {
    "doc": "Installation guide",
    "title": "Review the Azure Kubernetes tab",
    "content": "On the Azure Kubernetes tab, you can customize the number of nodes that you want to use in your CluedIn instance, and you can also define autoscaling parameters. In most cases, you do not need to adjust anything on this tab unless advised to do so by CluedIn support. The AKS Setup tab contains the following settings: . | General Node Size that runs the main application services. General Node Pool VM Count contains a minimum number of VMs to ensure that all resources can be scheduled. If you need to increase the VM count, consult with CluedIn support. | Data Node Size that runs any database and data collection services. Data Node Pool VM Count contains a minimum number of VMs to ensure that all resources can be scheduled. If you need to increase the VM count, consult with CluedIn support. | Data ES Node Size and Data ES Node Pool VM Count. | Processing Node Size that runs any CluedIn processing services. Processing Node Size depends on the license you have specified. If it is a pay-as-you-go installation, a basic SKU will be used at runtime. | . You can enable auto-scaling for the processing node pool. It means that when CPU pressure (high workload) builds up in this node pool, AKS can start up new nodes to compensate for the increase in load. When the load returns to normal, the extra nodes will be shut down. However, be aware that there are additional infrastructure and licensing costs associated with scaling up. For more information about auto-scaling, contact CluedIn support. You can enable auto-scaling after installation. ",
    "url": "/deployment/azure-marketplace/step-3#review-the-azure-kubernetes-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#review-the-azure-kubernetes-tab"
  },"1139": {
    "doc": "Installation guide",
    "title": "Review the Advanced tab",
    "content": "On the Advanced tab, you can find technical and debugging switches that are used by CluedIn support. Most of the options on this tab should not be adjusted. Any misconfiguration or changes to these options can cause the installation to fail. You can make changes on this tab only with the advice of CluedIn support. Bring Your Own Private DNS Zones (Optional) . You can enable the use of existing Private DNS Zones during installation. If you choose to bring your own zones, the deployment will reference them instead of creating new ones. Note: The following DNS zones must already exist: . | privatelink.blob.core.windows.net | privatelink.file.core.windows.net | privatelink.vaultcore.azure.net | . Prerequisites . | Virtual Network Link Each of the above DNS zones must have a virtual network link established with the Virtual Network (VNet) that will be used during the installation process. | Access Control The user-assigned managed identity used for deployment must be granted the following roles: . | Private DNS Zone Contributor on the respective Private DNS Zones. | Network Contributor on the target virtual network. | . | . ",
    "url": "/deployment/azure-marketplace/step-3#review-the-advanced-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#review-the-advanced-tab"
  },"1140": {
    "doc": "Installation guide",
    "title": "Installation Configuration",
    "content": "As part of the deployment workflow, ensure that the “Use Existing Private DNS Zones” option is enabled in the installer interface. This instructs the deployment process to reference the provided DNS zones rather than creating new ones. Jumpbox (Optional) . You can enable the Jumpbox feature during installation. This deploys a secure virtual machine inside your virtual network to provide administrative access for installation and troubleshooting. | The Jumpbox comes pre-installed with all required tools—no additional setup is needed. | You must provide a valid Jumpbox Admin Username and Password to access the VM. | . Bastion (Optional) . You can also choose to enable Azure Bastion for secure, browser-based RDP or SSH access to the Jumpbox (or other VMs) without exposing any public IPs. Note: If you are bringing your own virtual network, ensure that a separate subnet named AzureBastionSubnet has already been created in that VNet. This is a prerequisite for enabling Azure Bastion. For more information about using Jumpbox or Bastion, contact CluedIn Support. ",
    "url": "/deployment/azure-marketplace/step-3#installation-configuration",
    
    "relUrl": "/deployment/azure-marketplace/step-3#installation-configuration"
  },"1141": {
    "doc": "Installation guide",
    "title": "Review the Tags tab",
    "content": "On the Tags tab, you can add tags to categorize your resources and view consolidated billing by applying the same tag to multiple resources and resource groups. Tags do not get applied to the managed application resource itself. This may cause issues if you have comprehensive tagging policies in place. ",
    "url": "/deployment/azure-marketplace/step-3#review-the-tags-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#review-the-tags-tab"
  },"1142": {
    "doc": "Installation guide",
    "title": "Complete the Review + Create tab",
    "content": "On the Review + Create tab, review and accept the terms and conditions. To start the deployment process in your own cloud environment, select Create. ",
    "url": "/deployment/azure-marketplace/step-3#complete-the-review--create-tab",
    
    "relUrl": "/deployment/azure-marketplace/step-3#complete-the-review--create-tab"
  },"1143": {
    "doc": "Installation guide",
    "title": "Results",
    "content": "You have CluedIn running in your own cloud environment. ",
    "url": "/deployment/azure-marketplace/step-3#results",
    
    "relUrl": "/deployment/azure-marketplace/step-3#results"
  },"1144": {
    "doc": "Installation guide",
    "title": "Next steps",
    "content": "Customize CluedIn to meet your organization’s needs as described in our Post-installation guide. ",
    "url": "/deployment/azure-marketplace/step-3#next-steps",
    
    "relUrl": "/deployment/azure-marketplace/step-3#next-steps"
  },"1145": {
    "doc": "Installation guide",
    "title": "Installation guide",
    "content": " ",
    "url": "/deployment/azure-marketplace/step-3",
    
    "relUrl": "/deployment/azure-marketplace/step-3"
  },"1146": {
    "doc": "ADF pipeline automation",
    "title": "On this page",
    "content": ". | Preparation . | Preparation in Azure portal | Preparation in Purview | Preparation in CluedIn | . | Feature overview | . In this article, you will learn how to configure the Azure Data Factory (ADF) pipeline automation to sync data between Purview and CluedIn. To configure the ADF pipeline to sync data between Purview and CluedIn, complete three steps: . | Prepare ADF resource and ADF service principal in the Azure portal – register the ADF application and assign the appropriate permissions. | Prepare data source in Purview – create a glossary term and associate it with the data sources that you want to sync. | Configure settings in CluedIn – provide ADF credentials, enable the ADF pipeline automation feature, and provide the glossary term to identify the data sources for syncing. | . ",
    "url": "/microsoft-integration/purview/adf-pipeline-automation#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/adf-pipeline-automation#on-this-page"
  },"1147": {
    "doc": "ADF pipeline automation",
    "title": "Preparation",
    "content": "This section contains the steps required to prepare for syncing data from Purview to CluedIn with the help of ADF pipeline. Preparation in Azure portal . | Register an application for ADF following the steps described in Register an application and create a service principal. You will need the Application (client) ID and Directory (tenant) ID to configure ADF in CluedIn. | Create a client secret for ADF application following the steps described in Register an application and create a service principal. | Register ADF application in the key vault that you created in Create a key vault and register Purview: . | Go to Access policies and select Create. | On the Permissions tab, in the Secret permissions column, select the checkboxes for Get and List, and then select Next. | On the Principal tab, find and select the ADF service principal, and then select Next. | Select Create. | On the Application (optional) tab, select Next. | On the Review + create tab, select Create. | . | Give a Contributor role to the ADF application to the data factory resource: . | In the ADF resource, go to Access control (IAM), and then select Add &gt; Add role assignment. | On the Role tab, go to Privileged administrator roles, and review information about the Contributor role. | Select Next. | In Members &gt; Select members, find and select ADF service principal. | Select Review + assign. | . As a result, the ADF application now has the Contributor access to the data factory resource. | . Preparation in Purview . | In the Microsoft Purview portal, navigate to Unified Catalog &gt; Catalog management &gt; Classic types. | Find and select the glossary that you created in Sync data sources. | Add a term to the glossary: . | On the terms card, select View terms. | Select New term. | Select the System default term template and select Continue. | Enter the Name of the term and select Create. | . The new term is added to the glossary. Next, add the term to the asset that you want to sync with CluedIn. | To add the term to the asset that you want to sync with CluedIn: . | Navigate to Data Map &gt; Domains. In your default domain, select the collection that stores the assets from Azure Data Lake Storage. | Select the assets card. | Find and select the asset that you want to sync with CluedIn. | On the asset details page, select Edit. | In Glossary terms, find and select the term you created in step 3. | Select Save. | . The term is added to the asset that you want to sync with CluedIn. On the term details page, you can find the assets associated with the term. Once you have prepared the data sources that you want to sync, configure the appropriate settings in CluedIn. | . Preparation in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | In Azure Data Factory Base Url, enter the resource ID of your ADF resource. To find the resource ID, go to your ADF resource, select JSON View, and then copy the value of Resource ID. The resource ID should be in the following format: https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/providers/Microsoft.DataFactory/factories/{factoryName}/. | In Azure Data Factory Client ID, enter a unique identifier assigned to the ADF application when you registered it in Microsoft Entra ID. You can find this value on the application overview page, in Application (client) ID. | In Azure Data Factory Client Secret, enter a string value that your ADF application uses to prove its identity when requesting a token. | In Azure Data Factory Tenant ID, enter a unique identifier for your Microsoft Entra ID tenant in which your ADF application is registered. You can find this value on the application overview page, in Directory (tenant) ID. | Turn on the toggle in Azure Data Factory Pipeline Automation. | In ADF Pipeline Automation Term Pattern, enter the name of the term that is associated with assets you want to ingest into CluedIn via automated pipeline. | Select Save. Once you save the changes, the data will start to come forward to CluedIn ingestion endpoint. | . ",
    "url": "/microsoft-integration/purview/adf-pipeline-automation#preparation",
    
    "relUrl": "/microsoft-integration/purview/adf-pipeline-automation#preparation"
  },"1148": {
    "doc": "ADF pipeline automation",
    "title": "Feature overview",
    "content": "When the synchronization is completed, you will receive a notification. How to check the pipelines in ADF? . The ADF pipelines are created automatically for each Purview asset. To verify the pipelines in ADF, go to Author, and then do the following: . | Expand the Pipelines dropdown list – you will see the pipeline that has been created automatically. | Expand the Datasets dropdown list – you will see the dataset that has been sent to CluedIn as well as REST connection to CluedIn ingestion endpoint. | . Additionally, go to Manage &gt; Linked services. Here, you will see the linked services that define connection information to a data store. For example, in the following screenshot, there are three linked services: . | Azure Data Lake Storage – connects ADF to your Azure Data Lake Storage account and allows you to copy data from the storage account. | Azure Key Vault – connects ADF to Azure Key Vault, where ADF retrieves the credentials at runtime to access the data store. | REST – connects ADF to CluedIn ingestion endpoint. | . Finally, go to Monitor &gt; Pipeline runs to verify that the pipeline has run successfully. How to check the ingested data in CluedIn? . As a result of pipeline run, the data source in CluedIn now contains a data set. ",
    "url": "/microsoft-integration/purview/adf-pipeline-automation#feature-overview",
    
    "relUrl": "/microsoft-integration/purview/adf-pipeline-automation#feature-overview"
  },"1149": {
    "doc": "ADF pipeline automation",
    "title": "ADF pipeline automation",
    "content": " ",
    "url": "/microsoft-integration/purview/adf-pipeline-automation",
    
    "relUrl": "/microsoft-integration/purview/adf-pipeline-automation"
  },"1150": {
    "doc": "Azure Event Hub connector",
    "title": "Azure Event Hub connector",
    "content": "This article outlines how to configure the Azure Event Hub connector to publish data from CluedIn to Azure Event Hubs. Prerequisites: Make sure you have an existing Event Hubs namespace with a specific event hub where you want to store the data from CluedIn. In addition, the event hub must have a policy with Manage, Send, and Listen access. To configure Azure Event Hub connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Azure Event Hub Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | Connection String – connection string to the event hub within a namespace where you want to store the data from CluedIn. To find this value, select the needed event hub. Then, select Shared access policies on the left menu under Settings. Finally, select the policy, and then select the Copy button next to the Connection string-primary key field. For more information, see Microsoft documentation. | Name – name of the event hub in the Event Hubs namespace where you want to store the data from CluedIn. | . | Test the connection to make sure it works, and then select Add. Now, you can select the Azure Event Hub connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/azure-event-hub-connector",
    
    "relUrl": "/consume/export-targets/azure-event-hub-connector"
  },"1151": {
    "doc": "Brreg",
    "title": "On this page",
    "content": ". | Add Brreg enricher | Properties from Brreg enricher | . This article explains how to add the Brreg enricher. The purpose of this enricher is to provide a wide range of information about Norwegian and foreign businesses operating in Norway (for example, address, registration date, employee count, and so on). More details can be found in Properties from Brreg enricher. The Bregg enricher supports the following endpoints: . | http://data.brreg.no/enhetsregisteret/api/enheter/{id}, where {id} is the Brreg code. | http://data.brreg.no/enhetsregisteret/api/enheter?page=0&amp;size=30&amp;navn={name}, where {name} is the company name. | . ",
    "url": "/preparation/enricher/brreg#on-this-page",
    
    "relUrl": "/preparation/enricher/brreg#on-this-page"
  },"1152": {
    "doc": "Brreg",
    "title": "Add Brreg enricher",
    "content": "The enricher requires at least one of the following attributes for searching the Brreg register: . | Brreg Code – if your golden records have Brreg codes, you can enter the corresponding vocabulary key to configure the enricher. As a result, the enricher will use the Brreg code to search the Brreg register. | Name, Country Code, and Website – if your golden records do not have Brreg codes, you can enter these three attributes to configure the enricher. As a result, the enricher will use the combination of name, country, and website to search the Brreg register. For this method to work, the name must be a valid name—not empty, not numbers, not GUID, not equal to “Microsoft Office User”, not email. Additionally, at least one of the following conditions must be met: . | The name postfix contains one of the following values: “A/S”, “AS”, “ASA”, “I/S”, “IS”, “K/S”, “KS”, “ENK”, “ANS”, “NUF”, “P/S”, “PS”, “Enkeltpersonforetak”, “Ansvarlig Selskap”, “Aksjeselskap”, “Norskregistrert utenlandsk foretak”. | The name contains at least one of the following values: ” no”, “no “, “norway”, “norge”, “norsk”, “æ”, “ø”, “å”. | The country is one of the following: “no”, “NOR”, “Norway”, “Norge”. | The website is valid and ends with “.no” (for example: google.no). | . | . To add the Brreg enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Brreg, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Name Vocabulary Key – enter the vocabulary key that contains the names of companies that will be used for searching the Brreg register. | Country Code Vocabulary Key – enter the vocabulary key that contains the country codes of companies that will be used for searching the Brreg register. | Website Vocabulary Key – enter the vocabulary key that contains the websites of companies that will be used for searching the Brreg register. | Brreg Code Vocabulary Key – enter the vocabulary key that contains the Brreg codes of companies that will be used for searching the Brreg register. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Brreg enricher is added and has an active status. This means that it will enrich relevant golden records during processing or when you trigger external enrichment. | . After the Brreg enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/brreg#add-brreg-enricher",
    
    "relUrl": "/preparation/enricher/brreg#add-brreg-enricher"
  },"1153": {
    "doc": "Brreg",
    "title": "Properties from Brreg enricher",
    "content": "To quickly find the properties added to golden records from the Brreg enricher, use the integrations filter on the Properties page. For a more detailed information about the changes made to a golden record by the Brreg enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Brreg enricher. | Display name | Vocabulary key | . | Bankrupt | brreg.organization.bankrupt | . | Brreg Url | brreg.organization.brregUrl | . | Country | brreg.organization.businessPostAddress+country | . | Municipality | brreg.organization.businessPostAddress+municipality | . | Municipality Number | brreg.organization.businessPostAddress+municipalityNumber | . | Industry Code | brreg.organization.industryCode | . | Institution Sector Code | brreg.organization.institutionSectorCode | . | Institution Sector Description | brreg.organization.institutionSectorDescription | . | Language Variant | brreg.organization.languageVariant | . | Latest Filed Annual Accounts | brreg.organization.latestFiledAnnualAccounts | . | Organization Type | brreg.organization.organizationTyp | . | Address | brreg.organization.postAddress+address | . | Country | brreg.organization.postAddress+country | . | Country Code | brreg.organization.postAddress+countryCode | . | Municipality | brreg.organization.postAddress+municipality | . | Municipality Number | brreg.organization.postAddress+municipalityNumber | . | Postal Area | brreg.organization.postAddress+postalArea | . | Postal Code | brreg.organization.postAddress+postalCode | . | Registered Business Register | brreg.organization.registeredBusinessRegister | . | Registered Founding Register | brreg.organization.registeredFoundingRegister | . | Registered Im Goods Register | brreg.organization.registeredImGoodsRegister | . | Registration Date | brreg.organization.registrationDate | . | Under Liquidation | brreg.organization.underLiquidation | . | Under Liquidation Or Dissolution | brreg.organization.underLiquidationOrDissolution | . | Voluntary Registered | brreg.organization.voluntaryRegistered | . | Address | organization.address | . | Address Country Code | organization.address.countryCode | . | Address Postal Area | organization.address.postalArea | . | Address Zip Code | organization.address.zipCode | . | Codes Brreg | organization.codes.brreg | . | Employee Count | organization.employeeCount | . | Industry | organization.industry | . | Website | organization.website | . ",
    "url": "/preparation/enricher/brreg#properties-from-brreg-enricher",
    
    "relUrl": "/preparation/enricher/brreg#properties-from-brreg-enricher"
  },"1154": {
    "doc": "Brreg",
    "title": "Brreg",
    "content": " ",
    "url": "/preparation/enricher/brreg",
    
    "relUrl": "/preparation/enricher/brreg"
  },"1155": {
    "doc": "How to build an organizational hierarchy",
    "title": "On this page",
    "content": ". | Preparation . | Business Group data set | Company data set | Legal Entity data set | Purchasing Organization data set | Sales Organization data set | Warehouse data set | Preparation results | . | Hierarchy creation . | Hierarchy project configuration | Building organizational hierarchy | Hierarchy creation results | . | . The terminology and user interface described in this article might be slightly different depending on your version of CluedIn, but the overall logic remains the same. In this article, you will learn how to build an organizational hierarchy to reflect the relations of entities within an organization. We are going to build a hierarchy of the following entities: . | Business Group – the highest-level organizational unit, typically representing a major line of business or a holding structure. It may consist of multiple companies that operate under a shared strategic direction but may have separate operations, finances, and compliance structures. | Company – a legal and financial entity that operates under the business group. It typically has its own set of books for accounting and is responsible for reporting taxes, profits, and losses. | Legal Entity – an organization that has legal standing in the eyes of the law—it can enter contracts, own assets, and be held liable. In many contexts, it is synonymous with a company, but some companies may contain multiple legal entities for operational, tax, or regulatory reasons. | Purchasing Organization – responsible for procurement activities—purchasing goods and services, negotiating terms with vendors, and managing supplier relationships. | Sales Organization – responsible for the sale and distribution of products or services. It defines the selling structure within a company or legal entity, including sales regions, channels, and responsibilities. | Warehouse (or plant) – a warehouse is a specific location, dedicated to the storage and handling of goods; a warehouse is a specific location, often within or attached to a plant, dedicated to the storage and handling of goods. | . After completing all steps described in this article, you will get a hierarchy similar to the following. ",
    "url": "/kb/how-to-build-an-organizational-hierarchy#on-this-page",
    
    "relUrl": "/kb/how-to-build-an-organizational-hierarchy#on-this-page"
  },"1156": {
    "doc": "How to build an organizational hierarchy",
    "title": "Preparation",
    "content": "You need to have separate data sets representing specific entities in the organizational hierarchy. This means that you need to have 6 data sets: Business Group, Company, Legal Entity, Purchasing Organization, Sales Organization, and Warehouse. If you want to practice building organizational hierarchy, you can use the following files for training: . | business-group.json . | company.json . | legal-entity.json . | purchasing-organization.json . | sales-organization.json . | warehouse.json . | . Business Group data set . The Business Group data set consists of 2 columns: business_group_id and business_group_name. We mapped this data set to the Business Group business domain and the Training Business Group vocabulary. After processing the data set, we have 1 golden record of the Business Group business domain. Company data set . The Company data set consists of 3 columns: company_id, company_name, and business_group_id. We mapped this data set to the Company business domain and the Training Company vocabulary. The business_group_id column is used to establish a relationship between a company and its corresponding business group. Therefore, we need to create an edge relation between the Company and Business Group golden records to represent this connection in the organizational hierarchy. For this purpose, we can use the /CompanyOf edge type. After processing the data set, we have 2 golden records of the Company business domain. Legal Entity data set . The Legal Entity data set consists of 3 columns: legal_entity_id, legal_entity_name, and company_id. We mapped this data set to the Legal Entity business domain and the Training Legal Entity vocabulary. The company_id column is used to establish a relationship between a legal entity and its corresponding company. Therefore, we need to create an edge relation between the Legal Entity and Company golden records to represent this connection in the organizational hierarchy. For this purpose, we can use the /LegalEntityOf edge type. After processing the data set, we have 2 golden records of the Legal Entity business domain. Purchasing Organization data set . The Purchasing Organization data set consists of 3 columns: purchasing_organization_id, purchasing_organization_name, and legal_entity_id. We mapped this data set to the Purchasing Organization business domain and the Training Purchasing Org vocabulary. The legal_entity_id column is used to establish a relationship between a purchasing organization and its corresponding legal entity. Therefore, we need to create an edge relation between the Purchasing Organization and Legal Entity golden records to represent this connection in the organizational hierarchy. For this purpose, we can use the /PurchasingOrgOf edge type. After processing the data set, we have 2 golden records of the Purchasing Organization business domain. Sales Organization data set . The Sales Organization data set consists of 3 columns: sales_organization_id, sales_organization_name, and legal_entity_id. We mapped this data set to the Sales Organization business domain and the Training Sales Org vocabulary. The legal_entity_id column is used to establish a relationship between a sales organization and its corresponding legal entity. Therefore, we need to create an edge relation between the Sales Organization and Legal Entity golden records to represent this connection in the organizational hierarchy. For this purpose, we can use the /SalesOrgOf edge type. After processing the data set, we have 2 golden records of the Sales Organization business domain. Warehouse data set . The Warehouse data set consists of 3 columns: warehouse_id, warehouse_name, and sales_orgnization_id. We mapped this data set to the Warehouse business domain and the Training Warehouse vocabulary. The sales_orgnization_id column is used to establish a relationship between a warehouse and its corresponding sales organization. Therefore, we need to create an edge relation between the Warehouse and Sales Organization golden records to represent this connection in the organizational hierarchy. For this purpose, we can use the /WarehouseOf edge type. After processing the data set, we have 3 golden records of the Warehouse business domain. Preparation results . After preparing the data sets, you can view the relations between golden records—created using edges—on the Relations tab of a golden record. For example, the following screenshot shows how a company is related to its business group, as well as to the legal entities that are part of the company. To view all organizational relations in one place, create a hierarchy. ",
    "url": "/kb/how-to-build-an-organizational-hierarchy#preparation",
    
    "relUrl": "/kb/how-to-build-an-organizational-hierarchy#preparation"
  },"1157": {
    "doc": "How to build an organizational hierarchy",
    "title": "Hierarchy creation",
    "content": "The process of creating a hierarchy involves configuring a hierarchy project and loading entities of specific relation types. Hierarchy project configuration . Start by creating a hierarchy project. In the first step, you only need to provide the hierarchy name. You do not need to select the business domain because we will rely on the existing edge relations. Since the relations between golden records already exist in the system, we can use them as the starting for building the hierarchy. We will build the hierarchy from top to bottom, starting with the relations between Business Group and Company. These relations are represented by the /CompanyOf relation type with the Incoming direction. For the top-level hierarchy project configuration, we rely on the existing relations to Automatically identify top-level golden records. Regarding the hierarchy type configuration, we choose the Single hierarchy project option because there is only one top-level Business Group golden record. As a result, the hierarchy project displays the relations between Business Group and its Companies. Next, we will explain how to load other entities of the organizational hierarchy to the project one by one. Building organizational hierarchy . To build organizational hierarchy, we recommend loading entities using the existing relations. Since we already have the relations between Business Group and its Companies, next we need to display the relations between Companies and their Legal Entities. To do this, load entities that have the /LegalEntityOf relation type with the Incoming direction. As a result, the hierarchy is updated with the Legal Entity golden records. Next, we need to display the relations between Legal Entities and their Purchasing Organizations. To do this, load entities that have the /PurchasingOrgOf relation type with the Incoming direction. As a result, the hierarchy is updated with the Purchasing Organization golden records. Next, we need to display the relations between Legal Entities and their Sales Organizations. To do this, load entities that have the /SalesOrgOf relation type with the Incoming direction. As a result, the hierarchy is updated with the Sales Organization golden records. Finally, we need to display the relations between Sales Organizations and their Warehouses. To do this, load entities that have the /WarehouseOf relation type with the Incoming direction. As a result, the hierarchy is updated with the Warehouse golden records. This is the last level that we needed to add to the hierarchy, and now our hierarchy displays the relations between Business Group, Companies, Legal Entities, Procurement Organizations, Sales Organizations, and Warehouses. Hierarchy creation results . After creating a hierarchy, you need to save and publish it. As a result, you can view the hierarchy project on the Hierarchies tab of a golden record that is involved in the hierarchy. Additionally, the hierarchy relations are displayed on the Relations tab of a golden record. To sum up, in this article, we demonstrated how to create an organizational hierarchy using the existing relations between organizational entities within the system. To automate the hierarchy-building process, it is essential to define edge relations during mapping. CluedIn uses these edge relations to efficiently build and manage the organizational hierarchy. ",
    "url": "/kb/how-to-build-an-organizational-hierarchy#hierarchy-creation",
    
    "relUrl": "/kb/how-to-build-an-organizational-hierarchy#hierarchy-creation"
  },"1158": {
    "doc": "How to build an organizational hierarchy",
    "title": "How to build an organizational hierarchy",
    "content": " ",
    "url": "/kb/how-to-build-an-organizational-hierarchy",
    
    "relUrl": "/kb/how-to-build-an-organizational-hierarchy"
  },"1159": {
    "doc": "Claims",
    "title": "On this page",
    "content": ". | Integration | Governance | Preparation | Engine Room | Management | Workflow | Consume | Admin | . In this article, you’ll find detailed explanation of each claim used in CluedIn. Claims, together with access levels, define user access to the features in CluedIn. The purpose of this article is to help you understand what actions on the platform a particular claim is related to. ",
    "url": "/administration/roles/claims#on-this-page",
    
    "relUrl": "/administration/roles/claims#on-this-page"
  },"1160": {
    "doc": "Claims",
    "title": "Integration",
    "content": "This section focuses on getting data into CluedIn from various sources: files, endpoints, databases, enrichers, integrations, and manual data entry projects. Available Integrations . This claim governs access to all built-in integrations for connecting and pushing the data to CluedIn. It covers only one action—viewing available integrations. Configured Integrations . This claim governs access to the actions for configuring an integration to push the data into CluedIn. It covers the following actions: . | Adding and editing an integration. | Inviting a user to add an integration. | Managing permissions to the data from an integration: adding or removing users or roles. | Undoing a merge on the Topology tab of a golden record. | . Data Source Groups . This claim governs access to the actions for importing your data into CluedIn, managing the resulting data sources, and processing data. To perform all actions within this claim, you should also have at least Informed access to several other claims: Configured Integrations, Annotation, Data Catalog, and Export Targets. Learn more about getting your data into CluedIn in Data sources. This claim does not govern access to data from data sources. To get access to data, you need to have appropriate source control and access control permissions. This claim covers the following actions: . | Viewing data source groups. | Viewing data sources and data source details. Access to the Data tab is granted based on data access permissions. | Viewing data sets and data set details, except the Mapping tab (governed by the management.annotation claim). Additionally, access to the Data and Quarantine tabs is granted based on data access permissions. | Editing and deleting a data source group. | Editing a data source: changing name and logo. | Managing a data set: archiving or removing. | Editing the quality of a data source. | Importing data from a file, an endpoint, and a database. | Adding, editing, and removing pre-process rules. | Processing data to create new golden records or enhance existing ones. | Managing processing logs: configuring retention period and purging logs. | Managing data set logs: configuring retention period and purging logs. | Managing permissions to data from the data source: adding or removing users or roles. | . Enrichment . This claim governs access to enrichers, which improve the quality and completeness of your golden records with the help of third-party sources. Learn more about enhancing your golden records with external information in Enricher. This claim covers the following actions: . | Viewing enrichers. | Adding and editing enrichers. | Managing permissions to data from the enricher: adding or removing users or roles. | Managing the owners of the enricher: adding or removing users or roles. | . Manual Data Entry Input . This claim governs access to the actions for creating new records directly in CluedIn and adding or editing golden record properties. It covers the following actions: . | Creating a new record in a manual data entry project. | Editing the quality of a manual data entry project. | Managing logs of a manual data entry project: configuring retention period and purging logs. | Editing golden record property on the golden record details page. | Adding new golden record property on the golden record details page. | Viewing audit log for a golden record property. | . Manual Data Entry Project Management . This claim governs access to the actions for setting up a manual data entry project. To perform all actions within this claim, you should also have at least Informed access to several other claims: Configured Integrations, Annotation, Data Catalog, Entity Types, and Export Targets. This claim covers the following actions: . | Viewing a manual data entry project. | Creating and editing a manual data entry project and form fields. | Creating and editing form fields. | . ",
    "url": "/administration/roles/claims#integration",
    
    "relUrl": "/administration/roles/claims#integration"
  },"1161": {
    "doc": "Claims",
    "title": "Governance",
    "content": "This section focuses on data quality metrics, sensitive data identification, global data metrics, as well as global data model. Global Data Model . This claim governs access to global data model that allows you to explore connections between business domains in the platform. It covers only one action—viewing global data model. Metrics . This claim governs access to data quality metrics dashboards and global data metrics dashboards. It covers only one action—viewing respective dashboards. Personal Identifiers . This claim governs access to metrics related to sensitive data. It covers only one action—viewing sensitive data metrics. ",
    "url": "/administration/roles/claims#governance",
    
    "relUrl": "/administration/roles/claims#governance"
  },"1162": {
    "doc": "Claims",
    "title": "Preparation",
    "content": "This section focuses on access to clean projects and cleaning activities. Clean . This claim governs access to the actions for cleaning your golden records: creating a clean project, performing cleaning activities in the clean application, and sending cleaned golden records back to CluedIn. Learn more about data cleaning in Clean. This claim bypasses source control and access control permissions because access to data is essential to work in the clean project. This claim covers the following actions: . | Viewing a clean project. | Creating and editing a clean project. | Generating and regenerating the results for a clean project. | Performing clean activities in the clean application. | Exporting records from the clean application. | Processing cleaned records. | Generating and viewing data part rules based on cleaning activities. | Reverting changes to golden records as a result of cleaning activities. | Managing owners of the clean project: adding or removing users and roles. | Creating a clean project from search. | . ",
    "url": "/administration/roles/claims#preparation",
    
    "relUrl": "/administration/roles/claims#preparation"
  },"1163": {
    "doc": "Claims",
    "title": "Engine Room",
    "content": "This section focuses on various tools for monitoring what is happening inside CluedIn through real-time charts and summaries. Configuration Groups . This claim governs access to configuration groups that contain environment variables, feature flag settings, and other settings that are being used by your CluedIn instance. It covers only one action—viewing configuration groups. Processing . This claim governs access to the processing pipeline, where you can monitor high-level metrics of all the processing that happens in CluedIn such as ingestion, processing, and streaming activities. It covers only one action—viewing processing pipeline. Statistics . This claim governs access to real-time charts showing global CPU and memory stats on the nodes that run your CluedIn instance. It covers only one action—viewing respective charts and dashboards. ",
    "url": "/administration/roles/claims#engine-room",
    
    "relUrl": "/administration/roles/claims#engine-room"
  },"1164": {
    "doc": "Claims",
    "title": "Management",
    "content": "This section focuses on various possibilities for managing your golden records: applying business rules, identifying and merging duplicates, managing your data catalog, and more. Entity Types . This claim governs access to business domains (previously known as entity types) that describe the semantic meaning of golden records. Learn more about business domains in a dedicated article. This claim covers the following actions: . | Viewing a business domain. | Creating and editing a business domain. | . Access Control . This claim governs access to the feature that allows you to set up fine-grained control over access to specific golden records and vocabulary keys. This claim covers the following actions: . | Viewing access control policies. | Creating and editing an access control policy. | Managing owners of the access control policy: adding or removing users and roles. | . Annotation . This claim governs access to the mapping in data sets and manual data entry projects. Lean more about data set mapping in Create mapping and Review mapping details. This claim covers the following actions: . | Viewing data set mapping details. | Creating and editing data set mapping. | Viewing mapping details in the manual data entry project. | Editing mapping in the manual data entry project. | . Data Catalog . This claim governs access to vocabularies and vocabulary keys. Learn more in Data catalog. This claim covers the following actions: . | Viewing vocabularies and vocabulary keys. | Creating and editing a vocabulary and a vocabulary key. | Removing a vocabulary if it does not have vocabulary keys. | Removing a vocabulary key if it is not used anywhere in the system or if it has been remapped to another vocabulary key. | Managing owners of the vocabulary: adding or removing users and roles. | Viewing changes in audit log of a vocabulary or vocabulary key. | . Deduplication Project Management . This claim governs access to configuring a deduplication project and setting up matching rules. Learn more about deduplication project management in Deduplication. This claim bypasses source control and access control permissions because access to data is essential to work in the deduplication project. This claim covers the following actions: . | Viewing a deduplication project. | Creating and editing a deduplication project. | Archiving a deduplication project. | Generating or discarding matches in a deduplication project. | Viewing groups of duplicates that were identified in a deduplication project. | Managing owners of the deduplication project: adding or removing users and roles. | Viewing changes in audit log of a deduplication project. | . Deduplication Review . This claim governs access to reviewing and processing groups of duplicates that were identified in a deduplication project. Learn more about working in a deduplication project in Manage groups of duplicates. This claim bypasses source control and access control permissions because access to data is essential to work in the deduplication project. This claim covers the following actions: . | Processing groups of duplicates: selecting appropriate values among conflicting values, approving or rejecting a group for merge. | Revoking approval from a group of duplicates if you decide not to merge it. | Merging a group of duplicates to produce a consolidated golden record. | Unmerging a golden record that was merged in the deduplication project. | Viewing merged golden records produced in deduplication project. | . Glossary . This claim governs access to glossary that contains groups of golden records, called terms, that meet specific criteria. It covers the following actions: . | Viewing terms by categories. | Creating a category that acts as a folder for terms. | Changing the name of a category. | Creating a term within a category. | Editing a term to define conditions for including golden records in the term and to change term configuration. | Managing term status by activating or deactivating it as needed. | Removing a term if you no longer need it. | Endorsing a term to indicate to other users that it is reliable for their use. | Viewing golden records that match conditions from term configuration. | Managing owners of the term: adding or removing users and roles. | View changes in term audit log. | . Hierarchy Builder . This claim governs access to the actions for organizing, visualizing, and managing hierarchy relations between golden records within and across different business domains. It covers the following actions: . | Viewing all hierarchies and opening any hierarchy to view relations between its elements. | Creating a hierarchy either by dragging elements onto the canvas or by leveraging relations between golden records. | Editing hierarchy configuration: changing hierarchy properties, replacing nodes, deleting nodes or subtrees. | Deleting a hierarchy if you no longer need it. | Loading golden records connected by various types of relations so that these relations are automatically visualized in the hierarchy. | Publishing the hierarchy to make the hierarchy relations available on the Relations and Hierarchies tab of golden records. | Managing hierarchy owners: adding or removing users and roles. | . Rule Builder . This claim governs access to the actions for managing all types of business rules that help you modify golden records in an organized and controlled manner. Learn more in Rules. This claim is applicable to all types of rules: data part rules, survivorship rules, and golden record rules. If you can create a data part rule, it means you can do the same with survivorship and golden record rules. This claim covers the following actions: . | Viewing all rules and opening any specific rule to view its configuration. | Creating a rule, specifying the golden records to which the rule will be applied, and choosing the action to be performed by the rule. | Editing rule configuration: name, description, filters, and actions. | Managing rule status by activating or deactivating it as needed. | Removing a rule if you no longer need it. | Reprocessing a rule to apply the rule’s action to golden records associated with the rule. | Managing rule owners: adding or removing users and roles. | Viewing changes in the rule audit log. | Editing rule processing order to prioritize the execution of specific rules over others. | . ",
    "url": "/administration/roles/claims#management",
    
    "relUrl": "/administration/roles/claims#management"
  },"1165": {
    "doc": "Claims",
    "title": "Workflow",
    "content": "This section focuses on access to automated workflows and approvals. Workflow Approvals . This claim governs access to approval requests sent from the Power Automate widget in CluedIn. It covers the following actions: . | Viewing approval requests. | Approving or rejecting an approval request. | . Workflow Builder . This claim governs access to the actions for creating workflows in the Power Automate widget in CluedIn for automating specific approvals and processes. It covers the following actions: . | Viewing all workflows and opening any workflow to view its details. | Creating a workflow for automating the approval process for a specific action in CluedIn. | Editing a workflow. | Managing workflow status by activating or deactivating it as needed. | . ",
    "url": "/administration/roles/claims#workflow",
    
    "relUrl": "/administration/roles/claims#workflow"
  },"1166": {
    "doc": "Claims",
    "title": "Consume",
    "content": "This section focuses on access to tools for configuring connection with external systems and exporting golden records from CluedIn to those systems. Export Targets . This claim governs access to the actions for configuring external destinations where golden records from CluedIn can be sent. Learn more in Export targets. This claim covers the following actions: . | Viewing all export targets and their configuration details. | Adding and configuring an export target. | Managing export target status by activating or deactivating it. | Testing if CluedIn can connect to the export target. | Managing permissions to of the export target: adding or removing users or roles. | Managing export target owners: adding or removing users or roles. | . Graph QL . This claim governs access to the GraphQL tool, where you can execute queries directly in the UI. It is useful when you need to query data in CluedIn and verify if your GQL syntax is correct. This claim covers only one action—executing queries in GraphQL. Streams . This claim governs access to various actions for exporting golden records from CluedIn to the export target. Learn more in Streams. This claim covers the following actions: . | Viewing all streams and their configuration details. | Creating a stream. | Editing a stream. | Deleting a stream. | Starting, pausing, and stopping the stream. | Creating stream export target configuration: selecting a connector, specifying connector properties, and selecting golden record properties and relations for export. | Editing stream export target configuration. | Managing stream owners: adding or removing users or roles. | Viewing changes in stream audit log. | . ",
    "url": "/administration/roles/claims#consume",
    
    "relUrl": "/administration/roles/claims#consume"
  },"1167": {
    "doc": "Claims",
    "title": "Admin",
    "content": "This section focuses on various administrative tasks in CluedIn. Data Management . This claim governs access to various actions for managing golden records. This claim bypasses source control and access control permissions because access to data is essential to work with golden records. This claim covers the following actions: . | Deleting a golden record. | Triggering enrichment for a golden record. | Reprocessing a golden record. | Adding or deleting an edge to represent relations between golden records. | Deleting a data part that contributes to a golden record on the History and topology tabs of the golden record. | . Roles . This claim governs access to the roles in CluedIn, which in turn grant permissions to users to perform specific actions in the platform. Learn more about roles in a dedicated article. This claim covers the following actions: . | Viewing a role, including its claims and access levels as well as users who have this role. | Adding a new role and selecting the needed access level for each claim. | Editing a role. | Adding user to a role. | . Token Management . This claim governs access to various actions for customizing your CluedIn instance. It covers the following actions: . | Viewing API tokens. | Creating an API token. | Revoking an API token. | Viewing organization settings. | Editing organization settings and saving changes. | Viewing feature flags, as well as turning them on or off as needed. | Viewing theme configuration. | Changing theme configuration. | Viewing entity page layouts and their details. | Editing an entity page layout. | Adding or removing an entity page layout. | . Users . This claim governs access to actions for managing users in CluedIn. Learn more about user management in a dedicated article. This claim covers the following actions: . | Viewing users who have been added to CluedIn, as well as user settings and roles. | Inviting a user to CluedIn. | Deactivating a user to prevent them from signing in to CluedIn. | Viewing user invitations. | Removing user invitation. | Resending user invitation. | . ",
    "url": "/administration/roles/claims#admin",
    
    "relUrl": "/administration/roles/claims#admin"
  },"1168": {
    "doc": "Claims",
    "title": "Claims",
    "content": " ",
    "url": "/administration/roles/claims",
    
    "relUrl": "/administration/roles/claims"
  },"1169": {
    "doc": "Clean application reference",
    "title": "Clean application reference",
    "content": "In this article, you will find reference information about common transformation functions in the clean application. The purpose of these functions is to help you edit and improve the contents of cells automatically and efficiently. To access the common transformation functions, expand the menu in the column heading, and then select Edit cells &gt; Common transforms. The following table provides the description of common transformation functions. | Function | Description | . | Trim leading and trailing whitespace | Removes extra spaces and line breaks before or after visible text characters. | . | Collapse consecutive whitespace | Removes tabs and multiple spaces in a row and replaces them with a single space. | . | Unescape HTML entities | Replaces HTML character references with Unicode characters. For example, &amp;nbsp; will be replaced with a space. | . | Replace smart quotes with ASCII | Replaces curly quotation marks with straight double quote character (“). | . | To titlecase | Transforms text in the column into Title Case. | . | To uppercase | Transforms text in the column into UPPER CASE. | . | To lowercase | Transforms text in the column into lower case. | . | To number | Transforms the data in the cells and converts the data type to number. For example, “10” as a text string will be transformed to “10” as a number. | . | To date | Transforms the data in the cells and converts the data type into the ISO-8601-compliant extended format with time in UTC: YYYY-MM-DDTHH:MM:SSZ. For example, 10/3/2023 will be converted to 2023-10-03T00:00:00Z. | . | To text | Transforms the data in the cells and converts the data type to text. | . | To null | Converts the data in the cells into null values. | . | To empty string | Converts the data in the cells into an empty string. | . ",
    "url": "/preparation/clean/clean-application-reference",
    
    "relUrl": "/preparation/clean/clean-application-reference"
  },"1170": {
    "doc": "Common upgrade operations",
    "title": "On this page",
    "content": ". | Check CluedIn UI | Check CluedIn pods | Check CluedIn logs | Check CluedIn queues | . This section outlines common CluedIn activities you may need to carry out during the CluedIn upgrade process. It is designed to provide practical steps and references that help you keep your CluedIn environment running smoothly. ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations#on-this-page"
  },"1171": {
    "doc": "Common upgrade operations",
    "title": "Check CluedIn UI",
    "content": "A successful load of the CluedIn UI is a strong indication that your installation is functioning correctly. | To access the UI, use the URL that was configured during installation. Contact your administrator to get the URL. This URL may be either public or private: . | Public URL – Accessible from any network without restrictions. | Private URL – Requires that you are connected to the correct internal or VPN network. Make sure you have the necessary access before attempting to open the UI. | . | Ensure you have access to CluedIn. Users can be granted access to CluedIn in two main ways: . | Single sign-on (SSO) – Access is provisioned by your administrator through your organization’s SSO provider. | Invitation email – Your CluedIn administrator sends you an email invitation to join. | . | Check the UI: . | Sign in to CluedIn. The home dashboard should appear almost instantly. | Click through the left-hand menu. All pages should appear almost instantly. | Perform a search in the top menu, click on any of the entities in the results, and then browse through the entity tabs. All tabs should respond quickly. | . If any page takes longer than 5 seconds to load, it may indicate a performance issue or that the upgrade has not fully completed. | . ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-ui",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-ui"
  },"1172": {
    "doc": "Common upgrade operations",
    "title": "Check CluedIn pods",
    "content": "Monitoring the state of pods is a routine task in Kubernetes, and it is especially critical during installation and upgrades. Verifying that pods are running and healthy ensures that the CluedIn platform is functioning correctly and that new deployments are stable. | Before checking the pods, make sure you are already connected to the target Kubernetes cluster via a valid kubeconfig file. Without this connection, you will not be able to query the cluster. | To view the status of all CluedIn pods, run the following command: . kubectl get pods -n cluedin . The command returns the list of all pods in the CluedIn namespace (-n cluedin) along with their current state (for example, Running, Pending, CrashLoopBackOff, Completed, Succeeded, or Failed). This provides quick visibility into whether services are healthy or require troubleshooting. Ideally, all pods should be in a Running or Completed state. | Depending on what you see, do the following: . | All pods in Running or Completed state. This indicates that your CluedIn installation is healthy and the infrastructure is actively running. If all pods are Green/Running but the application still has issues, check the pod logs. See the Check CluedIn logs section for details. | Pods in Pending state. If some pods remain in the Pending state, allow a few minutes for them to transition to Running. If a pod is still pending after 5 minutes, review the logs to identify the cause. | Pods in CrashLoopBackOff state. When a pod is in the CrashLoopBackOff state, it means Kubernetes is repeatedly trying to start the pod, but the container inside it keeps failing and crashing. After each crash, Kubernetes waits a little longer before attempting to restart it again (the “backoff” part). Common causes: . | Configuration errors – Such as incorrect environment variables, secrets, or config maps. CluedIn config maps are complex. If changes have been made to the recommended configurations, revert them to the default settings. | Application errors – The application may crash on startup due to bugs, missing dependencies, or incompatible versions. | Resource limits (most common) – The container may not have sufficient CPU or memory and is killed by the system. | Permission or connectivity issues – For example, firewall changes or Azure policies may block the pod from starting. | . | . | To get more details about a pod that is not starting or is stuck in a crash loop, run the following command: . kubectl describe pod &lt;pod-name&gt; -n cluedin . | . ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-pods",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-pods"
  },"1173": {
    "doc": "Common upgrade operations",
    "title": "Check CluedIn logs",
    "content": "All CluedIn pods generate logs that provide detailed information about what the system is doing. These logs help you do the following: . | Confirm that services are running smoothly. | Diagnose issues when problems occur. | Gain visibility into the internal behavior of the system. | . Reviewing pod logs is an essential step in troubleshooting errors and verifying that your installation is functioning correctly. To check the logs of a pod, run the following command: . kubectl logs &lt;pod name&gt; -n cluedin . Pod logs are especially useful for troubleshooting in the following scenarios: . | CrashLoopBackOff – A pod repeatedly fails to start. | Running but Not Ready – Readiness probes fail even though the pod is running. | Running and Ready, but Application Misbehaving – The pod looks healthy but the application itself is not functioning correctly. | Init Containers Delayed or Failing – The initialization steps take too long or do not complete successfully. | . Logs stored in a Kubernetes pod are limited to the default 10 MB size. If logs grow beyond this limit, older entries will no longer be visible when using kubectl logs. If you have log analytics configured to collect pod logs, it is recommended to use that to read the pod logs. ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-logs",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-logs"
  },"1174": {
    "doc": "Common upgrade operations",
    "title": "Check CluedIn queues",
    "content": "CluedIn queues are powered by a messaging system called RabbitMQ. RabbitMQ is a message broker – it allows different parts of CluedIn to communicate by passing messages between them: . | Think of a queue as a waiting line for messages. | One service in CluedIn places messages onto the queue. | Another service takes messages off the queue and processes them. | . This setup helps CluedIn handle large volumes of data reliably and asynchronously. Why are queues important? . If queues stop working correctly, CluedIn may not be able to move data between services efficiently. Common symptoms include: . | Queues growing indefinitely (messages are piling up but not being processed). | Queues stuck (no new messages are being consumed). | . Services that depend on these messages may experience failures or degraded performance. Check the queues . By checking the queues, you can quickly determine if CluedIn’s internal messaging system is healthy, or if a backlog or failure might be affecting the platform. | Accesing RabbitMQ. Run the following command to get access the RabbitMQ credentials. kubectl get secret cluedin-rabbitmq -n cluedin -o jsonpath=\"{.data.rabbitmq-password}\" | base64 --decode . | RabbitMQ includes a built-in management UI. To access it, you first need to open a port forward to the RabbitMQ pod: . #kubectl port-forward service/cluedin-rabbitmq 15672:15672 -n cluedin . This will return an output similar to the following: . Forwarding from 127.0.0.1:15672 -&gt; 15672 Handling connection for 15672 Handling connection for 15672 Handling connection for 15672 Handling connection for 15672 Handling connection for 15672 Handling connection for 15672 . The RabbitMQ dashboard becomes available at http://localhost:15672. | Sign in to the RabbitMQ UI using the following credentials: . | Username – cluedin. | Password – Retrieve the password from your stored CluedIn credentials. | . | Check the queues. After the sign-in, the system typically directs you to the Overview tab. This dashboard provides a high-level summary of RabbitMQ, including the total number of messages currently in the system and key metrics related to queue activity. | You can sort the queues by total messages to quickly identify the largest queues within RabbitMQ. | You can also review message rates to determine whether queues are processing the messages as expected: . | Incoming – Indicates that messages are being published to the queue by a producer. These messages remain in the queue until they are consumed. | Deliver/Get – Indicates that messages from the queue are being consumed by a consumer. | . If you see a large number of Incoming messages but no corresponding Deliver/Get activity, it may indicate that no consumers are currently available to process those messages. | To check the number of consumers attached to a queue, click the +/– icon on the right-hand side to display the Consumers column. In the example above, the queue RemoteEvents_cluedin-server-processing is connected to a single consumer, with messages delivered at a rate of 224/s. This indicates that the queue is healthy and functioning as expected. | . | . ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-queues",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations#check-cluedin-queues"
  },"1175": {
    "doc": "Common upgrade operations",
    "title": "Common upgrade operations",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/common-upgrade-operations",
    
    "relUrl": "/paas-operations/upgrade/guide/common-upgrade-operations"
  },"1176": {
    "doc": "Email",
    "title": "On this page",
    "content": ". | Email configuration - local installation (Home) | Email configuration - Kubernetes | . Emails within CluedIn application are primarily used for inviting a user to the platform for management. When single-sign on becomes enabled, the use of emails is not really required anymore. In order to send emails, CluedIn must be configured with an SMTP server. This can be a company owned one for your organization or a temporary one using a service such as MailTrap or Sendgrid, for example, which are useful for simple testing. ",
    "url": "/deployment/infra-how-tos/configure-email#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-email#on-this-page"
  },"1177": {
    "doc": "Email",
    "title": "Email configuration - local installation (Home)",
    "content": "Email configuration is simplified when running locally by using the cluedin.ps1 helper script that comes part of the CluedIn Home repository. You can configure any email settings as environment variables that will be passed into the application at runtime. This is achieve by using the env command : ./cluedin.ps1 env -set [NAME]=[VALUE] . The possible values for configuring email in CluedIn are: . CLUEDIN_EMAIL_HOST (default: &lt;blank&gt;) CLUEDIN_EMAIL_PASS (default: &lt;blank&gt;) CLUEDIN_EMAIL_PORT (default: 587) CLUEDIN_EMAIL_SENDER (default: noreply@cluedin.com) CLUEDIN_EMAIL_USER (default: &lt;blank&gt;) . ",
    "url": "/deployment/infra-how-tos/configure-email#email-configuration---local-installation-home",
    
    "relUrl": "/deployment/infra-how-tos/configure-email#email-configuration---local-installation-home"
  },"1178": {
    "doc": "Email",
    "title": "Email configuration - Kubernetes",
    "content": "When using Kubernetes, the SMTP setting can be configured in the values.yaml. This can be done by setting the following properties: . boostrap: email: host: port: user: password: senderName: senderDisplayName: fromAddress: . This will create a secret, storing the user and password information. Should you want to pass a secret already containing those details, you can create a secret with the keys: . apiVersion: v1 kind: Secret metadata: name: &lt;my-email-secret&gt; type: Opaque data: EmailUserName: EmailPassword: . And pass the name of the secret in the property . email: secretRef: &lt;my-email-secret&gt; . Passing a secret in this way will override the use of explicit user/password properties. ",
    "url": "/deployment/infra-how-tos/configure-email#email-configuration---kubernetes",
    
    "relUrl": "/deployment/infra-how-tos/configure-email#email-configuration---kubernetes"
  },"1179": {
    "doc": "Email",
    "title": "Email",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-email",
    
    "relUrl": "/deployment/infra-how-tos/configure-email"
  },"1180": {
    "doc": "Deduplicate data",
    "title": "On this page",
    "content": ". | Create deduplication project | Configure matching rule | Fix duplicates | Results &amp; next steps | . Deduplication process helps you find and merge duplicate records based on a set of rules that you define. This process involves creating a deduplication project, configuring the matching rules for identifying duplicates, and fixing duplicates. In this guide, you will learn how to deduplicate the data that you have ingested into CluedIn. Before you start: Make sure you have completed all steps in the Ingest data guide. Context: This guide focuses on identifying duplicates based on the same first name and last name. ",
    "url": "/getting-started/data-deduplication#on-this-page",
    
    "relUrl": "/getting-started/data-deduplication#on-this-page"
  },"1181": {
    "doc": "Deduplicate data",
    "title": "Create deduplication project",
    "content": "As a first step, you need to create a deduplication project that allows you to check for duplicates that belong to a certain business domain. To create a deduplication project . | On the navigation pane, go to Management. Then, select Deduplication. | Select Create Deduplication Project. | On the Create Deduplication Project pane, do the following: . | Enter the name of the deduplication project. | Select the business domain that you want to use as a filter for all records. | In the lower-right corner, select Create. | . You created the deduplication project. Now, you can proceed to define the rules for checking duplicates within the selected business domain. | . ",
    "url": "/getting-started/data-deduplication#create-deduplication-project",
    
    "relUrl": "/getting-started/data-deduplication#create-deduplication-project"
  },"1182": {
    "doc": "Deduplicate data",
    "title": "Configure matching rule",
    "content": "When creating a matching rule, you need to specify certain criteria. CluedIn uses these criteria to check for matching values among records belonging to the selected business domain. To configure a matching rule . | Go to the Matching Rules tab and select Add Matching Rule. The Add Matching Rule pane opens on the right side of the page. | On the Matching Rule Name tab, enter the name of the matching rule, and then select Next. | On the Matching Criteria tab, do the following: . | Enter the name of the matching criteria. | Select the vocabulary key. All values associated with this vocabulary key will be checked for duplicates. | In the Matching Function dropdown list, select the method for detecting duplicates. | In the lower-right corner, select Next. | . | On the Preview tab, review the defined matching criteria. If you want to add more matching criteria to the rule, select Add Matching Criteria. | After you have added the needed matching criteria, in the lower-right corner of the Preview tab, select Add Rule. The status of the deduplication project becomes Ready to generate. | In the upper-right corner, select Generate Results. Then, confirm that you want to generate the results for the deduplication project. The process of generating results may take some time. After the process is completed, you will receive a notification. If duplicates are detected, the results will be displayed on the page. The results are organized into groups containing records that match your criteria. For example, on the following screenshot, the group consists of two duplicates. The name of the group corresponds to the value of the vocabulary key from the matching rule. Now, you can proceed to fix the duplicates. | . ",
    "url": "/getting-started/data-deduplication#configure-matching-rule",
    
    "relUrl": "/getting-started/data-deduplication#configure-matching-rule"
  },"1183": {
    "doc": "Deduplicate data",
    "title": "Fix duplicates",
    "content": "The process of fixing duplicates involves reviewing the values from duplicate records and selecting which values you want to merge into the deduplicated record. To fix duplicates . | Select the name of the group. The Fix Conflicts tab opens. Here, you can view the details of the duplicate records. In the Conflicting section, you can find the properties that have different values in the duplicate records. In the Matching section, you can find the properties that have the same values in the duplicate records. | In the Conflicting section, select the values that you want to merge into the deduplicated record. | In the upper-right corner of the page, select Next. The Preview Merge tab opens. Here, you can view the values that will be merged into the deduplicated record. | In the upper-right corner of the page, select Approve. Then, confirm that you want to approve your selection of values for the group. | Select the checkbox next to the group name. Then, select Merge. | Confirm that you want to merge the records from the group: . | Review the group that will be merged and select Next. | Select an option to handle the data merging process if more recent data becomes available for the golden record. Then, select Confirm. | . The process of merging data may take some time. After the process is completed, you will receive a notification. As a result, the duplicate records have been merged into one record. | . All changes to golden records in CluedIn are tracked. You can search for the needed golden record and on the Topology pane, you can view the visual representation of the records that were merged through the deduplication process. ",
    "url": "/getting-started/data-deduplication#fix-duplicates",
    
    "relUrl": "/getting-started/data-deduplication#fix-duplicates"
  },"1184": {
    "doc": "Deduplicate data",
    "title": "Results &amp; next steps",
    "content": "After you identified and merged duplicates, the count of golden records decreased. By following the steps outlined in this guide, you can conduct additional checks for duplicates in your data using various matching functions. The next item on the list of common data management tasks is data streaming. Now that your data has been cleaned and deduplicated, you can send it to a Microsoft SQL Server database. Learn how to send data from CluedIn to external systems in the Stream data guide. ",
    "url": "/getting-started/data-deduplication#results--next-steps",
    
    "relUrl": "/getting-started/data-deduplication#results--next-steps"
  },"1185": {
    "doc": "Deduplicate data",
    "title": "Deduplicate data",
    "content": " ",
    "url": "/getting-started/data-deduplication",
    
    "relUrl": "/getting-started/data-deduplication"
  },"1186": {
    "doc": "Disks",
    "title": "Disks",
    "content": "By default, AKS creates the disks to keep your data, but these disks will be deleted with the AKS cluster if you decide to recreate it. To preserve your data, you must create Azure Managed disks configure CluedIn instance to use it. We will cover the managed disks configuration in the Helm section. Also, see Sizing for the recommended disks sizes. ",
    "url": "/deployment/azure/disks",
    
    "relUrl": "/deployment/azure/disks"
  },"1187": {
    "doc": "Business domain",
    "title": "On this page",
    "content": ". | Business domain details page | Create a business domain | Manage a business domain | . A business domain is a well-known business object that describes the semantic meaning of golden records. Business domains can represent physical objects, locations, interactions, individuals, and more. In CluedIn, all golden records must have a business domain to ensure the systematic organization and optimization of data management processes. A well-named business domain is global (for example, Person, Organization, Car) and should not be changed across sources. In this article, you will learn how to create and manage business domains to enhance the efficiency and organization of golden records in CluedIn. ",
    "url": "/management/entity-type#on-this-page",
    
    "relUrl": "/management/entity-type#on-this-page"
  },"1188": {
    "doc": "Business domain",
    "title": "Business domain details page",
    "content": "On the business domain details page, you can view relevant information about the business domain and take other actions to manage it. Data . This tab contains all golden records that belong to the business domain. Vocabularies . This tab contains all vocabularies that are associated with the business domain. Configuration . This tab contains general information about the business domain, including: . | Display name – a user-friendly identifier of the business domain that is displayed throughout the system (for example, in rules, search, streams, and so on). | business domain identifier – a string that represents the business domain in code (for example, in clues). | Icon – a visual representation of the business domain that helps you quickly identify what kind of golden record it is. | Path – a URL path of the business domain. | Layout – a way in which information is arranged on the Overview tab of the golden records that belong to the business domain. | . ",
    "url": "/management/entity-type#business-domain-details-page",
    
    "relUrl": "/management/entity-type#business-domain-details-page"
  },"1189": {
    "doc": "Business domain",
    "title": "Create a business domain",
    "content": "CluedIn ships with some pre-defined values for common business domains such as Document, File, Organization. However, you might want to have some specific business domains that may not be configured by CluedIn. Depending on the selected data modeling approach, you can create a business domain in two ways: . | Automatically – this option is part of the data-first approach. When creating a mapping for a data set, you have the option to enter the name of a new business domain and select the icon. CluedIn will then automatically create the business domain. Once the mapping is created, you can then open the business domain and make any necessary adjustments. | Manually – this option is part of the model-first approach, which assumes that you need to create a business domain before using it in the mapping for a data set. The following procedure outlines the steps to manually create a business domain. | . To create a business domain . | On the navigation pane, go to Management &gt; Business domains. | Select Create business domain. | Enter the display name of the business domain. The Business domain identifier and the Path fields are filled in automatically based on the name that you entered. | Select the icon for the visual representation of the business domain. | Select the layout for arranging the information on the golden record overview page. | Select Create. The business domain opens, where you can view and manage business domain details. | . ",
    "url": "/management/entity-type#create-a-business-domain",
    
    "relUrl": "/management/entity-type#create-a-business-domain"
  },"1190": {
    "doc": "Business domain",
    "title": "Manage a business domain",
    "content": "You can change the following elements of the business domain configuration: display name, icon, layout, and description. You cannot change the business domain identifier and path. Also, you cannot delete the business domain. To edit business domain configuration . | In the upper-right corner of the business domain page, select Edit. | Make the needed changes, and then select Save. | . ",
    "url": "/management/entity-type#manage-a-business-domain",
    
    "relUrl": "/management/entity-type#manage-a-business-domain"
  },"1191": {
    "doc": "Business domain",
    "title": "Business domain",
    "content": " ",
    "url": "/management/entity-type",
    
    "relUrl": "/management/entity-type"
  },"1192": {
    "doc": "Explain Log",
    "title": "On this page",
    "content": ". | Overview | Structure of the Explain Log | How to read the Explain Log | Troubleshooting using the Explain Log | . In this article, you will learn how to use the Explain Log to track the sequence of processing steps and rules that shape a golden record. This will help you understand why a golden record appears in its current state. ",
    "url": "/key-terms-and-features/golden-records/explain-log#on-this-page",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log#on-this-page"
  },"1193": {
    "doc": "Explain Log",
    "title": "Overview",
    "content": "The Explain Log is a diagnostic and traceability tool that provides visibility into how data is transformed and processed with respect to golden records and their constituent records. This feature allows you to understand why a golden record appears in its current state and how the platform derived the final output values that you see. The Explain Log was designed to make making processing steps transparent, traceable, and understandable. Specifically, the Explain Log: . | Shows processing activities applied to a golden record and its constituent records. | Identifies business rules that are applied to a golden record and its constituent records. | Shows the origin of each value in a golden record, answering the question where a specific value came from. | Provides visibility into complex processing logic, helping you understand what is happening under the hood. | Details the execution order of processing steps and data transformations. | Displays transformations that values undergo throughout the processing pipeline. | . ",
    "url": "/key-terms-and-features/golden-records/explain-log#overview",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log#overview"
  },"1194": {
    "doc": "Explain Log",
    "title": "Structure of the Explain Log",
    "content": "The Explain Log consists of two main sections that represent logical processing steps: . | Golden Record – top-level section that contains decisions made on value selection and rule application. | Records – a section that tracks how each constituent record (also known as data part) contributed to the golden record. This section can contain multiple records. | . Both sections consist of many elements that represent various operations, which happened within a logical processing step. The operation is a small and logical computation and typically has a single responsibility. Each operation consists of following information: . | Operation name – name of the operation in the processing pipeline. | Status – indicates the outcome of the operation. The possible statuses include: . | Successful – the operation executed and completed without errors. This status reflects the execution of the operation, not whether it resulted in any changes. An operation may be marked as successful even if no changes were applied—for example, when the input did not require modification. | Successful with warnings – the operation was executed but encountered non-critical issues. | Skipped - the operation was not executed due to specified reason. | Failed - the operation encountered a critical error and did not complete successfully. | . | Duration – the time taken by the operation to execute within the processing pipeline. | Summaries – a tabular representation of actions made by the operation. This summary is derived from events and operations recorded during execution and provides a concise and user-friendly visual overview of outcomes. | Events and Operations – a detailed breakdown of actions made by the operation. These are the primary source of insight into what the operation actually did. Events and operations are also reflected in the summary table for easier interpretation. Note that the information in Summaries and Events and Operations is generally the same; the difference lies in how it is visually presented. In most cases, there is a one-to-one correspondence between the two sections. However, in less common cases, some events and operations may not be included in summaries. | . If an operation does not contain Summaries and Events and Operations, it means that no changes were applied during its execution. ",
    "url": "/key-terms-and-features/golden-records/explain-log#structure-of-the-explain-log",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log#structure-of-the-explain-log"
  },"1195": {
    "doc": "Explain Log",
    "title": "How to read the Explain Log",
    "content": "This section helps you interpret the Explain Log by focusing on two key aspects: understanding where golden record values come from and identifying rules applied to a golden record. Understanding where golden record values come from . To see which values are used in the golden record, review the Merge data parts operation in the Golden Record section. The Summaries table within this operation shows each property value along with the ID of the record from which it originated. Once you identify the record ID, you can further investigate the operations that were applied to that specific record throughout the processing pipeline. Identifying rules applied to a golden record . To determine which rules have been applied to a golden record, refer to the Evaluate rules operations in the Golden Record section. You may notice that this section contains two Evaluate rules operations: . | The first Evaluate rules operation is dedicated to survivorship rules. | The second Evaluate rules operation is dedicated to golden record rules. | . The structure of both operations in the Explain Log is the same. Let’s consider an example of the Summaries section for golden record rules. It includes two tables: . | Applied rules – lists all golden record rules along with their actions that were successfully applied to the golden record as well as actions that were skipped or failed. | Evaluated rules – lists all golden record rules along with their actions that exist in the system and indicates whether each rule was applied, partially applied, or skipped. If a rule was skipped, a reason is provided—for example, if the conditions defined in the rule’s action were not met. | . Note that the Golden Record section provides information only about survivorship rules and golden record rules. It does not include information about data part rules. To identify which data part rules have been applied to a constituent record, refer to the Evaluate rules operation in the Records section. ",
    "url": "/key-terms-and-features/golden-records/explain-log#how-to-read-the-explain-log",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log#how-to-read-the-explain-log"
  },"1196": {
    "doc": "Explain Log",
    "title": "Troubleshooting using the Explain Log",
    "content": "The Explain Log shows how a golden record would appear if it were reprocessed right now. This means the Explain Log reflects the current results of the processing pipeline—not necessarily the current state of the golden record itself. For example, if you modify a golden record rule but do not reprocess the affected golden record, the Explain Log will still show the updated rule logic, even though the golden record has not yet been updated accordingly. Recommendation: To ensure consistency between the Explain Log and the actual state of the golden record, always reprocess the golden record before performing troubleshooting based on the Explain Log. ",
    "url": "/key-terms-and-features/golden-records/explain-log#troubleshooting-using-the-explain-log",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log#troubleshooting-using-the-explain-log"
  },"1197": {
    "doc": "Explain Log",
    "title": "Explain Log",
    "content": " ",
    "url": "/key-terms-and-features/golden-records/explain-log",
    
    "relUrl": "/key-terms-and-features/golden-records/explain-log"
  },"1198": {
    "doc": "Export targets",
    "title": "On this page",
    "content": ". | Configure an export target | Manage an export target | . Export targets are used for connecting with an integration point (also referred to as target), enabling CluedIn to send records to it using a stream. Examples of integration points include the following: . | Business intelligence tools . | Data warehouse dimension tables . | Machine learning platforms . | Custom applications . | Databases . | . Here are two examples of how to build a new Export Target: . | SQL Server . | Snowflake . | . See the full list of public implementations of CluedIn Export Targets here. In this article, you will learn how to configure and manage an export target that can receive records sent from CluedIn. ",
    "url": "/consume/export-targets#on-this-page",
    
    "relUrl": "/consume/export-targets#on-this-page"
  },"1199": {
    "doc": "Export targets",
    "title": "Configure an export target",
    "content": "You can configure a predefined export target provided with CluedIn (such as SQL Server Connector or Azure Event Hub Connector). You can also write a custom connector using the CluedIn NuGet package; however, it requires C# knowledge and the actual external service. To configure an export target . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. The Add Export Target pane opens, where you can select and configure the needed export target. | On the Choose Target tab, select the external system you want to establish a connection with. Then, select Next. | On the Configure tab, enter the connection details. The list of fields that you need to complete depends on the selected export target. For example, for the Microsoft SQL Server export target, you need to provide the Name, Host, Database Name, Username, and Password. | Test the connection to make sure it works, and then select Add. You configured the export target. Now, it can be attached to the stream. The user who added the export target is the owner of the export target. | . ",
    "url": "/consume/export-targets#configure-an-export-target",
    
    "relUrl": "/consume/export-targets#configure-an-export-target"
  },"1200": {
    "doc": "Export targets",
    "title": "Manage an export target",
    "content": "On the export target details page, you can do the following: . | Test the connection to the export target. | View the streaming modes supported by the export target: . | Synchronized – the records in the export target mirror the records in CluedIn. | Event – the records in the export target are written as events (for example, Create, Insert, Update, Delete) each time an action occurs in CluedIn. | . | Activate and deactivate the export target. When activating or deactivating the export target, you can view the streams linked to it as well as start, pause, or stop the needed streams. | Review the export target health checks. Health checks for the export target run every 60 seconds. If, for example, somebody has changed the password for the user connecting to the export target, the health check status would change to Unhealthy, letting you know that you need to check on the export target. After the issue is fixed, the status will go back to Healthy again. The Partially Healthy status indicates that some pods are reporting healthy and others are reporting unhealthy (for example, if there is a network issue where one pod is struggling to connect but the other pods can still connect). If the export target is Unhealthy, you cannot select it in the export target configuration for a stream. Additionally, if you have already configured the stream and the export target becomes Unhealthy, you cannot start such stream. | Edit the connection details. | . ",
    "url": "/consume/export-targets#manage-an-export-target",
    
    "relUrl": "/consume/export-targets#manage-an-export-target"
  },"1201": {
    "doc": "Export targets",
    "title": "Export targets",
    "content": " ",
    "url": "/consume/export-targets",
    
    "relUrl": "/consume/export-targets"
  },"1202": {
    "doc": "External features",
    "title": "On this page",
    "content": ". | Dataverse data | Dataverse columns | Workflow | . This will enable the user to see the Power Apps and Power Automate features as an iFrame in CluedIn UI. The feature below will only appear on the business domain’s main page if that business domain is part of the synchronization. Dataverse data . This will enable you to see and navigate to Power Apps table data as well as add, edit, and delete data directly from a Power Apps platform. Dataverse columns . This will enable you to see and navigate to PowerApps columns as well as add, edit, and delete columns directly from a Power Apps platform. Workflow . This will enable you to see the workflow associated with it as well as wiew the columns that have been synced and the ingestion endpoint details in the HTTP event. ",
    "url": "/microsoft-integration/powerapps/external-features#on-this-page",
    
    "relUrl": "/microsoft-integration/powerapps/external-features#on-this-page"
  },"1203": {
    "doc": "External features",
    "title": "External features",
    "content": " ",
    "url": "/microsoft-integration/powerapps/external-features",
    
    "relUrl": "/microsoft-integration/powerapps/external-features"
  },"1204": {
    "doc": "Microsoft Fabric Integration",
    "title": "Microsoft Fabric Integration",
    "content": " ",
    "url": "/microsoft-integration/fabric",
    
    "relUrl": "/microsoft-integration/fabric"
  },"1205": {
    "doc": "Filters",
    "title": "On this page",
    "content": ". | Overview | Filters in search | Reference information . | Properties | Operations | . | . Filters play a significant role in various parts of CluedIn. Filters are used in advanced search, clean projects, streams, rules, and glossary. The purpose of a filter is to define a specific set of records for further activities in the system. In this article, you will learn how the filter in CluedIn works, what it consists of, and how to create efficient and complex filters to precisely define the data you require. ",
    "url": "/key-terms-and-features/filters#on-this-page",
    
    "relUrl": "/key-terms-and-features/filters#on-this-page"
  },"1206": {
    "doc": "Filters",
    "title": "Overview",
    "content": "Filters can be as simple or complex as you need. Simple filters consist of a single condition or several conditions combined with a logical operator. Complex filters consist of groups of conditions combined with a logical operator. The filter consists of the following elements: . | AND/OR operator – a logical operator that combines conditions expressed within rules or groups (if available). | Group – a container for rules, allowing you to create complex filters by combining multiple conditions. Each group has its own AND/OR operator, which dictates how the conditions within the group are combined. | Rule – a set of parameters that make up a filter condition. It defines what to look for in your data. | . Creating a filter is a step-by-step process in which parameters are defined one after another. The following diagram shows the sequence of defining filter parameters. Property type is a parameter that defines the type of object in CluedIn. There are the following property types: . | Property – a metadata about a golden record. You can find many properties on the golden record details page. For more information, see Properties. | Vocabulary – a schema that contains attributes (vocabulary keys) shared across different golden records. | Glossary – a group of records that meet specific criteria. A glossary is essentially a predefined subquery or a rule that can be combined into another rule to reduce complexity and promote reuse and consistency. | . Object is a parameter that defines a specific element you are filtering on. Operation defines the relationship between filter parameters. The available operations change based on the object’s data type (DateTime, Text, Integer, and so on). For more information, see Operations. Value is a criteria that you use to define the filter results. For example, the following filter will return all records where such Property (object type) as Tag (object) Contains (operation) the value Invalid (criteria). If you want to define several criteria in one filter, you can add more rules to the same filter. For example, the following filter will return all records where such Property (object type) as Tag (object) Contains (operation) the value Invalid or To review (criteria). When adding more rules to the filter, pay attention to the AND/OR operator in the upper-left corner of the filter. Selecting AND will return only those records where all combined criteria match. Selecting OR will return only those records where any of the combined criteria match. ",
    "url": "/key-terms-and-features/filters#overview",
    
    "relUrl": "/key-terms-and-features/filters#overview"
  },"1207": {
    "doc": "Filters",
    "title": "Filters in search",
    "content": "Filters in search help you narrow down the exact records you want to see or use for such activities as merge or clean. There are two filter modes in search: . | Basic – you can filter records by business domains, providers, sources, or tags (if available in the system). You can select multiple values in each filter parameter. | Advanced – you can filter records by various properties or vocabulary keys. You can also filter records by their association with a particular glossary. Advanced mode gives you more precision in defining filter criteria. The following screenshot displays the same filters as in the screenshot above, but they are represented in the advanced filter mode. | . After you set up your search filters, you can save them for future use. This eliminates the need to specify filter criteria each time you want to find the same set of records. You can also share saved searches within your organization, making them available to others who need to work with the same set of records. For more information, see Saved searches. Saved searches are particularly useful in clean projects, especially if you need to clean your records on a regular basis. You can set up and save a search filter to detect specific data issues, and when the saved search returns results, you can run the clean project to fix those issues. ",
    "url": "/key-terms-and-features/filters#filters-in-search",
    
    "relUrl": "/key-terms-and-features/filters#filters-in-search"
  },"1208": {
    "doc": "Filters",
    "title": "Reference information",
    "content": "This section includes descriptions of properties and operations available in filters. Refer to the relevant section to learn more about each option. Properties . | Property | Description | . | Aliases | An alternative or secondary name associated with the record. | . | Authors | User who created the record. | . | Created Date | Date when the record was created in the source system or date when the record was created via a manual data entry project in CluedIn. | . | Description | Description of the record. | . | Discovery Date | Date when the record was discovered in CluedIn during processing. | . | Display Name | Name of the record in CluedIn that is shown at the top of on the record details page next to the business domain. If the record in the source system does not contain the display name, then the Name is shown instead of Display Name. | . | Document Mime Type | A label that specifies the nature and format of the record. It is part of the metadata for records added via a crawler or posted to CluedIn. | . | Encoding | A type of encoding scheme (e.g., UTF-8) used to represent the characters in the record. It is part of the metadata for records added via a crawler or posted to CluedIn. | . | Entity Codes | Additional unique identifiers of the record. | . | Business Domain | An attribute of the record that corresponds to a specific business domain. You can set up a filter to return only those records that are associated with a particular business domain. | . | Last Changed By | User who was the last to edit the record. | . | Last Processed Date | Date when the record was processed for the last time. | . | Modified Date | Date when the record was modified in the source system or date when the record has been modified via clean, deduplication, or manual data entry project in CluedIn. | . | Name | Name of the record in CluedIn that is shown in the search results page and in the record properties of the record details page. | . | Origin Entity Code | Unique primary identifier of the record. | . | Origin Entity Codes in source records | Additional unique identifier of the record that is composed of the source name and the field used in the creation of origin entity code. | . | Property Count | The number of properties the record has. | . | Provider | A system from which the data came to CluedIn, such as SAP, HubSpot, or Azure Data Lake. You can set up a filter to return only those records that were created from a specific provider. | . | Revision | A version number of the record. It is part of the metadata for records added via a crawler or posted to CluedIn. | . | Source | A source of the incoming data, such as a specific file, endpoint, or database. You can set up a filter to return only those records that were created from a specific source. | . | Tag | A label that automatically categorizes records across business domains. You can set up a filter to return only those records that contain a specific tag. | . Operations . | Operation | Description | . | Begins With | Valid for a property or vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records where the selected property or vocabulary key begins with the value in the filter. | . | Between | Valid for a property that contains dates. You need to specify two values. The filter results include only those records where the selected property is between the two values in the filter. | . | Contains | Valid for a property or a vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records where the selected property or vocabulary key contains the value in the filter. | . | Ends With | Valid for a property or vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records where the selected property or vocabulary key ends with the value in the filter. | . | Equals | Valid for a property or a vocabulary key that contains text, numbers, or dates. You need to specify a single value. Filter results include only those records where the selected property or vocabulary key matches the value in the filter. | . | Exists/Is Not Null | Valid for a property or a vocabulary key that contains text, numbers, or dates. You do not have to specify a value. The operation checks for the presence of data in the property or vocabulary key. The filter results include only those records where the selected property or vocabulary key has a value. | . | Greater | Valid for a property that contains dates. You need to specify a single value. The filter results include only those records where the selected property is greater than the value in the filter. | . | Greater or Equal | Valid for a property that contains dates. You need to specify a single value. The filter results include only those records where the selected property is greater than or the same as the value in the filter. | . | In | Valid for a property or a vocabulary key that contains text or numbers. You can specify multiple values. The filter results include only those records where the selected property or vocabulary key contains the values in the filter. | . | Less | Valid for a property that contains dates. You need to specify a single value. The filter results include only those records where the selected property is less than the value in the filter. | . | Less or Equal | Valid for a property that contains dates. You need to specify a single value. The filter results include only those records where the selected property is less than or the same as the value in the filter. | . | Not Begins With | Valid for a property or vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records where the selected property or vocabulary key does not begin with the value in the filter. | . | Not Between | Valid for a property that contains dates. You need to specify two values. The filter results include only those records where the specified property is not between the two values in the filter. | . | Not Contains | Valid for a property or a vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records that do not contain the specified value. | . | Not Ends With | Valid for a property or vocabulary key that contains text or numbers. You have to specify a single value. The filter results include only those records where the selected property or vocabulary key does not end with the value in the filter. | . | Not Equal | Valid for a property or a vocabulary key that contains text, numbers, or dates. You need to specify a single value. Filter results include only those records where the selected property or vocabulary key does not match the value in the filter. | . | Does Not Exist/Is Null | Valid for a property or a vocabulary key that contains text, numbers, or dates. You do not have to specify a value. The operation checks for the presence of data in the property or vocabulary key. The filter results include only those records where the specified property or vocabulary key has no value. | . | Not In | Valid for a property or a vocabulary key that contains text or numbers. You can specify multiple values. The filter results include only those records where the selected property or vocabulary key does not contain the values in the filter. | . | Is Not True | Valid for a glossary. The filter results include only those records that do not belong to the specified glossary. | . | Is True | Valid for a glossary. The filter results include only those records that belong to the specified glossary. | . ",
    "url": "/key-terms-and-features/filters#reference-information",
    
    "relUrl": "/key-terms-and-features/filters#reference-information"
  },"1209": {
    "doc": "Filters",
    "title": "Filters",
    "content": " ",
    "url": "/key-terms-and-features/filters",
    
    "relUrl": "/key-terms-and-features/filters"
  },"1210": {
    "doc": "Home Screen",
    "title": "Home Screen",
    "content": "Your integration Home Screen will give you details of how many data sources you have connected and actions to be able to add new integrations or configure existing sources. You will also be able to see information on the amount of data that is stored in CluedIn based off the data you have ingested. Your integration home screen will notify and alert you if there is a data source that has stopped working. This could be for many reasons, but it typically falls within: . | The authentication has stopped working due to expiry of authentication tokens or change in authentication. | The configuration needs attention e.g. a data source has moved location and will need to be updated to support a new source location. | An Administrator has paused or stopped the integration. | . CluedIn will maintain the state of when the integration has stopped working and so on resolution, we know exactly the offset or point in time in which we need to sync data to get it back to a 100% sync state. ",
    "url": "/integration/home-screen",
    
    "relUrl": "/integration/home-screen"
  },"1211": {
    "doc": "Ingesting data to CluedIn with Python",
    "title": "On this page",
    "content": ". | CluedIn | Databricks | . In this article, we will explore how to ingest data to CluedIn Ingestion Endpoints with Python. We will use Databricks as a data source and CluedIn as a destination. The same approach can be used with any other data source like Microsoft Fabric, Azure Synapse Analytics, Snowflake, or any other data source that can run Python code. Let’s start with a simple example. We have a table named imdb_titles in Databricks with 601_222 rows: . ",
    "url": "/playbooks/data-engineering-playbook/ingestion#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/ingestion#on-this-page"
  },"1212": {
    "doc": "Ingesting data to CluedIn with Python",
    "title": "CluedIn",
    "content": "We want to load this data to CluedIn. To do that, we need to create an API token in CluedIn. Go to Administration &gt; API Tokens and create a new token: . Next, create an endpoint in CluedIn. From CluedIn’s main page, click “Import From Ingestion Endpoint” and create a new endpoint. You will need to enter the endpoint’s name, group name, and select business domain (previouisly entity type): . After you create the endpoint, you will find its URL in the “View Instructions” section: . We are ready to load data to CluedIn. Now it’s time to set up Databricks. ",
    "url": "/playbooks/data-engineering-playbook/ingestion#cluedin",
    
    "relUrl": "/playbooks/data-engineering-playbook/ingestion#cluedin"
  },"1213": {
    "doc": "Ingesting data to CluedIn with Python",
    "title": "Databricks",
    "content": "In Databricks, we create a Jupyter notebook and install the cluedin library: . %pip install cluedin . Next, we import the cluedin library and create a CluedIn context object: . import requests import cluedin ctx = cluedin.Context.from_dict( { \"domain\": \"51.132.187.83.sslip.io\", \"org_name\": \"foobar\", \"access_token\": \"(your token)\", } ) ENDPOINT_URL = \"https://app.51.132.187.83.sslip.io/upload/api/endpoint/9A327661-51FD-4FFC-8DF5-3F80746B996C\" DELAY_SECONDS = 5 BATCH_SIZE = 100_000 . In our example, the URL of our CluedIn instance is https://foobar.51.132.187.83.sslip.io/, so the domain is 51.132.187.83.sslip.io and the organization name is foobar. The access token is the one we created earlier. Next, we will pull data from Databricks and post it to CluedIn. Here is a simple method to select all rows from a table and yield them one by one: . from pyspark.sql import SparkSession def get_rows(): spark = SparkSession.builder.getOrCreate() imdb_names_df = spark.sql(\"SELECT * FROM hive_metastore.cluedin.imdb_titles\") for row in imdb_names_df.collect(): yield row.asDict() . Next, we create a method that posts a batch of rows to CluedIn: . import time from datetime import timedelta def post_batch(ctx, batch): response = requests.post( url=ENDPOINT_URL, json=batch, headers={\"Authorization\": f\"Bearer {ctx.access_token}\"}, timeout=60, ) time.sleep(DELAY_SECONDS) return response def print_response(start, iteration_start, response) -&gt; None: time_from_start = timedelta(seconds=time.time() - start) time_from_iteration_start = timedelta( seconds=time.time() - iteration_start) time_stamp = f'{time_from_start} {time_from_iteration_start}' print(f'{time_stamp}: {response.status_code} {response.json()}\\n') . print_response is a helper method that prints the response status code and the response body. Finally, we iterate over the rows and post them to CluedIn. Note that we are posting the rows in batches of BATCH_SIZE rows. DELAY_SECONDS is the number of seconds to wait between batches. However, we will need to post a small batch of rows first to set up mapping on the CluedIn side. We will post ten rows. To do that, we will add the following lines in the code below: . if i &gt;= 10: break . Here is the code that posts the rows to CluedIn: . batch = [] batch_count = 0 start = time.time() iteration_start = start for i, row in enumerate(get_rows()): if i &gt;= 10: break batch.append(row) if len(batch) &gt;= BATCH_SIZE: batch_count += 1 print(f'posting batch #{batch_count:_} ({len(batch):_} rows)') response = post_batch(ctx, batch) print_response(start, iteration_start, response) iteration_start = time.time() batch = [] if len(batch) &gt; 0: batch_count += 1 print(f'posting batch #{batch_count:_} ({len(batch):_} rows)') response = post_batch(ctx, batch) print_response(start, iteration_start, response) iteration_start = time.time() print(f'posted {(i + 1):_} rows') . After we run the code, we shall see ten rows in CluedIn: . Then, in the Map tab, we create an automatic mapping: . | Click “Add Mapping”. | Select “Auto Mapping” and “Next”. | Ensure the business domain (previously entity type) is selected or type a new business domain name and click “Create”. | Type the new vocabulary name, like imdb.title and click “Create”. | Click “Create Mapping”. | . | Click “Edit Mapping”. | On the “Map Entity” tab, select the property used as the entity name and the property used for the origin entity code: | . | Click “Next” and click “Finish”. | On the “Process” tab, enable “Auto submission” and then click “Switch to Bridge Mode”: | . | After you followed the instructions, you can remove or comment on these lines in the notebook and rerun it: | . if i &gt;= 10: break . All table rows will be posted to CluedIn. The output in the notebook will look like this: . 0:00:13.398879 0:00:13.398889: 200 {'success': True, 'warning': False, 'error': False} posting batch #2 (100_000 rows) 0:00:22.021498 0:00:08.622231: 200 {'success': True, 'warning': False, 'error': False} posting batch #3 (100_000 rows) 0:00:30.709844 0:00:08.687518: 200 {'success': True, 'warning': False, 'error': False} posting batch #4 (100_000 rows) 0:00:40.026708 0:00:09.316675: 200 {'success': True, 'warning': False, 'error': False} posting batch #5 (100_000 rows) 0:00:48.530380 0:00:08.503460: 200 {'success': True, 'warning': False, 'error': False} posting batch #6 (100_000 rows) 0:00:57.116517 0:00:08.585930: 200 {'success': True, 'warning': False, 'error': False} posting batch #7 (1_222 rows) 0:01:02.769714 0:00:05.652984: 200 {'success': True, 'warning': False, 'error': False} posted 601_222 rows . Give CluedIn some time to process the data, and you should see 601_222 rows in CluedIn. The same approach works not only with Databricks but also with any other data source. You must change the get_rows method to pull data from your source. ",
    "url": "/playbooks/data-engineering-playbook/ingestion#databricks",
    
    "relUrl": "/playbooks/data-engineering-playbook/ingestion#databricks"
  },"1214": {
    "doc": "Ingesting data to CluedIn with Python",
    "title": "Ingesting data to CluedIn with Python",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/ingestion",
    
    "relUrl": "/playbooks/data-engineering-playbook/ingestion"
  },"1215": {
    "doc": "Ingestion",
    "title": "Ingestion",
    "content": "In the Ingestion module, you can upload your data into CluedIn, map it to standard fields, and process is to turn your data into golden records. Data sources Get your data from external sources into CluedIn Operations Improve the quality of incoming data Crawlers Build robust integrations and crawlers When you open the Ingestion module, the first thing you see is the dashboard that can simplify and streamline your work with data sources and source records. The dashboard is a place where you can start the process of uploading the data into CluedIn as well as find general statistics about your data sources. It consists of three main sections. Source actions . At the top of the dashboard, you can find the actions to upload the data into CluedIn from a file, an ingestion endpoint, and a database. Additionally, you can add a manual data entry project and navigate to the list of installed crawlers. Each action card contains a number that indicates the count of data sources of a particular type that are currently in CluedIn. Selecting the number in the file, ingestion endpoint, or database action card will take you to the Sources page with data sources filtered by a specific type. To view the list of manual data entry projects, select the number in the corresponding action card. Data set records pending review . This table allows you to track the number of records per data set that are in quarantine or require approval. The table includes up to 20 latest sources, regardless of the owner. However, even if the table displays less than 20 sources, there might be additional sources requiring review. This is because after approving a specific source, only sources that were added later than the approved source will appear in the table. To view the records that are currently in quarantine, select the corresponding number in the Quarantine column. Similarly, to view the records that require approval, select the corresponding number in the Requires approval column. If you are not the owner of the data source, you cannot approve or reject the records on the Quarantine or Approval tabs of the data set. If you want to track records in data sources where you are the owner, go to Home &gt; My tasks. You will see a similar table with the number of records per data set that are in quarantine or require approval. Each item in the table comes from a data source where you are the owner, so you can go ahead and review the records. Manual data entry project records pending review . This table allows you to track the number of records per manual data entry project that require approval. The table includes up to 20 latest projects, regardless of the owner. However, even if the table displays less than 20 projects, there might be additional projects requiring review. This is because after approving records from a specific manual data entry project, only projects that were added later than the approved project will appear in the table. To view the records that require approval, select the corresponding number in the Requires approval column. If you are not the owner of the manual data entry project, you cannot approve or reject the records on the Approval tabs of the project. If you want to track records in manual data entry projects where you are the owner, go to Home &gt; My tasks. You will see a similar table with the number of records per manual data entry project that require approval. Since you are the owner of each project in the table, you can go ahead and review the records. ",
    "url": "/integration",
    
    "relUrl": "/integration"
  },"1216": {
    "doc": "Local upgrade",
    "title": "Local upgrade",
    "content": "In this article, you will learn how to upgrade your local instance of CluedIn to the new version. Before upgrading your local instance of CluedIn, consider whether it would be more convenient to create a new environment. Typically, you shouldn’t store many records in your local instance, so re-ingesting the records should be fast. To upgrade the local instance of CluedIn . | Depending on the release that you need, do one of the following: . | To get the latest release, run git pull on the master branch of CluedIn Home repository. | To get a specific release, in CluedIn Home repository, switch to the tag containing the needed release, and then check out the tag by running the following command: $ git checkout tags/&lt;tag&gt; -b &lt;branch&gt;. | . | Retrieve the SQL Init image by running the following command: docker pull {acr url}/cluedin/sqlserver-init:[VERSION] . You should use the version to which you want to upgrade (e.g., 3.7.4). | Perform the database update by running the following command: docker run --rm -it -e MSSQL_HOST=host.docker.internal {acr url}/cluedin/sqlserver-init:[VERSION] . You should use the version to which you want to upgrade (e.g., 3.7.4). Your environment’s SQL Server Docker image should be running for this image to run correctly. | In the .env file for your environment, change the tags to reference the version of CluedIn that you are upgrading to. | Using a diff tool, compare the .env file for your environment with the default/.env file. If some values are missing in the .env file for your environment, copy them from the default/.env file. | Run the following command to get container images: pwsh ./cluedin pull -env [NAME OF ENV] . | Run the following command to start your instance of CluedIn: pwsh ./cluedin up -env [NAME OF ENV] . | . ",
    "url": "/deployment/local/local-upgrade",
    
    "relUrl": "/deployment/local/local-upgrade"
  },"1217": {
    "doc": "Manage a deduplication project",
    "title": "On this page",
    "content": ". | Generate matches | Discard matches | Edit a deduplication project | Archive a deduplication project | Duplicate a deduplication project | . After you create and configure your deduplication project, you can begin the deduplication process to eliminate duplicates and produce a unified and accurate representation of a golden record. In this article, you will learn how to generate and discard matches in the deduplication project, as well as how to keep your project organized and aligned with your deduplication goals. The primary action on the deduplication project level is the generation of matches. Its purpose is to analyze a specific set of golden records and detect potential duplicates based on your matching rules. If such duplicates are detected, they are organized into groups. The following diagram illustrates basic project workflow. For more information about project statuses, see Deduplication reference. Instructions on how to process groups of duplicates, including merging and unmerging, are provided in the Manage groups of duplicates article. ",
    "url": "/management/deduplication/manage-a-deduplication-project#on-this-page",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#on-this-page"
  },"1218": {
    "doc": "Manage a deduplication project",
    "title": "Generate matches",
    "content": "The process of generating matches involves analyzing a specified set of golden records with the goal of finding duplicates among them based on your matching rules. To generate matches . | In the deduplication project, select Generate matches, and then confirm your choice. During the process of generating matches, you can view the number of groups found as well as the percentage of golden records that have been checked for duplicates. When the process is complete, the groups of duplicates are displayed on the page. A group contains duplicate records that can be potentially merged into one golden record. Next, process the groups of duplicates—open each group one by one to review duplicates and fix conflicting values. | . Sorting and filtering of groups . You can sort the groups of duplicates using column headers—Name and Number of matches. These column headers contain small arrows pointing up and down. A green arrow pointing up means that column is sorted in ascending order. A green arrow pointing down means that column is sorted in descending order. To change the order, click on the grey arrow in the needed column header. You can filter the groups of duplicates using the Min matches – Max matches slider. This allows you to focus on groups that contain a specific number of matches. For example, if you have a lot of groups of duplicates, you might want to start with small groups first. To do this, set the desired number of matches using the slider. Once you’ve processed the small groups, you can move on to larger groups by adjusting the slider accordingly. ",
    "url": "/management/deduplication/manage-a-deduplication-project#generate-matches",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#generate-matches"
  },"1219": {
    "doc": "Manage a deduplication project",
    "title": "Discard matches",
    "content": "If you want to change matching rules, modify project filters, or regenerate matches in the deduplication project, you can discard matches. This action does not affect merged golden records. To discard matches . | In the upper-right corner of the deduplication project, select Discard matches, and then confirm your choice. The project status is changed to Ready to generate. Now, you can edit the project as needed and generate matches again. | . ",
    "url": "/management/deduplication/manage-a-deduplication-project#discard-matches",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#discard-matches"
  },"1220": {
    "doc": "Manage a deduplication project",
    "title": "Edit a deduplication project",
    "content": "You can edit a deduplication project only when its status is Requires configuration or Ready to generate. In other statuses, you need to discard matches before editing the project. Editing a deduplication project involves two aspects: . | Editing the project configuration: project name, business domain, or advanced filters, and description. For example, if you used advanced filters to narrow down the number of golden records for the project and you have reached the desired matching rules configuration, you can now modify the filters and run the project on the entire set of data. | Editing the matching rules configuration: change rule name; add rule; deactivate or delete the rule; modify matching criteria (edit, delete, add). | . To edit the project configuration . | In the upper-right corner of a deduplication project, select Edit. | Make the needed changes, and then select Save. | . To edit the matching rules configuration . | On the Matching Rules tab, in the Status column, do one of the following: . | To change the rule name, select the vertical ellipsis button, and then select Edit name. Enter the new name and save your changes. | To deactivate the rule, turn off the toggle. Deactivated rules are not executed during the process of generating matches. | To delete the rule, select the vertical ellipsis button, and then select Delete and confirm your choice. If you delete the rule, it cannot be reverted. | To modify the matching criteria, expand the rule and select the vertical ellipsis button in the needed row. Then select the needed action: Edit or Delete. | . | . You can also add new matching criteria to the rule. To do that, expand the rule, and then select Add Matching Criteria. When you are satisfied with the project and matching rules configuration, proceed to generate matches. ",
    "url": "/management/deduplication/manage-a-deduplication-project#edit-a-deduplication-project",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#edit-a-deduplication-project"
  },"1221": {
    "doc": "Manage a deduplication project",
    "title": "Archive a deduplication project",
    "content": "You can archive a deduplication project if you no longer need it or if you created it by mistake. You can also archive a deduplication project if you’re confident that you won’t need to run it again in the future. Archiving does not affect merges that have been submitted to CluedIn. After a deduplication project is archived, it cannot be unarchived. Archived projects are available only for viewing. You can archive a deduplication project only when its status is Requires configuration or Ready to generate. In other statuses, you need to discard matches before archiving the project. To archive a deduplication project . | In the upper-right corner of a deduplication project, select Edit &gt; Archive. Then, confirm that you want to archive the project. The status of a deduplication project becomes Archived, and you can no longer work with the project. | . To view an archived deduplication project . | In the upper-right corner of the Deduplication page, select View Archived. | From the list of archived deduplication projects, select the one that you want to view. | . ",
    "url": "/management/deduplication/manage-a-deduplication-project#archive-a-deduplication-project",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#archive-a-deduplication-project"
  },"1222": {
    "doc": "Manage a deduplication project",
    "title": "Duplicate a deduplication project",
    "content": "Duplicating a deduplication project means creating a new deduplication project with the configuration of the existing project. This configuration includes filters and matching rules but does not include the deduplication activities performed in the project. By activities, we mean generated matches and merges. You can duplicate a deduplication project if you want to: . | Use the same matching rules for a different set of golden records. In this case, you only need to modify the filters in the duplicated project. | Use different matching rules for the same set of golden records. In this case, you only need to modify the matching rules in the duplicated project. | . Duplication is a beta feature. To access it, go to Administration &gt; Feature Flags, and enable the Duplicate Actions feature. To duplicate a deduplication project . | In the list of deduplication projects, find a project that you want to duplicate. Then, open the three-dot menu for the project, and select Duplicate. | In Display Name, review the default name of the new project and modify it if needed. The default name is created by adding _duplicate to the name of the project that you’re duplicating. | In Filters, review the filters that will be duplicated for the new project. | In Matching Rules, review the list of matching rules that will be duplicated for the new project. To view the details of a specific matching rule, select the angle bracket (&gt;) next to the name of the matching rule. | Select Duplicate. The new deduplication project is created, and it has the Ready to generate status. Now, you can modify the deduplication project configuration as needed. When you reach the desired configuration, generate matches, and then start working with the groups of duplicates. | . ",
    "url": "/management/deduplication/manage-a-deduplication-project#duplicate-a-deduplication-project",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project#duplicate-a-deduplication-project"
  },"1223": {
    "doc": "Manage a deduplication project",
    "title": "Manage a deduplication project",
    "content": " ",
    "url": "/management/deduplication/manage-a-deduplication-project",
    
    "relUrl": "/management/deduplication/manage-a-deduplication-project"
  },"1224": {
    "doc": "Manage approval requests",
    "title": "On this page",
    "content": ". | View pending approval requests | Approve or reject an approval request | . To use the Workflow module in CluedIn, you need to configure Power Automate integration. In this article, you will learn how to view, approve, and reject approval requests sent from the Power Automate widget in CluedIn. ",
    "url": "/workflow/manage-approval-requests#on-this-page",
    
    "relUrl": "/workflow/manage-approval-requests#on-this-page"
  },"1225": {
    "doc": "Manage approval requests",
    "title": "View pending approval requests",
    "content": "If you’re an approver in the approval flow or if you made some changes in CluedIn that require approval from somebody else, you can find your pending requests in CluedIn. To view pending approval requests . | On the navigation pane, go to Workflow &gt; Approvals. | To view the approval requests that are assigned to you, go to the Received tab. Here, you can also approve or reject a request right away. | To view the approval requests that were sent out on your behalf, go to the Sent tab. Here, you can view the status of each request, indicating whether they have been approved, rejected, or still pending. | . ",
    "url": "/workflow/manage-approval-requests#view-pending-approval-requests",
    
    "relUrl": "/workflow/manage-approval-requests#view-pending-approval-requests"
  },"1226": {
    "doc": "Manage approval requests",
    "title": "Approve or reject an approval request",
    "content": "When a specific activity in CluedIn requires your approval, you receive an email in Outlook or a message in the Approvals app in Teams. You can then approve or reject a request from the email, Teams, or the Power Automate widget in CluedIn (Workflow &gt; Approvals). To approve or reject a request from email . | Open the email and review the request details. | Make an approval decision by selecting Approve or Reject. Optionally, you can add a comment before sending your approval decision. | . To approve or reject a request from the Approvals app in Teams . | Open the request and review its details. | Make an approval decision by selecting Approve or Reject. Optionally, you can add a comment before sending your approval decision. | . If you approved the request, the action that required your approval is automatically performed in CluedIn. Regardless of the approval decision, the user who performed the action receives a notification in CluedIn. ",
    "url": "/workflow/manage-approval-requests#approve-or-reject-an-approval-request",
    
    "relUrl": "/workflow/manage-approval-requests#approve-or-reject-an-approval-request"
  },"1227": {
    "doc": "Manage approval requests",
    "title": "Manage approval requests",
    "content": " ",
    "url": "/workflow/manage-approval-requests",
    
    "relUrl": "/workflow/manage-approval-requests"
  },"1228": {
    "doc": "Manage hierarchies",
    "title": "On this page",
    "content": ". | Clone a hierarchy | Export a hierarchy | Delete a hierarchy | . In this article, you will learn how to manage hierarchies in order to keep them well-organized. ",
    "url": "/management/hierarchy-builder/manage-hierarchies#on-this-page",
    
    "relUrl": "/management/hierarchy-builder/manage-hierarchies#on-this-page"
  },"1229": {
    "doc": "Manage hierarchies",
    "title": "Clone a hierarchy",
    "content": "If you want to reuse the configuration of an existing hierarchy project, you can clone it. To clone a hierarchy . | On the navigation pane, go to Management &gt; Hierarchy Builder. | Find the hierarchy that you want to clone. In the right corner of the row, open the three-dot menu, and then select Clone. | Enter the name of the new hierarchy project. | Select Clone. Now, you can edit the cloned hierarchy project. | . ",
    "url": "/management/hierarchy-builder/manage-hierarchies#clone-a-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/manage-hierarchies#clone-a-hierarchy"
  },"1230": {
    "doc": "Manage hierarchies",
    "title": "Export a hierarchy",
    "content": "If you want to validate leaf nodes, you can export the hierarchy to a comma-separated values (CSV) file. The file will contain the following columns: . | Id – node ID. | Name – node name. | ParentId – ID of the parent node. | Level – node level in the hierarchy. | IsLeaf – false if the node has child nodes; true if the node does not have child nodes. | . To export a hierarchy . | On the navigation pane, go to Management &gt; Hierarchy Builder. | Find the hierarchy that you want to export. In the right corner of the row, open the three-dot menu, and then select Export. The hierarchy in the CSV format is downloaded to your computer. | . ",
    "url": "/management/hierarchy-builder/manage-hierarchies#export-a-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/manage-hierarchies#export-a-hierarchy"
  },"1231": {
    "doc": "Manage hierarchies",
    "title": "Delete a hierarchy",
    "content": "If you no longer need a hierarchy, you can delete it. The hierarchy is also deleted from the golden record page. To delete a hierarchy . | On the navigation pane, go to Management &gt; Hierarchy Builder. | Find the hierarchy that you want to delete, and then do one of the following: . | In the right corner of the row, open the three-dot menu, and then select Delete. | Select the checkbox next to the hierarchy, and then select Delete. | . | Confirm that you want to delete the hierarchy. Alternatively, you can open the hierarchy project, and then select Delete. | . ",
    "url": "/management/hierarchy-builder/manage-hierarchies#delete-a-hierarchy",
    
    "relUrl": "/management/hierarchy-builder/manage-hierarchies#delete-a-hierarchy"
  },"1232": {
    "doc": "Manage hierarchies",
    "title": "Manage hierarchies",
    "content": " ",
    "url": "/management/hierarchy-builder/manage-hierarchies",
    
    "relUrl": "/management/hierarchy-builder/manage-hierarchies"
  },"1233": {
    "doc": "Manage vocabulary keys",
    "title": "On this page",
    "content": ". | Create a vocabulary key | Edit a vocabulary key | Map one vocabulary key to another | Delete a vocabulary key | . In this article, you will learn how to efficiently create and manage vocabulary keys, helping you to structure and organize metadata within the system. ",
    "url": "/management/data-catalog/manage-vocabulary-keys#on-this-page",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys#on-this-page"
  },"1234": {
    "doc": "Manage vocabulary keys",
    "title": "Create a vocabulary key",
    "content": "Depending on the selected data modeling approach, you can create a vocabulary key in two ways: . | Automatically – this option is part of the data-first approach. When creating a mapping for a data set, you have the option to generate a new vocabulary. CluedIn will then automatically create vocabulary keys and assign data types. You can customize the names of vocabulary keys and data types. Once the mapping is created, you can then open the vocabulary and make additional adjustments to vocabulary keys if needed. | Manually – this option is part of the model-first approach, which assumes that you need to create vocabulary keys before creating the mapping for a data set. The following procedure outlines the steps to manually create a vocabulary key. | . To create a vocabulary key . | Go to the Vocabulary Keys tab, and then select Add Vocabulary Key. | Enter the display name of the vocabulary key. This name is displayed at the top of the vocabulary details page. | Enter the name of the vocabulary key. This name, together with the key prefix, forms a complete vocabulary key. | Enter the name of the group to which the vocabulary key will belong. | (Optional) If you don’t want to the vocabulary key to be displayed anywhere else in the system, turn off the corresponding toggle. | Select the data type of the vocabulary key. For more information, see Data types. | Review the storage for the vocabulary key. This is the type of field—keyword, typed, untyped—where the vocabulary key value is stored. The storage is selected by default according to the data type. The keyword storage option cannot be changed. If you configure the storage as typed but send untyped data—such as a combination of integer and string, as in ‘10-regular’—then such untyped values (‘regular’) would be excluded from the filter results. Even though these values are still stored, they are treated as anomalies. | (Optional) Select the classification for the vocabulary key. This is a way to categorize vocabulary keys, making them easier to search. | Select Create. The vocabulary key page opens, where you can view and manage vocabulary key details. | . ",
    "url": "/management/data-catalog/manage-vocabulary-keys#create-a-vocabulary-key",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys#create-a-vocabulary-key"
  },"1235": {
    "doc": "Manage vocabulary keys",
    "title": "Edit a vocabulary key",
    "content": "You can edit a vocabulary key to make necessary changes to its configuration, ensuring that metadata is organized and consistent. Changing the data type, storage, classification, and mapping will trigger the reprocessing of a vocabulary key. Additionally, if you change the name of the vocabulary key, a new key will be generated, with the existing key automatically mapped to the new one and marked as obsolete. To edit a vocabulary key . | Open the vocabulary key that you want to edit. | In the upper-right corner of the vocabulary key details page, select Edit. | Make the needed change and then save them. Depending on the section that you want to edit, you may have to follow different steps. For example, direct editing is not possible for the Name and Data Type sections; instead, you must select the corresponding button to open the pane for editing. | . ",
    "url": "/management/data-catalog/manage-vocabulary-keys#edit-a-vocabulary-key",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys#edit-a-vocabulary-key"
  },"1236": {
    "doc": "Manage vocabulary keys",
    "title": "Map one vocabulary key to another",
    "content": "The purpose of vocabulary key mapping is to keep data organized and consistent by preventing duplicate values in the system. For the practical application of vocabulary key mapping, check the video in Modeling approaches. To demonstrate the importance and efficiency of vocabulary key mapping, let’s look at an example. Suppose you have contact records coming into CluedIn from various sources—Salesforce, SAP, and MS SQL Server—and each source has its unique vocabulary. Some properties in data coming from these sources are the same, and some are different. For example, the gender property exists in data from all three sources. It means that gender values would be stored in three separate vocabulary keys. To keep all gender values under one vocabulary key instead of keeping them in each separate vocabulary key, you can map these three vocabulary keys to one, thus improving the organization and getting better visibility of your data. To map one vocabulary key to another . | Open the vocabulary key that you want to map to another vocabulary key. | In the upper-right corner of the vocabulary key details page, select Edit. | In the Maps to section, select Add mapping. | Find and select a vocabulary key to which you want to map the current vocabulary key. Then, select Add Key. | Select Confirm. | Select Save, and then confirm your choice. The Mapped label appears under the vocabulary key name. Hover over the label to find the vocabulary key to which the current vocabulary key is mapped. | . The vocabulary key mapping is executed on the clue level, so the History of the golden record does not show source and target properties. However, you can view the source and target properties in the Explain Log of the golden record (Records &gt; data source &gt; Translate properties &gt; Summaries). When you try to use a vocabulary key that is mapped to another vocabulary key, a prompt will inform you of the mapping. Throughout the system, the following guidelines are applied to mapped vocabulary keys: . | When you use a vocabulary key in rules, streams, glossary terms, or anywhere else in the system, and it is mapped to another vocabulary key, the values from the mapped vocabulary key will be used. Essentially, the system takes the left-most vocabulary key and returns the values from the right-most vocabulary key. For example, if vocabulary key A is mapped to vocabulary key B, and vocabulary key B is mapped to vocabulary key C, then when you use vocabulary key A in the filter rule, the values from the vocabulary key C will be returned. | When you request a specific vocabulary key for display purposes and it is mapped to another vocabulary key, the values from the source vocabulary key will be used. However, because the source vocabulary key is mapped, all its values are stored in the mapped vocabulary key, meaning you won’t see any values for the source vocabulary key. For example, if you add a vocabulary key column to the search results page, it will be empty because all values are stored in the mapped vocabulary key. | When you make changes to the current vocabulary key mapping or to the vocabulary key it is mapped to, the system will evaluate whether these changes are compatible with the vocabulary key configuration. | . ",
    "url": "/management/data-catalog/manage-vocabulary-keys#map-one-vocabulary-key-to-another",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys#map-one-vocabulary-key-to-another"
  },"1237": {
    "doc": "Manage vocabulary keys",
    "title": "Delete a vocabulary key",
    "content": "If you no longer need a vocabulary key, you can delete it. The process of deleting a vocabulary key is different depending on whether the vocabulary key is used anywhere in the system. If a vocabulary key is used somewhere in the system (for example, in a data set mapping, in a golden record, in a rule, and so on), you need to remap the vocabulary key first, and only then you can delete it. To delete a vocabulary key . | Open the vocabulary key that you want to delete. | In the upper-right corner of the vocabulary key details page, select Edit &gt; Remove. | On the Map to existing tab, choose whether you want to map the current vocabulary key to another vocabulary key: . | If the current vocabulary key is used somewhere in the system, the only available option is to map it to another vocabulary key. | If the current vocabulary key is not used anywhere in the system, you can delete it right away or you can map it to another vocabulary key. | . After you made your choice, select Next. | On the Select vocabulary key tab, use the search box to find the existing vocabulary key to which you want to map the current vocabulary key. Then, select the checkbox next to the needed key. Finally, select Next. | On the Confirm tab, review the details about the vocabulary key, and select Confirm. If a deleted vocabulary key has been used somewhere in the system, it will be marked as obsolete and will remain in the list of vocabulary keys. Hover over its label to find the vocabulary key it is mapped to. If the deleted vocabulary key is not used anywhere in the system and was not mapped to another vocabulary key, it will be removed from the list of vocabulary keys. | . ",
    "url": "/management/data-catalog/manage-vocabulary-keys#delete-a-vocabulary-key",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys#delete-a-vocabulary-key"
  },"1238": {
    "doc": "Manage vocabulary keys",
    "title": "Manage vocabulary keys",
    "content": " ",
    "url": "/management/data-catalog/manage-vocabulary-keys",
    
    "relUrl": "/management/data-catalog/manage-vocabulary-keys"
  },"1239": {
    "doc": "Power Automate private network configuration guide",
    "title": "On this page",
    "content": ". | Advanced Configuration: Power Automate / Power Apps in a Private Network . | Connectivity Requirements | CluedIn Server Configuration | Firewall Configuration | . | . In this guide, you will learn how configure access for power automate within a private instance of CluedIn. This guide is applicable to only private CluedIn instances. ",
    "url": "/microsoft-integration/power-automate/private-network#on-this-page",
    
    "relUrl": "/microsoft-integration/power-automate/private-network#on-this-page"
  },"1240": {
    "doc": "Power Automate private network configuration guide",
    "title": "Advanced Configuration: Power Automate / Power Apps in a Private Network",
    "content": "To enable Power Automate or Power Apps to communicate with CluedIn from within a private network, external access must be configured through a publicly accessible endpoint. Connectivity Requirements . Power Automate must access CluedIn through a public URL. The data flow diagram (see below) illustrates how Power Automate routes requests to CluedIn. To support this setup, the following components are required: . | Public DNS . | Public IP Address . | . CluedIn Server Configuration . The public DNS must be defined on the CluedIn server using an environment variable: . CLUEDIN_APPSETTINGS__PROXY_PUBLICURL = \"workflow-{env}.{your-domain}\" . | Replace {env} with your deployment environment (e.g., dev, prod) . | Replace {your-domain} with your organization’s domain name . | . Firewall Configuration . Power Automate operates from Microsoft-managed IP ranges, which must be allowed through your network perimeter. | Azure IP Ranges &amp; Service Tags . | Outbound Firewall rules . | . If you’re using Azure Firewall, Microsoft provides a service tag to simplify rule configuration: . | Use the service tag: AzureConnectors LogicApps . | This automatically includes all required outbound IP ranges for Power Automate and related services . | . ",
    "url": "/microsoft-integration/power-automate/private-network#advanced-configuration-power-automate--power-apps-in-a-private-network",
    
    "relUrl": "/microsoft-integration/power-automate/private-network#advanced-configuration-power-automate--power-apps-in-a-private-network"
  },"1241": {
    "doc": "Power Automate private network configuration guide",
    "title": "Power Automate private network configuration guide",
    "content": " ",
    "url": "/microsoft-integration/power-automate/private-network",
    
    "relUrl": "/microsoft-integration/power-automate/private-network"
  },"1242": {
    "doc": "Power Fx formulas",
    "title": "On this page",
    "content": ". | Power Fx formulas in rules | Power Fx formula examples | . In this article, you will learn about Power Fx formulas that you can use in the Rule Builder to set up filters, conditions, and actions in the Rule Builder. Power Fx is a general-purpose, low-code, strong-typed, and functional programming language developed by Microsoft. Power Fx enables you to build and customize applications, workflows, and other solutions by writing simple, declarative expressions rather than traditional code. You can work with Power Fx using Excel-like formulas, which makes it intuitive both for technical and business users. For more information, see Microsoft Power Fx overview. In CluedIn, you can use Power Fx formulas in the Rule Builder for querying, equality testing, decision making, type conversion, and string manipulation based on the supported properties of a data part or a golden record. ",
    "url": "/management/rules/power-fx-formulas#on-this-page",
    
    "relUrl": "/management/rules/power-fx-formulas#on-this-page"
  },"1243": {
    "doc": "Power Fx formulas",
    "title": "Power Fx formulas in rules",
    "content": "You can use Power Fx formulas in the following types of rules: . | Data part rules – formulas are available in filters and actions (conditions and formula action). | Survivorship rules – formulas are available in filters and actions (conditions). The formula action is not available in survivorship rules. | Golden record rules – formulas are available in filters and actions (conditions and formula action). | . In CluedIn, a Power Fx formula requires a context to work with. It is called an Entity, and it represents a data part or a golden record. Essentially, it is a global variable that holds a sandboxed version of a data part or a golden record in CluedIn. Consider the following example of a formula. Right(Entity.Name, 1) = \"t\" . This formula consists of the following elements: . | Right – a built-in function that retrieves the rightmost x characters in a string. | Entity – a context that the formula is working against. | Name – a string property of Entity. | 1 – the number of characters to retrieve from the right end of the string. | = \"t\" – the equality evaluator that checks if the retrieved character is equal to the letter “t”. | . The formula checks if the rightmost character of Entity.Name is t. If it is, the formula returns true, meaning that the rule will be applied to a specific golden record. If the formula returns false, the rule will not be applied to a specific golden record. Custom functions . Custom CluedIn functions are designed to help you with querying and setting data for a data part or a golden record. Custom functions include the following: . | AddTag – adds a tag to the golden record’s tag collection. This function is analogous to the Add Tag rule action. | GetVocabularyKeyValue – gets a value from the golden record’s properties if such value exists; otherwise, it returns Empty (null). | SetEntityProperty – sets a golden record metadata property (for example, Created Date, Aliases, Description). | SetVocabularyKeyValue – sets or adds a vocabulary key to the golden record’s properties. | . ",
    "url": "/management/rules/power-fx-formulas#power-fx-formulas-in-rules",
    
    "relUrl": "/management/rules/power-fx-formulas#power-fx-formulas-in-rules"
  },"1244": {
    "doc": "Power Fx formulas",
    "title": "Power Fx formula examples",
    "content": "This section contains some examples of Power Fx formulas in rules. | Set a value using an IF condition. SetVocabularyKeyValue(Entity, \"user.price\", If(((GetVocabularyKeyValue(Entity, \"user.price\") / 5) * 7) + 14 &gt; 50, 355, ((GetVocabularyKeyValue(Entity, \"user.price\") / 5) * 7) + 14)) . | Set contract status to “Expiring Soon” if the contract ends within 5 days; otherwise, set it to “Active”. SetVocabularyKeyValue(Entity, \"finance.contractStatus\", If(DateDiff(Today(), GetVocabularyKeyValue(Entity, \"finance.contractEndDate\")) &lt; 5, \"Expiring Soon\", \"Active\")) . | Set salary grade to “Above Target” if the salary is higher than the target salary; otherwise, set it to “Below Target”. SetVocabularyKeyValue(Entity, \"finance.salaryGrade\", If(Value(GetVocabularyKeyValue(Entity, \"finance.salary\")) &gt; Value(GetVocabularyKeyValue(Entity, \"finance.targetSalary\")), \"Above Target\", \"Below Target\")) . | Set a vocabulary key value to a date using type conversion and formatting it to ISO format. SetVocabularyKeyValue(Entity, \"user.startDate\", Text(DateValue(GetVocabularyKeyValue(Entity, \"user.startDate\")), \"yyyy-MM-ddTHH:mm:ssZ\")) . | Set full name to the combination of first name and last name, separated by a space. SetVocabularyKeyValue(Entity, \"employee.fullName\", GetVocabularyKeyValue(Entity, \"employee.firstName\") &amp; \" \" &amp; GetVocabularyKeyValue(Entity, \"employee.lastName\")) . | Add a tag. AddTag(Entity, \"ThisIsATag\") . | Check if the number of rows on a table/collection equals a value. CountRows(Entity.OutgoingEdges) = 1 . | Set a golden record property to a value. SetEntityProperty(Entity, \"Encoding\", \"utf-8\") . | Get a golden record by identifier (previously known as entity code). If found, returns a golden record; otherwise, returns an error. LoadEntityByEntityCode(\"5452407DH\") . | . ",
    "url": "/management/rules/power-fx-formulas#power-fx-formula-examples",
    
    "relUrl": "/management/rules/power-fx-formulas#power-fx-formula-examples"
  },"1245": {
    "doc": "Power Fx formulas",
    "title": "Power Fx formulas",
    "content": " ",
    "url": "/management/rules/power-fx-formulas",
    
    "relUrl": "/management/rules/power-fx-formulas"
  },"1246": {
    "doc": "Quarantine",
    "title": "Quarantine",
    "content": "Quarantine is an area in CluedIn where records that do not meet certain conditions are sent during processing. These conditions are usually set in pre-process rules or property rules. In this article, you will learn how to manage quarantined records. Why is the record in quarantine? To find out why the record is in quarantine, select the View details icon in the Details column. The Reason pane will display the rule that led to the record’s quarantine. The value that led to the record’s quarantine is marked with the information icon. What happens to the record after it has been fixed and processed? The record disappears from the quarantine table. On the Process tab of the data set, you will find a new entry including the number of records and the processing status. However, keep in mind that the record remains in its original state on the Preview tab of the data set. What happens to the record if you reject it? The record disappears from the quarantine table. However, the record is not lost, it remains in its original state on the Preview tab of the data set. If you re-process the data set without changing the previous rules, the record will appear in the quarantine table again. To manage quarantined records . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set. | Go to the Quarantine tab. All records are displayed in the table with editable cells. | Find values that need to be fixed, and then make changes directly in the cells. | When you are satisfied with changes, process the records in one of the following ways: . | To process the records one by one, select the check mark in the Actions column. | To process specific records, select checkboxes in the first column for the needed records. You can also process all records by selecting the checkbox in the first column header. Then, in the upper right corner of the table, open the three-dot menu, and select Process. | . | If you do not want to send the records from quarantine to CluedIn, reject them in one of the following ways: . | To reject the records one by one, select the cross mark in the Actions column. | To reject specific records, select checkboxes in the first column for the needed records. You can also reject all records by selecting the checkbox in the first column header. Then, in the upper-right corner of the table, open the three-dot menu, and select Reject. | . Rejected records are removed from quarantine. | . ",
    "url": "/integration/additional-operations-on-records/quarantine",
    
    "relUrl": "/integration/additional-operations-on-records/quarantine"
  },"1247": {
    "doc": "Restore runbook",
    "title": "On this page",
    "content": ". | Typical persistent disks | Automation account | Scaledown | Input parameters | Process | Permissions | . The restore runbook can be triggered manually or on a predefined schedule. It handles the removal of all persistent disks and restores them from snapshots. This runbook is a PowerShell script, which CluedIn will provide as needed. Prerequisites . | An active or passive CluedIn cluster with a valid license | The runbook script | An automation account | A storage account | Sufficient permissions | . ",
    "url": "/paas-operations/automation/restore-runbook#on-this-page",
    
    "relUrl": "/paas-operations/automation/restore-runbook#on-this-page"
  },"1248": {
    "doc": "Restore runbook",
    "title": "Typical persistent disks",
    "content": "CluedIn configurations may vary, but a typical instance includes the nine disks as shown below. All must be restore from snapshots. ",
    "url": "/paas-operations/automation/restore-runbook#typical-persistent-disks",
    
    "relUrl": "/paas-operations/automation/restore-runbook#typical-persistent-disks"
  },"1249": {
    "doc": "Restore runbook",
    "title": "Automation account",
    "content": "An automation account must be provided. The runbook will be installed into the the automation account. Typically, the runbook should be triggered following a backup/copy event. ",
    "url": "/paas-operations/automation/restore-runbook#automation-account",
    
    "relUrl": "/paas-operations/automation/restore-runbook#automation-account"
  },"1250": {
    "doc": "Restore runbook",
    "title": "Scaledown",
    "content": "The runbook will optionally scale down the target CluedIn instance after the restore. ",
    "url": "/paas-operations/automation/restore-runbook#scaledown",
    
    "relUrl": "/paas-operations/automation/restore-runbook#scaledown"
  },"1251": {
    "doc": "Restore runbook",
    "title": "Input parameters",
    "content": "| Parameter | Default | Description | . | LicenseKey | required | License key tag on snapshot | . | Timestamp | required | Timestamp on snapshot | . | HostResourceGroup | required | Name of source resource group | . | Subscription | required | ID of target Azure subscription | . | TargetResourceGroup | required | Name of target resource group | . | TargetAKSClusterName | required | Name of target AKS cluster | . | StorageAccountName | required | Name of storage account | . | Scaledown | false | Option to scale down after a successful restore | . ",
    "url": "/paas-operations/automation/restore-runbook#input-parameters",
    
    "relUrl": "/paas-operations/automation/restore-runbook#input-parameters"
  },"1252": {
    "doc": "Restore runbook",
    "title": "Process",
    "content": ". ",
    "url": "/paas-operations/automation/restore-runbook#process",
    
    "relUrl": "/paas-operations/automation/restore-runbook#process"
  },"1253": {
    "doc": "Restore runbook",
    "title": "Permissions",
    "content": "The runbook must be granted the following permissions: . | Resource | Assigned Role(s) | Why This Role is Needed | . | AKS Resource Group | Reader | Required to read AKS configurations and metadata. | . | AKS Instance | Contributor | Required to scale AKS Instance and send aks command. | . | Snapshot Resource Group | Reader, Disk Snapshot Contributor | Required to read snapshots and creating/managing disk snapshots in the resource group. | . | AKS Node Resource Group | Reader, Disk Snapshot Contributor, VM Restore Contributor | Required to read list of disk, delete old disk, and restore new Disk from snapshot. | . | Storage Account Resource Group | Reader | Required to read Storage account configurations and metadata. | . | Storage Account | Storage Blob Data Contributor, Storage Account Key Operator Service Role | Required to store pod replica configuration during scaling down. | . ",
    "url": "/paas-operations/automation/restore-runbook#permissions",
    
    "relUrl": "/paas-operations/automation/restore-runbook#permissions"
  },"1254": {
    "doc": "Restore runbook",
    "title": "Restore runbook",
    "content": " ",
    "url": "/paas-operations/automation/restore-runbook",
    
    "relUrl": "/paas-operations/automation/restore-runbook"
  },"1255": {
    "doc": "Rules reference",
    "title": "On this page",
    "content": ". | Actions in data part rules | Actions in survivorship rules | Actions in golden record rules | . In this article, you will find reference information to help you understand the actions associated with each type of rule. ",
    "url": "/management/rules/rules-reference#on-this-page",
    
    "relUrl": "/management/rules/rules-reference#on-this-page"
  },"1256": {
    "doc": "Rules reference",
    "title": "Actions in data part rules",
    "content": "The following table contains a list of actions that you can use within data part rules. | Action | Description | . | Add Alias | Adds an alias to the record. You need to specify the alias that you want to add. | . | Add Tag | Adds a tag to the record. You need to specify the tag that you want to add. | . | Add Value | Adds a value to the vocabulary key. You can select the existing value or create a new value. Use this action when the vocabulary key doesn’t contain any value. | . | Add Value with CluedIn AI | Adds a value to the property or vocabulary key according to your prompt. For example, you can check if the email address in the record is a personal address or business address. | . | Change Business Domain | Changes the business domain of the record. You can select the existing business domain or create a new business domain. | . | Copy Value | Copies the value from one field (source field) to another (target field). | . | Delete Value | Deletes the value that you select. | . | Evaluate Regex | Finds values that match a regular expression and replaces them with the needed value. | . | Mask Value | Applies a mask to the value. You can use this action to hide sensitive data. Note that this action will be deprecated in future releases; therefore, use the mask action in access control policy rules instead. | . | Move Value | Moves the value from one field (source field) to another (target field). | . | Normalize Date | Converts the values of the vocabulary key to ISO 8601 format (YYYY-MM-DDT00:00:00+00:00). You can enter the culture or input format to tell CluedIn that you expect dates for the specified vocabulary key to be in a certain way. If you don’t enter the culture or input format, CluedIn will analyze the values and determine their date format on its own before converting them to ISO 8601 format. This action gives you more control over how dates are interpreted before they are converted to ISO 8601 format. Important! To use this action, the Date Time option must be disabled in Administration &gt; Settings &gt; Processing Property Data Type Normalization. For dates that have already been converted when the Date Time option was enabled, the Normalize Date rule action has no effect at all because the dates are already normalized. | . | Remove Alias | Removes alias from the record. You need to specify the alias that you want to remove. | . | Remove All Tags | Removes all tags from the record. | . | Remove line breaks | Removes line breaks from the value. By default, line breaks are replaced with spaces, but you can specify other replacement options. | . | Remove Tag | Removes a tag from the record. You need to specify the tag that you want to remove. | . | Replace | Replaces one value with another. You need to provide both the value you want to replace and its corresponding replacement value. | . | Set Value | Changes the value of the vocabulary key. You can select the existing value or create a new value. Use this action when the vocabulary key contains a value and you want to change this value to another one. | . | Set Value with CluedIn AI | Changes the value of the property or vocabulary key according to your prompt. | . | To CamelCase | Changes the value to camel case and removes all spaces. | . | To LowerCase | Changes the value to lower case. | . | To TitleCase | Changes the value to title case. | . | To UpperCase | Changes the value to upper case. | . | Trim WhiteSpace | Removes white space from the value. | . | Unmask Value | Removes the mask from the value. | . ",
    "url": "/management/rules/rules-reference#actions-in-data-part-rules",
    
    "relUrl": "/management/rules/rules-reference#actions-in-data-part-rules"
  },"1257": {
    "doc": "Rules reference",
    "title": "Actions in survivorship rules",
    "content": "The following table contains a list of actions that you can use within survivorship rules. | Action | Description | . | Best Quality | The value from the source with the highest quality wins. | . | Latest Manual | The value from the record that was the last to be modified in CluedIn wins. | . | Latest Modified Date | The value from the record that was the last to be modified in the external system wins. | . | Most Frequent Value | The value that occurs most often in the records. | . | Most Properties | The value from the record that has the most properties wins. | . | Oldest Modified Date | The value from the record that was the first to be modified in the external system wins. | . | Provider Definition | The value from the record that has the specified provider definition wins. | . ",
    "url": "/management/rules/rules-reference#actions-in-survivorship-rules",
    
    "relUrl": "/management/rules/rules-reference#actions-in-survivorship-rules"
  },"1258": {
    "doc": "Rules reference",
    "title": "Actions in golden record rules",
    "content": "The following table contains a list of actions that you can use within golden record rules. | Action | Description | . | Add Alias | Adds an alias to the record. You need to specify the alias that you want to add. | . | Add Tag | Adds a tag to the record. You need to specify the tag that you want to add. | . | Add Value | Adds a value to the vocabulary key. You can select the existing value or create a new value. Use this action when the vocabulary key doesn’t contain any value. | . | Add Value with CluedIn AI | Adds a value to the property or vocabulary key according to your prompt. For example, you can check if the email address in the record is a personal address or business address. | . | Change Business Domain | Changes the business domain of the record. You can select the existing business domain or create a new business domain. | . | Mask Value | Applies a mask to the value. You can use this action to hide sensitive data. Note that this action will be deprecated in future releases; therefore, use the mask action in access control policy rules instead. | . | Normalize Date | Converts the values of the vocabulary key to ISO 8601 format (YYYY-MM-DDT00:00:00+00:00). You can enter the culture or input format to tell CluedIn that you expect dates for the specified vocabulary key to be in a certain way. If you don’t enter the culture or input format, CluedIn will analyze the values and determine their date format on its own before converting them to ISO 8601 format. This action gives you more control over how dates are interpreted before they are converted to ISO 8601 format. Important! To use this action, the Date Time option must be disabled in Administration &gt; Settings &gt; Processing Property Data Type Normalization. For dates that have already been converted when the Date Time option was enabled, the Normalize Date rule action has no effect at all because the dates are already normalized. | . | Remove Alias | Removes alias from the record. You need to specify the alias that you want to remove. | . | Remove All Tags | Removes all tags from the record. | . | Remove Tag | Removes a tag from the record. You need to specify the tag that you want to remove. | . ",
    "url": "/management/rules/rules-reference#actions-in-golden-record-rules",
    
    "relUrl": "/management/rules/rules-reference#actions-in-golden-record-rules"
  },"1259": {
    "doc": "Rules reference",
    "title": "Rules reference",
    "content": " ",
    "url": "/management/rules/rules-reference",
    
    "relUrl": "/management/rules/rules-reference"
  },"1260": {
    "doc": "Search the data catalog",
    "title": "On this page",
    "content": "Searching the data catalog is a valuable tool for streamlining data discovery. This article provides information on how to make the process of finding metadata faster and more efficient by using filters. Vocabularies . By default, the All Vocabularies page displays only used vocabularies. These are the vocabularies that contain the vocabulary keys used in golden records. You can use the search box to find the vocabulary you need. Enter either the full name or a partial name of the vocabulary and start the search. All vocabularies that match your input will be displayed on the page. You can filter the vocabularies using the filter pane in the upper-right corner of the page. The following filters are available: . | Business domain – filters vocabularies based on their primary business domain. By default, all business domains are selected. To narrow down your search results, you can opt for a specific business domain. | Integrations – filters vocabularies based on their source. By default, all integrations are selected. To narrow your search results, you can select an option to display vocabularies that are not associated with a specific source, or you can select a specific source. | Usage – filters vocabularies based on their usage in golden records. By default, only used vocabularies are shown, and you have the option to display all, used, or unused vocabularies. | . If the filtered results do not meet your requirements, you can reset the filters and try again. Vocabulary keys . By default, the All Vocabular Keys page displays only used vocabulary keys. These are the vocabulary keys used in golden records. You can use the search box to find the vocabulary key you need. Enter either the full name or a partial name of the vocabulary key and start the search. All the vocabulary keys that match your input will be displayed on the page. You can filter the vocabulary keys using the filter pane in the upper-right corner of the page. The following filters are available: . | Data type – filters vocabulary keys based on their data type. By default, vocabulary keys of all types are displayed. To narrow down your search results, you can opt for a specific data type. | Classification – filters vocabulary keys based on their classification. By default, vocabulary keys of all classifications are displayed. To narrow down your search results, you can opt for a specific classification. | Integrations – filters vocabulary keys by their source. By default, all integrations are selected. To narrow your search results, you can select an option to display vocabulary keys that are not associated with a specific source, or you can select a specific source. | Usage – filters vocabulary keys based on their usage in golden records. By default, only used vocabulary keys are shown, and you have the option to display all, used, or unused vocabulary keys. | . If the search results do not meet your requirements, you can reset the filters and try again. ",
    "url": "/management/data-catalog/search-data-catalog#on-this-page",
    
    "relUrl": "/management/data-catalog/search-data-catalog#on-this-page"
  },"1261": {
    "doc": "Search the data catalog",
    "title": "Search the data catalog",
    "content": " ",
    "url": "/management/data-catalog/search-data-catalog",
    
    "relUrl": "/management/data-catalog/search-data-catalog"
  },"1262": {
    "doc": "Stream logs",
    "title": "Stream logs",
    "content": "In this article, you will learn how to ensure that your golden records have been successfully exported and how to identify issues when something goes wrong. CluedIn offers three types of logs to assist in monitoring and troubleshooting your streams: . | Golden record stream logs – view the streams that exported a specific golden record. | Stream logs – view golden records exported by a specific stream. | Export target health checks – view the health check status of the export target, updated every minute. | . Golden record stream logs . Each golden record contains the Streams tab, where you can find the stream that exported the golden record and the date it was sent to the export target. This page lists active and paused streams that exported the golden record. However, if the stream is stopped, it is not displayed on the Streams tab because all of its logs are cleared. You can select the stream to view its details. Stream logs . Each stream contains the Stream Log tab, where you’ll find all golden records that were exported by this stream. If the stream encounters an error while exporting golden records, it will be displayed on the page. This way, you can quickly identify and address any issues. You can filter the stream logs by two categories: . | Severity – the level of importance or urgency of an event or issue. Learn about standard severity levels here. | Area – the place in the streaming pipeline that produces logs: . | Stream ingestion log – logs coming from the stream. | Health check – logs coming from the export target assigned to the stream. When the health check status of the export target changes, for example, from Healthy to Unhealthy, a new log is added to the page. | Export target – other messages that the export target itself would like to log in the stream. | . | . This page displays logs only when the stream is active or paused. If you stop the stream, the logs are cleared. Export target health checks . Each export target contains the Health Checks tab, where you can find the health check status of the export target. CluedIn runs health checks for the export target every 60 seconds. If the export target encounters any issues, its health status will be marked as Unhealthy. If the export target becomes Unhealthy, the stream associated with that export target is stopped and the corresponding log is added to the Stream Log page as well. ",
    "url": "/consume/streams/stream-logs",
    
    "relUrl": "/consume/streams/stream-logs"
  },"1263": {
    "doc": "Use CluedIn's Microsoft Fabric Workload",
    "title": "Use CluedIn's Microsoft Fabric Workload",
    "content": "CluedIn provides a native Microsoft Fabric Workload that can take data in your Fabric environment and clean it through the CluedIn Clean application. You can have the processing of the cleaning happen in CluedIn, or, you can choose to run the processing in Fabric. Install the Fabric Workload . You can install the CluedIn Cleanse Workload by first asking support@cluedin.com to invite you to the Private Preview of this Workload. Once you have been invited, you can search and find the CluedIn Cleanse Workload in your Workload Hub. You will need to attach this Workload to a Workspace, or your entire Tenant - but do make sure you are attaching it to an environment that has capacity assocaited with it. Trial capacties are supported. Use the Fabric Workload . Requirements . Firstly, please follow the Microsoft Fabric documentation in order to know if you have the rights and authorization to be able to install Workloads in your Fabric environment. Secondly, you will need to bring your own CluedIn instance and pass the details of it into the Workload (as you can see in the video above). If you do not have a CluedIn instance, you can get a free PAAS instance from the Azure Marketplace. ",
    "url": "/microsoft-integration/fabric/use-cluedin-fabric-workload",
    
    "relUrl": "/microsoft-integration/fabric/use-cluedin-fabric-workload"
  },"1264": {
    "doc": "Access control",
    "title": "Access control",
    "content": "Access control gives you a fine-grained control over who can view and modify specific golden records. Together with source control, access control helps you configure reliable and secure access to golden records in CluedIn. For more information about the combination of source control and access control, see Data access. Limitations of access control . Access control policies are not applied to the data in deduplication projects, clean projects, hierarchy projects, and manual merges. The reason is that the users who are processing groups of duplicates, cleaning data, creating hierarchies, or doing manual merges must be able to view all values to adequately execute their tasks. When you enable access control for the first time, all users in your organization lose access to golden records. To restore their access, you must create access control policies and ensure users have permissions to the sources of the golden records. This section covers the following topics: . | Create access control policy – learn how to create and configure an access control policy. | Manage access control policies – learn how to edit, deactivate, and delete an access control policy, as well how these actions affect user access to data. | Access control reference – learn about the structure of an access control policy and the available actions in the policy rule. | . ",
    "url": "/management/access-control",
    
    "relUrl": "/management/access-control"
  },"1265": {
    "doc": "Post-installation guide",
    "title": "On this page",
    "content": ". | Configure custom DNS | Configure custom SSL certificates | Configure alerts | Configure logging | Set up SSO | Set up a backup solution | Results | Next steps | . At this point, you have a working, secure CluedIn environment. However, the default configuration provided by installation from the Azure Marketplace may not fit your organization’s security policy. This guide will help you understand how to configure CluedIn to meet your needs. ",
    "url": "/deployment/azure-marketplace/step-4#on-this-page",
    
    "relUrl": "/deployment/azure-marketplace/step-4#on-this-page"
  },"1266": {
    "doc": "Post-installation guide",
    "title": "Configure custom DNS",
    "content": "If you did not set up CluedIn PaaS with a custom DNS, you may experience issues accessing CluedIn with sslip.io when using public Wi-fi (e.g. hotel, conferences,…). In such situations, if possible, use your mobile internet (3G/4G/5G). To avoid interruption in the adoption of CluedIn, external DNS names are provided by the sslip.io service by default without any upfront configuration. The sslip.io service is a DNS service that returns the IP address when queried with a host name that contains an embedded IP address. The default DNS configuration ensures security by using the Automated Certificate Management Environment (ACME) protocol to issue SSL Certificates via the HTTP challenge method. For more information about certificates, see Configure certificates. If you want to set up custom DNS entries, see Configure DNS. ",
    "url": "/deployment/azure-marketplace/step-4#configure-custom-dns",
    
    "relUrl": "/deployment/azure-marketplace/step-4#configure-custom-dns"
  },"1267": {
    "doc": "Post-installation guide",
    "title": "Configure custom SSL certificates",
    "content": "By default, CluedIn installation is secured by using TLS. CluedIn uses the Automated Certificate Management Environment (ACME) protocol and the public Let’s Encrypt certificate authority to issue certificates. However, this default configuration might not comply with your organization’s security policy. If you want to use a Subject Alternative Name (SAN) or wildcard certificate for you domain, see Configure certificates. ",
    "url": "/deployment/azure-marketplace/step-4#configure-custom-ssl-certificates",
    
    "relUrl": "/deployment/azure-marketplace/step-4#configure-custom-ssl-certificates"
  },"1268": {
    "doc": "Post-installation guide",
    "title": "Configure alerts",
    "content": "By default, CluedIn contains built-in alerts that are sent to our support team. You can configure your own alerts in the Azure portal. For more information about alerts, see Configure alerts. ",
    "url": "/deployment/azure-marketplace/step-4#configure-alerts",
    
    "relUrl": "/deployment/azure-marketplace/step-4#configure-alerts"
  },"1269": {
    "doc": "Post-installation guide",
    "title": "Configure logging",
    "content": "CluedIn uses structured logging, and only the console sink is enabled by default. If you want to use another sink, see Configure logging. By default, your CluedIn containers are configured to log at the production level. The production log level allows you to view high-level information about the server and the tasks it is performing. The production log level provides an output with the following log entry types: . | INF – informational messages | WRN – system warnings | ERR – system errors | FTL – fatal system logs | . ",
    "url": "/deployment/azure-marketplace/step-4#configure-logging",
    
    "relUrl": "/deployment/azure-marketplace/step-4#configure-logging"
  },"1270": {
    "doc": "Post-installation guide",
    "title": "Set up SSO",
    "content": "CluedIn does not set up SSO directly out of the box. If you want to use SSO, see Configure SSO. ",
    "url": "/deployment/azure-marketplace/step-4#set-up-sso",
    
    "relUrl": "/deployment/azure-marketplace/step-4#set-up-sso"
  },"1271": {
    "doc": "Post-installation guide",
    "title": "Set up a backup solution",
    "content": "Currently, a backup solution is a post-install step because Microsoft does not support the ability to deploy backup policies within AMA. To set up a backup solution, see AMA backup and restore. ",
    "url": "/deployment/azure-marketplace/step-4#set-up-a-backup-solution",
    
    "relUrl": "/deployment/azure-marketplace/step-4#set-up-a-backup-solution"
  },"1272": {
    "doc": "Post-installation guide",
    "title": "Results",
    "content": "At this point, your CluedIn instance is up and running and configured according to your organization’s needs. ",
    "url": "/deployment/azure-marketplace/step-4#results",
    
    "relUrl": "/deployment/azure-marketplace/step-4#results"
  },"1273": {
    "doc": "Post-installation guide",
    "title": "Next steps",
    "content": "Review procedures for basic maintenance operations: . | Connect to CluedIn cluster using Azure Cloud Shell | . ",
    "url": "/deployment/azure-marketplace/step-4#next-steps",
    
    "relUrl": "/deployment/azure-marketplace/step-4#next-steps"
  },"1274": {
    "doc": "Post-installation guide",
    "title": "Post-installation guide",
    "content": " ",
    "url": "/deployment/azure-marketplace/step-4",
    
    "relUrl": "/deployment/azure-marketplace/step-4"
  },"1275": {
    "doc": "Approval",
    "title": "On this page",
    "content": ". | Source record approval flow | Configure source record approval . | Source record approval in data sets | Source record approval in manual data entry project | . | Review records pending approval | . In this article, you will learn how to implement source record approval, which allows you to approve or reject specific records and ensure that only verified records are sent into processing. You can configure source record approval for data sources of any type (file, ingestion endpoint, database) and for manual data entry projects. ",
    "url": "/integration/additional-operations-on-records/approval#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/approval#on-this-page"
  },"1276": {
    "doc": "Approval",
    "title": "Source record approval flow",
    "content": "When source record approval is configured for a data source, each record goes through several stages. When a non-owner user processes the data set or when a new ingestion appears for the data set that is in the auto-submission mode, CluedIn first checks if the record should be sent to quarantine. If yes, the record is sent to quarantine and does not move forward in the processing pipeline. When a quarantined record is approved, it moves to the next stage where CluedIn checks if the record requires approval. If yes, the record is sent to the approval area and does not move forward in the processing pipeline. When the record is approved, it moves to processing, resulting in the creation of a new golden record or aggregation to an existing golden record. After source record approval, the record does not go back to any previous stages in the processing pipeline; it goes straight into processing. Difference between quarantine and approval . When the record is in quarantine, you can edit it and fix some data quality issues. When the record is in the approval area, you cannot edit it; you can only approve or reject it. ",
    "url": "/integration/additional-operations-on-records/approval#source-record-approval-flow",
    
    "relUrl": "/integration/additional-operations-on-records/approval#source-record-approval-flow"
  },"1277": {
    "doc": "Approval",
    "title": "Configure source record approval",
    "content": "In this section, you will learn how to configure source record approval in data sets and manual data entry projects. Only owners can configure source record approval. Source record approval in data sets . Source record approval is particularly useful for data sets created via the ingestion endpoint. If you have already completed the initial full data load and now want to ingest only delta records on a daily basis, the approval mechanism can help you ensure that only verified delta records are processed. The owners of the data source can then review such delta records and decide whether they should be processed. You can configure source record approval using one of the following methods: . | Property rules – use to define the conditions for sending the source record for approval based on the property values of the record. | Pre-process rules – use to define the conditions for sending the source record for approval based on the whole record, and not its specific property values. | Advanced mapping code – use to define complex logic for sending the source record for approval. | . To configure source record approval in a data set . | On the navigation pane, go to Ingestion &gt; Sources. | Find and open the data source and its data set for which you want to configure source record approval. | Define the rules for sending the source records for approval in one of the following ways: . | By adding a property rule: . | Go to the Map tab, and then select Edit mapping. Make sure you are on the Map columns to vocabulary key tab. | Find the vocabulary key by which you want to determine whether the record should be sent for approval. Then, next to this vocabulary key, select Add property rule. | In the Filter section, select whether you want to apply the rule to all values of the records (Apply action to all values) or to specific values (Add filter to apply action). For example, you might want to send the record for approval if the vocabulary key contains a specific value. | In the Action dropdown list, select Require approval. | Select Add Rule. | . | By adding a pre-process rule: . | Go to the Pre-process Rules tab, and then select Add Pre-process Rule. | Enter the name of the rule. | In the Filter section, select whether you want to apply the rule to all records (Apply action to all values) or to specific records (Add filter to apply action). | In the Action dropdown list, select Require approval. | Select Add Rule. | . | By writing advanced mapping code: . | Go to the Map tab, and then select Advanced. | On the left side of the page, write the code to define the conditions for sending the source records for approval. You can write any JavaScript code. | Select Save. | . | . | Go to the Process tab, and make sure that Auto-submission is enabled. This way only delta records will be sent for approval, rather than all records from the Preview tab. Now, when delta records are ingested to CluedIn and they meet the conditions in pre-process rules, property rules, or advanced mapping, a new entry appears on the Process tab, informing that these records have been sent for approval. Additionally, the owners of the data source will receive a notification in CluedIn about the records pending approval. The owners of the data source can now review these records and approve or reject them. | . Source record approval in manual data entry project . Source record approval is useful for manual data entry projects as it grants project owners full control over the records created in the project. The owners of the manual data entry project can review new records added by non-owner users and decide whether they should be processed and turned into golden records. To configure source record approval for manual data entry . | On the navigation pane, go to Ingestion &gt; Manual Data Entry. | Find and open the manual data entry project for which you want to configure source record approval. | In the upper-right corner, select Edit. | In the Require records approval section, select the checkbox to enable the approval mechanism. | Select Save. Now, when a non-owner user adds a new record in the manual data entry project, you will receive a notification about a record pending your approval. You can then review such record and approve or reject it. | . ",
    "url": "/integration/additional-operations-on-records/approval#configure-source-record-approval",
    
    "relUrl": "/integration/additional-operations-on-records/approval#configure-source-record-approval"
  },"1278": {
    "doc": "Approval",
    "title": "Review records pending approval",
    "content": "To review source records pending approval, you require the following: . | You should be added to the Permissions tab of the data source. This is necessary to view source records. For more information about access to data, see Source control. | You should be added to the Owners tab of the data set. This is necessary to approve or reject source records. For more information about the right to approve and reject, see Ownership. | . If you have the required permissions to the data source and you are among the owners of the data source, you will receive a notification every time the records pending your approval appear in CluedIn. Additionally, you will receive a daily email summarizing the records that require your approval. To track the records that require your approval, on the navigation pane, go to Home &gt; My Tasks. Here, you will find two tables: . | Data set records pending review – this table allows you to track the number of records per data set that are in quarantine or require approval. The table contains information about the data sources for which you are the owner. | Manual data entry project records pending review – this table allows you to track the number of records per manual data entry project that require approval. The table contains information about the manual data entry projects for which you are the owner. | . To review records pending approval . | Find the records pending approval by doing one of the following: . | In the notification about records being sent for approval, select View. | In the daily email about records pending approval, select the number of records that require your approval. | In the Data set records pending review or Manual data entry project records pending review table, select the number of records that require your approval. As a result, the Approval tab of the data set opens. | . | On the Approval tab, review the records. To find the reason why a record has been sent for approval, select the icon in the Details column. Then, do one of the following: . | If the source record should be processed, select the check mark in the Actions column for this record. You can approve records one by one, or you can approve them in bulk. To approve the records in bulk, select checkboxes in the first column for the needed records. Then, open the three-dot menu, and select Approve clues. To approve all records on the page, select the checkbox in the first column header. Then, open the three-dot menu, and select Approve clues. Keep in mind that this action approves only the records that are on the page, not all records on all the other pages. Every time you approve a record or a bunch of records, the corresponding entry is added to the Process tab meaning that these records have now been processed. | If the source record is invalid, wrong, and should not be processed, select the cross mark in the Actions column for this record. You can reject records one by one, or you can reject them in bulk. To reject the records in bulk, select checkboxes in the first column for the needed records. Then, open the three-dot menu above and select Reject clues. To reject all records on the page, select the checkbox in the first column header. Then, open the three-dot menu, and select Reject clues. Keep in mind that this action rejects only the records that are on the page, not all records on all the other pages. To reject all records, open the three-dot menu and select Reject all clues. | . | . ",
    "url": "/integration/additional-operations-on-records/approval#review-records-pending-approval",
    
    "relUrl": "/integration/additional-operations-on-records/approval#review-records-pending-approval"
  },"1279": {
    "doc": "Approval",
    "title": "Approval",
    "content": " ",
    "url": "/integration/additional-operations-on-records/approval",
    
    "relUrl": "/integration/additional-operations-on-records/approval"
  },"1280": {
    "doc": "Azure Service Bus connector",
    "title": "Azure Service Bus connector",
    "content": "This article outlines how to configure the Azure Service Bus connector to publish data from CluedIn to Service Bus. Prerequisites: Make sure you have an existing Service Bus namespace with a specific queue where you want to store the data from CluedIn. The namespace must have the RootManageSharedAccessKey policy with Manage, Send, and Listen access. In addition, the queue must have a policy with Manage, Send, and Listen access. To configure Azure Service Bus connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Azure Service Bus Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | Connection String – connection string to the Service Bus namespace or to the specific queue where you want to store the data from CluedIn. To get the connection string to the Service Bus namespace, follow this instruction from Microsoft. The following image shows an example of connection string for the namespace. Alternatively, you can use the connection string to the specific queue. | Queue Name – name of the queue in the Service Bus namespace where you want to store the data from CluedIn. If you provided the connection string to the namespace in step 2b, you can leave the Queue Name field empty. When configuring the export target for the stream, you’ll need to provide the name of the queue as the Target name. If you provided the connection string to the queue in step 2b, you can leave the Queue Name field empty since the connection string contains the queue name. | . | Test the connection to make sure it works, and then select Add. Now, you can select the Azure Service Bus connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/azure-service-bus-connector",
    
    "relUrl": "/consume/export-targets/azure-service-bus-connector"
  },"1281": {
    "doc": "Best practices for rules",
    "title": "On this page",
    "content": ". | Example 1 | Example 2 | Summary | . In this article, you will learn about best practices for data part and golden record rules. In particular, we will examine the case of tagging data quality issues and why golden record rules are the preferred way to do this. Generally, use data part rules for fixing data and golden record rules for tagging data quality issues. Next, we will provide two examples to illustrate this recommendation. ",
    "url": "/management/rules/best-practices-for-rules#on-this-page",
    
    "relUrl": "/management/rules/best-practices-for-rules#on-this-page"
  },"1282": {
    "doc": "Best practices for rules",
    "title": "Example 1",
    "content": "Suppose you want to tag golden records that contain invalid emails with “Invalid email”. Let’s start from a golden record that is built from one data part from the CRM source. The data part contains an email property with invalid value (jsmith&amp;cluedin.com). At this point, it does not matter if you use a data part rule or a golden record rule to tag records with invalid email because the golden record contains just one data part. If we use a data part rule, the golden record that contains invalid email will be tagged with “Invalid email”. As long as the golden record contains just one data part, the above setup is correct. However, what if the golden record contains multiple data parts, each with its own email property? Suppose we add another data part from a different source (ERP) to the golden record. This data part also contains the email property, but it has a valid value (jsmith@cluedin.com). At this point, we also add a survivorship rule that tells CluedIn to use the property from the ERP source in the golden record. As a result, the golden record contains a valid email from the data part of the ERP source, but it is still tagged with “Invalid email”. This is because the data part rule is applied to each data part of the golden record, even if the properties from a specific data part are not used in the golden record due to a survivorship rule. To avoid irrelevant tags in golden records, use golden record rules for tagging data quality issues. The golden record rules take into account only the properties of the golden record, not the properties of its specific data parts. As a result, the “Invalid email” tag is not added to the golden record because the email property used in the golden record is valid. ",
    "url": "/management/rules/best-practices-for-rules#example-1",
    
    "relUrl": "/management/rules/best-practices-for-rules#example-1"
  },"1283": {
    "doc": "Best practices for rules",
    "title": "Example 2",
    "content": "For this example, we will start with a data part rule that targets data parts that from a particular source—CRM. The data part contains an email property with invalid value (jsmith&amp;cluedin.com). This is the only data part in the golden record, so the golden record is tagged with “Invalid email”. Now that we’ve identified an invalid email, we can add a data part rule to replace invalid email with a valid one in data parts from the CRM source. When the email in the data part is replaced with a valid value, the previous data part rule that tagged invalid emails becomes irrelevant and the “Invalid email” tag disappears from the golden record. Next, we add another data part from a different source—ERP. This data part contains invalid email (jsmith&amp;cluedin.com). In this example, we are not using a custom survivorship rule. Instead, we rely on default survivorship mechanism, which picks the most recent value for the golden record. Since the ERP data part appeared after the CRM data part, it is used in the golden record. As a result, the golden record contains invalid email address, but it is not tagged with “Invalid email”. This is because the data part is applied only to data parts from the CRM source, it is not targeted at data parts from the ERP source. To make sure that invalid emails are flagged regardless of the source, use a golden record rule instead of the data part rule. The golden record rule is executed based on the properties of the golden record. As a result, the “Invalid email” tag is added to the golden record because the email property used in the golden record is invalid. ",
    "url": "/management/rules/best-practices-for-rules#example-2",
    
    "relUrl": "/management/rules/best-practices-for-rules#example-2"
  },"1284": {
    "doc": "Best practices for rules",
    "title": "Summary",
    "content": "To sum up, if you want to flag data quality issues, use golden record rules. This way, you can be sure that the tag is added based on the end values that are used in your source of truth—a golden record. For more information, see our article How to tag records with data quality issues. ",
    "url": "/management/rules/best-practices-for-rules#summary",
    
    "relUrl": "/management/rules/best-practices-for-rules#summary"
  },"1285": {
    "doc": "Best practices for rules",
    "title": "Best practices for rules",
    "content": " ",
    "url": "/management/rules/best-practices-for-rules",
    
    "relUrl": "/management/rules/best-practices-for-rules"
  },"1286": {
    "doc": "Clearbit",
    "title": "On this page",
    "content": ". | Add Clearbit enricher | Properties from Clearbit enricher | . This article outlines how to configure the Clearbit enricher. Clearbit is a marketing data engine that provides tools for data enrichment, lead generation, marketing intelligence, and more. The purpose of Clearbit enricher is to retrieve company logo and domain information. The Clearbit enricher supports the following endpoint: . | https://autocomplete.clearbit.com/v1/companies/suggest?query= | . ",
    "url": "/preparation/enricher/clearbit#on-this-page",
    
    "relUrl": "/preparation/enricher/clearbit#on-this-page"
  },"1287": {
    "doc": "Clearbit",
    "title": "Add Clearbit enricher",
    "content": "The enricher requires at least one of the following attributes to search for company logo and domain: . | Website – if your golden records have websites, you can enter the corresponding vocabulary key to configure the enricher. As a result, the enricher will use the website to search for domain and logo information. If your golden records do not have websites, then one of the other attributes will be used for search. | Organization Name – if your golden records have organization names, you can enter the corresponding vocabulary key to configure the enricher. As a result, the enricher will use the organization name to search for domain and logo information. If your golden records do not have organization names, then one of the other attributes will be used for search. | Email Domain – if your golden records have email domain names, you can enter the corresponding vocabulary key to configure the enricher. As a result, the enricher will use the email domain name to search for domain and logo information. If your golden records do not have email domain names, then one of the other attributes will be used for search. | . To add the Clearbit enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Clearbit, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Website Vocabulary Key – enter the vocabulary key that contains company websites that will be used to search for company domain and logo. | Organization Name Vocabulary Key – enter the vocabulary key that contains company names that will be used to search for company domain and logo. | Email Domain Vocabulary Key – enter the vocabulary key that contains company email domains that will be used to search for company domain and logo. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Clearbit enricher is added and has the active status. It means that it will enrich relevant golden records when they are processed or when your trigger external enrichment. | . After the Clearbit enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/clearbit#add-clearbit-enricher",
    
    "relUrl": "/preparation/enricher/clearbit#add-clearbit-enricher"
  },"1288": {
    "doc": "Clearbit",
    "title": "Properties from Clearbit enricher",
    "content": "To quickly find the properties added to golden records from the Clearbit enricher, use the integrations filter on the Properties page. For a more detailed information about the changes made to a golden record by the Clearbit enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Clearbit enricher. | Display name | Vocabulary key | . | Domain | clearbit.organization.domain | . | Logo | clearbit.organization.logo | . ",
    "url": "/preparation/enricher/clearbit#properties-from-clearbit-enricher",
    
    "relUrl": "/preparation/enricher/clearbit#properties-from-clearbit-enricher"
  },"1289": {
    "doc": "Clearbit",
    "title": "Clearbit",
    "content": " ",
    "url": "/preparation/enricher/clearbit",
    
    "relUrl": "/preparation/enricher/clearbit"
  },"1290": {
    "doc": "Logging",
    "title": "On this page",
    "content": ". | Overview of logging . | Log levels | Log format and examples | . | Apply your log level | Admin UIs . | Neo4j | RabbitMQ | Redis | ElasticSearch | SQL Server | . | . In this article, you will get an overview of the logging options and learn how to configure the logging level that you need. ",
    "url": "/deployment/infra-how-tos/configure-logging#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-logging#on-this-page"
  },"1291": {
    "doc": "Logging",
    "title": "Overview of logging",
    "content": "CluedIn uses structured logging. You can configure any sink, but only 3 sinks have been tested with the application: console, Seq, and Azure Application Insights. Console . This sink is enabled by default. Seq . By default, the Seq endpoint is protected with an OAuth2 proxy. To enable Seq, add the seq image that you want to use. seq: image: datalust/seq . You can access Seq using port-forwarding. Alternatively, you can enable an ingress route. seq: public_endpoint: /seq . Azure Application Insights . Add the key to the Application Insights instance that you want to use. logging: appInsightsKey: 'your-app-instance-key-guid' . By default, Azure Application Insights sends full telemetry of the front-end application. All the logs from the CluedIn server are sent as trace information. Log levels . By default, your CluedIn server containers are configured to log at the production level. The production-level log provides high-level information about the server and the tasks it is performing. The production-level log can include log message types as described in the following table. | Log message type | Description | . | INF | Informational messages | . | WRN | System warnings | . | ERR | System errors | . | FTL | Fatal system logs | . You can adjust the verbosity of the log messages that your system generates. To do that, change the value of the ASPNETCORE_ENVIRONMENT variable to one of the following values: . | Production | Development or debug | Verbose or trace | . If you change the value of ASPNETCORE_ENVIRONMENT to development or debug, you will see log messages of the DBG type in addition to the four default types (INF, WRN, ERR, FTL). If you need a more granular level of detail, set the value of ASPNETCORE_ENVIRONMENT to verbose or trace. As a result, the VRB type will be added in addition to DBG and the four default types. Log format and examples . By default, CluedIn provides logs to the console in the following format: [#{ThreadId:000} {Timestamp:HH:mm:ss} {Level:u3}][{SourceContext}] {Message:lj}{NewLine}{Exception} . Examples of log messages are provided in the following table. | Log message type | Example | . | Information log message created by thread 001 at 11:38 | [#001 11:38:53 INF] Operating System: Unix 5.15.0.58 | . | Development/debug log message | [#001 10:36:35 DBG] [ComponentHost] : Starting Metrics | . | Verbose/trace log message | [#015 10:42:11 VRB][CluedIn.Core.ExecutionContext] Operation GetByEntityCode (/Organization#CluedIn xxxxx-XXXX-xxxx-xxxx) : 5475 | . ",
    "url": "/deployment/infra-how-tos/configure-logging#overview-of-logging",
    
    "relUrl": "/deployment/infra-how-tos/configure-logging#overview-of-logging"
  },"1292": {
    "doc": "Logging",
    "title": "Apply your log level",
    "content": "The following procedure shows how to get the current cluedin-server config map, edit the configuration, and apply the configuration to your Kubernetes cluster. The example below uses a target namespace called cluedin. You may need to change the namespace to fit your implementation. To apply your log level . | Get the current configuration by running the following command: . kubectl get configmap cluedin-server -o yaml &gt; cluedin-server.yaml --namespace cluedin . This command downloads the current cluedin-server config map into a local cluedin-server.yaml file. | Open the downloaded file in the text editor of your choice. | Change the value of ASPNETCORE_ENVIRONMENT to your required log level. You can use one of the following values: production, development, debug, verbose, or trace. apiVersion: v1 Data: ASPNETCORE_ENVIRONMENT: debug . | Apply the changed values from the local cluedin-server.yaml file to your Kubernetes cluedin-server config map by running the following command: . kubectl apply -f cluedin-server.yaml --namespace cluedin . | . After you apply the values, they won’t become active until the pod is restarted. This is because the values are applied during the pod startup process. After the required pod is restarted, you should see additional log types in your logging target or in the pod logs. ",
    "url": "/deployment/infra-how-tos/configure-logging#apply-your-log-level",
    
    "relUrl": "/deployment/infra-how-tos/configure-logging#apply-your-log-level"
  },"1293": {
    "doc": "Logging",
    "title": "Admin UIs",
    "content": "For debugging purposes, it is helpful to be able to log in to some of the tools/dependencies used by CluedIn. The easiest way to do this is to set up a proxy using a machine that has kubectl configured to access the cluster. You can use the following tools: . | Neo4J | RabbitMQ | Redis | ElasticSearch | SQL Server | . In the following statements, &lt;name-of-release&gt; is how you named your Helm deployment. You can see the list of releases using helm list. If you want to use several tools simultaneously, you can proxy several ports at the same time. The port-forward command used to set up the proxy will remain running. The proxy will be available until you stop the port-forward command. Neo4j . Neo4J is a graph database used to store the relationships between entities. To connect to Neo4j via port forwarding . | Run the following command: kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=neo4j') 7474 7687 . | Point your browser to localhost:7474 . | . RabbitMQ . RabbitMQ is a messaging bus. To connect to RabbitMQ via port forwarding . | Run the following command: kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=rabbitmq') 15672 . | Point your browser to localhost:15672 . | . Redis . Redis is a storage of cache and key-value pairs. To connect to Redis via port forwarding . | Run the following command: powershell kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=redis') 6379 . | Set up the front end for Redis on your computer using Docker. To do that, run the following command: docker run --rm -p 8081:8081 -e REDIS_HOSTS=local:host.docker.internal:6379 rediscommander/redis-commander . | Point your browser to localhost:8081 | . ElasticSearch . ElasticSearch is a search index. To connect to ElasticSearch via port forwarding . | Run the following command: kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=elasticsearch') 9200 . | Point your browser to localhost:9200/_plugin/inquisitor/#/ | . SQL Server . SQL Server is relational database. To connect to SQL Server via port forwarding . | Depending on the tool that you use, retrieve the password by doing one of the following actions: . | If you are using bash shell, run the following command: kubectl get secret &lt;release-name&gt;-cluedin-sql-password -o jsonpath=\"{.data.SA_PASSWORD}\" | base64 --decode . | If you are using PowerShell, run the following command: [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String($(kubectl get secret &lt;release-name&gt;-cluedin-sql-password -o jsonpath=\"{.data.SA_PASSWORD}\"))) . | . | To expose the port locally using regular Kubernetes port-forwarding, run the following command: kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=sqlserver') 1433 | . Then, you can use either Visual Studio or the MS SQL Management Studio to connect to the database on localhost. If there is already a SQL Server instance on your machine, there will be a port clash. If there is a conflict with the existing open ports on your machine, you can map the port to a different local port. To do that, use the following syntax: kubectl port-forward &lt;pod&gt; &lt;local-port&gt;:&lt;remote-port&gt; . ",
    "url": "/deployment/infra-how-tos/configure-logging#admin-uis",
    
    "relUrl": "/deployment/infra-how-tos/configure-logging#admin-uis"
  },"1294": {
    "doc": "Logging",
    "title": "Logging",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-logging",
    
    "relUrl": "/deployment/infra-how-tos/configure-logging"
  },"1295": {
    "doc": "Stream data",
    "title": "On this page",
    "content": ". | Set up an export target | Create a stream | Results &amp; next steps | . Streaming data from CluedIn involves setting up an export target and creating a stream. In this guide, you will learn how to stream your records from CluedIn to a Microsoft SQL Server database. Before you start: Make sure you have completed all steps in the Ingest data guide. ",
    "url": "/getting-started/data-streaming#on-this-page",
    
    "relUrl": "/getting-started/data-streaming#on-this-page"
  },"1296": {
    "doc": "Stream data",
    "title": "Set up an export target",
    "content": "An export target is a place where you can send the data out of CluedIn after it has been processed. In the following procedure, we are going to establish a connection with a Microsoft SQL Server database to use it as an export target. To set up an export target . | On the navigation pane, go to Consume &gt; Export Targets. | Select Add Export Target. | On the Choose Target tab, find and select Sql Server Connector. Then, select Next. | On the Configure tab, enter the database connection details such as Name, Host, Database Name, Username, and Password. Optionally, you may add Port Number, Schema, and Connection pool size. | Select Test Connection. After you receive a notification that the connection is successful, select Add. The export target is added. Now, you can create a stream. | . ",
    "url": "/getting-started/data-streaming#set-up-an-export-target",
    
    "relUrl": "/getting-started/data-streaming#set-up-an-export-target"
  },"1297": {
    "doc": "Stream data",
    "title": "Create a stream",
    "content": "A stream is a trigger that starts the process of sending the data to the export target. In the following procedure, we are going to configure the stream and define the records that will be streamed to a Microsoft SQL Server database. To create a stream . | On the navigation pane, go to Consume &gt; Streams. | Select Create Stream. | Enter the Stream Name, and then select Create. The stream details page opens. | On the Configuration tab, in the Filters section, select Add First Filter, and then specify what data you want to share. The fields for configuring a filter appear one by one. After you complete the previous field, the next field appears. | Select Save. To view the records that match stream filters, go to the Preview Condition tab. | Go to the Export Target Configuration tab. | On the Choose Connector tab, select the Sql Server connector, and then select Next. | On the Connector Properties tab, do the following: . | Enter the Target Name. This will be the name of the table in the database. | Select the Streaming Mode. Two streaming modes are available: . | Synchronized stream – the database and CluedIn contain the same data that is synchronized. For example, if you edit the record in CluedIn, the record is also edited in the database. | Event log stream – every time you make a change in CluedIn, a new record is added to the database instead of replacing the existing record. | . | . | On the Properties to Export tab, click Auto-select. All vocabulary keys associated with the records in the strem filter will be displayed on the page. | Select Save. To view the data that will be sent to the database, go to the Data tab. | Start the stream by selecting the play button. Then, confirm that you want to start the stream. The Streaming and Exporting labels appear under the stream name, indicatig that the stream is active. On the Monitoring tab, you can view different data performance metrics. | Go to the database and open the table. The records have been sent to the database. | . ",
    "url": "/getting-started/data-streaming#create-a-stream",
    
    "relUrl": "/getting-started/data-streaming#create-a-stream"
  },"1298": {
    "doc": "Stream data",
    "title": "Results &amp; next steps",
    "content": "After completing all steps outlined in this guide, you learned how to configure the connection to a Microsoft SQL Server database and how to send data from CluedIn to the database. If you make any changes to the records in CluedIn, they will be automatically updated in the database. For example, if you fix the job title values in CluedIn, they will be automatically corrected in the database. Next, learn how to create rules for automating data transformation and capturing data quality issues in the Create rules guide. ",
    "url": "/getting-started/data-streaming#results--next-steps",
    
    "relUrl": "/getting-started/data-streaming#results--next-steps"
  },"1299": {
    "doc": "Stream data",
    "title": "Stream data",
    "content": " ",
    "url": "/getting-started/data-streaming",
    
    "relUrl": "/getting-started/data-streaming"
  },"1300": {
    "doc": "Data types",
    "title": "Data types",
    "content": "In this article, you will find reference information about data types that can be used in vocabulary keys. Specifying the data type correctly facilitates the use of filter operations for more precise filter results. Each data type is associated with a specific storage type that defines how the data is stored: . | Keyword – represents the storage of data in text format. Storing data as a keyword doesn’t allow you to query properties using numeric and date operators such as Between, Equals, Greater or Equal. | Typed – represents the storage of data in its native form. The Typed storage is selected by default for data types that can be typable. The Typed storage allows you to query properties using numeric and date operators such as Between, Equals, Greater or Equal. | Untyped – represents the storage of typable data in text format. If you don’t want to query properties using numeric and date operators, then you can change the storage from Typed to Untyped. | . The following table provides the description of data types along with the type of storage that applies to each data type. | Data type | Description | Storage | . | Text | A Unicode text string. | Keyword | . | DateTime | A date with a time, in the time zone of your computer. If no time zone is specified, then UTC is assumed when we try to normalize* the date. | Typed | . | Time | A time without a date, in the time zone of your computer | Typed | . | Duration | A span of time. It is used to store and manipulate time intervals, which could be measured in seconds, minutes, hours, days, or other units of time. | Typed | . | Boolean | A true or false value. Data will only be strong-typed if it is stored as the values “true” or “false”. For all other values such as “yes”, “no”, “1”, “0” and so on, the data must be normalized using a rule or a clean project. | Typed | . | Integer | A whole number without any fractional or decimal part. | Typed | . | Number | Any numeric value, including integers and numbers with decimal parts. | Typed | . | Uri | A Universal Resource Identifier (URI) text string to an image. | Keyword | . | Guid | A Globally Unique Identifier. | Keyword | . | Email | An email address. | Keyword | . | PhoneNumber | A phone number. | Keyword | . | TimeZone | A geographical region’s standard time offset from UTC or a specific named time zone, such as those defined by the IANA Time Zone Database. | Keyword | . | GeographyCity | A name of a city. | Keyword | . | GeographyState | A name of a state, county, or province. | Keyword | . | GeographyCountry | A name of a country in ISO code. | Keyword | . | GeographyCoordinates | A coordinate represented as a latitude and longitude, or other coordinate system. Can also be used for individual latitude or longitude properties. | Keyword | . | GeographyLocation | A generic location such as “Street 1, Apt 2-B”. | Keyword | . | Json | An object or object graph represent in JSON format. | Keyword | . | Xml | An object or object graph represent in XML format. | Keyword | . | Html | A page or snippet of HTML. | Keyword | . | IPAddress | An IP Address in either v4 or v6 format. | Keyword | . | Color | A color specification. | Keyword | . | Money | A monetary value. | Typed | . | Currency | Represents currency values. | Keyword | . | PersonName | A name of a person. | Keyword | . | OrganizationName | A name of an organization. | Keyword | . | Identifier | An identifier representing the key of a record, normally as a GUID or Integer. | Keyword | . | Lookup | A custom list of possible values, which is defined with a glossary. | Keyword | . *Date normalization occurs when the Date Time option is enabled in Administration &gt; Settings &gt; Processing Property Data Type Normalization. In this case, CluedIn analyzes the incoming date format and converts it to ISO 8601 format (YYYY-MM-DDT00:00:00+00:00). If you want to instruct CluedIn how to interpret dates before converting them to ISO 8601 format, create a rule with the Normalize Date action. ",
    "url": "/management/data-catalog/data-types",
    
    "relUrl": "/management/data-catalog/data-types"
  },"1301": {
    "doc": "Email",
    "title": "Email",
    "content": "To receive email notifications and send invites to new CluedIn users, you need an email address and credentials (username and password) to the email server. We discuss the email configuration in the Helm section. ",
    "url": "/deployment/azure/email",
    
    "relUrl": "/deployment/azure/email"
  },"1302": {
    "doc": "Event Hub Integration",
    "title": "Event Hub Integration",
    "content": "Azure Event Hub integration enables the transmission of workflow events from CluedIn to your event hub. These events include various actions, among which are the following: . | Adding a data source, updating a data source . | Adding a business domain . | Adding a role . | Adding a stream, updating a stream, changing stream state . | Adding a vocabulary, updating a vocabulary key . | Creating a clean project, generating the results of the clean project, committing the results of the clean project . | Adding a glossary category, adding a glossary term, updating a glossary term . | Registering an export target . | Creating a rule, changing the rule state . | Publishing a hierarchy . | Creating a deduplication project . | . Essentially, any task featuring a progress bar in CluedIn is classified as a workflow event because it allows us to track the workflow’s start and completion times. To connect CluedIn to Event Hub . | Create an event hub following the steps from Microsoft tutorial. | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the EventHub section. | Provide the Connection String to your event hub. To find the connection string: . | In your event hub, on the Event Hubs Namespace page, select Shared Access Policies on the left menu. | Select a shared access policy in the list of policies. The default one is named RootManageSharedAccessPolicy. You can add a policy with appropriate permissions, and use that policy. | Select the copy button next to the Connection string-primary key field. For more details, see Get Event Hubs connection string. | . | Enter the Name of your event hub that you created in step 1. | Choose the workflow events that you want to send from CluedIn to event hub. Enabling Send Events will automatically activate all events, while disabling it will deactivate all other events accordingly. Enabling Include Origin Host Information will send host details such as the pod’s name, its allocated memory, and its IP address. This feature is useful for diagnostic purposes. | . After you complete all the required fields in CluedIn settings and enable the necessary features, you’ll begin receiving incoming messages on the Event Hubs Namespace page in the Azure portal. After at least one event has been generated, it will be displayed in the Event Hub section of CluedIn settings. All generated events will dynamically appear there. You can also connect VS Code to your event hub to see the XML payload. The following screenshot shows an example of the message you will receive. ",
    "url": "/microsoft-integration/event-hub-integration",
    
    "relUrl": "/microsoft-integration/event-hub-integration"
  },"1303": {
    "doc": "Exporting data to and from CluedIn with Python",
    "title": "On this page",
    "content": ". | CluedIn | Databricks . | Install dependencies | Import libraries | Connect to CluedIn | Explore data | Create schema | . | . In this article, we will load data from CluedIn to a Databricks notebook, do basic data exploration and transformation, and save data in a Delta Lake table. The same approach can be used with any other data destination like Microsoft Fabric, Azure Synapse Analytics, Snowflake, or any other data destination that can run Python code. ",
    "url": "/playbooks/data-engineering-playbook/export#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/export#on-this-page"
  },"1304": {
    "doc": "Exporting data to and from CluedIn with Python",
    "title": "CluedIn",
    "content": "Our CluedIn instance has 601_222 entities of /IMDb/Title. To load them in Databricks, we need to create an API token in CluedIn. Go to Administration &gt; API Tokens and create a new token: . ",
    "url": "/playbooks/data-engineering-playbook/export#cluedin",
    
    "relUrl": "/playbooks/data-engineering-playbook/export#cluedin"
  },"1305": {
    "doc": "Exporting data to and from CluedIn with Python",
    "title": "Databricks",
    "content": "Install dependencies . To connect to CluedIn API, we need to install the cluedin library. %pip install cluedin==2.2.0 . Import libraries . We will need the following libraries: . import pandas as pd import matplotlib.pyplot as plt import cluedin . Connect to CluedIn . To connect to CluedIn, we need to provide the URL of our CluedIn instance and the API token we created earlier: . # CluedIn URL: https://foobar.mycluedin.com/: # - foobar is the organization's name # - mycluedin.com is the domain name cluedin_context = { 'domain': 'mycluedin.com', 'org_name': 'foobar', 'access_token': '(your token)' } . Now, let’s pull some data from CluedIn. We will fetch only one row to see what data we have: . # Create a CluedIn context object. ctx = cluedin.Context.from_dict(cluedin_context) # GraphQL query to pull data from CluedIn. query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query cursor: $cursor pageSize: $pageSize sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} ) { totalResults cursor entries { id name entityType properties } } } \"\"\" # Fetch the first record from the `cluedin.gql.entries` generator. next(cluedin.gql.entries(ctx, query, { 'query': 'entityType:/IMDb/Title', 'pageSize': 1 })) . Output: . {'id': '00001e32-9bae-53b9-a30f-cf30ed66c360', 'name': 'Murder, Money and a Dog', 'entityType': '/IMDb/Title', 'properties': {'attribute-type': '/Metadata/KeyValue', 'property-imdb.title.endYear': '\\\\N', 'property-imdb.title.genres': 'Comedy,Drama,Thriller', 'property-imdb.title.isAdult': '0', 'property-imdb.title.originalTitle': 'Murder, Money and a Dog', 'property-imdb.title.primaryTitle': 'Murder, Money and a Dog', 'property-imdb.title.runtimeMinutes': '65', 'property-imdb.title.startYear': '2010', 'property-imdb.title.tconst': 'tt1664719', 'property-imdb.title.titleType': 'movie'}} . For performance reasons and to avoid collisions, it’s important to sort the results by a unique field in the GraphQL query. Entity ID works just fine: . sort: FIELDS sortFields: {field: \"id\", direction: ASCENDING} . Now, let’s pull the whole dataset in a pandas DataFrame. However, we’ll need to flatten the properties and remove unnecessary property name prefixes. and replace dots with underscores to make it compatible with the Spark schema: . ctx = cluedin.Context.from_dict(cluedin_context) query = \"\"\" query searchEntities($cursor: PagingCursor, $query: String, $pageSize: Int) { search( query: $query sort: FIELDS cursor: $cursor pageSize: $pageSize sortFields: {field: \"id\", direction: ASCENDING} ) { totalResults cursor entries { id properties } } } \"\"\" def flatten_properties(d): for k, v in d['properties'].items(): if k == 'attribute-type': continue if k.startswith('property-'): k = k[9:] # len('property-') == 9 k = k.replace('.', '_') d[k] = v del d['properties'] return d df_titles = pd.DataFrame( map( flatten_properties, cluedin.gql.entries(ctx, query, { 'query': 'entityType:/IMDb/Title', 'pageSize': 10_000 }))) df_titles.head() . One thing to fix here, let’s set the DataFrame’s index to Entity Id: . df_titles.set_index('id', inplace=True) df_titles.head() . Explore data . Let’s see how many movies we have by genre: . df_titles['imdb_title_genres'].str.split(',', expand=True).stack().value_counts().plot(kind='bar') plt.title('Distribution of genres') plt.xlabel('Genres') plt.ylabel('Count') plt.show() . Create schema . Now, let’s create a schema for our data (mind that imdb_title_genres is a string, not an array, so we need to split it): . from pyspark.sql import SparkSession from pyspark.sql.types import StructType,StructField, StringType, ArrayType, IntegerType from pyspark.sql.functions import split spark = SparkSession.builder.getOrCreate() schema = StructType([ StructField('id', StringType(), True), StructField('imdb_title_endYear', StringType(), True), StructField('imdb_title_genres', ArrayType(StringType()), True), StructField('imdb_title_isAdult', StringType(), True), StructField('imdb_title_originalTitle', StringType(), True), StructField('imdb_title_primaryTitle', StringType(), True), StructField('imdb_title_runtimeMinutes', StringType(), True), StructField('imdb_title_startYear', StringType(), True), StructField('imdb_title_tconst', StringType(), True), StructField('imdb_title_titleType', StringType(), True) ]) df_spark_titles = spark.createDataFrame(df_titles) df_spark_titles = df_spark_titles.withColumn('imdb_title_genres', split(df_spark_titles.imdb_title_genres, ',')) spark.sql('CREATE DATABASE IF NOT EXISTS cluedin') df_spark_titles.write.mode('overwrite').format('parquet').saveAsTable('cluedin.imdb_titles', schema=schema) display(df_spark_titles) . Now, we can see our data in the Catalog: . ",
    "url": "/playbooks/data-engineering-playbook/export#databricks",
    
    "relUrl": "/playbooks/data-engineering-playbook/export#databricks"
  },"1306": {
    "doc": "Exporting data to and from CluedIn with Python",
    "title": "Exporting data to and from CluedIn with Python",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/export",
    
    "relUrl": "/playbooks/data-engineering-playbook/export"
  },"1307": {
    "doc": "Golden records",
    "title": "On this page",
    "content": ". | Concept of golden record . | Linking similar records together | Linking golden records together | . | Golden record step by step . | Source record (bronze) | Data part (silver) | Golden record (golden) | . | Building a golden record . | One source | Multiple sources | . | Golden record page | . In this article, you will learn what a golden record is, how it is generated, and how you can control the sources contributing to a golden record. A golden record is an accurate and consolidated representation of a data subject, such as an organization or an employee, derived from multiple sources. It provides a 360-degree view of a data subject, facilitating a deeper understanding of its current state and the reasons behind it. A golden record is created through a process of data integration, enrichment, cleaning, deduplication, and manual data entry. Each step in the process is registered as a separate element called a data part. A golden record is usually made up of multiple data parts. The purpose of creating a golden record is to provide a single source of truth, ensuring data consistency, improving data quality, and enabling better decision-making. A golden record serves as a reliable reference point that can be used by different systems, departments, or stakeholders within the organization. Golden records in CluedIn are stored for performance reasons, but essentially, they are a projection of all the sources and rules that you have created. That is why golden records adapt based on the changes that are applied within CluedIn. This makes golden records very agile—you can easily revert changes and re-shape your golden records at any point in time. ",
    "url": "/key-terms-and-features/golden-records#on-this-page",
    
    "relUrl": "/key-terms-and-features/golden-records#on-this-page"
  },"1308": {
    "doc": "Golden records",
    "title": "Concept of golden record",
    "content": "A golden record is a multi-level graph. A graph structure consists of nodes (discrete objects) that can be connected by relations. Even though this concept can be a bit hard to comprehend at first, once you understand it, you’ll appreciate the great flexibility that it gives you. Golden records can have 2 types of relations: . | Linking similar records together. | Linking golden records together. | . Linking similar records together . In CluedIn, similar records are grouped together under the same banner called golden record. Generally, the golden record is composed of records from multiple sources. For example, suppose we have a golden record that is composed of 2 records from different sources: one record from CRM and the other record from ERP. These distinct nodes are referred to as data parts. The more sources you add, the bigger your golden record model becomes. Since the golden record is a projection of the sources, adding or removing a source is not an issue. This is what gives great flexibility of golden records in CluedIn. Linking golden records together . We always say that a golden record is a graph of a graph. What it means is that when the golden record is being produced, CluedIn has the ability to link golden records together using edges. An edge is simply a relation. You can define an edge using rules or during the mapping. When you define an edge, CluedIn can connect golden records together as shown in the example. So, when we say that a golden record is a graph of a graph, it is because at the end, the entire view of the models is as follows. ",
    "url": "/key-terms-and-features/golden-records#concept-of-golden-record",
    
    "relUrl": "/key-terms-and-features/golden-records#concept-of-golden-record"
  },"1309": {
    "doc": "Golden records",
    "title": "Golden record step by step",
    "content": "To make the golden record generation process easier to understand, we’ll start with the simplified explanation that does not include various types of rules. We’ll focus on the records and use the concepts of bronze, silver, and golden layers from the medallion architecture for visual assistance. | Medallion architecture term | CluedIn term | Definition | . | Bronze layer | Source record | This is a record in its basic, raw format as it was in the source system. | . | Silver layer | Data part | This is a mapped record with all of the pre-processing rules and changes applied to it. | . | Golden layer | Golden record | This is a record that you can trust, usually formed by aggregating data parts to the existing golden record. | . Generally, a new record is associated with the bronze layer. This is the record that comes from a specific source—it may come from a file, an Azure Data Factory pipeline, or a database. We call this record a source record. The process of generating a golden record spans from source records, through data parts, to the golden record. Next, we’ll describe each step that the record goes through to become a new golden record or aggregate into the existing golden record. Source record (bronze) . A source record is a raw record that has been ingested into CluedIn from a source system. It is generally stored in JSON format. Such record has not been modified in any way. You can see source records on the Preview tab of the data set. Data part (silver) . When records appear in CluedIn, you need to add a semantic layer to transform them into a format that CluedIn can understand. This process is called mapping. Once all the steps of the mapping process have been performed, you get what we call a data part. Essentially, a data part is an aggregation of all changes to the record coming from a single source after it has been mapped. During the mapping process, the source records can go through multiple steps: . | Changes to the values of mapped records via property rules. | Changes to the mapped records via pre-process rules. | Changes to mapped records via advanced mapping code. | If the mapped record gets into the quarantine, it should be approved to get to the next stage. | . All in all, a data part is a record in a format that CluedIn can understand and that has already gone through multiple processes to ensure it is valid and ready for the production of a golden record. Golden record (golden) . When you process the data set, CluedIn checks if the data part can be associated with the existing golden record. If the data part can be associated with the existing golden record—they share the same identifiers—then it is aggregated to the existing golden record. In this case, the golden record is re-processed. If the data part cannot be associated with the existing golden record, then a new golden record is created. What happens when golden record is re-processed? . When a new data part is added to the existing golden record, CluedIn incorporates all of its properties into the golden record. However, conflicts can arise between new and existing data parts when the values for the same property differ. We describe the scenarios of handling different values for the same property later in this article. How can you re-shape a golden record? . Depending on the projects and processes that are running in CluedIn—clean projects, deduplication projects, enrichers, manual modifications, data ingestion—the golden record can be changed. For example, if you create a clean project and it affects a golden record, a new data part is added to that golden record. Similarly, if you have an enricher that affects a golden record, a new data parts with the properties from third-party source is added to the golden record. If you are not satisfied with the changes made to a golden record by a specific data part, you can simply delete such data part. Whenever a new data part or changes to an existing data part appear in CluedIn, they are ordered by the sort date. The sort date is determined by selecting the first available date among modified, created, and discovery dates: . | If there is a modified date, then this date is used as the sort date. | If there is no modified date, but there is a created date, then this date is used as the sort date. | If there is no modified or created date, then the discovery date is used as the sort date. The discovery date is always present in the data part as it is the date when the data part was created in CluedIn. | . The sort date is important because CluedIn uses it by default to determine the winning value for each specific data part, as well as the winning value between different data parts. We’ll explain the specifics of this mechanism in the following sections. ",
    "url": "/key-terms-and-features/golden-records#golden-record-step-by-step",
    
    "relUrl": "/key-terms-and-features/golden-records#golden-record-step-by-step"
  },"1310": {
    "doc": "Golden records",
    "title": "Building a golden record",
    "content": "In CluedIn, we track the lineage of source records with the help of version branches. Each source has its own version branch that consists of parts. Whenever the source record is changed, a new part is added to the corresponding version branch. This way you can track the changes of values per source. The parts within a version branch are ordered by sort date. The values from the part with the most recent sort date are used in the data part that can contribute to the golden record. To explain how a golden record is built, we’ll start from the way CluedIn handles changes in source records for a golden record with just one source. Then, we’ll move on to discussing a golden record built from multiple sources. One source . Let’s consider a simple example of a golden record that is formed by one data part from CRM. Whenever the source record is changed, a new part appears in the version branch of the corresponding source. In case of conflicting values for the same property—in our case, Job Title—CluedIn takes the value from the part with the most recent sort date and uses this value in a data part. Multiple sources . Now, let’s consider an example of a golden record formed from multiple sources. In this case, each source has its own version branch, and each version branch behaves the same way as described in the section above. However, if there is a property that has different values in each source, then how does CluedIn determine which value should be used in the golden record? The answer is by applying a default survivorship rule. The default survivorship rule is CluedIn’s mechanism for determining the winning value among conflicting values from different sources. According to this rule, manually added changes are prioritized; otherwise, the most recent value wins. To explain how CluedIn determines the winning value among data parts, we’ll use an example of a golden record that is formed from two data parts: one from CRM and the other one from ERP. Each data part has its own version branch that contains a collection of parts. From the section above, you already know how the winning value is defined for a data part. Now, let’s find out how CluedIn defines the winner between data parts. In our example, each data part has a different value for the same property—Job Title. To determine the winning value, CluedIn defines the winner between version branches. It is important to emphasize that CluedIn does not evaluate each part from the version branch individually. CluedIn only evaluates the values from each data part. The value from the data part with the most recent sort date wins and is used in the golden record. The following gif animates the previous diagram, illustrating how the golden record is changed according to default survivorship rule. If the default survivorship rule is not suitable for you, you can set up your own strategy for defining the winning value using survivorship rules. For example, you can create a custom survivorship rule to prioritize a specific source for determining the value for a specific property. Suppose you want CRM to be the source of truth for the Job Title property. In this case, even though the value from ERP is the most recent one, the golden record uses the value from CRM as defined by the custom survivorship rule. The following gif animates the previous diagram, illustrating how the golden record is changed according to custom survivorship rule. ",
    "url": "/key-terms-and-features/golden-records#building-a-golden-record",
    
    "relUrl": "/key-terms-and-features/golden-records#building-a-golden-record"
  },"1311": {
    "doc": "Golden records",
    "title": "Golden record page",
    "content": "In CluedIn, you can find a golden record using search. The golden record page contains several tabs where you can find all relevant information about a golden record: . | Overview – here you can view general information about a golden record, such as entity properties, vocabularies, sources, and more. | Properties – here you can view all properties that the golden record has as well as add new properties. | Relations – here you can view which golden records the current golden record is related to. | Pending changes . | History – here you can view all data parts (versions of clues that make up a data part) of a golden record as well as all outgoing relations (edges) of a golden record. | Explain log – here you can view detailed information about the operations performed on a golden record and its data parts. | Topology – here you can view the visualization of data parts that form a golden record. | Hierarchy – here you can view the hierarchy projects that the current golden record is a part of. | . ",
    "url": "/key-terms-and-features/golden-records#golden-record-page",
    
    "relUrl": "/key-terms-and-features/golden-records#golden-record-page"
  },"1312": {
    "doc": "Golden records",
    "title": "Golden records",
    "content": " ",
    "url": "/key-terms-and-features/golden-records",
    
    "relUrl": "/key-terms-and-features/golden-records"
  },"1313": {
    "doc": "GraphQL actions",
    "title": "On this page",
    "content": ". | Split entities in bulk | Delete entities in bulk | Run post-processing | Run entity metrics processing | Run edge processing | Run enrichment | Search with filter | . CluedIn supports GraphQL actions so that you can run commands in bulk from our GraphQL endpoint. You will need to be in the Admin role to even see these commands as they allow you to run operations in bulk. ",
    "url": "/consume/graphql/graphql-actions#on-this-page",
    
    "relUrl": "/consume/graphql/graphql-actions#on-this-page"
  },"1314": {
    "doc": "GraphQL actions",
    "title": "Split entities in bulk",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { splitEntity } } } } . ",
    "url": "/consume/graphql/graphql-actions#split-entities-in-bulk",
    
    "relUrl": "/consume/graphql/graphql-actions#split-entities-in-bulk"
  },"1315": {
    "doc": "GraphQL actions",
    "title": "Delete entities in bulk",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { deleteEntity } } } } . ",
    "url": "/consume/graphql/graphql-actions#delete-entities-in-bulk",
    
    "relUrl": "/consume/graphql/graphql-actions#delete-entities-in-bulk"
  },"1316": {
    "doc": "GraphQL actions",
    "title": "Run post-processing",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { postProcess } } } } . ",
    "url": "/consume/graphql/graphql-actions#run-post-processing",
    
    "relUrl": "/consume/graphql/graphql-actions#run-post-processing"
  },"1317": {
    "doc": "GraphQL actions",
    "title": "Run entity metrics processing",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { processEntityMetrics } } } } . ",
    "url": "/consume/graphql/graphql-actions#run-entity-metrics-processing",
    
    "relUrl": "/consume/graphql/graphql-actions#run-entity-metrics-processing"
  },"1318": {
    "doc": "GraphQL actions",
    "title": "Run edge processing",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { processEdges } } } } . ",
    "url": "/consume/graphql/graphql-actions#run-edge-processing",
    
    "relUrl": "/consume/graphql/graphql-actions#run-edge-processing"
  },"1319": {
    "doc": "GraphQL actions",
    "title": "Run enrichment",
    "content": "{ search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { enrich } } } } . ",
    "url": "/consume/graphql/graphql-actions#run-enrichment",
    
    "relUrl": "/consume/graphql/graphql-actions#run-enrichment"
  },"1320": {
    "doc": "GraphQL actions",
    "title": "Search with filter",
    "content": "{ search( query:\"entityType:/Customer\", filter: \"(properties.customer.addressCountry:CHN OR JPN) AND -properties.customer.addressZipCode:*\" ) { totalResults entries { id name properties } } } . In -properties.customer.addressZipCode:*, the -properties means where value does not exist; and if it is +properties, it would mean where value exists. Note that the filter uses Lucene syntax. Also, you can test the filter query directly in the search bar. ",
    "url": "/consume/graphql/graphql-actions#search-with-filter",
    
    "relUrl": "/consume/graphql/graphql-actions#search-with-filter"
  },"1321": {
    "doc": "GraphQL actions",
    "title": "GraphQL actions",
    "content": " ",
    "url": "/consume/graphql/graphql-actions",
    
    "relUrl": "/consume/graphql/graphql-actions"
  },"1322": {
    "doc": "Manage groups of duplicates",
    "title": "On this page",
    "content": ". | Process a group of duplicates | Merge groups | Check merged record | Unmerge records | . In this article, you will learn how to process and merge groups of duplicates, as well as how to undo a merge if it did not go as expected. When you generate matches and CluedIn detects duplicates, they are organized into groups. These groups contain golden records identified as potential duplicates based on your matching rules. The following diagram illustrates basic group workflow. For more information about group statuses, see Deduplication reference. When you open a group of duplicates, you can find the following details: . | Golden records that were identified as possible duplicates. | Conflicting values – variations in the same property or vocabulary key across all golden records within the group. | Matching values – consistent values for the same property or vocabulary key across all golden records within the group. | . ",
    "url": "/management/deduplication/manage-groups-of-duplicates#on-this-page",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates#on-this-page"
  },"1323": {
    "doc": "Manage groups of duplicates",
    "title": "Process a group of duplicates",
    "content": "To ensure the merged golden records meet your requirements, you need to process each group of duplicates. Processing a group of duplicates involves three key steps: . | Fixing duplicates – select the appropriate values from conflicting values that will be used in the merged golden record. You can fix duplicates in one of the following ways: . | Manually – by selecting the appropriate values on your own. | Automatically – by turning on the Auto-select toggle. CluedIn will select the appropriate values based on your survivorship rules. | . | Defining if a group is suitable for merging – approve or reject the group. | . To process a group of duplicates . | On the Matches tab of the deduplication project, select the group name. | On the Fix Conflicts tab, review conflicting values and choose the appropriate values to be used in the merged golden record. If you are not certain about the correctness of the selected values, you can Reset Conflicts and start again. Turning on the Auto-select toggle prevents the display of selected values on the Fix Conflicts tab. Instead, you can review those values on the next tab (Preview Merge). | Go to the Preview Merge tab and review the property or vocabulary keys values that the merged golden record will hold. | Depending on the results of your review, choose if a group is suitable for merging: . | If you want to merge the group, select Approve, and then confirm your choice. This is a preliminary step before merging. If you change your mind, you can Revoke the approval, which brings the group back to the New status. | If you don’t want to merge the group, select Reject, and then confirm your choice. Rejected groups cannot be merged. However, if you change your mind, you can Approve the rejected group later. | . Once all groups within the project have been processed, proceed to merge the groups. | . ",
    "url": "/management/deduplication/manage-groups-of-duplicates#process-a-group-of-duplicates",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates#process-a-group-of-duplicates"
  },"1324": {
    "doc": "Manage groups of duplicates",
    "title": "Merge groups",
    "content": "Only groups with the Approved status can be merged. For additional details on group statuses, see Deduplication reference. To merge groups . | Select the checkboxes next to the groups that you want to merge, and then select Merge. Alternatively, you can use the Merge All option, and CluedIn will automatically select groups suitable for merging. | Review the groups that will be merged, and then select Next. | Choose the strategy for dealing with stale data if it is identified during merging. | Select Confirm. The groups are merged, each generating merged golden records. Next, check the merged golden records to ensure they align with your specific requirements. If you are not satisfied, you have the option to revert changes by unmerging records. | . ",
    "url": "/management/deduplication/manage-groups-of-duplicates#merge-groups",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates#merge-groups"
  },"1325": {
    "doc": "Manage groups of duplicates",
    "title": "Check merged record",
    "content": "All merged golden records produced within the deduplication project are listed on the Merges tab. Selecting a golden record takes you to the Topology tab, where you can view the records that make up the golden record, linked together in a visual representation. The link between such records is the deduplication project. Having such link allows you to easily unmerge records in needed. ",
    "url": "/management/deduplication/manage-groups-of-duplicates#check-merged-record",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates#check-merged-record"
  },"1326": {
    "doc": "Manage groups of duplicates",
    "title": "Unmerge records",
    "content": "If merged golden record is not as you expected, you can revert the changes by unmerging records. The unmerging option enables dynamic testing of your deduplication project configuration, allowing you to iteratively refine it until you achieve the desired results. You can unmerge records in one of the following ways: . | On the Topology tab of the golden record – select the deduplication project that links the records together and then undo deduplication merge. | On the Matches tab of the deduplication project – select the checkboxes next to the groups that you want to unmerge, and then select Unmerge. | On the Merges tab of the deduplication project – select Unmerge All. After you unmerge the records, the group status is changed to Unmerged, and you can no longer process the group. If you want to continue working with the same group, you can discard matches and start again. | . ",
    "url": "/management/deduplication/manage-groups-of-duplicates#unmerge-records",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates#unmerge-records"
  },"1327": {
    "doc": "Manage groups of duplicates",
    "title": "Manage groups of duplicates",
    "content": " ",
    "url": "/management/deduplication/manage-groups-of-duplicates",
    
    "relUrl": "/management/deduplication/manage-groups-of-duplicates"
  },"1328": {
    "doc": "Preparation",
    "title": "Preparation",
    "content": "The Preparation module allows you to clean your data and enrich it with additional information from third-party sources. Clean Identify and fix data quality issues in your golden records Enrich Enhance your golden records with information from third-party sources ",
    "url": "/Preparation",
    
    "relUrl": "/Preparation"
  },"1329": {
    "doc": "Process role requests",
    "title": "Process role requests",
    "content": "In this article, you will learn how to process the role requests you receive from users to grant them the access they need. When users are browsing CluedIn and find that they do not have access to certain sections or modules, they can send a request to get access. When this happens, you’ll receive a new role request notification. The following diagram shows the flow of processing role requests. To process a role request . | In the upper-right corner, select the ellipsis button, and then select My Tasks. | Select Role Requests, and then find the needed request. | In the Actions column, select Grant Access. The dialog opens where you can view the details of the access request. | In the Granting Access To dialog box, do the following: . | Review the access level and select another level if necessary. The information message below the selected access level helps you understand the permissions that will be granted to the user. | Select a role to assign to the user. | In the lower-right corner, select Grant Access. | . The user will receive a notification about the role changes. For the changes to take effect, the user has to sign out and sign in again. | . ",
    "url": "/administration/roles/process-role-requests",
    
    "relUrl": "/administration/roles/process-role-requests"
  },"1330": {
    "doc": "Processing logic in clean projects",
    "title": "On this page",
    "content": ". | Processing clean project changes | Pre-processing: comparing values | Processing: applying survivorship rules | . In this article, you will learn about the underlying mechanisms involved in processing clues generated by the clean project. The following diagram illustrates the processing logic for clues generated from the clean project. ",
    "url": "/preparation/clean/processing-logic-in-clean-projects#on-this-page",
    
    "relUrl": "/preparation/clean/processing-logic-in-clean-projects#on-this-page"
  },"1331": {
    "doc": "Processing logic in clean projects",
    "title": "Processing clean project changes",
    "content": "When you generate a clean project, all data is copied to the clean application. While working in the clean project, the golden records in CluedIn may be modified by other processes. Therefore, when submitting your clean project changes, you must decide how to handle golden records that have been changed outside the clean project: . | Push – apply all cleaning changes, even if golden records have been modified outside the clean project. This action forces the application of all changes without checking for outdated data. | Skip – do not apply cleaning changes to golden records that have been modified outside the clean project. This action checks for outdated data and ignores changes for stale records. | . If you choose to push all cleaning changes, then CluedIn will first verify whether any modifications were made within the clean project. Each row is evaluated based on whether it was modified in the clean project: . | If a row has not been changed in the clean project, it will be skipped during processing. | If a row has been changed in the clean project, then a new clue will be generated with the data from the clean application. Then such clue will be sent to the pre-processing step. | . After checking for changes in the clean project, CluedIn compares values in the clean project clue and the golden record. ",
    "url": "/preparation/clean/processing-logic-in-clean-projects#processing-clean-project-changes",
    
    "relUrl": "/preparation/clean/processing-logic-in-clean-projects#processing-clean-project-changes"
  },"1332": {
    "doc": "Processing logic in clean projects",
    "title": "Pre-processing: comparing values",
    "content": ". During the pre-processing step, CluedIn checks the golden record that the clue is going to be merged into and compares golden record values with those in the clue: . | If the values are identical, CluedIn trims these values from the clue and passes it through the regular processing pipeline. Essentially, CluedIn checks the persist hash of each row—if a hash already exists in the golden record, the clue is not merged. | If the values are different, the clue is added to the history of the golden record. At this point, CluedIn compares the clue with the previous revision in the same version branch. It is important to note that the comparison takes place with the previous revision in the same version branch and not with another version in the golden record history. If CluedIn finds insignificant changes—Created Date, Modified Date, Received Date, Sent Date—they are skipped. Generally, the first version of such properties is used in the golden record. Note that you can configure the vocabulary keys that are considered insignificant. This can be useful when you have frequent changes sent from the source system or when the source system generates hashes. If a clue consists only of insignificant changes, then such clue will not be added to the golden record. If a clue consists of both significant and insignificant changes, then such clue will be added to the golden record. | . After the pre-processing step, CluedIn applies survivorship rules. ",
    "url": "/preparation/clean/processing-logic-in-clean-projects#pre-processing-comparing-values",
    
    "relUrl": "/preparation/clean/processing-logic-in-clean-projects#pre-processing-comparing-values"
  },"1333": {
    "doc": "Processing logic in clean projects",
    "title": "Processing: applying survivorship rules",
    "content": ". Changes from the clean project are treated as manually added. This means that the default survivorship mechanism is applied to the clean project clues—manually added changes always win over changes from other sources. If you have multiple manually added changes, the most recent one wins. Learn more about default survivorship rule here. During processing, CluedIn initially applies the default survivorship mechanism, followed by custom survivorship rules. Therefore, if you want to override default survivorship and always use values from the source system, you can create a custom survivorship rule. ",
    "url": "/preparation/clean/processing-logic-in-clean-projects#processing-applying-survivorship-rules",
    
    "relUrl": "/preparation/clean/processing-logic-in-clean-projects#processing-applying-survivorship-rules"
  },"1334": {
    "doc": "Processing logic in clean projects",
    "title": "Processing logic in clean projects",
    "content": " ",
    "url": "/preparation/clean/processing-logic-in-clean-projects",
    
    "relUrl": "/preparation/clean/processing-logic-in-clean-projects"
  },"1335": {
    "doc": "Resolve common upgrade issues",
    "title": "On this page",
    "content": ". | Scenario 1: CrashLoopBackOff state | Scenario 2: Pod not ready | Scenario 3: Pod running and ready, but application exhibits unexpected behaviour | Scenario 4: Pod pending due to init container issues | . Even with careful preparation, upgrades may sometimes encounter issues. This section describes the most common issues you might face during or after the CluedIn upgrade process and provides guidance on how to resolve them quickly. ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#on-this-page"
  },"1336": {
    "doc": "Resolve common upgrade issues",
    "title": "Scenario 1: CrashLoopBackOff state",
    "content": "A pod is in a CrashLoopBackOff state, the container keeps starting, failing, and restarting in a loop. In this case, when you run kubectl get pods –n cluedin, you will see similar output showing a high number of restarts and the CrashLoopBackOff status. NAME READY STATUS RESTARTS AGE ---- ----- ------ -------- --- cluedin-ui-7d9f8d7c9d-abc12 0/1 CrashLoopBackOff 9 5m . To troubleshoot this, it is important to review the logs of the previous container instance, not the current one that is restarting. The previous logs usually contain the exact error message that caused the crash. These logs often appear near the last few lines of the output. To check previous logs from before a pod crashed, add the –p (stands for previous) flag at the end of the kubectl logs command: . kubectl logs &lt;pod name&gt; -n cluedin –p . ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-1-crashloopbackoff-state",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-1-crashloopbackoff-state"
  },"1337": {
    "doc": "Resolve common upgrade issues",
    "title": "Scenario 2: Pod not ready",
    "content": "A pod can be in the Running state but still marked as Not Ready if it is failing its readiness probes. This situation occurs when Kubernetes has successfully started the pod, but the application inside is not yet prepared to handle traffic. In other words, the container is alive, but it cannot serve requests. In this case, when you run kubectl get pods –n cluedin, you will see similar output: . NAME READY STATUS RESTARTS AGE ---- ----- ------ -------- --- cluedin-ui-7d9f8d7c9d-abc12 0/1 Running 0 5m . To investigate whether a pod is failing due to a readiness probe, do the following: . | Describe the pod and review the events section at the bottom of the output. You may see warnings similar to the following: . Warning Unhealthy 2m (x4 over 4m) kubelet Readiness probe failed: . If you find repeated Readiness probe failed events, this confirms that the pod is starting but failing to pass the readiness check. For example, a pod might be running but remain Not Ready until it successfully connects to its database. In this case, the readiness probe will continue to fail until the dependency becomes available. Example: . kubectl logs &lt;pod-name&gt; -n cluedin . Returns the following: . 2025-09-19T10:25:12Z INFO Starting CluedIn ... 2025-09-19T10:25:15Z WARN Waiting for database connection... 2025-09-19T10:25:30Z ERROR Timeout connecting to SQL at db-service:4133 . | Examine the container logs, which may provide additional details on why the application is not ready to serve traffic. | Resolve the issue. In this example, the issue must be resolved by fixing the connectivity between the pod and the database. Common causes include: . | A misconfigured connection string (for example, wrong host, port, username, or password). | The database may be under resource pressure (for example, CPU or memory exhaustion), which can prevent it from accepting new connections. | . Addressing these problems will allow the pod to pass its readiness probe and become ready to serve traffic. | . ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-2-pod-not-ready",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-2-pod-not-ready"
  },"1338": {
    "doc": "Resolve common upgrade issues",
    "title": "Scenario 3: Pod running and ready, but application exhibits unexpected behaviour",
    "content": "In some cases, a pod may be in the Running state and marked as Ready, but the application inside still shows unexpected or faulty behaviour. This indicates that the pod has passed its liveness and readiness probes, but the underlying issue lies within the application itself. | To begin diagnosing the issue, run the following command: . kubectl get pods –n cluedin . The output will be similar to the following: . NAME READY STATUS RESTARTS AGE ---- ----- ------ -------- --- cluedin-ui-7d9f8d7c9d-abc12 1/1 Running 0 5m . This usually means that the problem is not with Kubernetes itself, but with the application inside the pod, or with network access between the user and the pod. | Even if a pod appears healthy, the application inside might be failing silently. To check for hidden errors, review the pod logs by running the following command: . kubectl logs &lt;pod name&gt; -n cluedin . If you want to read the log in a more convenient way, you can download it to a file and open it with any file reader. kubectl logs &lt;pod name&gt; -n cluedin &gt; &lt;podname&gt;.log . | . ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-3-pod-running-and-ready-but-application-exhibits-unexpected-behaviour",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-3-pod-running-and-ready-but-application-exhibits-unexpected-behaviour"
  },"1339": {
    "doc": "Resolve common upgrade issues",
    "title": "Scenario 4: Pod pending due to init container issues",
    "content": "A pod can contain one or more application containers, and may also include one or more init containers. | Init containers run sequentially before the main application containers start. Each must complete successfully before any main container in the pod can begin running. | If an init container fails or cannot complete, the main container responsible for serving traffic may remain stuck in the Pending state. This means that the pod never progresses to running the main workload. | . To verify whether a pod is unable to start because of a failing init container, describe the pod with the following command: . kubectl describe pod &lt;pod-name&gt; -n cluedin . The output will be similar to the following: . Name: cluedin-ui-879c4db6b-8jzks Namespace: cluedin Status: Pending Controlled By: ReplicaSet/cluedin-ui-879c4db6b Init Containers: wait-cluedin-gql: Image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 State: Terminated Reason: Error Exit Code: 1 Restart Count: 3 Args: service cluedin-gql -n cluedin Containers: ui: Image: cluedinprod.azurecr.io/cluedin/ui:2024.12.02 State: Waiting Reason: PodInitializing Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Failed 5m kubelet Init container \"wait-cluedin-gql\" failed . In the example above: . | The main container named ui is in the Waiting state. This usually means it is waiting for the init containers to complete successfully. | The events show that the init container wait-cluedin-gql has failed. In such cases, the pod cannot progress to running the main container until the init container issue is resolved. | . Sometimes, an init container may run indefinitely without explicitly failing. In both scenarios, it is useful to inspect the init container logs for more details. You can view the logs of a specific init container by adding the -c &lt;init-container-name&gt; flag to the kubectl logs command: . kubectl logs &lt;pod-name&gt; -n cluedin -c &lt;init-container-name&gt; . This will help you understand why the init container is failing or stuck, and therefore why the main container cannot proceed. ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-4-pod-pending-due-to-init-container-issues",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues#scenario-4-pod-pending-due-to-init-container-issues"
  },"1340": {
    "doc": "Resolve common upgrade issues",
    "title": "Resolve common upgrade issues",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues",
    
    "relUrl": "/paas-operations/upgrade/guide/resolve-common-upgrade-issues"
  },"1341": {
    "doc": "Set Retention Policies on Data",
    "title": "Setting Retention Policies for Records in CluedIn",
    "content": "Retention policies define how long data is stored in CluedIn before it is automatically archived or deleted. They are essential for managing compliance requirements, controlling storage costs, and ensuring that outdated or irrelevant data does not clutter your environment. This article explains what retention policies are, why they matter, and how to configure them in CluedIn. ",
    "url": "/kb/set-retention#setting-retention-policies-for-records-in-cluedin",
    
    "relUrl": "/kb/set-retention#setting-retention-policies-for-records-in-cluedin"
  },"1342": {
    "doc": "Set Retention Policies on Data",
    "title": "What Are Retention Policies?",
    "content": "A retention policy specifies: . | Which records the policy applies to (e.g., Customers, Orders, Products). | How long the data should be retained. | What action to take when the retention period expires (archive, delete, or move to another system). | . Retention policies ensure that CluedIn automatically manages the lifecycle of your records in line with business and regulatory requirements. ",
    "url": "/kb/set-retention#what-are-retention-policies",
    
    "relUrl": "/kb/set-retention#what-are-retention-policies"
  },"1343": {
    "doc": "Set Retention Policies on Data",
    "title": "Why Retention Policies Matter",
    "content": ". | Regulatory Compliance: Frameworks like GDPR and CCPA require organizations to only retain personal data for as long as it is needed. | Data Governance: Reduces clutter by removing outdated or irrelevant data. | Cost Efficiency: Helps manage storage costs by preventing unbounded data growth. | Performance: Keeping datasets lean improves ingestion, rule processing, and matching speeds. | . ",
    "url": "/kb/set-retention#why-retention-policies-matter",
    
    "relUrl": "/kb/set-retention#why-retention-policies-matter"
  },"1344": {
    "doc": "Set Retention Policies on Data",
    "title": "How to Configure Retention Policies in CluedIn",
    "content": "Step 1: Identify Applicable Entity Types . | Decide which entities should be subject to retention rules (e.g., Customer, Order, LogEntry). | Review regulatory requirements or internal governance guidelines for each entity type. | . Step 2: Navigate to Retention Policy Settings . | Log in to the CluedIn Portal. | Go to Governance &gt; Retention Policies. | Click Create Policy. | . Step 3: Define the Policy Scope . | Choose the Entity Type (e.g., Customers). | Select optional filters to refine scope (e.g., customers without activity in 5 years). | . Step 4: Set Retention Duration . | Define the number of days, months, or years records should be retained. | Example: Retain inactive customer records for 7 years. | . Step 5: Define the Action . | Delete: Permanently removes records and associated data. | Archive: Moves data into an archived state (still accessible but not active in workflows). | Move: Exports data to an external storage system before removal from CluedIn. | . Step 6: Publish &amp; Activate . | Save the retention policy. | Review the summary screen and activate the policy. | Policies will automatically apply to existing and new records within scope. | . ",
    "url": "/kb/set-retention#how-to-configure-retention-policies-in-cluedin",
    
    "relUrl": "/kb/set-retention#how-to-configure-retention-policies-in-cluedin"
  },"1345": {
    "doc": "Set Retention Policies on Data",
    "title": "Best Practices for Retention Policies",
    "content": ". | Start with Non-Destructive Actions: Use archiving before permanent deletion to reduce risk. | Align with Legal Requirements: Consult compliance and legal teams to set retention durations. | Test on a Subset First: Apply new policies to a limited dataset before rolling out broadly. | Communicate Changes: Inform stakeholders about data lifecycle policies to avoid surprises. | Monitor Policy Impact: Review retention reports to ensure policies are working as intended. | . ",
    "url": "/kb/set-retention#best-practices-for-retention-policies",
    
    "relUrl": "/kb/set-retention#best-practices-for-retention-policies"
  },"1346": {
    "doc": "Set Retention Policies on Data",
    "title": "Common Issues and Troubleshooting",
    "content": "Policy Not Applying . | Cause: Entity type or filters are misconfigured. | Fix: Double-check policy scope and filters. | . Records Not Being Deleted . | Cause: Policy action is set to Archive instead of Delete. | Fix: Review action type in the policy definition. | . Unexpected Data Removal . | Cause: Policy conditions too broad. | Fix: Narrow scope with additional filters (e.g., “inactive &gt; 5 years”). | . ",
    "url": "/kb/set-retention#common-issues-and-troubleshooting",
    
    "relUrl": "/kb/set-retention#common-issues-and-troubleshooting"
  },"1347": {
    "doc": "Set Retention Policies on Data",
    "title": "Summary",
    "content": "Retention policies in CluedIn allow you to automate record lifecycle management, ensuring compliance, efficiency, and improved system performance. By carefully scoping policies, aligning them with business/legal requirements, and monitoring their impact, you can maintain a clean and compliant data environment. ",
    "url": "/kb/set-retention#summary",
    
    "relUrl": "/kb/set-retention#summary"
  },"1348": {
    "doc": "Set Retention Policies on Data",
    "title": "Set Retention Policies on Data",
    "content": " ",
    "url": "/kb/set-retention",
    
    "relUrl": "/kb/set-retention"
  },"1349": {
    "doc": "Sync manual data entry to Purview",
    "title": "On this page",
    "content": ". | Preparation in CluedIn | Feature demonstration | . In this article, you will learn how to sync CluedIn manual data entry projects to Purview assets. ",
    "url": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#on-this-page"
  },"1350": {
    "doc": "Sync manual data entry to Purview",
    "title": "Preparation in CluedIn",
    "content": ". | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Manual Data Entry to Purview. | Select Save. | Make sure you have an existing manual data entry project that contains some processed data. | . ",
    "url": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#preparation-in-cluedin",
    
    "relUrl": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#preparation-in-cluedin"
  },"1351": {
    "doc": "Sync manual data entry to Purview",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of manual data entry projects to Purview, you will receive a notification when the project is synced. To find the asset in Purview . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the manual data entry project. | On the asset details page, go to Lineage. Here, you can view a visual representation of how a manual data entry project moves in the CluedIn processing pipeline. | . ",
    "url": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-manual-data-entry-to-purview#feature-demonstration"
  },"1352": {
    "doc": "Sync manual data entry to Purview",
    "title": "Sync manual data entry to Purview",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-manual-data-entry-to-purview",
    
    "relUrl": "/microsoft-integration/purview/sync-manual-data-entry-to-purview"
  },"1353": {
    "doc": "Traffic manager runbook",
    "title": "On this page",
    "content": ". | Automation account | Input parameters | Process | . The traffic manager runbook will swap the endpoints or the traffic manager following a disaster event. The runbook will switch from the active region to the passive region (and back again). This runbook is a PowerShell script, which CluedIn will provide as needed. This runbook should be run manually following a disaster event. Prerequisites . | An active CluedIn cluster with a valid license (and/or a passive CluedIn cluster) | The runbook script | An automation account | A storage account | Sufficient permissions | . ",
    "url": "/paas-operations/automation/traffic-manager-runbook#on-this-page",
    
    "relUrl": "/paas-operations/automation/traffic-manager-runbook#on-this-page"
  },"1354": {
    "doc": "Traffic manager runbook",
    "title": "Automation account",
    "content": "An automation account must be provided. The runbook will be installed into the automation account. Typically, the runbook should only be run following a disaster event and after discussions with relevant stakeholders. ",
    "url": "/paas-operations/automation/traffic-manager-runbook#automation-account",
    
    "relUrl": "/paas-operations/automation/traffic-manager-runbook#automation-account"
  },"1355": {
    "doc": "Traffic manager runbook",
    "title": "Input parameters",
    "content": "| Parameter | Default | Description | . | Subscription | required | ID of the Azure subscription where the traffic manager is located | . | ResourceGroupName | required | Name of resource group where the traffic manager is located | . | FailToDRDR | true | Move to the DR endpoints; when false move back to original endpoints | . ",
    "url": "/paas-operations/automation/traffic-manager-runbook#input-parameters",
    
    "relUrl": "/paas-operations/automation/traffic-manager-runbook#input-parameters"
  },"1356": {
    "doc": "Traffic manager runbook",
    "title": "Process",
    "content": ". Scaling down of the DR environment following a successful restore is optional. ",
    "url": "/paas-operations/automation/traffic-manager-runbook#process",
    
    "relUrl": "/paas-operations/automation/traffic-manager-runbook#process"
  },"1357": {
    "doc": "Traffic manager runbook",
    "title": "Traffic manager runbook",
    "content": " ",
    "url": "/paas-operations/automation/traffic-manager-runbook",
    
    "relUrl": "/paas-operations/automation/traffic-manager-runbook"
  },"1358": {
    "doc": "How to add an Integration",
    "title": "How to add an Integration",
    "content": "Introduction . An integration should not be specific to a user. When an integration is installed, it generally takes a number of parameters to be able to add it multiple times for different user. Eg: Slack account of multiple organizations, multiple shared email inbox from the same Exchange server, multiple Office 365 accounts… . Please refer to the documentation on How to build an integration . When installing an integration, if you want CluedIn to only read, add the integration with a user that has read-only on ALL information if you want CluedIn to push-back to the integration, add it with an administrator account . Types of integration . Cloud integration . Cloud integration are generally integration for existing SaaS product such as HubSpot, Slack, Dropbox. To authenticate to this integration, we have 3 authentication methods which variates based on the product. Notice, if you are running CluedIn, on-prem, you will need to setup the Oauth process with the product you want to add. Refer to the provider’s documentation and follow the steps. Oauth authentication . In this type of integration, you will be redirect to the Integration’s website where he will ask your permission for CluedIn to access the data. API Token . In this type of integration, you will need to provide a valid API token so CluedIn could access the data. Form Fields . Sometimes, the integration required multiple fields such as a ‘URL’, a username… Be sure to have the correct information before adding them. On-premise integration . Another type of integration are ‘on-prem’, they are integration that you need to install on your servers. A good example is a File system provider which will scan all the files located into a physical hard-drive. Adding an Integration . | Login to CluedIn | Go to the integration section | Click on Available integrations. | . | Click on ‘Add configuration’ | . | Follow the authentication process | . | Configure your integration and add it . | Congratz, your configuration is now added . | . Data coming in . Once the integration added, CluedIn will ingest the date, once that is done, you will receive a notification. Product Owner . You can set the Product Owner when you add an integration. You can have multiple Product Owners and it can change throughout the lifetime of the integration. Setting the Product Owner will dictate certain actions and responsibilities. These responsibilities include: . | They are responsible for accepting or rejecting Mesh Commands. | They will be notified when a Subject Request Access contains data for this integration point. | They will be responsible for the CluedIn Clean projects that contain data from this integration point. | They will be notified when their system is involved in a Data Breach. | They will be responsible for accepting or rejecting the Data involved in a retention setup. | They are responsible for setting the Consent for the properties in their integration. | They are responsible for resolving duplicates in their integration point. | They are responsible for the provider specific Vocabularies and the mappings to Core Vocabulary Keys. | . Integration Access . ",
    "url": "/integration/add-integration",
    
    "relUrl": "/integration/add-integration"
  },"1359": {
    "doc": "Billable records",
    "title": "Billable records",
    "content": "A billable record is a unique data part that forms a golden record. In this case, an ingested record is considered a data part. For more information about records, data parts, and golden records, see Data life cycle. The following video illustrates the difference between billable records and golden records. Not all data parts are considered billable records. The data parts that appeared from the following processes are not billable records: . | Enrichment | Clean projects | User input (editing properties manually) | Shadow entities | Any updates to the source record | . Every time a golden record is processed, the count of billable records is recalculated. If you remove records from CluedIn, the count of billable records would go down as it is recalculated. Each record that you ingest and process from your source counts as a billable record. If 2 records come from the same source and share the same identifier (primary identifier and origin) then they are considered exact duplicates and are merged into one golden record. This is 1 billable record. If 2 similar records come from different sources and are identified as duplicates, they are also merged into 1 golden record, but they will be counted as 2 billable records. If 2 unique records come from the same or different sources, then 2 golden records are created. Therefore, there are 2 data parts, which means that there are 2 billable records. When you ingest a record for the first time, a unique primary identifier is created in the mapping to identify this record once processed. If you later change the value that was used to create the primary identifier in the golden record and re-ingest the original record with the updated value, this will be considered as 2 data parts, resulting in 2 billable records. This is because CluedIn perceives the identifier that was initially created during the mapping as different from the updated record. ",
    "url": "/key-terms-and-features/billable-records",
    
    "relUrl": "/key-terms-and-features/billable-records"
  },"1360": {
    "doc": "Companies House",
    "title": "On this page",
    "content": ". | Add Companies House enricher | Properties from Companies House enricher | . This article outlines how to configure the Companies House enricher. The purpose of this enricher is to get information about companies registered in the UK (for example, address, date of creation, company status, and so on). More details can be found in Properties from Companies House enricher. The Companies House enricher supports the following endpoints: . | https://api.companieshouse.gov.uk/search/company/{companyNumber}, where {companyNumber} is the Companies House number – this endpoint is called when the Companies House number is provided. | https://api.companieshouse.gov.uk/search/companies?q={name}, where {name} is the company name – this endpoint is called when the Companies House number is not provided. | . ",
    "url": "/preparation/enricher/companies-house#on-this-page",
    
    "relUrl": "/preparation/enricher/companies-house#on-this-page"
  },"1361": {
    "doc": "Companies House",
    "title": "Add Companies House enricher",
    "content": "To use the Companies House enricher, you must provide the API key. To get it, you need to register a user account with Companies House. You can add input parameters for the enricher (organization name, country, and Companies House number). The enricher will use either organization name or Companies House number to retrieve information from the Companies House website. However, this step is optional. If you do not provide any input parameters, the following parameters will be used by default: . | Organization Name Vocab Key – organization.name . | Country Vocab Key – organization.address.countryCode . | Companies House Number Vocab Key - organization.codes.companyHouse . | . To add the Companies House enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Companies House, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API key for retrieving information from the Companies House website. | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Companies House Number Vocabulary Key – enter the vocabulary key that contains the Companies House number that will be used for searching the Companies House website. | Country Vocabulary Key – enter the vocabulary key that contains the countries of companies that will be used for searching the Companies House website. | Organization Name Vocabulary Key – enter the vocabulary key that contains the names of companies that will be used for searching the Companies House website. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Companies House enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Companies House enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/companies-house#add-companies-house-enricher",
    
    "relUrl": "/preparation/enricher/companies-house#add-companies-house-enricher"
  },"1362": {
    "doc": "Companies House",
    "title": "Properties from Companies House enricher",
    "content": "You can find the properties added to golden records from the Companies House enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Companies House enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Companies House enricher. | Display name | Vocabulary key | . | Address Line 1 | companyHouse.organization.address+addressLine1 | . | Address Line 2 | companyHouse.organization.address+addressLine2 | . | Locality | companyHouse.organization.address+locality | . | Post Code | companyHouse.organization.address+postCode | . | Charges | companyHouse.organization.charges | . | Company Number | companyHouse.organization.companyNumber | . | Company Status | companyHouse.organization.companyStatus | . | Date Of Creation | companyHouse.organization.dateOfCreation | . | Has_been_liquidated | companyHouse.organization.has_been_liquidated | . | Has_insolvency_history | companyHouse.organization.has_insolvency_history | . | Jurisdiction | companyHouse.organization.jurisdiction | . | Registered_office_is_in_dispute | companyHouse.organization.registered_office_is_in_dispute | . | Type | companyHouse.organization.type | . ",
    "url": "/preparation/enricher/companies-house#properties-from-companies-house-enricher",
    
    "relUrl": "/preparation/enricher/companies-house#properties-from-companies-house-enricher"
  },"1363": {
    "doc": "Companies House",
    "title": "Companies House",
    "content": " ",
    "url": "/preparation/enricher/companies-house",
    
    "relUrl": "/preparation/enricher/companies-house"
  },"1364": {
    "doc": "Dataverse connector",
    "title": "Dataverse connector",
    "content": "This article outlines how to configure the Dataverse connector to publish data from CluedIn to Microsoft Dataverse. Prerequisites: . | Create a service principal (app registration) following the instruction in this article. This step is needed to get Client ID, Tenant ID, and Client Secret for connector configuration. | Make sure you use a service principal to authenticate and access Dataverse. | Make sure you have a Power Apps account. For more information on how to sign up for Power Apps, see Microsoft documentation. | Create a security role in Power Platform Admin Center following the instruction in this article. | Create an application user and tag it with 2 security roles—the role you created in the previous requirement and the System Administrator role—following the instruction in this article. | . To configure Dataverse connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Dataverse Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | URL – you can find this value in Power Apps, in the environment that contains your Dataverse instance. In the upper-right corner of the Power Apps page, select the settings icon, and then select Developer Resources. Copy the value in Web API endpoint and paste it to the URL field in CluedIn. You do not need to copy the version of the API (/api/data/v9.2). | TenantID – unique identifier for your Microsoft Entra tenant. You can find this value in the Overview section of app registration. | ClientID – unique identifier assigned to the Dataverse app when it was registered in the Microsoft identity platform. You can find this value in the Overview section of app registration. | ClientSecret – confidential string used by your Dataverse app to authenticate itself to the Microsoft identity platform. You can find this value in the Certificates &amp; secrets section of app registration. | . | Test the connection to make sure it works, and then select Add. | . ",
    "url": "/consume/export-targets/dataverse-connector",
    
    "relUrl": "/consume/export-targets/dataverse-connector"
  },"1365": {
    "doc": "Deduplication reference",
    "title": "On this page",
    "content": ". | Matching rules | Matching functions | Normalization rules . | Street address normalization | . | Deduplication project . | Project statuses | Project status workflow | . | Group of duplicates . | Group statuses | Group status workflow | . | Deduplication project audit log actions | . In this article, you will find reference information to help you understand matching rules and functions, normalization rules, deduplication project statuses, and group statuses. ",
    "url": "/management/deduplication/deduplication-reference#on-this-page",
    
    "relUrl": "/management/deduplication/deduplication-reference#on-this-page"
  },"1366": {
    "doc": "Deduplication reference",
    "title": "Matching rules",
    "content": "Matching rules allow you to set up complex logic for detecting duplicates. In the deduplication project, matching rules are combined using the OR logical operator, while matching criteria within a rule are combined using the AND logical operator. So, if you want a record to be identified as a duplicate if it meets at least one of your criteria, then you need a separate rule for each criteria. On the other hand, if you want a record to be identified as a duplicate if it meets all of your criteria, then you need one rule that includes all of your criteria. For example, in the following configuration, either records with the same email address or records with the same first and last name are identified as duplicates. ",
    "url": "/management/deduplication/deduplication-reference#matching-rules",
    
    "relUrl": "/management/deduplication/deduplication-reference#matching-rules"
  },"1367": {
    "doc": "Deduplication reference",
    "title": "Matching functions",
    "content": "Matching functions specify how values of a vocabulary key or property should be compared. The following table provides the description of matching functions. | Function | Description | . | Equals | Identifies records as duplicates if two values are the same. | . | Fuzzy match - Sift4 | Detects approximate duplicates using a string similarity algorithm. This algorithm takes into account the number of matching characters and the transpositions (swaps) needed to make the strings identical. For example, algorithm and algorrithm would be identified as fuzzy matches. In this method, you have to specify the threshold— a parameter that sets the maximum number of characters by which two strings can differ and still be considered a match. | . | Fuzzy match - phonetic, DoubleMetaphone | Detects approximate duplicates using a phonetic matching algorithm. This algorithm encodes words into a phonetic representation, allowing for matching based on pronunciation rather than spelling. For example, night and knight would be identified as fuzzy matches. | . | Contains | Matches two values if one contains the other. | . | Starts with | Matches two values if one starts with the other. | . | Ends with | Matches two values if one ends with the other. | . | First token equals | Extracts the first word from both values and compares for exact match ignoring casing. | . | Email | Matches two email address values if they are semantically equal. | . Watch the following video where we provide an explanation of matching functions along with practical examples. ",
    "url": "/management/deduplication/deduplication-reference#matching-functions",
    
    "relUrl": "/management/deduplication/deduplication-reference#matching-functions"
  },"1368": {
    "doc": "Deduplication reference",
    "title": "Normalization rules",
    "content": "Normalization rules clean up values before comparing them to identify duplicates. The following table provides the description of normalization rules. | Normalization rule | Description | . | To lowercase | Converts values to lover case. | . | Trim whitespace | Removes whitespace characters from values. | . | Remove punctuation | Removes punctuation marks from the values. For example, CluedIn.!?, will be converted to CluedIn. | . | Remove special characters | Removes special characters from the values. For example, CluedIn”#€%&amp;(/)€ will be converted to CluedIn. | . | Remove diacritic marks | Removes diacritic marks from the values. For example, spăn'ĭsh will be converted to spanish. | . | Person name variations | Generates person name variations. For example, Timothy Daniel Ward will be converted to Tim D. Ward, Tim Ward. | . | First name nickname variants | Generates nickname variations for the person’s first name. For example, Timothy will be converted to Tim, Timmy. | . | Organization name | Normalizes organization name. For example, CluedIn ApS will be converted to CluedIn. | . | Phone number | Removes well-known phone number formatting characters. For example, +45-123 456 789 (001) will be converted to 45123456789001. | . | Email mask | Normalizes email address to match other variants. | . | Street address | Normalizes common street name tokens. | . | Geography country | Converts country to ISO code. For example, Australia will be converted to AU. | . | Replace text | Replaces text using configured pattern. You need to enter both the value to be replaced and the replacement value. | . | Regex replace text | Replaces text using configured regex pattern. | . | Split text | Splits text using configured separator. | . | Regex split text | Splits text using configured regex separator pattern. | . | Split text by whitespace | Splits text by whitespace into tokens. | . | Transliterate | Transliterates text value to an ASCII string. For example, Ελληνική Δημοκρατία will be converted to Ellēnikē Dēmokratia. | . Watch the following video where we provide an explanation of normalization rules along with practical examples. Street address normalization . Street address normalization converts input addresses into standardized versions before comparing them to identify duplicates. For example, let’s consider two addresses: 123 Fantasy strabe and 123 Fantasy street. When street address normalization is selected, any time CluedIn sees strabe, strasse, street, or str, it turns it into [STR]. If the only thing that differs in an address is the way street is spelled, then the addresses will match. | Input | Normalized version | . | strabe, strasse, street, str\\.?, st | [STR] | . | Allee, aly | [ALY] | . | Boulevard, BLVD | [BLVD] | . | Lane, Ln | [LANE] | . | Drive, Dr | [DRIVE] | . | avenue, ave | [AVE] | . | University, univ | [UNIVERSITY] | . | West, w | [WEST] | . | North, n | [NORTH] | . | East, e | [EAST] | . | South, s | [SOUTH] | . | Floor, flr | [FLOOR] | . | First, 1st | [1ST] | . | Second, 2nd, 2rd | [2ND] | . | Third, 3rd | [3RD] | . | Fourth, 4th | [4TH] | . | Fifth, 5th | [5TH] | . | p\\.?\\s*o\\.?\\s+box, post\\s*box, post\\s*boks, GPO\\s*Box | [POBOX] | . | Private\\sbag, Locked\\sBag, Private\\sMail\\sBag, Locked\\sBag, PMB | [PMB] | . | Plaza, PLAZ, PLZ | [PLZ] | . | Suite, STE | [STE] | . | Trailer, TRLR | [TRLR] | . | Apartment, APT | [APT] | . | Basement, BSMT | [BSMT] | . | Building, BLDG | [BLDG] | . | Department, DEPT | [DEPT] | . | Hanger, HNGR | [HNGR] | . | Lobby, LBBY | [LBBY] | . | Lower, LOWR | [LOWR] | . | Office, OFC | [OFC] | . | Penthouse, PH | [PH] | . | Space, SPC | [SPC] | . | Level, LVL | [LEVEL] | . | Highway, Highwy, Hiway, Hiwy, Hway, Hwy | [HWY] | . ",
    "url": "/management/deduplication/deduplication-reference#normalization-rules",
    
    "relUrl": "/management/deduplication/deduplication-reference#normalization-rules"
  },"1369": {
    "doc": "Deduplication reference",
    "title": "Deduplication project",
    "content": "This section contains reference information about deduplication project statuses. Project statuses . The following table provides descriptions of deduplication project statuses. | Status | Description | . | Requires configuration | The deduplication project has been created, no matching rules have been added yet. | . | Ready to generate | The deduplication project has been configured, no matches have been generated yet. This status also appears when you discard matches at any further stage of the project. | . | Generating | CluedIn is analyzing the specified set of golden records with the aim to detect duplicates based on your matching rules. | . | Aborting generation | The process of generating matches of the deduplication project is being cancelled. The status will shortly change to Ready to generate. | . | Ready for review | CluedIn has generated matches of the deduplication project, you can start processing groups of duplicates. | . | Committing | CluedIn is merging records from selected groups. This status is applicable whether you are merging a single group, multiple groups, or all groups in the project. | . | Merged | CluedIn has merged records from all groups in the project. If the project contains groups that were not selected for merge, the project status remains Ready for review. | . | Unmerging | CluedIn is reverting the results of merge. When records are unmerged, the project status becomes Ready for review. | . | Abort unmerging | The process of reverting changes is being cancelled. The status will shortly change to Ready to generate. | . Project status workflow . The following diagram shows the deduplication project workflow along with its statuses and main activities. ",
    "url": "/management/deduplication/deduplication-reference#deduplication-project",
    
    "relUrl": "/management/deduplication/deduplication-reference#deduplication-project"
  },"1370": {
    "doc": "Deduplication reference",
    "title": "Group of duplicates",
    "content": "This section contains reference information about the statuses of a group of duplicates. Group statuses . The following table provides descriptions of the statuses of groups of duplicates. | Status | Description | . | New | The group with potential duplicates has been generated. This status is also applicable when you revoke the group’s approval. | . | Approved | The group has been approved and it is ready for merge. If you change your mind, you can reject the group. If you are uncertain about the selection of values in the group, you can revoke your approval and start processing the group from scratch. | . | Rejected | The group has been rejected and it cannot be merged. If you change your mind, you can approve the group. | . | Merge Committed | The group has been merged. When all groups in the project have this status, the status of the project becomes Merged. If you are not satisfied with the merged golden record, you can revert the changes by unmerging the records. | . | Unmerged | The changes made to the golden record through merging have been reverted, restoring duplicate records to their previous state before the merge. | . Group status workflow . The following diagram shows the group of duplicates workflow along with its statuses and main activities. ",
    "url": "/management/deduplication/deduplication-reference#group-of-duplicates",
    
    "relUrl": "/management/deduplication/deduplication-reference#group-of-duplicates"
  },"1371": {
    "doc": "Deduplication reference",
    "title": "Deduplication project audit log actions",
    "content": "Whenever some changes or actions are made in the deduplication project, they are recorded and can be found on the Audit Log tab. These actions include the following: . | Create a deduplication project | Add users to owners | Update a deduplication project | Create a deduplication rule | Update a deduplication rule | Activate a matching rule | Deactivate a matching rule | Generate matches | Discard matches | Manual conflict resolution | Reset manual conflict resolutions | Remove entity from a group | Approve one group | Approve multiple groups | Approve all groups | Remove (revoke) approval from all groups | Remove (revoke) approval from groups (one or multiple groups) | Reject groups (one or multiple groups) | Merge groups (one or multiple selected groups) | Merge approved groups | Undo merged entities | Undo merge groups | Abort undo | Cancel generating matches | Cancel merge | Archive a deduplication project | . ",
    "url": "/management/deduplication/deduplication-reference#deduplication-project-audit-log-actions",
    
    "relUrl": "/management/deduplication/deduplication-reference#deduplication-project-audit-log-actions"
  },"1372": {
    "doc": "Deduplication reference",
    "title": "Deduplication reference",
    "content": " ",
    "url": "/management/deduplication/deduplication-reference",
    
    "relUrl": "/management/deduplication/deduplication-reference"
  },"1373": {
    "doc": "Hierarchy builder",
    "title": "Hierarchy builder",
    "content": "Hierarchy Builder helps you organize, visualize, and manage relations between golden records within and across different business domains. This section covers the following topics: . | Concept of hierarchy – explore hierarchies through an example of Organization-Project-Employee hierarchy. | Create a hierarchy – learn about various options for creating a hierarchy. | Work in a hierarchy project – learn how to modify a hierarchy, load golden records of other business domains into the hierarchy, and use various tools to make working in the hierarchy project more convenient. | Manage hierarchies – learn how to clone, export, and delete a hierarchy. | . Useful links: . | How to build an organizational hierarchy | . ",
    "url": "/management/hierarchy-builder",
    
    "relUrl": "/management/hierarchy-builder"
  },"1374": {
    "doc": "Logs",
    "title": "Logs",
    "content": "Logs inform you about warnings or errors in your records based on the built-in logic and limitations of CluedIn. These logs are generated for records only, not for the infrastructure. Be sure to check the logs before processing the data. There are two levels of logs: . | Warning – the data has been processed, but some values have been changed. | Error – the data cannot be processed because it does not correspond to CluedIn business rules or limitations. | . CluedIn performs validations on the following stages of the record life cycle: parsing, storing, mapping, and processing. If an inconsistency is detected at any of these stages, a new log entry will appear on the Logs tab of the data set. By selecting the log message, you will find the error or warning details. In case of an error, related records will also be displayed, providing information for debugging purposes. If a record contains an error, it is not saved in CluedIn. However, if CluedIn recognizes an invalid character, it will convert it automatically to a valid one and the record will be stored in the platform. The following table contains examples of validation messages explaining what has gone wrong with the record. | Data source type | Log level | Validation message | . | Endpoint | Error | JSON is not an array. | . | Endpoint | Error | JSON is not a valid JSON object. | . | Endpoint | Error | JSON array contains more than 100,000 records. | . | File | Error | File could not be parsed. | . | Endpoint/File/SQL | Error | Record has more than 500 columns. | . | Endpoint/File/SQL | Error | The object is either null or empty or is an empty object ({}). | . | Endpoint/File/SQL | Error | Property in the record is invalid. | . | Endpoint/File/SQL | Error | Property in the record has Arabic letters (not supported), Asian letters (not supported), or Cyrillic letters (not supported). | . | Endpoint/File/SQL | Warning | Property in the record had the key named “id” and it was renamed to “_id”. | . | Endpoint/File/SQL | Warning | Property in the record had spaces and they were removed. | . | Endpoint/File/SQL | Warning | Property in the record had dots and they were removed. | . The reason for strict rules for property names is that property names are translated to vocabulary keys using the same pattern as the Microsoft Common Data Model. You can delete the logs by selecting Purge Logs. This action does not affect the data set, it only deletes all logs for the data set. ",
    "url": "/integration/additional-operations-on-records/logs",
    
    "relUrl": "/integration/additional-operations-on-records/logs"
  },"1375": {
    "doc": "Lookup data type",
    "title": "On this page",
    "content": ". | Overview | Add reference data . | Upload a file with reference data | Create a manual data entry project | . | Create a glossary term | Change data type of vocabulary key | Find anomalies in lookup vocabulary keys | Fix invalid values in lookup vocabulary keys | Example of using countries reference data | . In this article, you will learn how to work with vocabulary keys that use the Lookup data type to ensure that only valid values are entered in golden records. You will also learn how to identify and fix anomalies in lookup vocabulary keys. ",
    "url": "/management/data-catalog/lookup-data-type#on-this-page",
    
    "relUrl": "/management/data-catalog/lookup-data-type#on-this-page"
  },"1376": {
    "doc": "Lookup data type",
    "title": "Overview",
    "content": "A Lookup data type is a data type that allows a vocabulary key to get its value from a predefined list of values, typically stored in a glossary term. When a vocabulary key is configured with the Lookup data type, it means that its values are restricted to this controlled list. Instead of manually entering free-form text, users select from a dropdown list of values. This helps maintain consistency, standardization, and data quality across your golden records. The Lookup data type provides an effective way to implement and manage reference data in CluedIn. Reference data consists of standardized, stable values used to classify or categorize other types of data. Common examples of reference data include country codes, currency codes, units of measurement, industry types, or product categories. The process of creating a vocabulary key of the Lookup data type consists of the following steps: . | Adding reference data to CluedIn . | Creating a glossary term that lists all reference data values that can be used in a vocabulary key of the Lookup data type. | Changing the data type of the vocabulary key and associating it with the relevant glossary term. | . ",
    "url": "/management/data-catalog/lookup-data-type#overview",
    
    "relUrl": "/management/data-catalog/lookup-data-type#overview"
  },"1377": {
    "doc": "Lookup data type",
    "title": "Add reference data",
    "content": "You can add reference data to CluedIn in one of the following ways: . | By uploading a file with reference data. | By ingesting reference data from a database or an ingestion endpoint. | By creating a manual data entry project for adding reference records. | . In this article, we’ll explore two options for adding reference data: uploading a file and creating a manual data entry project. Note that the processing of reference data ingested from a database or an ingestion endpoint follows the same steps as file-based ingestion. Upload a file with reference data . If you have a file with reference data that you want to use in your project, you can upload it to CluedIn. Once the file is uploaded, map it to standard fields. When creating mapping for the data set, make sure you use a business domain and a vocabulary that represent the concept of your reference data. For more information about getting your data into CluedIn, see Getting started with data ingestion. In this article, we’ll use the example of the currency reference data. We mapped the reference data to the Currency business domain and the ProjectCurrency vocabulary. After processing the data set, we have a certain number of valid currency records. These represent all the currencies permitted for use in the project. To add these currencies to the vocabulary key of the Lookup data type, first create a glossary category, and then define a term under that category to represent the currency list. Create a manual data entry project . If you do not have the existing reference data that you can ingest to CluedIn, you can create reference data records with the help of a manual data entry project. First, create a manual data entry project using the business domain and a vocabulary that represent the concept of your reference data. Then, add the form fields for entering the reference data. For example, if you want to enter currency reference data, you can add two fields: ID and Name. Finally, add the currency reference records manually one by one in the manual data entry project. When you create all the needed currencies, create a glossary category and then define a term under that category to represent the currency list. ",
    "url": "/management/data-catalog/lookup-data-type#add-reference-data",
    
    "relUrl": "/management/data-catalog/lookup-data-type#add-reference-data"
  },"1378": {
    "doc": "Lookup data type",
    "title": "Create a glossary term",
    "content": "Now that you have the needed reference data in CluedIn, create a glossary category and define a term under that category to represent the currency list. When creating a glossary term, define which records should be included the term. In this case, we are listing all currencies that belong to the Currency business domain. Once you configure and activate the glossary term, you can see the list of acceptable currency values on the Matches tab. For more information, see Getting started with Glossary in CluedIn. A glossary term must be active for its values to appear as the allowed options in vocabulary keys that use the Lookup data type. ",
    "url": "/management/data-catalog/lookup-data-type#create-a-glossary-term",
    
    "relUrl": "/management/data-catalog/lookup-data-type#create-a-glossary-term"
  },"1379": {
    "doc": "Lookup data type",
    "title": "Change data type of vocabulary key",
    "content": "Now that you have created and activated a glossary term containing the list of acceptable currencies, the next step is to decide which vocabulary key in your project should be associated with this currency list. For example, suppose you have a trainingcompany.currency vocabulary key used in golden records. You can configure this vocabulary key to the Lookup data type so that users can select values from the predefined currency list, ensuring consistency and accuracy. To do this, edit the vocabulary key, change the data type to Lookup, and then select the glossary term that contains the list of allowed values. After saving changes, the vocabulary key is reprocessed and its data type is changed. As a result, 3 computed keys are added to the golden record that contains a vocabulary key of the Lookup data type: Entity Code, Entity ID, and Name. These computed keys are parts of the lookup vocabulary key. You may notice these keys on the search results page, on the golden record Overview page, in the data catalog, and in filters in some places of the platform. Note that at time of processing a rule, the -Entity Code part of the lookup vocabulary key is the only property available, not the -Name or the -Entity ID. So, if you are trying to do comparisons across two lookup vocabulary keys, then you must use the -Entity Code to do the comparison. When lookup reference data is changed or updated, it will be automatically reflected anywhere the vocabulary key of the Lookup data type is used. This means that you do not need to take any additional actions after modifying reference data. When you try to edit the currency property in a golden record, you can select a currency from the predefined list, but you cannot create a new currency value. If the currency is not in the glossary term, it will be marked as an invalid value. To fix this, edit the property and select the value from the predefined list. To learn how to quickly identify invalid reference data, see Find anomalies in reference data . ",
    "url": "/management/data-catalog/lookup-data-type#change-data-type-of-vocabulary-key",
    
    "relUrl": "/management/data-catalog/lookup-data-type#change-data-type-of-vocabulary-key"
  },"1380": {
    "doc": "Lookup data type",
    "title": "Find anomalies in lookup vocabulary keys",
    "content": "This feature is available starting from 2025.05 release. To quickly find anomalies in vocabulary keys that use the Lookup data type, create a golden record rule with the corresponding action. First, define which golden records the rule should be applied to. Then, add the rule action: . | Enter the action name. | Specify the condition for identifying invalid values: find and select the needed lookup vocabulary key and use the Is Invalid Lookup operator. | Enter the tag that should be added to golden records that contain invalid values. | . After saving, activating, and reprocessing the rule, the tag will be added to golden records that contain invalid values in the lookup vocabulary key. To verify that the rule has been applied, go to search and use the Tags filter. As a result, all golden records that contain invalid values in the lookup vocabulary key will be displayed on the page. Learn how to fix invalid values in the following section. ",
    "url": "/management/data-catalog/lookup-data-type#find-anomalies-in-lookup-vocabulary-keys",
    
    "relUrl": "/management/data-catalog/lookup-data-type#find-anomalies-in-lookup-vocabulary-keys"
  },"1381": {
    "doc": "Lookup data type",
    "title": "Fix invalid values in lookup vocabulary keys",
    "content": "Once you have identified invalid values that are used in the vocabulary key of the Lookup data type, create a clean project. If is convenient to create a clean project from the search results page that displays tagged golden records. In the clean project configuration, add the vocabulary key that contains invalid values. In this case, it is trainingcompany.currency. Once the records are loaded into the clean project, you can start fixing invalid values. For example, you can group invalid values using text facet and edit them in bulk. To learn more about fixing values in a clean project, see Clean data. After correcting all invalid values, process your changes. If the updated values match the allowed values in the lookup vocabulary key, they will no longer be marked as invalid. ",
    "url": "/management/data-catalog/lookup-data-type#fix-invalid-values-in-lookup-vocabulary-keys",
    
    "relUrl": "/management/data-catalog/lookup-data-type#fix-invalid-values-in-lookup-vocabulary-keys"
  },"1382": {
    "doc": "Lookup data type",
    "title": "Example of using countries reference data",
    "content": "Let’s consider an example of using the countries reference data in scenarios where a country can be represented in multiple formats—such as a 2-letter code, 3-letter code, or the full country name. In this case, the lookup vocabulary key serves not only to retrieve the standardized value from the list, but also to ensure the appropriate display of the full country name wherever the country reference is used. For example, instead of displaying a code like US, the system would use the lookup to show United States of America. To illustrate this example, we’ll use company records. Notice the Country column, which contains various formats for referring to a country—such as two-letter codes, three-letter codes, and full country names. To ensure consistency and use standardized values for countries, we need to add and configure the country reference data accordingly. The first step is to add reference data to CluedIn—a list of countries that includes the following columns: ID, DisplayName, TwoLetter, ThreeLetter, and Population. This reference data captures multiple formats for referring to a country, allowing CluedIn to recognize and standardize country values across different representations. We mapped the reference data to the Country business domain and the ProjectCountry vocabulary. Next, we need to review the mapping details on the Map Entity tab and make several changes: . | In the General Details section, in the Entity Name dropdown field, select the vocabulary key that contains the full name of the country (DisplayName). | In the Identifiers section, add the following properties for generating additional identifiers: DisplayName, TwoLetter, ThreeLetter. This allows CluedIn to recognize and resolve country references expressed in any of these formats. When CluedIn encounters a country reference—whether as a two-letter code, three-letter code, or full name—it will match it against the appropriate identifier and return the corresponding value from the DisplayName property, ensuring that the full country name is displayed consistently. Make sure to use a custom origin (for example, country) when generating these identifiers to maintain proper context. | . Once the mapping is complete, process the data set and create a glossary term to represent the list of countries. Ensure the glossary is activated so that its values can be used as allowed options in vocabulary keys that use the Lookup data type. The values from the Name column in the glossary will then be available for selection in a dropdown list wherever the vocabulary key is applied. Once the glossary term containing the list of acceptable countries is ready, the next step is to configure the vocabulary key that should be linked to this country list (for example, trainingcompany.country). To do this, edit the vocabulary key, change its data type to Lookup, and select the appropriate glossary term that contains the allowed country values. After saving the changes, the vocabulary key is reprocessed and its data type is updated accordingly. As a result, the Lookup Data vocabulary is added to the company golden record Overview page, reflecting the country value selected through the lookup vocabulary key. This ensures that the golden record includes standardized, structured information about the associated country, based on the predefined list from the glossary. The initial two-letter and three-letter country codes have been changed to the full country name. To sum up, when working with reference data that includes multiple formats for referring to the same entity—such as country codes and names—you can use a Lookup vocabulary key to standardize these references. For example, a country like Australia might be represented by several valid country names: AU, AUS, or Australia. The Lookup vocabulary key will attempt to resolve the reference by cycling through all available identifiers—including the two-letter code, three-letter code, and full country name. This approach ensures flexible and accurate matching against the standardized country list. The same approach can be applied to currency reference data, where values such as currency codes (for example, USD, EUR) and names (for example, US Dollar, Euro) are resolved through a Lookup vocabulary key to maintain consistency and accuracy across records. ",
    "url": "/management/data-catalog/lookup-data-type#example-of-using-countries-reference-data",
    
    "relUrl": "/management/data-catalog/lookup-data-type#example-of-using-countries-reference-data"
  },"1383": {
    "doc": "Lookup data type",
    "title": "Lookup data type",
    "content": " ",
    "url": "/management/data-catalog/lookup-data-type",
    
    "relUrl": "/management/data-catalog/lookup-data-type"
  },"1384": {
    "doc": "CluedIn Magic",
    "title": "On this page",
    "content": ". | Get CluedIn context | Search | . IPython, the command shell behind Jupyter notebooks, provides an awesome feature called magics. In short, you can skip writing Python code and use more like command line syntax. This approach can simplify many repeating tasks, including work with CluedIn Python SDK. In this article, I want to introduce you to CluedIn Magic - the package that lets you work with CluedIn API with minimal code. CluedIn Magic depends on CluedIn Python SDK, so you only need to install one package to get them both: . %pip install cluedin-magic . When working with products like Microsoft Fabric, Synapse Analytics, Databricks, etc., I usually pre-install packages in an environment so you don’t have to run the above line. Now, we can load CluedIn Magic by calling the %load_ext magic: . %load_ext cluedin_magic . After this, you can call %cluedin magic. If you do it without parameters or with wrong parameters, it will give you a brief help: . Available commands: get-context, search Usage: %cluedin get-context --jwt &lt;jwt&gt; %cluedin search --context &lt;context&gt; --query &lt;query&gt; [--limit &lt;limit&gt;] . ",
    "url": "/playbooks/data-engineering-playbook/magic#on-this-page",
    
    "relUrl": "/playbooks/data-engineering-playbook/magic#on-this-page"
  },"1385": {
    "doc": "CluedIn Magic",
    "title": "Get CluedIn context",
    "content": "When you work with CluedIn API, you need a context—a domain, organization name, email, password, or API token. What if I tell you that you just need the API token, and then CluedIn Magic will automagically resolve the rest? Let’s try it! . At first, you only need an API token — you can get one from Administration -&gt; API Tokens in CluedIn. In the example below, I store it in an environment variable and then can get into a variable: . access_token = %env ACCESS_TOKEN . ctx = %cluedin get-context --jwt $access_token . Now, just give it to CluedIn Magic and it will give you a working CluedIn context: . ctx = %cluedin get-context --jwt eyJhbGci...5Odvpr1g . You can use this context now with CluedIn Python SDK or CluedIn Magic. ",
    "url": "/playbooks/data-engineering-playbook/magic#get-cluedin-context",
    
    "relUrl": "/playbooks/data-engineering-playbook/magic#get-cluedin-context"
  },"1386": {
    "doc": "CluedIn Magic",
    "title": "Search",
    "content": "Say you want to load all /Infrastructure/User entities — provide a context and a query, and get a pandas DataFrame with your data: . %cluedin search --context ctx --query +entityType:/Infrastructure/User . You can get a sample by providing a limit if you have millions of entities. In the next example, I get ten entities out of all entities in the system: . %cluedin search --context ctx --query * --limit 10 . In the next example, I get ten records of type /IMDb/Name where imdb.name.birthYear vocabulary key property does not equal \\\\N: . %cluedin search --context ctx --query +entityType:/IMDb/Name -properties.imdb.name.birthYear:\"\\\\\\\\N\" --limit 10 . You can also save the results in a variable and use it as a usual pandas DataFrame: . pd = %cluedin search --context ctx --query +entityType:/IMDb/Name +properties.imdb.name.birthYear:1981 pd.head() . ",
    "url": "/playbooks/data-engineering-playbook/magic#search",
    
    "relUrl": "/playbooks/data-engineering-playbook/magic#search"
  },"1387": {
    "doc": "CluedIn Magic",
    "title": "CluedIn Magic",
    "content": " ",
    "url": "/playbooks/data-engineering-playbook/magic",
    
    "relUrl": "/playbooks/data-engineering-playbook/magic"
  },"1388": {
    "doc": "Azure Open AI Integration",
    "title": "Azure Open AI Integration",
    "content": "With Azure OpenAI integration, you can leverage AI capabilities to analyze data and determine mapping for you. In this article, you will learn what you need to do to make the AI mapping available in CluedIn. For Azure Administrators and CluedIn Organization Administrators . | In CluedIn, go to Administration &gt; Feature Flags, and then enable the AI Mapping feature. | Go to Administration &gt; Settings. Scroll down to the Open AI section and complete the following fields: . | API Key – you can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Keys and Endpoint. You can use either KEY 1 or KEY 2. | Base URL – you can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Keys and Endpoint, and then get the value from the Endpoint field. Alternatively, you can find this value in Azure OpenAI Studio by going to Playground &gt; View code. | Resource Key – you can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Keys and Endpoint. Since you have already used one key in the API Key field, use the other key in the Resource Key field. | Deployment Name – this is the custom name you chose for your deployment when you deployed a model. You can find this value in your Azure OpenAI resource. In Azure portal, go to Resource Management &gt; Model Deployments. Alternatively, you can find this value in Azure OpenAI Studio by going to Management &gt; Deployments. For more information about the required variables, see Microsoft documentation. | . | . Notes on CluedIn’s integration with Azure OpenAI . CluedIn integrates with Azure OpenAI services solely to enable you to make requests and store responses through our platform. To interact with Azure OpenAI services via CluedIn’s Rule Engine, AI mapping, or Copilot, you must decide where to host Azure OpenAI, including which tenants and locations to use. CluedIn does not provide a default Azure OpenAI token; you must configure CluedIn with your own Azure OpenAI token. Selection of Azure OpenAI services and models within your environment is entirely at your discretion. CluedIn transmits your prompts and the data contained within them to Azure OpenAI, giving you full control over the models and deployments used within your environment. Your prompts and results are also stored in CluedIn’s SQL server to preserve your prompt history. Please note that CluedIn cannot comment on how Azure OpenAI manages or processes data internally. Data handling by Azure OpenAI is governed solely by the agreement established directly between you and Microsoft Azure. ",
    "url": "/microsoft-integration/open-ai-integration",
    
    "relUrl": "/microsoft-integration/open-ai-integration"
  },"1389": {
    "doc": "PAT",
    "title": "PAT",
    "content": "To deploy CluedIn, you need a Personal Access Token (PAT) to access CluedIn NuGet feeds. You can get one from our support: support@cluedin.com. ",
    "url": "/deployment/azure/pat",
    
    "relUrl": "/deployment/azure/pat"
  },"1390": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "On this page",
    "content": ". | Azure Kubernetes Service | kubectl | Helm | Visual Studio Code | Lens or Freelens | . Before starting CluedIn upgrade, make sure you have the necessary tools installed and ready: . | kubectl . | Helm . | (Optional) Visual Studio Code . | (Optional) Lens or Freelens . | . ",
    "url": "/paas-operations/upgrade/guide/required-tools#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#on-this-page"
  },"1391": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "Azure Kubernetes Service",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/required-tools#azure-kubernetes-service",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#azure-kubernetes-service"
  },"1392": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "kubectl",
    "content": "kubectl is command-line tool for interacting with Kubernetes clusters. It lets you deploy applications, inspect and manage cluster resources, and view logs. When it comes to CluedIn, kubectl lets you communicate directly with the Kubernetes API server defined in your kubeconfig file. This means that you can: . | Inspect cluster resources (pods, services, deployments, and nodes). | Apply configuration files (kubectl apply -f deployment.yaml). | Scale applications up or down. | Restart, delete, or debug the workloads. | . Without kubectl, there is no simple way to manage or query what’s running inside your AKS cluster. Required version: 1.30 or higher. Installation instructions: See Kubernetes documentation. ",
    "url": "/paas-operations/upgrade/guide/required-tools#kubectl",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#kubectl"
  },"1393": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "Helm",
    "content": "Helm is a package manager for Kubernetes. It simplifies the deployment, upgrade, and management of applications by using reusable, versioned packages called charts. When it comes to CluedIn, we use Helm for upgrades because it makes updating applications simple, consistent, and reversible. With a single command, you can apply changes while keeping version history for easy rollbacks. Required version: 3.x.x. Installation instructions: See Helm documentation. ",
    "url": "/paas-operations/upgrade/guide/required-tools#helm",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#helm"
  },"1394": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "Visual Studio Code",
    "content": "Visual Studio Code is lightweight, cross-platform code editor. The editor is useful for editing YAML files and reviewing configuration files during the upgrade. This tool is optional to use. Download link: Official website. ",
    "url": "/paas-operations/upgrade/guide/required-tools#visual-studio-code",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#visual-studio-code"
  },"1395": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "Lens or Freelens",
    "content": "Lens and Freelens are powerful, free tools designed to monitor and manage Kubernetes clusters. They provide a user-friendly graphical interface that simplifies multiple everyday Kubernetes tasks. By reducing the need to recall and execute long or complex command-line instructions, these tools improve productivity and save valuable time. Reasons to use Lens: . | Ease of use – It offers an intuitive dashboard to view and manage cluster resources. | Productivity boost – It eliminates the need to memorize kubectl commands for common tasks. | Built-in logs – The Freelens version includes built-in log viewing, which makes it especially useful for troubleshooting. | . We recommend using Freelens, as it includes built-in log access and offers a more complete out-of-the-box experience. For teams that work regularly with Kubernetes, Lens can quickly become an indispensable daily tool for monitoring and troubleshooting clusters. Once you connect Lens (or Freelens) your CluedIn cluster, it allows you to: . | View and manage pods, services, deployments, and namespaces. | Monitor CPU, memory, and other resource usage. | Access and search through logs directly from the UI. | Inspect and edit Kubernetes objects without leaving the dashboard. | . Lens and Freelens are optional to use. You can download them from: . Download links: . | Official Lens website . | Official Freelens website . | . ",
    "url": "/paas-operations/upgrade/guide/required-tools#lens-or-freelens",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools#lens-or-freelens"
  },"1396": {
    "doc": "Required tools for CluedIn upgrade",
    "title": "Required tools for CluedIn upgrade",
    "content": " ",
    "url": "/paas-operations/upgrade/guide/required-tools",
    
    "relUrl": "/paas-operations/upgrade/guide/required-tools"
  },"1397": {
    "doc": "Create rules",
    "title": "On this page",
    "content": ". | Create rule | Reprocess records | Modify rule . | Edit rule | Inactivate rule | Delete rule | . | Results &amp; next steps | . Rule Builder allows you to create rules for cleaning, transforming, normalizing, and modifying the data. &lt; In this article, you will learn how to create rules in CluedIn using the Rule Builder tool. You can create a rule either before or after processing the data. Before you start: Make sure you have completed all steps in the Ingest data guide. ",
    "url": "/getting-started/rule-builder#on-this-page",
    
    "relUrl": "/getting-started/rule-builder#on-this-page"
  },"1398": {
    "doc": "Create rules",
    "title": "Create rule",
    "content": "Creating a rule involves configuring a filter and defining the rule action. To create a rule . | On the navigation pane, go to Management &gt; Rule Builder. | Choose the type of rule that you want to create. Then, select Create Rule. | Enter the name of the rule. Then, select Create. The rule details page opens. | In the Filters section, select Add First Filter, and then specify to which items the rule will be applied: . | Select the type of property (Property or Vocabulary). | Depending on the type of property that you selected before, find and select the needed property or vocabulary key. | Select the operation. | Select the value of the property or vocabulary key. If you want to create a rule for a value that does not yet exist in CluedIn, then enter a new value. The fields for configuring a filter appear one by one. After you complete the previous field, the next field appears. For more information, see Filters. | . | In the Actions section, select Add Action, and then configure the action that CluedIn can perform on the filtered items: . | Enter the name of the action. | (Optional) If you want to narrow down your filter query, specify additional conditions for the rule. To do this, in the Conditions section, select Add first rule, and then configure the condition according to the instructions from step 4. | In the Action section, select the action to be performed by the rule. To learn more about what each action does, see Rules reference. | Depending on the action that you selected, provide the required information. For example, if you selected the Add tag action, specify the value of the tag. | In the lower-right corner, select Add Action. You can add multiple actions to the rule. | . | In the upper-right corner of the rule details page, select Save, and then confirm your choice. | Activate the rule by turning on the toggle next to the rule status. | Depending on whether the rule applies to the processed or unprocessed data, do one of the following: . | If the rule applies to the processed data, reprocess the records. | If the rule applies to the unprocessed data, process the data as described in the Ingest data guide. In this case, the rule will be applied to the records automatically during processing. | . | . ",
    "url": "/getting-started/rule-builder#create-rule",
    
    "relUrl": "/getting-started/rule-builder#create-rule"
  },"1399": {
    "doc": "Create rules",
    "title": "Reprocess records",
    "content": "After you created the rule for the processed data, you need to reprocess the records to apply the rule. You can reprocess the records in one of the following ways: . | Reprocess records via the rule details page. | Reprocess records using the GraphQL tool. | Reprocess each record manually. | . To reprocess records via the rule details page . | Near the upper-right corner of the rule details page, select the reprocess icon. | Confirm that you want to reprocess the records associated with the rule. After the reprocessing is completed, the records associated with the rule are updated in accordance with the rule’s actions. | . To reprocess records using the GraphQL tool . | On the navigation pane, go to Consume &gt; GraphQL. | Enter a query to reprocess all records that belong to a certain business domain. Replace TrainingContact with the needed name of business domain. { search(query: \"entityType:/TrainingContact\") { entries { actions { postProcess } } } } . | Execute the query. You reprocessed all records that belong to a certain business domain. Now, the action from the rule is applied to those records. | . To reprocess a record manually . | Find and open the needed record. | In the upper-right corner of the record details page, select More &gt; Reprocess entity. You reprocessed the record. Now, you can view the result of an action performed by the rule. | To reprocess other records, repeat steps 1–2. | . ",
    "url": "/getting-started/rule-builder#reprocess-records",
    
    "relUrl": "/getting-started/rule-builder#reprocess-records"
  },"1400": {
    "doc": "Create rules",
    "title": "Modify rule",
    "content": "After you created the rule, you can edit, inactivate, or delete it. Edit rule . If you want to change the rule—name, description, filters, or actions—edit the rule. To edit the rule . | In the rule details page, make the needed changes. | Near the upper-right corner of the rule details page, select Save. If you edited filters or actions, the confirmation dialog appears where you have to decide what to do with the records associated with the rule. | In the confirmation dialog, do one of the following: . | If you want to reprocess the records affected both by the previous and current rule configuration, select the checkbox, and then confirm your choice. For example, in the previous configuration, the rule added the tag Prospect to all records of the TrainingContact business domain. If you edit the rule filter and change the business domain to Contact, then selecting the checkbox will remove the tag from the records of the TrainingContact business domain and add it to the records of the Contact business domain. | If you don’t want to reprocess the records affected both by the previous and current rule configuration, leave the checkbox unselected, and then confirm your choice. You can reprocess such records later. However, note that reprocessing via the rule details page applies only to the records matching the current rule configuration. To revert rule actions on records matching the previous rule configuration, you’ll need to reprocess such records via GraphQL or manually. | . | . Inactivate rule . If you currently do not need the rule, but might need it in future, inactivate the rule. To inactivate the rule . | Open the rule. | Inactivate the rule by turning on the toggle next to the rule status. You inactivated the rule, but the records to which the rule was applied still contain the changes made by the rule. | To return the records to which the rule was applied to their original state, reprocess the records. | . Delete rule . If you no longer need the rule, delete it. To delete the rule . | Open the rule. | On the rule details page, select the delete icon, and then confirm that you want to delete the rule. You deleted the rule, but the records to which the rule was applied still contain the changes made by the rule. | To return the records to which the rule was applied to their original state, reprocess the records. | . ",
    "url": "/getting-started/rule-builder#modify-rule",
    
    "relUrl": "/getting-started/rule-builder#modify-rule"
  },"1401": {
    "doc": "Create rules",
    "title": "Results &amp; next steps",
    "content": "After completing all steps outlined in this guide, you learned how to create rules to manage your records in CluedIn and how to apply the actions of the rule to the records associated with the rule. Next, learn how to visualize relations between golden records with the help of Hierarchy Builder in our Create hierarchies guide. ",
    "url": "/getting-started/rule-builder#results--next-steps",
    
    "relUrl": "/getting-started/rule-builder#results--next-steps"
  },"1402": {
    "doc": "Create rules",
    "title": "Create rules",
    "content": " ",
    "url": "/getting-started/rule-builder",
    
    "relUrl": "/getting-started/rule-builder"
  },"1403": {
    "doc": "Sync processing rules to Purview",
    "title": "On this page",
    "content": ". | Preparation in CluedIn | Feature demonstration | . In this article, you will learn how to sync CluedIn rules (data part rules, survivorship rules, golden records rules) to Purview assets. ",
    "url": "/microsoft-integration/purview/sync-processing-rules-to-purview#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-processing-rules-to-purview#on-this-page"
  },"1404": {
    "doc": "Sync processing rules to Purview",
    "title": "Preparation in CluedIn",
    "content": ". | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Processing Rules to Purview. | Select Save. | Make sure you have existing rules of any type. | . ",
    "url": "/microsoft-integration/purview/sync-processing-rules-to-purview#preparation-in-cluedin",
    
    "relUrl": "/microsoft-integration/purview/sync-processing-rules-to-purview#preparation-in-cluedin"
  },"1405": {
    "doc": "Sync processing rules to Purview",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of rules to Purview, you will receive a notification when the rule is synced. To find the asset in Purview . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the data set in CluedIn. | On the asset details page, go to Lineage. Here, you can view a visual representation of how rules are applied to the data set within the CluedIn processing pipeline. The following screenshot shows the application of a data part rule, a survivorship rule, and a golden record rule within the CluedIn processing pipeline. | . ",
    "url": "/microsoft-integration/purview/sync-processing-rules-to-purview#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-processing-rules-to-purview#feature-demonstration"
  },"1406": {
    "doc": "Sync processing rules to Purview",
    "title": "Sync processing rules to Purview",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-processing-rules-to-purview",
    
    "relUrl": "/microsoft-integration/purview/sync-processing-rules-to-purview"
  },"1407": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Working with Tags in Golden Record and Data Part Rules",
    "content": "Tags in CluedIn allow you to add metadata labels to records that can drive automation, filtering, and reporting. When used together with Golden Record Rules and Data Part Rules, tags become a powerful way to classify, prioritize, and enrich your data lifecycle. ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#working-with-tags-in-golden-record-and-data-part-rules",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#working-with-tags-in-golden-record-and-data-part-rules"
  },"1408": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "What Are Tags?",
    "content": ". | Tags are labels you can attach to entities, golden records, or data parts. | They provide contextual information (e.g., VIP Customer, DoNotContact, HighRisk). | Tags can be added automatically by Rules or manually by data stewards. | Tags are non-destructive – they don’t overwrite existing values but enrich records with extra classification. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#what-are-tags",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#what-are-tags"
  },"1409": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Why Use Tags with Rules?",
    "content": ". | Golden Record Rules: Apply tags to identify or categorize the final, trusted version of a record. Example: Mark customers who pass validation as Verified. | Data Part Rules: Apply tags at the data part (source record) level to flag issues or classify records before they are merged. Example: Tag data from a specific system as LegacySource. | . Benefits: . | Drive workflow automation (e.g., escalate tagged entities to specific teams). | Improve search and filtering in the CluedIn portal. | Support compliance and governance (e.g., tag PII records). | Provide visibility into how records are sourced and resolved. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#why-use-tags-with-rules",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#why-use-tags-with-rules"
  },"1410": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "How to Work with Tags in Golden Record Rules",
    "content": "Step 1: Navigate to Rules . | Log in to the CluedIn Portal. | Go to Governance &gt; Rules. | Choose Golden Record Rules. | . Step 2: Create or Edit a Rule . | Click New Rule or select an existing Golden Record Rule to edit. | . Step 3: Define Conditions . | Configure the conditions that determine when a record should be tagged. | Example: If Customer.Spend &gt; 10000, then apply tag HighValue. | . Step 4: Add Tag Action . | In the Actions section, choose Add Tag. | Enter one or more tags to apply (e.g., VIP, PrioritySupport). | . Step 5: Save &amp; Activate . | Save the Rule and publish it. | Once active, matching golden records will automatically be tagged. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#how-to-work-with-tags-in-golden-record-rules",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#how-to-work-with-tags-in-golden-record-rules"
  },"1411": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "How to Work with Tags in Data Part Rules",
    "content": "Step 1: Navigate to Rules . | In the CluedIn Portal, go to Governance &gt; Rules. | Choose Data Part Rules. | . Step 2: Create or Edit a Rule . | Click New Rule or edit an existing Data Part Rule. | . Step 3: Define Conditions . | Set up conditions to identify specific source records. | Example: If SourceSystem = LegacyCRM, then apply tag LegacySource. | . Step 4: Add Tag Action . | In the Actions section, choose Add Tag. | Apply tags such as NeedsValidation, LowConfidence, or HighRisk. | . Step 5: Save &amp; Activate . | Save and publish the Rule. | Tags will now apply to data parts that meet your conditions. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#how-to-work-with-tags-in-data-part-rules",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#how-to-work-with-tags-in-data-part-rules"
  },"1412": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Best Practices for Using Tags",
    "content": ". | Use Clear Naming Conventions: Keep tag names short, consistent, and meaningful (e.g., HighValueCustomer, not customer_high_value_flag). | Automate Where Possible: Use Rules to apply tags automatically rather than relying on manual tagging. | Leverage Tags in Search &amp; Filters: Use the portal’s search filters to find records with specific tags. | Combine with Workflows: Route tagged records into downstream workflows (e.g., send DoNotContact to marketing suppression lists). | Audit Regularly: Review tag usage periodically to ensure consistency and remove obsolete tags. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#best-practices-for-using-tags",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#best-practices-for-using-tags"
  },"1413": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Common Use Cases",
    "content": ". | Tagging VIP customers in Golden Record Rules for reporting. | Flagging low-confidence data parts for stewardship review. | Marking legacy system records during migration projects. | Adding compliance-related tags like ContainsPII. | Tagging products as Discontinued for downstream systems. | . ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#common-use-cases",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#common-use-cases"
  },"1414": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Summary",
    "content": "Tags in CluedIn provide an effective way to enrich, classify, and control data across Golden Records and Data Parts. By combining Tags with Rules, you can ensure that important metadata is applied consistently, driving better governance, compliance, and automation. ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts#summary",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts#summary"
  },"1415": {
    "doc": "Working with Tags on Golden Records and Data Parts",
    "title": "Working with Tags on Golden Records and Data Parts",
    "content": " ",
    "url": "/kb/how-to-use-tags-with-golden-records-and-data-parts",
    
    "relUrl": "/kb/how-to-use-tags-with-golden-records-and-data-parts"
  },"1416": {
    "doc": "Azure Data Factory Integration",
    "title": "Azure Data Factory Integration",
    "content": "With over 90 built-in connectors, Azure Data Factory (ADF) enables you to acquire data from a wide range of sources such as relational databases, big data and NoSQL systems, data lakes and storages, and many more. These connectors empower ADF to extract, transform, and load data from diverse systems for seamless integration and analytics. With ADF integration, you can leverage CluedIn’s core capabilities in data quality and enrichment to ensure that the data is clean, reliable, and ready for insights. For successful integration between ADF and CluedIn, you need to meet a couple of prerequisites: . | Your CluedIn instance must be accessible by ADF. For this, you might need to configure a private link as described here. | You need to create an ingestion endpoint in CluedIn where the data from ADF will be sent. For more information, see Endpoint. | You need to create an API token in CluedIn that will be used to authenticate your post requests. | You need to have a source dataset in ADF ready to be sent to CluedIn. | . ",
    "url": "/microsoft-integration/adf-integration",
    
    "relUrl": "/microsoft-integration/adf-integration"
  },"1417": {
    "doc": "TLS certificates",
    "title": "On this page",
    "content": ". | Create your own certificates and keys | Update your server configuration via Helm | Alternative certificate providers . | Let’s Encrypt | Self-signed certificates | . | . The CluedIn front-end application uses Transport Layer Security (TLS) to encrypt access to the application over the network using HTTPS. CluedIn uses the Automated Certificate Management Environment (ACME) protocol and the public Let’s Encrypt certificate authority to issue certificates. While there are no specific requirements regarding the issuer and source of your certificates and keys, it is recommended that all TLS certificates and keys meet your organization’s requirements and comply with any security and compliance policies and regional laws. In this article, you will learn how to create your own certificates and keys and update your server configuration with the newly generated certificates and keys. Also, you will learn about alternative certificate providers. Prerequisites . | You should be comfortable working in either PowerShell or bash terminal via Azure Cloud Shell. | You should be connected to your AKS cluster. See Connect to CluedIn cluster for detailed instructions. | Your Helm repository is set up. | . If you have any questions, you can request CluedIn support by sending an email to support@cluedin.com (or reach out to your delivery manager if you have a committed deal). ",
    "url": "/deployment/infra-how-tos/configure-certificates#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-certificates#on-this-page"
  },"1418": {
    "doc": "TLS certificates",
    "title": "Create your own certificates and keys",
    "content": "If you want to use a Subject Alternative Name (SAN) or wildcard certificate for you domain, create your own certificates and keys. CSR requirements . The following FQDN are a requirement in order for the application to function correctly. In the example below, we use a dev environment. Your subdomains (app-dev, clean-dev) may differ and should match what is set in the DNS section of your values file. If you are unsure, please view the Configure DNS page. SAN . cluedin-dev.company.com app-dev.company.com clean-dev.company.com . Wildcard . cluedin-dev.company.com *.cluedin-dev.company.com . To create certificates and keys . | From a suitable provider, obtain the following files: TLS certificate, TLS private key (without password), and Certificate authority’s public certificate. The TLS certificates and keys must contain the DNS names for the CluedIn services as described above. | After you obtain the required files, convert the content of each file to base64 string using the output.txt command. For example: bas64 /path/to/file &gt; output.txt . | Add the strings to your values.yaml file under the Platform section as shown in the example below. platform: extraCertificateSecrets: cluedin-frontend-crt: tlsKey: LS0tLS1CRUdJTiB0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ0lVTjU1RW95TkVPK3= tlsCrt: S0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ= caCrt: LS0tLS1CRUdJTiB0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ0lVTjU1RW95TkVPK3= . | Save the file. | . ",
    "url": "/deployment/infra-how-tos/configure-certificates#create-your-own-certificates-and-keys",
    
    "relUrl": "/deployment/infra-how-tos/configure-certificates#create-your-own-certificates-and-keys"
  },"1419": {
    "doc": "TLS certificates",
    "title": "Update your server configuration via Helm",
    "content": "After you added the certificates and keys to your values.yaml file, you need to update the server configuration with your new TLS certificates and keys. To update the server configuration via Helm . | Get the current TLS values by running the following command: . helm get values cluedin-platform -n cluedin -o yaml &gt; Cluster-Current-values.yaml . This command downloads the current cluster configuration that you can use to update your server configuration. | Open the file in the text editor of your choice (for example, nano). nano Cluster-Current-values.yaml . | Add the section with base64 encoded values for the keys and secrets. platform: extraCertificateSecrets: cluedin-frontend-crt: tlsKey: LS0tLS1CRUdJTiB0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ0lVTjU1RW95TkVPK3= tlsCrt: S0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ= caCrt: LS0tLS1CRUdJTiB0tLS0tCk1JSUZuekNDQTRlZ0F3SUJBZ0lVTjU1RW95TkVPK3= # Optional. Used for self-signed or missing CA certificates. Needs global.ingress.tls.hasClusterCA set to 'true' to be used. | Remove the following section of configuration. issuer: configuration: acme: email: aba@cluedin.com privateKeySecretRef: name: letsencrypt-production server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: ingressTemplate: metadata: annotations: kubernetes.io/ingress.class: haproxy isWildcard: false . We recommend that you remove Let’s Encrypt issuer because you are configuring the system to use your own certificates and keys. | Update the ingress controller to use the new certificate: . global: ingress: tls: hasClusterCA: true # Only set to 'true' if the CA certificate is not publicly trusted. secretName: cluedin-frontend-crt # Must match name of secret in platform.extraCertificateSecrets . | Finally, update the hostname field to match the DNS for ingress. global: dns: hostname: mydomain.com # By default will be sslip.io subdomains: application: app-env openrefine: clean-env # It's good to append what type of environment (ie. prod) to the end of app and clean. # This is due to having multiple cluedin environments. Often the base domain is shared between all 3, but sub-domains shouldn't clash. | Save the file. | Post the new configuration to your cluster by running the following command: . helm upgrade -i cluedin-platform cluedin/cluedin-platform -n cluedin --create-namespace --values Cluster-Current-values.yaml --set application.system.runDatabaseJobsOnUpgrade=false . After a short time, you’ll see the confirmation of your update in the console. CluedIn is now configured to use your new TLS certificate and keys. | . ",
    "url": "/deployment/infra-how-tos/configure-certificates#update-your-server-configuration-via-helm",
    
    "relUrl": "/deployment/infra-how-tos/configure-certificates#update-your-server-configuration-via-helm"
  },"1420": {
    "doc": "TLS certificates",
    "title": "Alternative certificate providers",
    "content": "If you can’t obtain a certificate from a commercial certificate authority or from your internal public key infrastructure (PKI) service, you can use other methods to generate certificates. For example, you can generate certificates via Let’s Encrypt or you can generate self-signed certificates. While these methods provide the same level of encryption as the commercial and PKI issued certificates, they don’t provide the same level of validation. Make sure that any certificates and keys that you use meet your organization’s security policies. Let’s Encrypt . Let’s Encrypt provides the ability to generate the required certificates and keys to be used for free. These certificates are issued by a widely accepted certificate authority managed by Internet Security Research Group. Certificates from Let’s Encrypt provide a low-cost, low-maintenance alternative to commercial or internal PKI providers. For more information about Let’s Encrypt, visit their website. Self-signed certificates . Self-signed certificates should only be used in non-production environments where organizational policies approve this approach. The following procedure shows how to create a self-signed certificate using OpenSSL. In the procedure, the certificate is created for mycompany.com and the expiration period is 10 years. To create self-signed certificate . | Generate the certificate: . openssl req -x509 -newkey rsa:4096 -keyout domain.key -out domain.crt -sha256 -days 3650 -nodes -subj \"/CN=mycompany.com\" . | Verify the certificate: . openssl x509 -text -noout -in domain.crt . | Convert the certificate into the .pfx format: . openssl pkcs12 -inkey domain.key -in domain.crt -export -out domain.pfx . | . ",
    "url": "/deployment/infra-how-tos/configure-certificates#alternative-certificate-providers",
    
    "relUrl": "/deployment/infra-how-tos/configure-certificates#alternative-certificate-providers"
  },"1421": {
    "doc": "TLS certificates",
    "title": "TLS certificates",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-certificates",
    
    "relUrl": "/deployment/infra-how-tos/configure-certificates"
  },"1422": {
    "doc": "Connectors",
    "title": "Connectors",
    "content": "In this article, you will learn about the differences and similarities between the following types of connectors in CluedIn—a crawler, an enricher, and an export target. All connectors link CluedIn with an external system, yet each one has a distinct purpose. | Crawler – pulls data from an external source into CluedIn. A crawler does not require any vocabulary key value to get the data. As soon as you get access to the crawler, the data starts to get into CluedIn. | Enricher – pulls additional data from an external source into CluedIn based on the specified vocabulary key value. For example, if you don’t have a VAT number, you can’t use the VAT enricher. For more information, see Enricher. | Export target – pushes golden records from CluedIn to an external system. For more information, see Export targets. | . All of these connectors are similar in that they require access to an external system. At the same time, they are different based on their purpose and prerequisites. Whether it’s an SQL crawler, SQL enricher, or SQL export target, each connector performs a unique function, even when using the same technology. ",
    "url": "/key-terms-and-features/connectors",
    
    "relUrl": "/key-terms-and-features/connectors"
  },"1423": {
    "doc": "CVR",
    "title": "On this page",
    "content": ". | Add CVR enricher | Properties from CVR enricher | . This article outlines how to configure the CVR enricher. CVR is the official Danish register of information about businesses. The purpose of CVR enricher is to provide a wide range of information about companies registered in Denmark (for example, CVR numbers, addresses, business type, and so on). More details can be found in Properties from CVR enricher. To use the CVR enricher, you need to have a valid CVR account. Also, you need to generate the URL for your account, which should look similar to the following: http://cvruser:password12345@distribution.virk.dk/cvr-permanent/_search. Then, reach out to CluedIn support at support@cluedin.com to set up the URL for the CVR enricher in your CluedIn instance. The CVR enricher supports the following endpoint: . | http://{username}:{password}@distribution.virk.dk/cvr-permanent/_search, where {username} and {password} are the valid credentials for your CVR account. | . ",
    "url": "/preparation/enricher/cvr#on-this-page",
    
    "relUrl": "/preparation/enricher/cvr#on-this-page"
  },"1424": {
    "doc": "CVR",
    "title": "Add CVR enricher",
    "content": "The enricher requires at least one of the following attributes for searching the CVR register: . | CVR Code – if your golden records have CVR codes, you can enter the corresponding vocabulary key to configure the enricher. As a result, the enricher will use the CVR code to search the CVR register. | Organization Name, Country, and Website – if your golden records do not have CVR codes, you can enter these three attributes to configure the enricher. As a result, the enricher will use the combination of organization name, country, and website to search the CVR register. However, there are some requirements for each attribute. | . Requirements for Organization Name . | The organization name must be a valid company name (with Ltd, Pty, and so on). | The organization name must contain at least one of the following values: “dk”, “dk “, “denmark”, “danmark”, “dansk”, “æ”, “ø”, “å”; or the postfix must contain one of the following values: “A/S”, “AS”, “ApS”, “IVS”, “I/S”, “IS”, “K/S”, “KS”, “G/S”, “GS”, “P/S”, “PS”, “Enkeltmandsvirksomhed”, “Forening”, “Partsrederi”, “Selskab”, “virksomhed”. | . Requirement for Country . | The country must be either “dk”, “denmark”, or “danmark”. If the country does not meet this requirement, then the requirement for website must be met. | . Requirement for Website . | The website’s top-level domain must end with “.dk” (for example, abc.dk). | . To add the CVR enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select CVR, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Organization Name Vocabulary Key – enter the vocabulary key that contains company names that will be used for searching the CVR register. | Organization Name Normalization – turn on the toggle if you want to normalize company names that will be used for searching the CVR register. The normalization removes trailing backslashes (\\), slashes (/), and vertical bars; also, it changes the names to lowercase. The normalization does not affect company names in CluedIn. | Match Past Organization Names – turn on the toggle if you want to allow the enricher to accept data that matches the search text (organization name), even if the name of the latest data in the CVR register doesn’t exactly match the search text. For example, Pfizer ApS is one of the old names of Pfizer A/S; by turning on the toggle, you can search for Pfizer ApS using the search text (Pfizer ApS). | CVR Vocabulary Key – enter the vocabulary key that contains company CVR codes that will be used for searching the CVR register. | Country Vocabulary Key – enter the vocabulary key that contains company countries that will be used for searching the CVR register. | Website Vocabulary Key – enter the vocabulary key that contains company websites that will be used for searching the CVR register. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The CVR enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the CVR enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/cvr#add-cvr-enricher",
    
    "relUrl": "/preparation/enricher/cvr#add-cvr-enricher"
  },"1425": {
    "doc": "CVR",
    "title": "Properties from CVR enricher",
    "content": "You can find the properties added to golden records from the CVR enricher on the Properties page. For a more detailed information about the changes made to a golden record by the CVR enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the CVR enricher. | Display name | Vocabulary key | . | Address Id | cvr.address.addressId | . | Address Line 1 | cvr.address.addressLine1 | . | City | cvr.address.city | . | Co Name | cvr.address.coName | . | Country Code | cvr.address.countryCode | . | District | cvr.address.district | . | Door | cvr.address.door | . | Floor | cvr.address.floor | . | Formatted | cvr.address.formatted | . | Free Text | cvr.address.freeText | . | Municipality Code | cvr.address.municipalityCode | . | Municipality Name | cvr.address.municipalityName | . | Po Box | cvr.address.poBox | . | Postal Code | cvr.address.postalCode | . | Street Code | cvr.address.streetCode | . | Street Letter From | cvr.address.streetLetterFrom | . | Street Letter To | cvr.address.streetLetterTo | . | Street Name | cvr.address.streetName | . | Street Number From | cvr.address.streetNumberFrom | . | Street Number To | cvr.address.streetNumberTo | . | Name | cvr.organization.name | . | Cvr Number | cvr.organization.cvrNumber | . | Opt Out Sales And Advertising | cvr.organization.optOutSalesAndAdvertising | . | Number Of Employees | cvr.organization.numberOfEmployees | . | Start Date | cvr.organization.startDate | . | End Date | cvr.organization.endDate | . | Founding Date | cvr.organization.foundingDate | . | Company Type Code | cvr.organization.companyTypeCode | . | Company Type Short Name | cvr.organization.companyTypeShortName | . | Company Type Long Name | cvr.organization.companyTypeLongName | . | Status | cvr.organization.status | . | Credit Status Code | cvr.organization.creditStatusCode | . | Credit Status Text | cvr.organization.creditStatusText | . | Is Bankrupt | cvr.organization.isBankrupt | . | Phone Number | cvr.organization.phoneNumber | . | Fax Number | cvr.organization.faxNumber | . | Email | cvr.organization.email | . | Website | cvr.organization.website | . | Municipality | cvr.organization.municipality | . | Main Industry Code | cvr.organization.mainIndustryCode | . | Main Industry Description | cvr.organization.mainIndustryDescription | . | Other Industry 1 Code | cvr.organization.otherIndustry1.code | . | Other Industry 1 Description | cvr.organization.otherIndustry1.description | . | Other Industry 2 Code | cvr.organization.otherIndustry2.code | . | Other Industry 2 Description | cvr.organization.otherIndustry2.description | . | Other Industry 3 Code | cvr.organization.otherIndustry3.code | . | Other Industry 3 Description | cvr.organization.otherIndustry3.description | . | Production Unit Count | cvr.organization.productionUnitCount | . | Purpose | cvr.organization.purpose | . | Has Share Capital Classes | cvr.organization.hasShareCapitalClasses | . | Registered Capital | cvr.organization.registeredCapital | . | Registered Capital Currency | cvr.organization.registeredCapitalCurrency | . | Fiscal Year Start | cvr.organization.fiscalYearStart | . | Fiscal Year End | cvr.organization.fiscalYearEnd | . | First Fiscal Year Start | cvr.organization.firstFiscalYearStart | . | First Fiscal Year End | cvr.organization.firstFiscalYearEnd | . | Financial Report Summary Year | cvr.organization.financialReportSummary.year | . | Financial Report Summary Equity | cvr.organization.financialReportSummary.equity | . | Financial Report Summary Gross Profit Loss | cvr.organization.financialReportSummary.grossProfitLoss | . | Financial Report Summary Liabilities And Equity | cvr.organization.inancialReportSummary.liabilitiesAndEquity | . | Financial Report Summary Profit Loss | cvr.organization.financialReportSummary.profitLoss | . | Financial Report Summary Profit Loss From Ordinary Activities Before Tax | cvr.organization.financialReportSummary.profitLossFromOrdinaryActivitiesBeforeTax | . | Financial Report Summary Currency | cvr.organization.financialReportSummary.currency | . | Financial Report Summary Link | cvr.organization.financialReportSummary.link | . ",
    "url": "/preparation/enricher/cvr#properties-from-cvr-enricher",
    
    "relUrl": "/preparation/enricher/cvr#properties-from-cvr-enricher"
  },"1426": {
    "doc": "CVR",
    "title": "CVR",
    "content": " ",
    "url": "/preparation/enricher/cvr",
    
    "relUrl": "/preparation/enricher/cvr"
  },"1427": {
    "doc": "Engine Room",
    "title": "Engine Room",
    "content": "The processing pipeline in CluedIn can be described as a tree of different processing steps. Each processing step has dependencies on previous steps being run and hence you can conceptualise it as a dependency tree of processing steps. The Engine Room is a visualisation of this processing workflow. It will give you statistics on amount of data processing in a particular state, error rates and will even allow you to introduce new processing sub-workflows. If you have added an integration to your account, the Engine Room will be able to report on what state that data is in the overall process. It will also help you gauge how much infrastructure you will need to scale to the point where you can keep up with the number of processing servers required. You can click on any of the processing steps and you will see details as to the state and processing speed of that particular step. You can use this to gauge what is happening in CluedIn at any moment of processing. It is often required to understand what is happening under the hood of CluedIn. A lot of this can be sourced from the many Adminstrator screens that come with CluedIn for the underlying systems. Due to complex security an infrastructure setups, many times you might find that you don’t have access to these systems, but would still like to see some metrics and progress statistics. For this, we have our Statistics API which aggregates the statistics from across the different underlying stores. There are 4 types of statistics that we offer through this API: . Processing Crawling Configuration Footprint CluedIn uses a queuing system that operates the many different operations that CluedIn does on your data. This can be thought of as a Tree of processes. You can see that process tree below or by calling our /api/queue/map endpoint. { \"CluedIn\": { \"Incoming\": { \"CluedIn.Core.Messages.Processing.ProcessBigClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessLowPriorityClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessPrioritizedClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.IProcessingCommand:CluedIn.Core_CluedIn_Clues\": { \"CluedIn.ExternalSearch.ExternalSearchCommand:CluedIn.ExternalSearch_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.SplitEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeduplicateCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ParentsProcessingCommand:CluedIn.Core_CluedIn_ParentIds\": {}, \"CluedIn.Core.Messages.Processing.ProcessEdgesCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessVersionHistoryCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.SaveEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.PercolateEntityUpdateCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.MergeEntitiesCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeduplicateEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeleteEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.PostProcessingEntityCommand:CluedIn.Core_CluedIn\": {} }, \"Webhooks\": { \"CluedIn.Core.Messages.Processing.WebhookDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessWebHookClueCommand:CluedIn.Core_CluedIn\": {} } }, \"Events\": { \"CluedIn.Core.Messages.Processing.AnonymiseDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeAnonymiseDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Export.IExportCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.MeshDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.SendMailCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.AgentController.EnqueueAgentJobCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RemoveDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RemoveFromProcessingDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ResyncEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.WebApi.IWebApiCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RefreshEntityBlobCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn\": {} }, \"Errors\": { \"CluedIn.Logging.Errors.ErrorPacket:CluedIn.Logging.Errors_CluedIn\": {} }, \"Outgoing\": { \"CluedIn.WebHooks.Commands.OutgoingCustomWebHookCommand:CluedIn.WebHooks_CluedIn\": {} }, \"Metrics\": { \"CluedIn.Core.Messages.Processing.Metrics.ProcessEntityMetricsCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Metrics.ProcessGlobalMetricsCommand:CluedIn.Core_CluedIn\": {} } } } . Each Queue will have its own statistics and you can either call /api/queue/statistics to get all statistics of all queues or you can get an individual queue by calling /api/queue/statistics?queueName=CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn . You can also add an “expand=true” which will give you the aggregate values of all child items or a parent (and child’s children etc.) . It will respond with the following details. Comments are for guidance in the user interface . { \"Queues\": { \"memory\": 17500, \"message_stats\": { \"disk_reads\": 0, \"disk_reads_details\": null, \"disk_writes\": 0, \"disk_writes_details\": null, \"publish\": 5, \"publish_details\": { \"rate\": 0.0 }, \"publish_in\": 0, \"publish_in_details\": null, \"publish_out\": 0, \"publish_out_details\": null, \"ack\": 5, \"ack_details\": { \"rate\": 0.0 }, \"deliver_get\": 6, \"deliver_get_details\": { \"rate\": 0.0 }, \"confirm\": 0, \"confirm_details\": null, \"return_unroutable\": 0, \"return_unroutable_details\": null, \"redeliver\": 1, \"redeliver_details\": { \"rate\": 0.0 }, \"deliver\": 6, \"deliver_details\": { \"rate\": 0.0 }, \"deliver_no_ack\": 0, \"deliver_no_ack_details\": { \"rate\": 0.0 }, \"get\": 0, \"get_details\": { \"rate\": 0.0 }, \"get_no_ack\": 0, \"get_no_ack_details\": { \"rate\": 0.0 } }, \"reductions\": 255588, \"reductions_details\": { \"rate\": 0.0 }, \"messages\": 0, \"messages_details\": { \"rate\": 0.0 }, \"messages_ready\": 0, \"messages_ready_details\": { \"rate\": 0.0 }, \"messages_unacknowledged\": 0, \"messages_unacknowledged_details\": { \"rate\": 0.0 }, \"idle_since\": \"2019-12-21 7:32:15\", \"consumer_utilisation\": null, \"policy\": null, \"exclusive_consumer_tag\": null, \"consumers\": 1, \"recoverable_slaves\": null, \"state\": \"running\", \"garbage_collection\": { \"max_heap_size\": 0, \"min_bin_vheap_size\": 46422, \"min_heap_size\": 233, \"fullsweep_after\": 65535, \"minor_gcs\": 34 }, \"messages_ram\": 0, \"messages_ready_ram\": 0, \"messages_unacknowledged_ram\": 0, \"messages_persistent\": 0, \"message_bytes\": 0, \"message_bytes_ready\": 0, \"message_bytes_unacknowledged\": 0, \"message_bytes_ram\": 0, \"message_bytes_persistent\": 0, \"head_message_timestamp\": null, \"disk_reads\": 0, \"disk_writes\": 0, \"backing_queue_status\": { \"mode\": \"default\", \"q1\": 0, \"q2\": 0, \"delta\": [ \"delta\", 0, 0, 0, 0 ], \"q3\": 0, \"q4\": 0, \"len\": 0, \"target_ram_count\": \"infinity\", \"next_seq_id\": 5, \"avg_ingress_rate\": 5.5897681059900364E-190, \"avg_egress_rate\": 5.5897681059900364E-190, \"avg_ack_ingress_rate\": 5.5897681059900364E-190, \"avg_ack_egress_rate\": 1.2720273332920905E-189 }, \"node\": \"rabbit@cluedin-dev\", \"exclusive\": false, \"auto_delete\": false, \"durable\": true, \"vhost\": \"/\", \"name\": \"CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn\" } } . The Crawling statistics will report on the metrics for the fetching and mapping part of the process. It is often that Crawlers are run using Agents and these Agents may run on-premise and separate to the processing boxes. The Crawling statstics will report on many things including: . Number of Tasks generated by Crawl (number of records to crawl) Number of Completed Tasks (number of records that successfully crawled) Number of Failed Tasks (number of records that failed to crawl) Status Estimated Number of Records (optional) Configuration = [HttpGet] /api/configuration . Configuration is managed using Yaml and .Config files, however these are closed down from the User Interface. It is however very useful to explore the possible configuration options and also to be aware of the current state of the application. For example, we may want to know if a certain feature is enabled or not. The Configuration Endpoint exposes parts of the Configuration that are for read only access. We will not expose Secrets, Passwords, API Tokens or anything that exposes credentials. This is also useful for debugging and exploring potential issues. For example, you can configure CluedIn with many different parameters and features, but if there are settings that are against our recommended settings, we can expose this in the configuration user interface to alert the first places to potentially look. This is also important to validate if the configuration that has been set is actually in action in the running state of the application. All settings a read-only. Footprint . Graph = api/graph/configuration Search = api/search/health . It is valuable to know the running state of the CluedIn application. It is not as important for Data Stewards and Business Users to know this, but very much more adminstrators or systems owners that are not necessarily aware of how to operate the sub systems. For this, we expose some underlying metrics and statistics around memory, desk, cpu and utilisation. This will help you to understand if it might be necessary to increase the infrastructure of your CluedIn installation or potentially to dedicate more resources to a particular process. All values are read-only. For more advanced exploration, please use the underlying system adminstrator interfaces. ",
    "url": "/engine-room",
    
    "relUrl": "/engine-room"
  },"1428": {
    "doc": "Helm",
    "title": "On this page",
    "content": "Helm is the package manager for Kubernetes. CluedIn provides a Helm chart to install CluedIn quickly in a Kubernetes cluster. Helm is a package manager that simplifies complex applications’ installation and management in a Kubernetes environment. The Helm charts (i.e., packages) are stored in a repository, and you can install a chart by invoking a command-line tool from a terminal that has access to a Kubernetes cluster. You can find CluedIn Helm Charts in the public GitHub repository. Also, please check the getting started manual for CluedIn Helm Charts on this page: https://cluedin-io.github.io/Charts/. ",
    "url": "/deployment/azure/helm#on-this-page",
    
    "relUrl": "/deployment/azure/helm#on-this-page"
  },"1429": {
    "doc": "Helm",
    "title": "Helm",
    "content": " ",
    "url": "/deployment/azure/helm",
    
    "relUrl": "/deployment/azure/helm"
  },"1430": {
    "doc": "Create hierarchies",
    "title": "On this page",
    "content": ". | Build a hierarchy | Manage a hierarchy | Update stream configuration | Results &amp; next steps | . Hierarchy Builder allows you to visualize relations between golden records. For example, you can create a corporate structure hierarchy within a company, or a hierarchy representing relations between companies (parent, subsidiary, branch). In this article, you will learn how to create hierarchies in CluedIn using the Hierarchy Builder tool. Before you start: Make sure you have completed all steps in the Ingest data guide and Stream data guide. ",
    "url": "/getting-started/hierarchy-builder#on-this-page",
    
    "relUrl": "/getting-started/hierarchy-builder#on-this-page"
  },"1431": {
    "doc": "Create hierarchies",
    "title": "Build a hierarchy",
    "content": ". | On the navigation pane, go to Management &gt; Hierarchy Builder. | Select Create Hierarchy. | On the Create Hierarchy pane, specify the details of the hierarchy: . | Enter the name of the hierarchy. | If you want to limit the records for building the hierarchy, find and select the business domain. All records belonging to the selected business domain will be available to build the hierarchy. If you do not select the business domain, then all records existing in the system will be available to build the hierarchy. | In the lower-right corner, select Next. | Select the starting point for the hierarchy project: Blank (if you do not have existing relations between golden records) or From existing relations (if you have existing relations between golden records). | Select Create. The hierarchy builder page opens. | . | Build the visual hierarchy by dragging the records from the left pane to the canvas. | In the upper-right corner of the page, select Save. The status of the hierarchy becomes Draft. Although the hierarchy is in CluedIn, the relations between the elements within the hierarchy have not been established yet. To establish the relations between the elements of the hierarchy, publish the hierarchy. | In the upper-right corner of the page, select Publish. Then, confirm that you want to publish the hierarchy. You created the hierarchy. | . You can view the hierarchy on the Hierarchy Builder page or on the Hierarchies tab of the golden record page. In addition, you can view the relations between the records on the Relations tab of the golden record page. To make sure that the data in the Microsoft SQL Server database reflects the relations that you set up, update the stream configuration. ",
    "url": "/getting-started/hierarchy-builder#build-a-hierarchy",
    
    "relUrl": "/getting-started/hierarchy-builder#build-a-hierarchy"
  },"1432": {
    "doc": "Create hierarchies",
    "title": "Manage a hierarchy",
    "content": "After you created the hierarchy, you can do the following actions with the elements of the hierarchy: . | Replace elements. | Collapse and expand elements. To collapse all elements below a certain element, point to the needed element and select Collapse. To expand the collapsed elements, point to the parent element, and select Expand. You can also collapse all elements under a parent element. To do that, in the lower-right corner, select Collapse all. | View the data associated with the elements. To do that, point to the element, select the three-dot menu, and then select Open Entity. | . ",
    "url": "/getting-started/hierarchy-builder#manage-a-hierarchy",
    
    "relUrl": "/getting-started/hierarchy-builder#manage-a-hierarchy"
  },"1433": {
    "doc": "Create hierarchies",
    "title": "Update stream configuration",
    "content": "After you published the hierarchy, update the stream to ensure that the data in the database reflects the relations between the records that you set up. To update the stream . | On the navigation pane, go to Consume &gt; Streams. | Open the needed stream. | Go to the Export Target Configuration pane. Then, select Edit Export Configuration and confirm that you want to edit the stream. | Go to the Properties to export tab. To do this, select Next two times. | In the Export Edges section, turn on the Outgoing and Incoming toggles. | In the upper-right corner, select Save. Then, confirm that you want to save your changes. The stream is updated with the relations between records. As a result, new tables are created in the database: dbo.xyzOutgoingEdges and dbo.xyzIncomingEdges, where xyz is the target name. If you update the hierarchy, the relations between records will be automatically updated in the database. | . ",
    "url": "/getting-started/hierarchy-builder#update-stream-configuration",
    
    "relUrl": "/getting-started/hierarchy-builder#update-stream-configuration"
  },"1434": {
    "doc": "Create hierarchies",
    "title": "Results &amp; next steps",
    "content": "After completing all steps outlined in this guide, you learned how to visualize relations between golden records with the help of Hierarchy Builder and how to send these relations to a Microsoft SQL Server database. If you make any changes to the relations in CluedIn, they will be automatically updated in the database. For more details about hierarchies, refer to How to build an organizational hierarchy. Next, learn how to use a glossary to document groups of golden records that meet specific criteria in the Work with glossary guide. ",
    "url": "/getting-started/hierarchy-builder#results--next-steps",
    
    "relUrl": "/getting-started/hierarchy-builder#results--next-steps"
  },"1435": {
    "doc": "Create hierarchies",
    "title": "Create hierarchies",
    "content": " ",
    "url": "/getting-started/hierarchy-builder",
    
    "relUrl": "/getting-started/hierarchy-builder"
  },"1436": {
    "doc": "HTTP connector",
    "title": "HTTP connector",
    "content": "This article outlines how to configure the HTTP connector to publish data from CluedIn to an external HTTP endpoint. We’ll use the URL generated on the Webhook.site as an example of an external endpoint, but you should use your own API endpoint that can be called from CluedIn. Note that as a free user of the Webhook.site, your URL will stop accepting new requests after reaching the limit of 100 requests. Once this limit is reached, the status of the HTTP export target in CluedIn will become Unhealthy. Prerequisites: Make sure you generate a unique URL for your external HTTP endpoint. To configure HTTP connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Http Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | Url – unique URL of the external HTTP endpoint. The following screenshot shows the unique URL from the Webhook.site, which you can use for testing HTTP POST requests. You should provide the URL of your own endpoint. | Authorization – authorization header value. You should provide this value if your endpoint requires it. | . | Test the connection to make sure it works, and then select Add. Now, you can select the HTTP connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/http-connector",
    
    "relUrl": "/consume/export-targets/http-connector"
  },"1437": {
    "doc": "Monitoring",
    "title": "On this page",
    "content": ". | Common monitoring elements | Monitoring for ingestion endpoints . | Overview | Ingestion reports | Ingestion anomalies | . | . In this article, you will learn about the features on the Monitoring tab to gain insight into what is happening with your records and help you quickly identify issues. ",
    "url": "/integration/additional-operations-on-records/monitoring#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/monitoring#on-this-page"
  },"1438": {
    "doc": "Monitoring",
    "title": "Common monitoring elements",
    "content": "Regardless of the data source type—file, ingestion endpoint, or database—the Monitoring tab in the data set includes the following sections: . | Total – here you can view general information about the records from the current data set, including the total number of records, original columns, mapped columns, and records in quarantine. This is a useful tool to compare the number of original columns and mapped columns. | Global queues – here you can view global statistics on ingestion and processing requests from all data sets. This is a useful tool to ensure that the system runs correctly. | Data set queues – here you can view statistics on the records in the current data set during different stages of their life cycle (loading, mapping, processing). | . If the number of messages of any type is greater than 0 while the number of consumers is 0, there may be an issue with your data. The following screenshot illustrates a situation where troubleshooting is needed to fix the processing of records. To get better visibility into what is happening with your records, in addition to monitoring, use system healthchecks. You can find them in the upper-right corner of CluedIn. If the status of any item is red or orange, it means that something is wrong, and some services probably need to be restarted. To fix the problem, contact the person responsible for maintaining CluedIn for your organization (for example, system administrator) who can restart the needed service. The following table provides the description of each queue and corresponding troubleshooting guidelines. If the number of messages doesn’t return to 0 or if the number of consumers remains 0, refer to the Troubleshooting column for recommended actions. | Queue | Description | Troubleshooting | . | Ingestion data set | Messages representing JSON objects sent to various endpoints. | If you are a system administrator, restart the pod named “datasource-processing”. | . | Commit data set | Messages representing requests for data set processing. Messages can be added by selecting the Process button on the Process tab of the data set or each time the endpoint receives data with the auto-submission enabled. | If you are a system administrator, restart the pod named “datasource-processing”. | . | Submitting Messages | Messages containing JSON objects sent to the mapping service to be converted into records during processing. | Go to the Process tab and select Cancel. If you are a system administrator, verify the status of the mapping service and restart the pod named “annotation”. | . | Processing Messages | Messages containing records sent to the processing pipeline. | If you are a system administrator, restart the pod named “submitter”. | . | Quarantine Messages | Messages containing records that were approved on the Quarantine tab and sent to the processing pipeline. | If you are a system administrator, restart the pod named “submitter”. | . | Loading Failures | Messages containing records from the data set that cannot be fully loaded. | Go to the Preview tab and select Retry. | . | Error Processing Messages | Messages containing records that could not be processed by the processing pipeline because it does not respond. | Go to the Process tab and select Retry. If you are a system administrator, verify the status of processing pods. | . ",
    "url": "/integration/additional-operations-on-records/monitoring#common-monitoring-elements",
    
    "relUrl": "/integration/additional-operations-on-records/monitoring#common-monitoring-elements"
  },"1439": {
    "doc": "Monitoring",
    "title": "Monitoring for ingestion endpoints",
    "content": "In a data set created from an ingestion endpoint, the Monitoring tab consists of three areas: . | Overview . | Ingestion reports . | Ingestion anomalies . | . Overview . The Overview area contains general statistics about the records and queues described in Common monitoring elements. Ingestion reports . The Ingestion reposts area contains information about each request sent to the specific ingestion endpoint in CluedIn. This information is presented in a table that consists of the following columns: . | ReceiptID – unique identifier of a request. Each request you send to an ingestion endpoint in CluedIn, whether successful or not, has a unique receipt ID. This ID allows you to quickly locate the request report in CluedIn. Simply copy the receipt ID from the request response and paste it to the search box above the table. To view the records that were sent to CluedIn in that specific request, select the receipt ID. As a result, the records appear in the Ingested records pane. Alternatively, you can copy the receipt ID, go to the Preview tab, paste the receipt ID in the search box, and start to search. As a result, only the records sent in a specific request will be displayed on the page. | Received – the number of records received by CluedIn. If the number is 0, it means that the request contained errors and CluedIn rejected it. | Loaded – the number of records loaded into CluedIn. This column contains three categories: . | Success – the number of records that were successfully loaded into CluedIn. | Failed – the number of records that failed to load into CluedIn. | Retry – the number of records that attempted to reload into CluedIn. | . | Logs – the number of logs generated for a specific request. You can view the log details by selecting the content of the cell. Keep in mind that for ingestion endpoints, we only log warnings. These logs are the same as those found on the Logs tab of the dataset. The difference is that the Logs tab contains logs for all requests, while the Ingestion reports table provides logs for each specific request. For more information on how to read logs, see the Logs documentation. | Processed – the number of times the records from a specific request have been processed in CluedIn. | Created at – the timestamp indicating when the ingestion report was generated. This corresponds to the time when the HTTP request was executed. | Produced golden records – the golden records generated from a specific request or those to which records from the request were aggregated as data parts. To view a list of golden records produced from a specific request, select View ingested records. As a result, the golden records appear in the Produced golden records pane. | Actions – currently, this column provides the possibility to remove records—golden records or data parts that were aggregated to the existing golden records—produced from a specific request. If you want to remove the records produced from a specific request, select Remove records. In the confirmation dialog, you can select the checkbox to remove the source records from the temporary storage on the Preview tab. To confirm your choice, enter DELETE, and then start the removal process. Once the records are removed, they will no longer be displayed when you select View ingested records for a specific request. The records removal mechanism is similar to the one described in Remove records, with the difference being that the article describes removing all records produced from the data set, while you have the option to remove records produced from a specific request that contributes to the data set. | . Ingestion anomalies . The Ingestion anomalies area contains a list of potential errors that can occur with the data set, along with remediation steps and the status for each error. This area provides a quick and easy way to monitor the data set for any problems, ensuring you have complete visibility. If you notice that the status of any error indicates a problem, refer to the remediation steps. The following table contains the list of errors and remediation steps. You can find a similar table on the Monitoring tab in CluedIn. | Error name | Remediation | . | Error in logsThis can occur due to various elements, generally data-related. For example, an invalid field name in the record or an unsupported value for a given data type. | Go to the Logs tab and filter the logs by the Error level. Then, select a log to view the detailed reason for the error. | . | Loading failureThis can occur when the search databases are under heavy load. | Go to the Preview tab. You should see the Retry button there. Select the Retry button to attempt reloading your records. If this does not resolve the issue, please reach out to our support team. | . | Error in submissionsThis can occur when the CluedIn processing pipeline is under intense load and cannot handle all the incoming requests. | Go to the Process tab. You should see the Retry button for the submission in the Error state. Select the Retry button to attempt processing your records. If this does not resolve the issue, please reach out to our support team. | . | Ingestion consumer lostIngestion consumer is used to ingest any payload sent to CluedIn. If the ingestion consumer is lost, it means that CluedIn will not ingest any new records. | CluedIn has a self-healing mechanism that checks if a consumer exists for a queue every 5 minutes. If a consumer is lost, the administrator will receive a notification. | . | Process consumer lostProcess consumer is used to process any payload sent to CluedIn. If the process consumer is lost, it means that CluedIn will not process any new records. | CluedIn has a self-healing mechanism that checks if a consumer exists for a queue every 5 minutes. If a consumer is lost, the administrator will receive a notification. | . | Commit consumer lostCommit consumer is used to map any payload sent to CluedIn. If the commit consumer is lost, it means that CluedIn will not map any new records. | CluedIn has a self-healing mechanism that checks if a consumer exists for a queue every 5 minutes. If a consumer is lost, the administrator will receive a notification. | . | Submission consumer lostSubmission consumer is used to send records to the processing pipeline. If the submission consumer is lost, it means that CluedIn will not send records to processing, and they will not become golden records. | CluedIn has a self-healing mechanism that checks if a consumer exists for a queue every 5 minutes. If a consumer is lost, the administrator will receive a notification. | . | Quarantine consumer lostQuarantine consumer is used to send records to quarantine. If the quarantine consumer is lost, it means that CluedIn will not send any records to quarantine. | CluedIn has a self-healing mechanism that checks if a consumer exists for a queue every 5 minutes. If a consumer is lost, the administrator will receive a notification. | . | Submissions stuckThis can occur either because some messages were manually deleted or because the cluster is down. | If you encounter this error, please reach out to our support team. | . ",
    "url": "/integration/additional-operations-on-records/monitoring#monitoring-for-ingestion-endpoints",
    
    "relUrl": "/integration/additional-operations-on-records/monitoring#monitoring-for-ingestion-endpoints"
  },"1440": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": " ",
    "url": "/integration/additional-operations-on-records/monitoring",
    
    "relUrl": "/integration/additional-operations-on-records/monitoring"
  },"1441": {
    "doc": "Sync clean projects to Purview",
    "title": "On this page",
    "content": ". | Preparation in CluedIn | Feature demonstration | . In this article, you will learn how to sync CluedIn clean projects to Purview assets. ",
    "url": "/microsoft-integration/purview/sync-clean-projects-to-purview#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-clean-projects-to-purview#on-this-page"
  },"1442": {
    "doc": "Sync clean projects to Purview",
    "title": "Preparation in CluedIn",
    "content": ". | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Clean Projects to Purview. | Select Save. | Make sure you have an existing clean project. | . ",
    "url": "/microsoft-integration/purview/sync-clean-projects-to-purview#preparation-in-cluedin",
    
    "relUrl": "/microsoft-integration/purview/sync-clean-projects-to-purview#preparation-in-cluedin"
  },"1443": {
    "doc": "Sync clean projects to Purview",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of rules to Purview, you will receive a notification when the rule is synced. To find the asset in Purview . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the clean project in CluedIn. | On the asset details page, go to Lineage. Here, you can view a visual representation of how clean projects are applied to the data set within the CluedIn processing pipeline. The following screenshot shows the application of a clean project within the CluedIn processing pipeline. The clean project is executed on processed entities, resulting in generating clues. The clues generated from the clean project are then sent back to the beginning of the processing pipeline. | . ",
    "url": "/microsoft-integration/purview/sync-clean-projects-to-purview#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-clean-projects-to-purview#feature-demonstration"
  },"1444": {
    "doc": "Sync clean projects to Purview",
    "title": "Sync clean projects to Purview",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-clean-projects-to-purview",
    
    "relUrl": "/microsoft-integration/purview/sync-clean-projects-to-purview"
  },"1445": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Rolling Back and Undoing Changes in CluedIn",
    "content": "Mistakes happen — whether it’s applying a transformation you didn’t mean to, merging the wrong records, or running a cleaning project with unintended results. CluedIn provides several built-in features to rollback or undo changes, helping you recover quickly and maintain trust in your data. ",
    "url": "/kb/how-to-rollback-or-undo#rolling-back-and-undoing-changes-in-cluedin",
    
    "relUrl": "/kb/how-to-rollback-or-undo#rolling-back-and-undoing-changes-in-cluedin"
  },"1446": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Why Rollback Matters",
    "content": ". | Data Governance: Ensure that incorrect changes don’t become permanent. | User Confidence: Experiment safely, knowing that changes can be reversed. | Audit &amp; Compliance: Maintain a clear history of what was changed and when. | . ",
    "url": "/kb/how-to-rollback-or-undo#why-rollback-matters",
    
    "relUrl": "/kb/how-to-rollback-or-undo#why-rollback-matters"
  },"1447": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Features That Support Rollback",
    "content": "1. Undo Changes to Data . | Where: Entity Explorer or Data Stewardship workspace. | What It Does: . | If you merge records incorrectly, you can unmerge them. | If you update or overwrite a field manually, you can revert to the previous value. | If enrichment or transformation introduces bad data, you can undo the applied changes. | . | How to Use: . | Open the affected record in Entity Explorer. | Navigate to the History tab. | Select the change you want to reverse. | Click Undo or Restore Previous Value. | . | Notes: . | Undo actions are logged in the audit trail. | Some automated transformations may re-apply on the next pipeline run, so check active Rules. | . | . 2. Undo a Cleaning Project . | Where: Cleaning Projects workspace. | What It Does: . | Reverts the results of a data cleaning project if the changes were incorrect or incomplete. | Removes tags, corrections, or transformations applied during the project. | . | How to Use: . | Go to Data Quality &gt; Cleaning Projects. | Open the completed project. | Click Rollback / Undo Project. | Confirm the rollback action. | . | Notes: . | Undoing a project restores the dataset to its pre-cleaning state. | You can always re-run the project later with updated rules or corrections. | . | . 3. Undo Record Merges (Golden Record Rollback) . | Where: Data Stewardship workspace. | What It Does: . | Unmerges records that were incorrectly consolidated into a Golden Record. | . | How to Use: . | Navigate to the merged entity in Entity Explorer. | Open the History tab. | Select the merge event. | Click Unmerge. | . | Notes: . | The original source records are preserved, so unmerging restores them. | Any downstream systems synced with the Golden Record will be updated after rollback. | . | . 4. Audit Trail &amp; Versioning . | Where: Entity Explorer → History. | What It Does: . | Lets you review all changes made to a record over time. | Provides the option to revert specific attributes to previous values. | . | Notes: . | Every change is timestamped and attributed to the user or process that made it. | This supports granular rollback (attribute-level undo) as well as record-level undo. | . | . ",
    "url": "/kb/how-to-rollback-or-undo#features-that-support-rollback",
    
    "relUrl": "/kb/how-to-rollback-or-undo#features-that-support-rollback"
  },"1448": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Limitations of Rollback",
    "content": ". | Not all actions are reversible (e.g., permanent deletion of data after retention policy expiry). | Automated pipelines may re-apply transformations after rollback unless the underlying Rule or cleaning logic is adjusted. | Large rollbacks (e.g., undoing a massive cleaning project) may take time and resources to complete. | . ",
    "url": "/kb/how-to-rollback-or-undo#limitations-of-rollback",
    
    "relUrl": "/kb/how-to-rollback-or-undo#limitations-of-rollback"
  },"1449": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Best Practices for Safe Rollbacks",
    "content": ". | Preview Before Applying: Use preview functionality in cleaning projects and rules to validate expected outcomes. | Start Small: Test rules and cleaning projects on subsets of data before applying at scale. | Monitor Audit Logs: Regularly review audit trails to catch unexpected changes early. | Coordinate with Teams: Inform downstream system owners if a rollback will affect synchronized data. | Disable Problematic Rules: If a rule caused incorrect changes, disable or adjust it before rolling back, to avoid reapplication. | . ",
    "url": "/kb/how-to-rollback-or-undo#best-practices-for-safe-rollbacks",
    
    "relUrl": "/kb/how-to-rollback-or-undo#best-practices-for-safe-rollbacks"
  },"1450": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "Summary",
    "content": "CluedIn offers multiple ways to undo or rollback changes, including undoing data changes, unmerging Golden Records, and reverting entire cleaning projects. Combined with detailed audit trails and version history, these features give you confidence to manage your data safely — knowing that mistakes can be corrected without permanent damage. ",
    "url": "/kb/how-to-rollback-or-undo#summary",
    
    "relUrl": "/kb/how-to-rollback-or-undo#summary"
  },"1451": {
    "doc": "How to rollback or undo changes that have been made",
    "title": "How to rollback or undo changes that have been made",
    "content": " ",
    "url": "/kb/how-to-rollback-or-undo",
    
    "relUrl": "/kb/how-to-rollback-or-undo"
  },"1452": {
    "doc": "Build Integration",
    "title": "Build Integration",
    "content": "Introduction . CluedIn official integrations are one size fits all integrations. They will generally try to ingest as much data as they can. If you want to ingest data in a precise fashion or want to ingest data from an in-house tool, an old tool, from some custom APIs, you will need to create your own integration. Pre-requesite . CluedIn is a .NET platform. So you will need: . | .NET installed | Visual Studio installed | Docker | . Creating initial template . To avoid cumbersome boilerplating, CluedIn provides you a script to generate a working Visual studio solution. | Create a folder for your provider mkdir my-first-integration cd my-first-integration . | Run the generator docker run --rm -ti -v ${PWD}:/generated cluedin/generator-crawler-template . The generator will ask some questions and then generate all your solution files: . _-----_ ╭──────────────────────────╮ | │ Welcome to the awesome │ |--(o)--| │ CluedIn integration │ `---------´ │ generator! │ ( _´U`_ ) ╰──────────────────────────╯ /___A___\\ / | ~ | __'.___.'__ ´ ` |° ´ Y ` ? Name of this crawler? MyFirstIntegration ? Will it support webhooks? No ? Does it require OAuth? No . | Initialize a git repo git init git add . git commit -m \"Initial commit\" . | Open the solution in Visual Studio and build it or alternatively you should also build it from the command line using the dotnet cli: dotnet build | . Adding a Model . There are several steps needed to create a crawler that fetches data, creates Clues and passes them back to CluedIn for processing. Please refer to our Hello World sample repository for a working example. This is based on a simple external JSON service . The following is the minimal steps required to replicate the Hello World example: . | Create model classes. You can use a subgenerator for this: docker run --rm -ti -v ${PWD}:/generated cluedin/generator-crawler-template crawler-template:model . | Answer the questions as follows, to create a User model and vocabulary, similar to the one in the example User.cs _-----_ ╭──────────────────────────╮ | │ This sub-generator │ |--(o)--| │ allows to create new │ `---------´ │ vocabularies │ ( _´U`_ ) ╰──────────────────────────╯ /___A___\\ / | ~ | __'.___.'__ ´ ` |° ´ Y ` ? What is the model name? User ? What is the business domain? Person ? Enter a comma separated list of properties to add to the model id,name,username,email ? Choose the visibility for key: id(undefined) Visible ? Choose the type for key id Integer ? Should key id map to a common vocab? None ? Choose the visibility for key: name(undefined) Visible ? Choose the type for key name Text ? Should key name map to a common vocab? None ? Choose the visibility for key: username(undefined) Visible ? Choose the type for key username Text ? Should key username map to a common vocab? None ? Choose the visibility for key: email(undefined) Hidden ? Choose the type for key email Email ? Should key email map to a common vocab? ContactEmail create src/MyFirstIntegration.Core/Models/User.cs create src/MyFirstIntegration.Crawling/ClueProducers/UserClueProducer.cs create src/MyFirstIntegration.Crawling/Vocabularies/UserVocabulary.cs create test/MyFirstIntegration.Crawling.Unit.Test/ClueProducers/UserClueProducerTests.cs . This will generate 4 files as shown above. If you try to run the tests you will notice there is a failing one, as we need to complete some work in the ClueProducer. | Go to the src/MyFirstIntegration.Crawling/ClueProducers/UserClueProducer.cs file, in line 29 uncomment the following code: if(input.Name != null) data.Name = input.Name; . | Delete all other comments in the UserClueProducer.cs file. | Open the src/MyFirstIntegration.Infrastructure/MyFirstIntegrationClient.cs and modify line 16 with the URL for the endpoint: private const string BaseUri = \"https://jsonplaceholder.typicode.com\"; . | Since this is a public endpoint we don’t need to pass any tokens. Remove or comment out line 42 . // client.AddDefaultParameter(\"api_key\", myfirstintegrationCrawlJobData.ApiKey, ParameterType.QueryString);` . | Add a method to retrieve users (you will need to import some namespaces too): . public async Task&lt;IList&lt;User&gt;&gt; GetUsers() =&gt; await GetAsync&lt;IList&lt;User&gt;&gt;(\"users\"); . | In the src/MyFirstIntegration.Crawling/MyFirstIntegrationCrawler.cs you retrieve the data you want to insert in CluedIn. Add the following inside the GetData method: //retrieve data from provider and yield objects foreach( var user in client.GetUsers().Result) { yield return user; } . | In order to test the provider, you can use the Integration test provided. Open the test/integration/Crawling.MyFirstIntegration.Integration.Test/MyFirstIntegrationDataIngestion.cs file, and in the CorrectNumberOfEntityTypes method add a new annotation to indicate the expectation of receiving 10 Persons (that’s what the sample endpoint returns by default): [Theory] [InlineData(\"/Provider/Root\", 1)] [InlineData(\"/Person\", 10)] public void CorrectNumberOfEntityTypes(string entityType, int expectedCount) . | Execute the tests - they should all pass. | Before adding the integration to CluedIn, open the file src\\MyFirstIntegration.Core\\MyFirstIntegrationConstants.cs and modify the values for the constants before the TODO comment. This information will be used in the GUI of CluedIn to show information about the integration. In particular you should set the CrawlerDescription, Integration, Uri (if this integration corresponds to an online tool), and IconResourceName. This last property corresponds to the path of an embedded resource in the Provider project. | . Architecture . As you can see in the example - these are the main components: . | A client that knows how to retrieve data from your source (e.g. MyFirstIntegrationClient.cs). It has methods to produce plain objects with the information. | The method GetData in the main Crawling class MyFirstIntegrationCrawler.cs - you can consider this as the entry point for the provider. This method will invoke the correct methods of the client, in order to yield plain objects. | A Vocabulary class (e.g. UserVocabulary.cs) which is for the most part generated automatically. This class defines the different keys of the data you are processing and how they map to generic terms (email, address, company) also in use in other sources. In addition it can define the relationship with other Vocabularies (also known as edges). For example the relationship between a user and a company. | A ClueProducer (e.g. UserClueProducer.cs) which essentially translates the plain object (retrieved by the client) into a clue, which is the object understood by CluedIn. It uses the keys from the Vocabulary to map the data from the object to the clue. | . In this case the sample API was very open and generic, however in other cases you may need extra information (credentials, data sources, etc.) on how to connect to the source, or what data to retrieve. This can be captured in the CrawlJobData (e.g. MyFirstIntegrationCrawlJobData.cs). You can enrich it with whatever properties you need. However, you will also need to expand two methods in the Provider (e.g. MyFirstIntegrationProvider.cs): . | GetCrawlJobData which translates the keys from a generic dictionary into the CrawlJobData object and | GetHelperConfiguration which performs the opposite translation (from the CrawlJobData to a dictionary) | . Deploying the provider locally . If you are running CluedIn locally for testing purposes using Docker, you can follow these instructions to add the integration. You most likely used the Home GitHub repo to pull your CluedIn environment down and boot it up. You can now use this to inject extra components into CluedIn. Under the env folder you can use the default folder or you can create new environments (See Home GitHub Readme). Within this folder there is a components folder. Create a new folder in here called ServerComponent. This is essentially a folder in which you can inject your own DLL files and CluedIn will look in this folder on boot of the CluedIn Server Docker Container and load these assemblies as well. In the example of a Crawler, you will need to copy the DLL files produced by your different projects (not including the test DLLs), the .json dependency file, any third party libraries you used in your crawler (e.g. a custom NuGet package for talking to a service) and optionally you will want the PDB files if you would like to debug. Copy all of these into your newly created ServerComponent folder and restart the CluedIn Server Docker container. Make sure that the version of your CluedIn dependencies are exactly the same as the version you are running of CluedIn. You can check this in your packages.props file. &lt;PropertyGroup Label=\"Dependency Versions\"&gt; &lt;_ComponentHost&gt;2.0.0-alpha-14&lt;/_ComponentHost&gt; &lt;_AutoFixture&gt;4.11.0&lt;/_AutoFixture&gt; &lt;_CluedIn&gt;3.2.2&lt;/_CluedIn&gt; &lt;/PropertyGroup&gt; . Testing the provider in your environment . Please refer to install an integration . Generating Models, Vocabularies and ClueProducers . Please refer to the FileGenerator GitHub Repository. This can be used to generate basic models, vocabularies and clue producers using one of three options: Metadata file; CSV files with data; Microsoft SQL Server. The generators need to be updated depending on each data source - more details can be found in the README section of the repository. ",
    "url": "/integration/build-integration",
    
    "relUrl": "/integration/build-integration"
  },"1453": {
    "doc": "CluedIn Setup",
    "title": "On this page",
    "content": ". | Introduction | Pre-requisites &amp; Preparation | Option 1: Step-by-Step Setup | Option 2: PowerShell Setup | . Introduction . In order to prepare the setup of CluedIn in Azure, you need to have all required tools ready on your machine. Once the pre-requisites are met, you can choose between a manual step-by-step, or a guided PowerShell installation. Pre-requisites &amp; Preparation . | Install PowerShell 7 locally. | Install Azure CLI for the specific OS you are using (Windows, MacOS or Linux) | Create a folder where you can store the different tools &amp; files you will need for the installation, for example C:\\Users\\$env:username\\AzureTools, where $env:username contains your user name. | Open a PowerShell 7 session | Create a new folder using the following command: mkdir C:\\Users\\$env:username\\AzureTools . Navigate to the new folder . cd C:\\Users\\$env:username\\AzureTools . | Assign the path of your folder to a new Environment Variable, that you can name AzureTools for example: $env:AzureTools = \"C:\\Users\\$env:username\\AzureTools\" . | Add the newly created variable to your machine’s PATH variable: $env:PATH += \";C:\\Users\\$env:username\\AzureTools\" . Moving forwards, make sure you are inside the newly created folder. | . | Install kubectl locally: Windows, MacOS or Linux. | Example for Windows: . | In your PowerShell session, run the following commands one by one: . Command to install kubectl.exe . curl -LO \"https://dl.k8s.io/release/v1.22.0/bin/windows/amd64/kubectl.exe\" . Command to install the kubectl Checksum file . curl -LO \"https://dl.k8s.io/v1.22.0/bin/windows/amd64/kubectl.exe.sha256\" . Command to check the validity of the installation (should return True) . $($(CertUtil -hashfile .\\kubectl.exe SHA256)[1] -replace \" \", \"\") -eq $(type .\\kubectl.exe.sha256) . Command to check the installed version . kubectl version --client . The results should look like the following Make sure kubectl is added to your PATH through the AzureTools folder . $env:PATH -split ';' . | . | . | Install Helm . | Choose the latest release that suits your OS here | Download the appropriate zip, for example: helm-vX.Y.Z-windows-amd64.zip for Windows x64 | Extract the content on the zip into your AzureTools folder | . After performing the above steps, your Azure Folder should look like the following: . | Check your Azure access &amp; permissions: . | You must be added as a user in your company’s Azure tenant | You must have the Contributor role on the Azure Subscription you will be using for the installation. | . | Provision a hostname for the CluedIn application, for example: cluedin-dev.companyName.com, this will be the hostname used in the remainder of the installation. | Purchase or generate an SSL certificate bound to *.hostname, for example: *.cluedin-dev.companyName.com. If you choose not to use HTTPS immediately (despite its importance, especially for production environments), you can reconfigure CluedIn later to use HTTPS. | You own a Docker Hub account, for which you requested access to CluedIn’s Docker images. Please contact support@cluedin.com if you have not gotten the access enabled yet. | . Option 1: Step-by-Step Setup . | Go to Step-by-Step Setup | . Option 2: PowerShell Setup . | Go to PowerShell Setup | . ",
    "url": "/deployment/azure/setup#on-this-page",
    
    "relUrl": "/deployment/azure/setup#on-this-page"
  },"1454": {
    "doc": "CluedIn Setup",
    "title": "CluedIn Setup",
    "content": " ",
    "url": "/deployment/azure/setup",
    
    "relUrl": "/deployment/azure/setup"
  },"1455": {
    "doc": "DNS",
    "title": "On this page",
    "content": ". | DNS entries | Update DNS configuration for CluedIn | . As part of the CluedIn configuration, a base URL is used to make the application easily accessible by your browser. For proper configuration of CluedIn, you need to update the DNS settings. Specifically, you need to ensure that the A-records are configured to point either to your public IP address or a private IP address. Changing your DNS settings could have an impact on your TLS/SSL configuration. If you are also using specific TLS ingress hosts, they will also need to be changed to reflect your new DNS. ",
    "url": "/deployment/infra-how-tos/configure-dns#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-dns#on-this-page"
  },"1456": {
    "doc": "DNS",
    "title": "DNS entries",
    "content": "In this article, we’ll use cluedin as the example organization name that was entered during the installation of CluedIn and yourdomain.com as the example main domain. CluedIn requires the following 3 DNS entries which can be A-records: . cluedin.yourdomain.com app.yourdomain.com clean.yourdomain.com . At CluedIn, we recommend the following environments: . | Development – for partners and developers to customize CluedIn. | Test – for business users to validate the work implemented by development teams (with non-production data). | Production – for business users to operate CluedIn using real-world data. | . We recommend that you set up these environments (development, test, and production) using one the following naming conventions: . Split by subdomain . cluedin.dev.yourdomain.com app.dev.yourdomain.com clean.dev.yourdomain.com cluedin.test.yourdomain.com app.test.yourdomain.com clean.test.yourdomain.com cluedin.prod.yourdomain.com app.prod.yourdomain.com clean.prod.yourdomain.com . Distinct subdomains . cluedin.yourdomain.com app.yourdomain.com clean.yourdomain.com cluedin-dev.yourdomain.com app-dev.yourdomain.com clean-dev.yourdomain.com cluedin-test.yourdomain.com app-test.yourdomain.com clean-test.yourdomain.com . ",
    "url": "/deployment/infra-how-tos/configure-dns#dns-entries",
    
    "relUrl": "/deployment/infra-how-tos/configure-dns#dns-entries"
  },"1457": {
    "doc": "DNS",
    "title": "Update DNS configuration for CluedIn",
    "content": "After you have decided what kind of DNS setup works for your business, the next steps are to add the needed DNS entries and update your DNS configuration for CluedIn. Prerequisites . | You should be comfortable working in either PowerShell or bash terminal via Azure Cloud Shell. | You should be connected to your AKS cluster. See Connect to CluedIn cluster for detailed instructions. | Your Helm repository is set up. | . If you have any questions about DNS configuration, you can request CluedIn support by sending an email to support@cluedin.com (or reach out to your delivery manager if you have a committed deal). To update the DNS configuration for CluedIn . | Download the current cluster configuration file by running the following command: helm get values cluedin-platform -n cluedin -o yaml &gt; Cluster-Current-values.yaml | Open the file in a text editor of your choice | In the file, find the block of code similar to what is shown below. Look for the entry named dns. It is the configuration that controls the DNS address used by CluedIn. global: dns: hostname: 1.2.3.4.sslip.io # subdomains: # openrefine: clean # application: app # If you want to deviate away from the standard 'app' and 'clean' subdomains, you need to add the # `subdomains` block of code and ensure that they match in the global.ingress.tls.hosts section as well. For environments sharing a second-level domain such as yourdomain.com, it’s important to plan the domains in advance to avoid issues during the onboarding process. | Edit the value of hostname to reflect your desired domain for the given environment. | Find the TLS hosts section. The example of the section is shown below. global: ingress: tls: hosts: - cluedin.1.2.3.4.sslip.io - app.1.2.3.4.sslip.io - clean.1.2.3.4.sslip.io - '*.1.2.3.4.sslip.io' . | Replace the hosts section as shown below. global: ingress: tls: hosts: - cluedin.yourdomain.com - app.yourdomain.com - clean.yourdomain.com - '*.yourdomain.com' . | Save the file and apply changes from the local file to the CluedIn cluster by running the following command: helm upgrade -i cluedin-platform cluedin/cluedin-platform -n cluedin --values Cluster-Current-values.yaml | . After a short time, you’ll see the confirmation of your update in the console. CluedIn is now configured to use your new DNS address. This will use the LetsEncrypt service in the cluster to do an HTTP request to validate the certificate. If you would like to use a self-provided certificate, please review the Configure TLS Certificates page. ",
    "url": "/deployment/infra-how-tos/configure-dns#update-dns-configuration-for-cluedin",
    
    "relUrl": "/deployment/infra-how-tos/configure-dns#update-dns-configuration-for-cluedin"
  },"1458": {
    "doc": "DNS",
    "title": "DNS",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-dns",
    
    "relUrl": "/deployment/infra-how-tos/configure-dns"
  },"1459": {
    "doc": "DuckDuckGo",
    "title": "On this page",
    "content": ". | Add DuckDuckGo enricher | Properties from DuckDuckGo enricher | . This article outlines how to configure the DuckDuckGo enricher. The purpose of this enricher is to get general information about the organization from the DuckDuckGo search engine. More details can be found in Properties from DuckDuckGo enricher. The DuckDuckGo enricher supports the following endpoint: . | https://api.duckduckgo.com | . ",
    "url": "/preparation/enricher/duckduckgo#on-this-page",
    
    "relUrl": "/preparation/enricher/duckduckgo#on-this-page"
  },"1460": {
    "doc": "DuckDuckGo",
    "title": "Add DuckDuckGo enricher",
    "content": "The enricher uses the organization name and/or the website to search for information in the DuckDuckGo engine. To add the DuckDuckGo enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select DuckDuckGo, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Organization Name Vocabulary Key – enter the vocabulary key that contains the names of companies that will be used for searching the DuckDuckGo engine. | Website Vocabulary Key – enter the vocabulary key that contains the websites of companies that will be used for searching the DuckDuckGo engine. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The DuckDuckGo enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the DuckDuckGo enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/duckduckgo#add-duckduckgo-enricher",
    
    "relUrl": "/preparation/enricher/duckduckgo#add-duckduckgo-enricher"
  },"1461": {
    "doc": "DuckDuckGo",
    "title": "Properties from DuckDuckGo enricher",
    "content": "You can find the properties added to golden records from the DuckDuckGo enricher on the Properties page. For a more detailed information about the changes made to a golden record by the DuckDuckGo enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the DuckDuckGo enricher. | Display name | Vocabulary key | . | Abstract | duckDuckGo.organization.abstract | . | Abstract Source | duckDuckGo.organization.abstractSource | . | Abstract Text | duckDuckGo.organization.abstractText | . | Abstract Url | duckDuckGo.organization.abstractURL | . | Answer | duckDuckGo.organization.answer | . | Answer Type | duckDuckGo.organization.answerType | . | Definition | duckDuckGo.organization.definition | . | Definition Source | duckDuckGo.organization.definitionSource | . | Definition Url | duckDuckGo.organization.definitionURL | . | Employees | duckDuckGo.organization.employees | . | Entity | duckDuckGo.organization.entity | . | Facebook Profile | duckDuckGo.organization.facebookProfile | . | Founded | duckDuckGo.organization.founded | . | Git Hub Profile | duckDuckGo.organization.gitHubProfile | . | Heading | duckDuckGo.organization.heading | . | Image | duckDuckGo.organization.image | . | Image Height | duckDuckGo.organization.imageHeight | . | Image Is Logo | duckDuckGo.organization.imageIsLogo | . | Image Width | duckDuckGo.organization.imageWidth | . | Industry | duckDuckGo.organization.industry | . | Instagram Profile | duckDuckGo.organization.instagramProfile | . | Redirect | duckDuckGo.organization.redirect | . | Revenue | duckDuckGo.organization.revenue | . | Score | duckDuckGo.organization.score | . | Twitter Profile | duckDuckGo.organization.twitterProfile | . | Type | duckDuckGo.organization.type | . | Websites | duckDuckGo.organization.websites | . | Youtube Channel | duckDuckGo.organization.youtubeChannel | . Additionaly, the DuckDuckGo enricher can add various infobox properties (for example, Formerly Called, Founders, Products, and so on) and related topic properties (brief description with a link to the source). These properties depend on the information that the enricher can find. ",
    "url": "/preparation/enricher/duckduckgo#properties-from-duckduckgo-enricher",
    
    "relUrl": "/preparation/enricher/duckduckgo#properties-from-duckduckgo-enricher"
  },"1462": {
    "doc": "DuckDuckGo",
    "title": "DuckDuckGo",
    "content": " ",
    "url": "/preparation/enricher/duckduckgo",
    
    "relUrl": "/preparation/enricher/duckduckgo"
  },"1463": {
    "doc": "Eventual connectivity",
    "title": "Eventual connectivity",
    "content": "The eventual connectivity pattern is at the heart of CluedIn. This pattern of integration makes it simple to blend data from many different systems into a unified and connected data set. Example . Let’s explore the concept of eventual connectivity in CluedIn through an example. We have a golden record that mentions an email address. This email address indicates that there might be another data set where the email is also used. So, CluedIn creates a shadow entity, which acts as a placeholder, with the email address. This shadow entity may have several codes, one of which is built using the email address. At this point, the document golden record is related to the shadow entity. When a new record containing the same email address appears in the system, CluedIn recognizes matching emails, and replaces the shadow entity with a new golden record. In the end, the document record is now related to another golden record, and not to the shadow entity. Goal of eventual connectivity . The goal of eventual connectivity is not to model your data into a final form but to provide a flexible model inferred from the source systems. This means that you don’t have control over the modelling within CluedIn up-front, but you will have control later in the process. You won’t have to conduct long and tedious architecture meetings to discuss how different systems will connect with each other. The eventual connectivity pattern will do the blending of data for you, as long as you can instruct it and guide it using its underlying concepts. Underlying concepts . | Identifiers – universally unique identifiers of a record. Read more about identifiers here. | Edges – a way to instruct CluedIn that there is a reference from one object to another. Edges are the key behind the eventual connectivity pattern. Read more about edge here. | Shadow entities (also known as floating edges) – records that are constructed from edges, with the expectation that the system will eventually find a link between these records. | Aliases – a way to instruct CluedIn that there is a property that can fuzzily lead to a linkage between golden records. | Vocabularies – a way to map properties from the source system to a common vocabulary that CluedIn understands. The vocabularies are key to bridge data across systems. For more information, see a video about vocabulary key mapping here. | . Main advantage . The main advantage of the eventual connectivity pattern is that you can take one system or even one object within a system at a time, determine the codes, aliases, and edges for an object, and then let the pattern manage how everything will blend together. It is also important to remember that with edges, we do not expect you to know where the reference is pointing to. Rather we would ask you to instruct us that a particular field or column should point to another record. This record could be in the current system, or it could be in another system all together. ",
    "url": "/key-terms-and-features/eventual-connectivity",
    
    "relUrl": "/key-terms-and-features/eventual-connectivity"
  },"1464": {
    "doc": "Feature flags",
    "title": "Feature flags",
    "content": "In this article, you will learn about the features in CluedIn that you can turn on or off as needed. Since not all features are suitable for every business use case, they are stored separately in Administration &gt; Feature Flags. This allows you to turn on only those features that are most relevant to your needs. However, you are free to try out any feature and see how it works for yourself. Turning on these features is safe and will not cause any disruptions to your data or affect the stability of your instance of CluedIn. If you encounter any issues or do something wrong while using these features, remember that you can always revert your actions. Each feature has a status to indicate the state of the feature’s readiness for production. The following table provides a description of each feature status. | Feature status | Description | . | Deprecated | The feature is no longer recommended for use, and it will be eventually removed in future versions. | . | Development | The feature is in the process of development. It has been tested but not yet validated. You can turn on such feature only in the test environment. You might encounter issues or bugs when using such feature. | . | Alpha | The feature is in the process of validation. You can turn on such feature only in the test environment. You might encounter issues or bugs when using such feature. | . | Beta | The feature has been partially validated and tested; no user documentation has been prepared yet. You can turn on such feature only in the test environment. To make sure the feature works as expected, we use this feature with selected customers in the production environment. | . | Release Candidate | The feature has been validated and thoroughly tested, but it still might have some outstanding minor issues. You can turn on such feature only in the test environment. | . | Production | The feature has passed all validations and tests and can be used in the production environment. If the feature has this status and is still listed on the Feature Flags page, it means that this feature might not be suitable for all business use cases. So, you can turn it on if you need it for your specific use case. | . If you want to use a feature in the production environment, but the feature status is other than Production, reach out to CluedIn support at support@cluedin.com and we’ll help you do it. ",
    "url": "/administration/feature-flags",
    
    "relUrl": "/administration/feature-flags"
  },"1465": {
    "doc": "Work with glossary",
    "title": "On this page",
    "content": ". | Create category | Create term | Manage glossary | Update stream configuration | Results &amp; next steps | . Glossary can help you in documenting groups of golden records that meet specific criteria, simplifying the process of cleaning and streaming these groups of records. In this article, you will learn how to work with the glossary in CluedIn. Glossary consists of terms that are grouped into categories. Each term contains a list of golden records that correspond to your conditions. Working with the glossary in CluedIn includes creating categories and creating terms within those categories. You can have multiple terms under one category. Before you start: Make sure you have completed all steps in the Ingest data guide and Stream data guide. ",
    "url": "/getting-started/glossary#on-this-page",
    
    "relUrl": "/getting-started/glossary#on-this-page"
  },"1466": {
    "doc": "Work with glossary",
    "title": "Create category",
    "content": "Category refers to a logical grouping of terms. For example, you can have a category named Customer, which would contain customer records organized by regions. Each region is a separate term within the category. In other words, a category acts as a folder for terms. To create a category . | On the navigation pane, go to Management &gt; Glossary. | Depending on whether or not you have created categories before, do one of the following: . | If you haven’t created any categories before, then in the middle of the page, select Create Category. | If you already created some categories, then, in the upper-left corner of the page, select Create &gt; Create Category. | . | Enter the category name. Then, in the lower-right corner, select Create. You created the category. Now, you can create a term in the category. | . ",
    "url": "/getting-started/glossary#create-category",
    
    "relUrl": "/getting-started/glossary#create-category"
  },"1467": {
    "doc": "Work with glossary",
    "title": "Create term",
    "content": "Term refers to the list of golden records that meet specific conditions. For example, within the Customer category, you can have a term named North America encompassing customer records where the BusinessRegion vocabulary key is set to North America. To create a term . | Select Create Term. | On the Create Term pane, do the following: . | Enter the term name. | In the Category section, leave the Choose an existing Category checkbox selected. | Select the category that you created before. | In the Filters section, define which golden records should be included in the term. | In the lower-right corner, select Create. You created the term. | . | (Optional) Specify additional details about the term: . | Select Edit. | In the Certification Level dropdown list, select the quality level of the records. | Enter the Short Description and Long Description of the term. | . | In the upper-right corner of the term details page, select Save. | Go to the Matches tab to view the records that meet the condition that you set up. By default, all records are displayed in the following columns: Name, Business Domain, and Description. To add more columns to the table, see step 3 of Find data. | Activate the term by turning on the toggle next to the term status. You created the term and defined the records that are included in this term. | . Now, you can clean the glossary term and stream it to a Microsoft SQL Server database. As an example, update the configuration of the stream that you created in the Stream data guide. ",
    "url": "/getting-started/glossary#create-term",
    
    "relUrl": "/getting-started/glossary#create-term"
  },"1468": {
    "doc": "Work with glossary",
    "title": "Manage glossary",
    "content": "You can do the following actions with the terms in the glossary: . | Endorse the term when you are confident that the records are of good quality, signaling to other users that the term is reliable for their use. To do that, in the upper-right corner of the term page, select Endorse. | Rate the term, enabling other users to view that it is reliable for their use. | . ",
    "url": "/getting-started/glossary#manage-glossary",
    
    "relUrl": "/getting-started/glossary#manage-glossary"
  },"1469": {
    "doc": "Work with glossary",
    "title": "Update stream configuration",
    "content": "Streaming the glossary terms to the database is more convenient than streaming specific records based on filters. You only need to specify the name of the glossary term, rather than setting filters for properties or vocabulary values. To update the stream . | On the navigation pane, go to Consume &gt; Streams. | Open the needed stream. | On the Configuration pane, in the Filters section, delete the existing filter. | Select Add First Filter, and then specify the glossary term that you created: . | In the Select Property Type dropdown list, select Glossary. | In the All Glossary dropdown list, select the glossary term that you created. | In the Choose Operation dropdown list, select Is True. | . | In the upper-right corner select Save. Then, confirm that you want to save your changes. The stream is updated with the new filter. As a result, the database table now contains the records from the glossary term. If you update the glossary term, the records will be automatically updated in the database. | . ",
    "url": "/getting-started/glossary#update-stream-configuration",
    
    "relUrl": "/getting-started/glossary#update-stream-configuration"
  },"1470": {
    "doc": "Work with glossary",
    "title": "Results &amp; next steps",
    "content": "After completing all steps outlined in this guide, you learned how to create a glossary term to document a group of golden records and how to send those golden records from the glossary term to a Microsoft SQL Server database. If you make any changes to the glossary term in CluedIn, the associated records will be automatically updated in the database. Next, learn how to add edges to build relations between golden records in the Add relations between records. ",
    "url": "/getting-started/glossary#results--next-steps",
    
    "relUrl": "/getting-started/glossary#results--next-steps"
  },"1471": {
    "doc": "Work with glossary",
    "title": "Work with glossary",
    "content": " ",
    "url": "/getting-started/glossary",
    
    "relUrl": "/getting-started/glossary"
  },"1472": {
    "doc": "Management",
    "title": "Management",
    "content": "The Management module allows you to efficiently handle your data, eliminate duplicates, and apply business rules to your records. Rules Set up rules to automate data transformation Deduplication Identify and merge duplicate records Data catalog Manage vocabularies and vocabulary keys ",
    "url": "/management",
    
    "relUrl": "/management"
  },"1473": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "How to Automatically Match Lookup or Reference Data to a Known List of Values",
    "content": "Many business processes rely on lookup tables or reference data (e.g., country codes, product categories, departments, payment terms). In practice, data sources often contain values that don’t exactly match your standard lists. CluedIn provides tools to automatically align these values to your canonical reference set. ",
    "url": "/kb/how-to-match-reference-data#how-to-automatically-match-lookup-or-reference-data-to-a-known-list-of-values",
    
    "relUrl": "/kb/how-to-match-reference-data#how-to-automatically-match-lookup-or-reference-data-to-a-known-list-of-values"
  },"1474": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Why Match Reference Data?",
    "content": ". | Consistency: Ensure all systems use the same codes and descriptions. | Analytics: Prevent fragmented reporting (e.g., UK, U.K., United Kingdom). | Interoperability: Map local or legacy codes to global standards (e.g., currency codes, ISO country codes). | Data Quality: Reduce manual stewardship by automating corrections. | . ",
    "url": "/kb/how-to-match-reference-data#why-match-reference-data",
    
    "relUrl": "/kb/how-to-match-reference-data#why-match-reference-data"
  },"1475": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Approaches in CluedIn",
    "content": "1. Deterministic Matching . If the incoming values are clean and predictable: . | Use Data Part Rules to map exact matches (UK → GB). | Apply Normalisers to ignore casing, whitespace, or punctuation (us = US). | . This works best when the variety of input formats is limited. 2. Fuzzy &amp; AI-Powered Matching with Find Closest Match . For more complex or inconsistent input values, use the Find Closest Match Rule Action. | What it does: . | Compares the incoming value against your known list of canonical values. | Uses fuzzy string similarity and semantic AI to suggest the closest aligned match. | Automatically replaces or tags the value with the matched canonical reference. | . | Example: . | Input: U.K., Britain, Great Britan | Canonical List: United Kingdom, Germany, France | Find Closest Match Result: → United Kingdom | . | Advantages: . | Handles typos, spelling variations, abbreviations, and synonyms. | Reduces manual stewardship workload. | Consistently maps new variations as they appear. | . | . ",
    "url": "/kb/how-to-match-reference-data#approaches-in-cluedin",
    
    "relUrl": "/kb/how-to-match-reference-data#approaches-in-cluedin"
  },"1476": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Step-by-Step: Using Find Closest Match",
    "content": ". | Prepare your reference list . | Define your canonical values in a Vocabulary or controlled lookup table. | . | Create or edit a Rule . | Go to Governance → Rules. | Add a new Data Part Rule or edit an existing one. | . | Add the Find Closest Match Action . | Choose Find Closest Match as the action. | Select the attribute (e.g., Country, Currency, Category). | Point it to your canonical list of values. | . | Configure thresholds (optional) . | Adjust similarity thresholds to control how strict/loose matches are. | Lower thresholds = more automated matches (but higher false positives). | Higher thresholds = more accuracy, but some unmatched values may require stewardship. | . | Test &amp; Validate . | Run the rule on sample data. | Verify results and adjust thresholds if needed. | . | Operationalize . | Apply the rule to incoming data streams. | Review unmatched or low-confidence cases in Stewardship. | . | . ",
    "url": "/kb/how-to-match-reference-data#step-by-step-using-find-closest-match",
    
    "relUrl": "/kb/how-to-match-reference-data#step-by-step-using-find-closest-match"
  },"1477": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Best Practices",
    "content": ". | Start strict, then relax: Begin with higher thresholds to build trust. Lower gradually if too many values remain unmatched. | Combine with cleaning: Standardize casing, whitespace, and punctuation before running Find Closest Match. | Audit results: Periodically review mappings to catch drift (new slang, abbreviations, or vendor codes). | Log &amp; tag: Keep the original raw value stored for lineage and auditing. | . ",
    "url": "/kb/how-to-match-reference-data#best-practices",
    
    "relUrl": "/kb/how-to-match-reference-data#best-practices"
  },"1478": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Example Use Cases",
    "content": ". | Mapping country names from free-text CRM fields to ISO 3166-1 codes. | Aligning product categories from multiple e-commerce sites to a master taxonomy. | Standardizing department names from HR systems (HR, Human Res, People Ops). | Normalizing payment terms (Net30, 30 Days, Thirty-day terms). | . ",
    "url": "/kb/how-to-match-reference-data#example-use-cases",
    
    "relUrl": "/kb/how-to-match-reference-data#example-use-cases"
  },"1479": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "Summary",
    "content": "When working with lookup or reference data in CluedIn: . | Use deterministic rules for exact, clean matches. | Apply normalisers to ignore irrelevant differences (case, punctuation). | Leverage the Find Closest Match Rule Action to automatically map inconsistent, fuzzy, or semantically similar values to your canonical list. | . This hybrid approach ensures maximum automation with minimum risk, reducing manual stewardship and improving data consistency across your ecosystem. ",
    "url": "/kb/how-to-match-reference-data#summary",
    
    "relUrl": "/kb/how-to-match-reference-data#summary"
  },"1480": {
    "doc": "How to match reference data or lookup data automatically",
    "title": "How to match reference data or lookup data automatically",
    "content": " ",
    "url": "/kb/how-to-match-reference-data",
    
    "relUrl": "/kb/how-to-match-reference-data"
  },"1481": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "On this page",
    "content": ". | Set up Azure Relay | Create WCF Relay | Get management credentials | Configure on-premises MDS server | Verify configuration | . In this article, you will learn how to expose on-premises Master Data Services (MDS) to CluedIn in Azure by using Azure Relay. Prerequisites: . | An outbound internet connection from on-premises MDS server. | Administrator access to on-premises MDS server for one-time reconfiguration. | An Azure subscription. If you don’t have one, create a free account before you begin. | . ",
    "url": "/microsoft-integration/mds-integration#on-this-page",
    
    "relUrl": "/microsoft-integration/mds-integration#on-this-page"
  },"1482": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Set up Azure Relay",
    "content": ". | Sign in to the Azure portal. | Select Create a resource. | Search for Relay. Then, select Create. | In the Subscription field, select an Azure subscription in which to create the namespace. | In the Resource group field, choose an existing resource group in which to place the namespace, or create a new one. | In the Name field, enter a namespace name. | In the Location field, select the country or region in which your namespace should be hosted. | Select Create. The Azure portal creates your namespace and enables it. After a few minutes, the system provisions resources for your account. | . ",
    "url": "/microsoft-integration/mds-integration#set-up-azure-relay",
    
    "relUrl": "/microsoft-integration/mds-integration#set-up-azure-relay"
  },"1483": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Create WCF Relay",
    "content": ". | Select WCF Relays, and then select + WCF Relay. | In the Name field, enter mds. | In Relay Type, select Http. | Clear the Requires Client Authorization checkbox. Authorization will be performed by the MDS service itself. | Select Create. | . ",
    "url": "/microsoft-integration/mds-integration#create-wcf-relay",
    
    "relUrl": "/microsoft-integration/mds-integration#create-wcf-relay"
  },"1484": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Get management credentials",
    "content": ". | Select All resources, and then select the newly created namespace name. | Select Shared access policies, and then select RootManageSharedAccessKey. | In the SAS Policy: RootManageSharedAccessKey pane, do the following: . | Select the Copy button next to Primary Connection String. This action copies the connection string to your clipboard for later use. Paste this value into Notepad or some other temporary location. | Select the Copy button next to Primary Key. This action copies the primary key to your clipboard for later use. Paste this value into Notepad or some other temporary location. | . | . ",
    "url": "/microsoft-integration/mds-integration#get-management-credentials",
    
    "relUrl": "/microsoft-integration/mds-integration#get-management-credentials"
  },"1485": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Configure on-premises MDS server",
    "content": ". | Using IIS, locate the physical path for the MDS website. The default location for SQL Server 2019 Master Data Services is C:\\Program Files\\Microsoft SQL Server\\150\\Master Data Services\\WebApplication. | Download WindowsAzure.ServiceBus from nuget and extract Microsoft.ServiceBus.dll to the bin folder in the previously located path. | Make the following modifications to the web.config file in the previously located path. Note that you may need to create the parent nodes if not already present. | . Add the following behaviour to system.serviceModel/behaviors/endpointBehaviors. &lt;behavior name=\"AzureRelay\"&gt; &lt;transportClientEndpointBehavior&gt; &lt;tokenProvider&gt; &lt;sharedAccessSignature keyName=\"RootManageSharedAccessKey\" key=\"[YOUR_KEY]\"/&gt; &lt;/tokenProvider&gt; &lt;/transportClientEndpointBehavior&gt; &lt;/behavior&gt; . Add the basicHttpRelayBinding to system.serviceModel/bindings. &lt;basicHttpRelayBinding&gt; &lt;binding isDynamic=\"false\"&gt; &lt;security mode=\"TransportWithMessageCredential\" relayClientAuthenticationType=\"None\"&gt; &lt;message clientCredentialType=\"UserName\"/&gt; &lt;/security&gt; &lt;/binding&gt; &lt;/basicHttpRelayBinding&gt; . Add the following to the system.serviceModel/services/service node whose name attribute matches your MDS service name. For a default install the name attribute will be MDS1. &lt;endpoint address=\"https://[YOUR_NAMESPACE].servicebus.windows.net/mds\" binding=\"basicHttpRelayBinding\" contract=\"Microsoft.MasterDataServices.Services.ServiceContracts.IService\" behaviorConfiguration=\"AzureRelay\"/&gt; . Add the following to system.serviceModel/extensions/behaviorExtensions . &lt;add name=\"transportClientEndpointBehavior\" type=\"Microsoft.ServiceBus.Configuration.TransportClientEndpointBehaviorElement, Microsoft.ServiceBus\"/&gt; . Add the following to system.serviceModel/extensions/bindingExtensions . &lt;add name=\"basicHttpRelayBinding\" type=\"Microsoft.ServiceBus.Configuration.BasicHttpRelayBindingCollectionElement, Microsoft.ServiceBus\"/&gt; . ",
    "url": "/microsoft-integration/mds-integration#configure-on-premises-mds-server",
    
    "relUrl": "/microsoft-integration/mds-integration#configure-on-premises-mds-server"
  },"1486": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Verify configuration",
    "content": ". | Verify the configuration by loading the service endpoint. For this, perform the sequence of steps as displayed on the following screenshot. As a result, you will see the screenshot similar to the following. | Verify that the on-premises MDS service has registered successfully with the Relay. | . ",
    "url": "/microsoft-integration/mds-integration#verify-configuration",
    
    "relUrl": "/microsoft-integration/mds-integration#verify-configuration"
  },"1487": {
    "doc": "Master Data Services (MDS) Integration",
    "title": "Master Data Services (MDS) Integration",
    "content": " ",
    "url": "/microsoft-integration/mds-integration",
    
    "relUrl": "/microsoft-integration/mds-integration"
  },"1488": {
    "doc": "OneLake connector",
    "title": "OneLake connector",
    "content": "This article outlines how to configure the OneLake connector to push data from CluedIn to Microsoft’s OneLake. Prerequisites . | Make sure you have a service principal to authenticate and access OneLake. | In PowerBI/Fabric, enable the following settings: . | Service principals can use Fabric APIs . | Users can access data stored in OneLake with apps external to Fabric . | . | In the workspace that you want to use in OneLake connector, give at least Contributor access to the OneLake service principal. To do that, in the workspace, select Manage access. Then, add the OneLake service principal and assign it the Contributor role to the workspace. | . To configure OneLake connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select OneLake Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | WorkspaceName – name of the workspace where you want to store the data from CluedIn. To find the workspace, sign in to Microsoft Fabric, and then select Workspaces from the left-hand menu. In the list of workspaces, find the needed workspace and select it. | ItemName – name of the data item within the workspace where you want to store the data from CluedIn. | ItemType – type of the data item within the workspace where you want to store the data from CluedIn (for example, Lakehouse). | ItemFolder – directory within a data item where you want to store the data from CluedIn (for example, Files/FirstLevel/SecondLevel). | ClientID – unique identifier assigned to the OneLake app when it was registered in the Microsoft identity platform. You can find this value in the Overview section of app registration. | ClientSecret – confidential string used by your OneLake app to authenticate itself to the Microsoft identity platform. You can find this value in the Certificates &amp; secrets section of app registration. | TenantID – unique identifier for your Microsoft Entra tenant. You can find this value in the Overview section of app registration. | Enable Stream Cache (Sync mode only) – when stream cache is enabled, CluedIn caches the golden records at intervals, and then writes out accumulated data to one file (JSON, Parquet, or CSV). When stream cache is not enabled, CluedIn streams out golden records one by one, each in a separate JSON file. Stream cache is available only for the synchronized stream mode. | Output Format – file format for the exported data. You can choose between JSON, Parquet, and CSV. However, Parquet and CSV formats are available only if you enabled stream cache. If stream cache is not enabled, JSON is the default format. | Export Schedule – schedule for sending the files from CluedIn to the export target. The files will be exported based on Coordinated Universal Time (UTC), which has an offset of 00:00. You can choose between the following options: . | Hourly – files will be exported every hour (for example, at 1:00 AM, at 2:00 AM, and so on). | Daily – files will be exported every day at 12:00 AM. | Weekly – files will be exported every Monday at 12:00 AM. | Custom Cron – you can create a specific schedule for exporting files by entering the cron expression in the Custom Cron field. For example, the cron expression 0 18 * * * means that the files will be exported every day at 6:00 PM. | . | (Optional) File Name Pattern – a file name pattern for the export file. For more information, see File name patterns. For example, in the {ContainerName}.{OutputFormat} pattern, {ContainerName} is the Target Name in the stream, and {OutputFormat} is the output format that you select in step 3j. In this case, every time the scheduled export occurs, it will generate the same file name, replacing the previously exported file. If you do not specify the file name pattern, CluedIn will use the default file name pattern: {StreamId:N}_{DataTime:yyyyMMddHHmmss}.{OutputFormat}. | . | Test the connection to make sure it works, and then select Add. Now, you can select the OneLake connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/onelake-connector",
    
    "relUrl": "/consume/export-targets/onelake-connector"
  },"1489": {
    "doc": "Preview",
    "title": "On this page",
    "content": ". | Search through source records | Analyze source records . | Profiling for data set | Profiling for golden records | Duplicates | . | Modify source records . | Filter records | Edit values manually | Transform values using operations | Add columns | Remove source records | . | Additional actions . | File: Download original file | Endpoint: Clear records | . | . In this article, you will learn about various actions available on the Preview tab of the data set to help you analyze the uploaded records and improve their quality before processing. When you upload the data into CluedIn, it appears on the Preview tab that displays the original, raw records. At this stage, you can perform various actions to identify data quality issues and prepare your data set for processing. ",
    "url": "/integration/additional-operations-on-records/preview#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/preview#on-this-page"
  },"1490": {
    "doc": "Preview",
    "title": "Search through source records",
    "content": "You can search through all source records to quickly locate and review particular records. Just enter a key word in the search box and start to search. As a result, all relevant records will be displayed on the page. If you are working with a data set created from an ingestion endpoint, you can search for records from a specific request send to the endpoint. You can find and copy the request ID on the Monitoring tab, and then paste it to the search box on the Preview and start to search. As a result, all records that were sent to CluedIn in a specific request will be displayed on the page. For more information about the request ID, see Monitoring. ",
    "url": "/integration/additional-operations-on-records/preview#search-through-source-records",
    
    "relUrl": "/integration/additional-operations-on-records/preview#search-through-source-records"
  },"1491": {
    "doc": "Preview",
    "title": "Analyze source records",
    "content": "Profiling allows you to analyze the uploaded records, identify issues and anomalies, and ensure data quality. By detecting inconsistencies and errors early, you can take corrective actions to improve the accuracy and reliability of your data. On the Preview tab, there are two types of profiling: . | Profiling for data set . | Profiling for golden records . | . Profiling for data set . Profiling for the data set allows you to analyze key metrics for each column of the uploaded records and identify data gaps, errors, and overall quality issues. By analyzing data set profiling, you can better understand the source records, evaluate their quality, and make informed decisions for mapping. To view data set profiling . | Find the column for which you want to view data set profiling. In the column header, open the three-dot menu, and then and select View profiling for data set. The Key metrics pane opens, where you can view the details about column values. | . The key data set profiling metrics include the following: . | Number of values – the total number of source records that have a value in a given column. | Number of empty values – the total number of source records that have an empty value in a given column. | Completeness – the percentage of non-empty or non-null values in a given column. | Incompleteness – the percentage of empty or null values in a given column. | Cardinality – the number of unique values in a given column. | Redundancy – the number that represents the frequency with which unique values are repeated in a given column. | . By analyzing data set profiling metrics, you can quickly identify if a particular column contains empty fields. Once aware of this issue, you can fix it by providing the necessary values as described in Modify source records. In general, a high percentage of incompleteness and a large number of repeated values may indicate that the field is not a good choice for producing identifiers. Profiling for golden records . Profiling for golden records provides a breakdown and distribution of all vocabulary key values in a graphic format. It can help you identify issues and anomalies in data quality. Profiling is not limited to a data set, meaning that it displays information about vocabulary key values coming from different data sources. To access profiling for golden records, go to Administration &gt; Feature Flags, and enable the Profiling dashboards feature. To view profiling . | Find the column for which you want to view golden record profiling. In the column header, open the three-dot menu, and then and select View profiling for golden records. The Profiling pane opens, where you can view the details about the vocabulary key values in a graphic format. | . The main profiling details for golden records include the following: . | Total values – the total number of values that the vocabulary key has in the system. | Values over time – a time-series visualization displaying the count of values that appeared in CluedIn over time. The x axis represents the time, and the y axis represents the count of values. For a more granular view, you can select a specific time period using the mouse. | Distribution of values (bar gauge) – the distribution of vocabulary key values based on the number of records where each value is used. Each bar represents a distinct value, with the color gradient indicating the frequency of that value’s occurrence in the records: green indicates a lower number of records, while red indicates a higher number of records. This gradient provides a quick visual cue for identifying the most and least common values. | Distribution of values (pie chart) – the distribution of vocabulary key values based on the number of records where each value is used. Each slice of the pie represents a distinct key value, with the area of the slice indicating the frequency of that value’s occurrence in the records. The larger the slice, the higher the number of records that contain the value. This type of visualization helps to quickly understand the relative proportions of different vocabulary key values. | . You can view profiling for golden records at any stage of the data ingestion process: immediately after uploading the data, after creating the mapping, or after processing the data. Profiling is type-specific, so if you change the data type during mapping and then process the data, the profiling dashboard may look different from when you first uploaded the data. For example, if you change the data type for the Revenue column from text to integer, the profiling dashboard will display metrics such as minimum value, maximum value, and average value instead of a breakdown and distribution of text values. You can also view profiling on the vocabulary key page. Duplicates . Once you have created the mapping for the data set, you can check if a specific column contains duplicate values. To view duplicates . | Find the column for which you want to view duplicates. In the column header, open the three-dot menu, and then and select View duplicates. The Duplicates preview pane opens, where you can view the total number of duplicate values in the column (a), identify which values are duplicates (b), and see how many times each duplicate value occurs (c). | . ",
    "url": "/integration/additional-operations-on-records/preview#analyze-source-records",
    
    "relUrl": "/integration/additional-operations-on-records/preview#analyze-source-records"
  },"1492": {
    "doc": "Preview",
    "title": "Modify source records",
    "content": "If you identify data gaps, errors, and overall quality issues in your source records and want to fix them before processing, you can do so in the edit mode of the data set. The edit mode allows you to perform various operations and validations on the source records to address these issues. The edit mode is available in data sets created from files and endpoints. When you switch to the edit mode, the original data set is cloned, so you can revert back to it at any time. However, reverting to the original data set means losing all changes made in the edit mode. To switch to the edit mode . | In the upper-right corner of the data set page, select Switch to edit mode. | Review the instructions about the edit mode, and then confirm that you want to switch to the edit mode. Depending on the size of your data set, switching to the edit mode might take some time. You’ll receive a notification when the data set has been switched to the edit mode. If you switch to edit mode for a data set that has already been processed, keep in mind that editing and re-processing this data set might lead to changes in primary identifiers. For example, if the primary identifiers are auto-generated, re-processing the data set will produce new golden records. | . If you decide to go back to the original data set, exit the edit mode. To do that, in the upper-right corner of the data set page, select Switch to original. Then, confirm that you want to switch to the original data set. If you made any changes in the edit mode, they will not be available in the original data set. When your data set is in the edit mode, you can do the following actions: . | Filter records . | Edit values manually . | Transform values using operations . | Add columns . | Remove source records . | . Filter records . Filters help you quickly find specific source records, identify records with empty values, and view column value aggregation. You can apply the following filters to any column: . | Search – select or enter a specific value to search for all records containing that value. You can choose to display the results that precisely match the entered value (Precise match) or those that exclude it (Inverse). | Is empty – display all records where the column contains empty values. | Is not empty – display all records where the column does not contain empty values. | Aggregation – consolidate and summarize all values contained in the column. | Invalid fields – display all records that contain invalid fields according to column validation result. If no validation is applied to the column, this filtering option is unavailable. Learn more about validation in Source record validation. | Valid fields – display all records that contain valid fields according to column validation result. If no validation is applied to the column, this filtering option is unavailable. Learn more about validation in Source record validation. | . You can use filters when you need to find incorrect or empty values. Once you find them, you can fix them by editing the values manually. To apply a filter . | Find the column you want to filter. In the column header, open the three-dot menu, select Filter, and then select the needed filtering option. As a result, the records matching the filter are displayed on the page. All applied filters are listed on the Filters pane. If you don’t need a filter temporarily, you can disable it and enable it again when needed. If you no longer need a filter, you can delete it by selecting the delete icon. | . Edit values manually . If you notice that your source records contain invalid values, missing values, or other issues and want to fix them before processing, you can do it by modifying the values manually. This way, you can fix incorrect spelling, provide missing values, or make any other changes. To edit a value manually . | Find a record containing a value that you want to edit. To quickly find the needed record, use filters. | Click on the cell containing a value that you want to edit and make the needed changes. The edited value is formatted in bold. | Select Save. The bold formatting of the changed value disappears. The history of your manual changes is displayed on the Operations tab. If you no longer need the change or you made it by mistake, you can revert the change. To do it, select the delete icon next to the operation name or the revert icon in the upper-right corner. Note that changes can only be reverted consecutively, one by one, and not selectively. | . Transform values using operations . You can transform and improve the contents of columns automatically and efficiently using operations. For example, you can transform the values to upper case, remove extra spaces, replace one value with another, and more. The following table shows all available operations and their description. | Operation | Description | . | Camel case | Transform all values in the column to camel case (directorOfSales). | . | Capitalize | Change the first letter of the first word to uppercase (Director of sales). | . | Decapitalize | Change the first letter of the first word to lowercase (director of sales). | . | Auto-generated value | Replace values with automatically created unique values. | . | Kebab case | Transform all values in the column to kebab case (director-of-sales). | . | Lower case | Transform all values in the column to lowercase (director of sales). | . | Set value | Replace all values in the column with the specified value. | . | Slugify | Convert a string into a lowercase, hyphen-separated version of the string, with all special characters removed. | . | Swap case | Invert the case of each letter in a string. This means that all uppercase letters are converted to lowercase, and all lowercase letters are converted to uppercase. | . | Trim | Remove all leading and trailing whitespace from a string. | . | Trim Left | Remove only the leading whitespace from a string. | . | Trim Right | Remove only the trailing whitespace from a string. | . | Upper case | Convert all letters of each word to uppercase (DIRECTOR OF SALES). | . | Title case | Convert the first letter of each word to uppercase (Director Of Sales). | . | Keep only numbers | Extract and retain only the numeric characters from a string, removing any non-numeric characters. | . | To Boolean | Convert a value to a Boolean data type, which can be either true or false. | . | Replace | Replace the first occurrence of a specified pattern within a string with another character or sequence of characters. | . | Replace all | Replace all occurrences of a specified pattern within a string with another character or sequence of characters. | . | Replace spaces | Replace spaces in a string with another character or sequence of characters. | . | Replace character(s) with space | Replace specified characters with a space. | . To transform values in a column using an operation . | Find the column containing values that you want to transform. In the column header, open the three-dot menu, select Operations, and then select the needed operation. As a result, the values in the column are automatically transformed. All applied operations are displayed on the Operations pane. If you no longer need the change or you made it by mistake, you can revert the change. To do it, select the delete icon next to the operation name or the revert icon in the upper-right corner. Note that changes can only be reverted consecutively, one by one, and not selectively. | . Add columns . You can add new columns to your dataset. This is useful for combining values from other columns or creating an empty column for new values. There are two types of columns that you can add: . | Stored column – the values are generated only at the column creation time. If new records appear in the data set afterwards, the stored column in such records will contain empty values. Stored column may be a good choice when working with a data set created from a file, which will not change over time. | Computed column – the values are combined from already existing columns into one new column. Computed column may be a good choice when working with a data set created from an endpoint, which will receive new records over time. | . To add a column to the data set . | Select Add column. | Enter the column name and select the column type. | Select Next. | Choose an option for generating the values for the column: . | (Stored column) Empty – a new column with empty fields will be added to the data set. | (Stored column or computed column) From existing fields – select the fields from the data set that you want to combine to create values for a new column. You can add multiple fields. By default, the values are separated with a space, but you can enter another delimiter if needed. | (Stored column or computed column) Expression – enter a C.E.L supported expression to create values for a new column. | . | Select Save. The new column is added to the data set and is marked with the information icon. If you no longer need the column, you can delete it. To delete the computed column, open the three-dot menu in the column header, select Delete computed field, and then confirm your choice. To delete the stored column, open the Operations pane, and then select the delete icon for the corresponding operation or the revert icon in the upper-right corner. Note that operations can only be reverted consecutively, one by one, and not selectively. | . Remove source records . If you do not need specific source records, you can remove them from the data set. To remove source records . | Select the checkbox next to the source record that you want to remove. | Select the delete icon. | In the confirmation dialog, enter DELETE, and then select Confirm. As a result, the records are removed from the data set. This change is displayed on the Operations tab. If you removed the records by mistake, you can revert the change. To do it, select the delete icon for the corresponding operation or the revert icon in the upper-right corner. Note that changes can only be reverted consecutively, one by one, and not selectively. | . ",
    "url": "/integration/additional-operations-on-records/preview#modify-source-records",
    
    "relUrl": "/integration/additional-operations-on-records/preview#modify-source-records"
  },"1493": {
    "doc": "Preview",
    "title": "Additional actions",
    "content": "Apart from analyzing and modifying source records, you can perform additional actions on the Preview tab. These actions depend on the data source type. File: Download original file . If you are working with a data set created from a file, you can download the original file. This is useful if you have modified the source records in the edit mode and want to keep the original file for reference. To download the original file . | Near the sorting dropdown list, open the three-dot menu, and then select Download original file. As a result, the original file is downloaded to your computer. | . Endpoint: Clear records . If you are working with a data set created from an endpoint, you can delete the records from temporary storage on the Preview tab. This is useful if you send many requests to the same endpoint and want to avoid processing each record every time. When processing records, CluedIn checks if they have been processed before. If they have, they won’t be processed again. To reduce processing time, you can delete already processed records from the Preview tab. You can clear the records regardless of whether the records have been processed or not, but if you haven’t processed the records, they will be permanently deleted. To clear records . | Near the sorting dropdown list, open the three-dot menu, and then select Clear records. | In the confirmation dialog, enter DELETE, and then select Confirm. As a result, all records from the Preview tab are deleted, and you can send more data to the endpoint. | . ",
    "url": "/integration/additional-operations-on-records/preview#additional-actions",
    
    "relUrl": "/integration/additional-operations-on-records/preview#additional-actions"
  },"1494": {
    "doc": "Preview",
    "title": "Preview",
    "content": " ",
    "url": "/integration/additional-operations-on-records/preview",
    
    "relUrl": "/integration/additional-operations-on-records/preview"
  },"1495": {
    "doc": "Sync deduplication projects to Purview",
    "title": "On this page",
    "content": ". | Preparation in CluedIn | Feature demonstration | . In this article, you will learn how to sync CluedIn deduplication projects to Purview assets. ",
    "url": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#on-this-page"
  },"1496": {
    "doc": "Sync deduplication projects to Purview",
    "title": "Preparation in CluedIn",
    "content": ". | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Deduplications to Purview. | Select Save. | Make sure you have an existing deduplication project. | . ",
    "url": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#preparation-in-cluedin",
    
    "relUrl": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#preparation-in-cluedin"
  },"1497": {
    "doc": "Sync deduplication projects to Purview",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of deduplication projects to Purview, you will receive a notification when the project is synced. To find the asset in Purview . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the deduplication project in CluedIn. | On the asset details page, go to Lineage. Here, you can view a visual representation of how deduplication projects are applied to the data set within the CluedIn processing pipeline. The following screenshot shows the application of a deduplication project within the CluedIn processing pipeline. The deduplication project is executed on processed entities, resulting in generating clues. The clues generated from the deduplication project are then sent back to the beginning of the processing pipeline. | . ",
    "url": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-deduplication-projects-to-purview#feature-demonstration"
  },"1498": {
    "doc": "Sync deduplication projects to Purview",
    "title": "Sync deduplication projects to Purview",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-deduplication-projects-to-purview",
    
    "relUrl": "/microsoft-integration/purview/sync-deduplication-projects-to-purview"
  },"1499": {
    "doc": "PVC",
    "title": "On this page",
    "content": ". | Resizing | Use existing volume claims | Getting AKS to create the disks on another resource group | Different SKUs in Azure . | SSD/Premium | Link directly to Azure Disk | . | References | Examples . | values.yml | cluedin-elastic-pvc.yml | cluedin-neo4j-pvc.yml | cluedin-rabbitmq-pvc.yml | cluedin-redis-pvc.yml | cluedin-seq-pvc.yml | cluedin-sqlserver-pvc.yml | . | . ",
    "url": "/deployment/kubernetes/persistence#on-this-page",
    
    "relUrl": "/deployment/kubernetes/persistence#on-this-page"
  },"1500": {
    "doc": "PVC",
    "title": "Resizing",
    "content": "By default, all deployments that store state (sqlserver, elasticsearch, rabbitmq, redis, openrefine) are defined with some storage in order to persist the state. We deploy with a recommended size by default, but there may be times where you want to adjust the size of the storage. You can do this by setting the following property (for each deployment) in the values file: . infrastructure: neo4j: #name of the deployment core: persistentVolume: size: 1G . Using persistence in this manner, a volume can only be linked to a single pod; so you won’t be able to scale the number of pods. In addition, the strategy for updating the pods is set to Recreate for exactly the same reason (as setting it to Rollout would require to have two pods accessing the volume simultaneously). The deployment to Azure will automatically handle the sizing. However, you cannot size down easily due to the way infrastructure disks work on Azure. Therefore, it is recommended to only size up. When working locally, you have more freedom as this is controlled by the user. ",
    "url": "/deployment/kubernetes/persistence#resizing",
    
    "relUrl": "/deployment/kubernetes/persistence#resizing"
  },"1501": {
    "doc": "PVC",
    "title": "Use existing volume claims",
    "content": "If you don’t want a service to create the PVC disks for you, you can: . | Create the PVCs ahead of time (if possible - see examples in this directory for each service) . | Update values.yaml with the names of the PVCs (see example values.yaml in this article - as different charts work differently the syntax is not consistent.). | . For certain services (ElasticSearch, Neo4J) you have to use specific names for the PVC in order to get them to work. These are listed in the example values.yaml. ",
    "url": "/deployment/kubernetes/persistence#use-existing-volume-claims",
    
    "relUrl": "/deployment/kubernetes/persistence#use-existing-volume-claims"
  },"1502": {
    "doc": "PVC",
    "title": "Getting AKS to create the disks on another resource group",
    "content": "If you want to create a disk on a different resource group (for increased resilience against cluster death), you can create a new StorageClass resource, in the cluedin namespace, with the configuration you want in it. | Create a new StorageClass object which links to the resource group | . kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: external-rg-hdd provisioner: kubernetes.io/azure-disk parameters: skuname: Standard_LRS kind: Managed cachingMode: ReadOnly resourceGroup: RESOURCE_GROUP_NAME allowVolumeExpansion: true . | Update the storageClass / storageClassName in the appropriate chart .. | . mssql: persistence: storageClass: \"external-rg-hdd\" . When it provisions the PVC for the first time it will provision them on a different resource group. Note: Its is unsupported/not possible to create disks in another subscription. Microsoft have said they have no plans to support this. ",
    "url": "/deployment/kubernetes/persistence#getting-aks-to-create-the-disks-on-another-resource-group",
    
    "relUrl": "/deployment/kubernetes/persistence#getting-aks-to-create-the-disks-on-another-resource-group"
  },"1503": {
    "doc": "PVC",
    "title": "Different SKUs in Azure",
    "content": "SSD/Premium . kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ssd provisioner: kubernetes.io/azure-disk parameters: skuname: Premium_LRS kind: managed cachingMode: ReadOnly . Link directly to Azure Disk . apiVersion: v1 kind: PersistentVolume metadata: name: pv-azuredisk spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain azureDisk: diskName: {disk-id} diskURI: /subscriptions/{sub-id}/resourcegroups/{group-name}/providers/microsoft.compute/disks/{disk-id} kind: Managed cachingMode: ReadOnly fsType: ext4 readOnly: false . ",
    "url": "/deployment/kubernetes/persistence#different-skus-in-azure",
    
    "relUrl": "/deployment/kubernetes/persistence#different-skus-in-azure"
  },"1504": {
    "doc": "PVC",
    "title": "References",
    "content": "Lots of example of different Azure/AKS StorageClass definitions can be found here: https://github.com/andyzhangx/demo/tree/master/pv. ",
    "url": "/deployment/kubernetes/persistence#references",
    
    "relUrl": "/deployment/kubernetes/persistence#references"
  },"1505": {
    "doc": "PVC",
    "title": "Examples",
    "content": "values.yml . mssql: persistence: existingDataClaim: \"cluedin-sqlserver-data\" existingTransactionLogClaim: \"cluedin-sqlserver-translog\" existingBackupClaim: \"cluedin-sqlserver-backup\" existingMasterClaim: \"cluedin-sqlserver-master\" seq: persistence: existingClaim: \"cluedin-seq-data\" rabbitmq: persistence: existingClaim: \"cluedin-rabbitmq-data\" # Note: If you are going to scale REDIS up and run multiple replicas etc then just create PVCs with the same name # as the PVC its *going* to create ahead of installation. redis: master: persistence: existingClaim: \"cluedin-redis-data\" # Note: For ELASTICSEARCH you need to create a PVC with the same name as the one its *going* to create ahead of # installation. This is because it uses a volumeClaimTemplate. cluedin-elastic-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-elasticsearch-cluedin-elasticsearch-0 labels: app: elasticsearch spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi . cluedin-neo4j-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: datadir-cluedin-neo4j-core-0 labels: app: neo4j spec: accessModes: - ReadWriteOnce resources: requests: storage: 12Gi . cluedin-rabbitmq-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-rabbitmq-data labels: app: rabbitmq spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi . cluedin-redis-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-redis-data labels: app: redis spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi . cluedin-seq-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-seq-data labels: app: seq spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi . cluedin-sqlserver-pvc.yml . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-sqlserver-master labels: app: sqlserver spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-sqlserver-backup labels: app: sqlserver spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-sqlserver-data labels: app: sqlserver spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cluedin-sqlserver-translog labels: app: sqlserver spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi . ",
    "url": "/deployment/kubernetes/persistence#examples",
    
    "relUrl": "/deployment/kubernetes/persistence#examples"
  },"1506": {
    "doc": "PVC",
    "title": "PVC",
    "content": " ",
    "url": "/deployment/kubernetes/persistence",
    
    "relUrl": "/deployment/kubernetes/persistence"
  },"1507": {
    "doc": "Open Mirroring connector",
    "title": "Open Mirroring connector",
    "content": "This article outlines how to configure the Open Mirroring connector to push data from CluedIn to Microsoft’s Fabric. Prerequisites: Make sure you use a service principal to authenticate and access Open Mirroring. | In PowerBI/Fabric, enable the following settings: . | Service principals can use Fabric APIs . | Users can access data stored in OneLake with apps external to Fabric . | . | In the workspace that you want to use in Open Mirroring connector, give at least Contributor access to the Open Mirroring service principal. To do that, in the workspace, select Manage access. Then, add the Open Mirroring service principal and assign it the Contributor role to the workspace. | . To configure Open Mirroring connector . | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select Fabric Open Mirroring Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | WorkspaceName – name of the workspace where you want to store the data from CluedIn. To find the workspace, sign in to Microsoft Fabric, and then select Workspaces from the left-hand menu. In the list of workspaces, find the needed workspace and select it. | Create Mirrored Database – Automatically creates the Mirrored Database if it doesn’t exist. Requires ‘Admin’ permission in the workspace. | Mirrored Database Name – name of the Mirrored Database if it already exists in Microsoft Fabric. Leaving this empty will create a new Mirrored Database. | ClientID – unique identifier assigned to the Open Mirroring app when it was registered in the Microsoft identity platform. You can find this value in the Overview section of app registration. | ClientSecret – confidential string used by your Open Mirroring app to authenticate itself to the Microsoft identity platform. You can find this value in the Certificates &amp; secrets section of app registration. | TenantID – unique identifier for your Microsoft Entra tenant. You can find this value in the Overview section of app registration. | Output Format – file format for the exported data. You can choose between JSON, Parquet, and CSV. However, Parquet and CSV formats are available only if you enabled stream cache. If stream cache is not enabled, JSON is the default format. | Export Schedule – schedule for sending the files from CluedIn to the export target. The files will be exported based on Coordinated Universal Time (UTC), which has an offset of 00:00. You can choose between the following options: . | Hourly – files will be exported every hour (for example, at 1:00 AM, at 2:00 AM, and so on). | Daily – files will be exported every day at 12:00 AM. | Weekly – files will be exported every Monday at 12:00 AM. | Custom Cron – you can create a specific schedule for exporting files by entering the cron expression in the Custom Cron field. For example, the cron expression 0 18 * * * means that the files will be exported every day at 6:00 PM. | . | (Optional) File Name Pattern – a file name pattern for the export file. For more information, see File name patterns. For example, in the {ContainerName}.{OutputFormat} pattern, {ContainerName} is the Target Name in the stream, and {OutputFormat} is the output format that you select in step 3j. In this case, every time the scheduled export occurs, it will generate the same file name, replacing the previously exported file. If you do not specify the file name pattern, CluedIn will use the default file name pattern: {StreamId:N}_{DataTime:yyyyMMddHHmmss}.{OutputFormat}. | . | Test the connection to make sure it works, and then select Add. | . You can follow this video to see how it works and what settings to provide to CluedIn. ",
    "url": "/consume/export-targets/openmirroring-connector",
    
    "relUrl": "/consume/export-targets/openmirroring-connector"
  },"1508": {
    "doc": "Alerts",
    "title": "Alerts",
    "content": "By default, CluedIn contains built-in alerts that are sent to our support team. If you need to set up additional alerts or modify the existing configuration of alerts, you can do it your CluedIn AKS cluster. To configure additional alerts . | In the Azure portal, navigate to the needed AKS cluster. | In the left pane, under Monitoring, select Alerts. You will see the alerts for your CluedIn AKS cluster. | To configure additional alerts, create a new alert rule and set the actions for the alerts rule as described in Microsoft documentation. | . ",
    "url": "/deployment/infra-how-tos/configure-alerts",
    
    "relUrl": "/deployment/infra-how-tos/configure-alerts"
  },"1509": {
    "doc": "Delta Crawls",
    "title": "Delta Crawls",
    "content": "What are delta crawls . By default, when adding a new provider, CluedIn sets up two recurring jobs - one that runs every 4 hours and one that runs every 7 days. These jobs can be customised for different intervals or simply disabled. The weekly job is supposed to run a full crawl again. The one that runs every 4 hours is called delta crawl - a crawl job designed to bring only the data that is new and hasn’t been ingested into CluedIn yet. To setup the crawler to support delta crawls as well as full crawls, you can use the LastCrawlFinishTime property of the JobData object that is available in the crawlers. Once a crawl job has finished, this property is updated with the corresponding date and time and it is available at runtime in the crawler. An example of how delta crawls can be implemented while still accepting full crawls can be found below: . In the crawler’s JobData, we can add a new configurable property as such: . public bool FullCrawl { get; set; } = false; . And in the JobData’s constructor, we can give the property the value from configuration: . if (configuration.ContainsKey(\"FullCrawl\") &amp;&amp; !string.IsNullOrEmpty(configuration[\"FullCrawl\"].ToString())) FullCrawl = GetValue&lt;bool&gt;(configuration, \"FullCrawl\"); . In the Client class we check for the FullCrawl’s value - if it’s false, we continue with the delta crawl; if it’s true, we continue with the full crawl. Below we can find two examples of how this can be implemented for an API and for SQL. Using Eloqua’s Bulk API publicly available filters documentation: https://docs.oracle.com/en/cloud/saas/marketing/eloqua-develop/Developers/BulkAPI/Tutorials/Filtering.htm. var exportDefinitionRequest = new ExportDefinitionRequest { Name = \"CluedIn Contact Export\", Fields = new Dictionary&lt;string, string&gt; { { \"EmailAddress\", \"Contact.Field(C_EmailAddress)\"}, { \"FirstName\", \"Contact.Field(C_FirstName)\"}, { \"LastName\", \"Contact.Field(C_LastName)\"} } }; if (!_jobData.FullCrawl &amp;&amp; _jobData.LastCrawlFinishTime &gt; DateTime.MinValue) { exportDefinitionRequest.Filter = \"'Contact.Field(C_DateModified)' &gt; '\" + $\"{_jobData.LastCrawlFinishTime.ToString()}\" + \"'\"; } var exportDefinitionResponse = await PostAsync&lt;ExportDefinitionResponse&gt;(\"contacts/exports\", exportDefinitionRequest); return exportDefinitionResponse; . Using an imaginary Oracle SQL database example: . public IEnumerable&lt;SqlEntity&gt; GetObject() { var offsetInitValue = GetInitialOffset(); var maxNumberOfRows = GetMaxNumberOfRows(tableName); var whereStatement = string.Empty; if (!_jobData.FullCrawl &amp;&amp; _jobData.LastCrawlFinishTime &gt; DateTime.MinValue) whereStatement = $\"WHERE ModifiedDateColumn &gt; {_jobData.LastCrawlFinishTime}\"; for (var offset = offsetInitValue; offset &lt; maxNumberOfRows; offset += _jobData.PageSize) { using (var connection = new OracleConnection(_jobData.ConnectionString)) using (var command = connection.CreateCommand()) { OracleDataReader reader = null; try { connection.Open(); reader = ActionExtensions.ExecuteWithRetry(() =&gt; { command.CommandText = $@\"SELECT f.* FROM ( SELECT t.*, rownum r FROM ( SELECT * FROM SqlTable {whereStatement} ORDER BY ModifiedDateColumn) t WHERE rownum &lt;= {offset + _jobData.PageSize}) f WHERE r &gt; {offset}\"; command.CommandTimeout = 180; return command.ExecuteReader(); }, isTransient: ex =&gt; ex is OracleException || ex.IsTransient()); } catch (Exception exception) { _log.LogError(exception.Message, exception); yield break; } while (reader.Read()) { SqlEntity sqlEntity = null; try { sqlEntity = new SqlEntity(reader); } catch (Exception exception) { _log.LogError(exception.Message, exception); continue; } if(sqlEntity != null) yield return sqlEntity; } } } } . Please note that the syntax Contact.Field(&lt;field name&gt;) should be put in double brackets (as per API’s documentation), which have not been included in the code snippet for formatting reasons. Stream Delta Crawls . Delta crawls can also be setup as consumers for stream platforms such as Kafka. If a Kafka stream has not been set up yet, make sure Change Data Capture is enabled for the database and CluedIn can implement Debezium as a Kafka stream (if the database of choice is supported). After we implemented Debezium, we then integrate the crawlers with the stream. One simple example of how a Client can be implemented with Kafka can be found in our public Kafka example crawler. ",
    "url": "/integration/delts-crawls",
    
    "relUrl": "/integration/delts-crawls"
  },"1510": {
    "doc": "Business domain",
    "title": "On this page",
    "content": ". | Business domain usage . | Adding semantic context to golden records | Filtering golden records | Producing primary identifier | . | Business domain properties and characteristics . | Business domain code | Nested business domains | . | Useful resources | . A business domain represents a specific business object that describes the semantic meaning of golden records. It can signify physical objects, individuals, locations, and more. Business domains should be global, timeless, and independent of specific data sources (e.g., Contact, Organization, Car). Each golden record is associated with exactly one business domain. You can leverage built-in business domains or define your own custom business domains. A business domain is assigned to a clue during the mapping process, and it plays a critical role in various identifiers, including primary identifiers and additional identifiers. ",
    "url": "/key-terms-and-features/entity-type#on-this-page",
    
    "relUrl": "/key-terms-and-features/entity-type#on-this-page"
  },"1511": {
    "doc": "Business domain",
    "title": "Business domain usage",
    "content": "Adding semantic context to golden records . The business domain provides metadata, such as a display name, icon, and description, adding a semantic layer to golden records. Golden records sharing the same business domain inherently share the same semantic meaning. When defining business domains, it’s essential to balance specificity and genericness. Use terminology familiar to your line of business (LOB) to help identify records intuitively. CluedIn is flexible—if you choose an initial business domain that needs adjustment, you can change it. However, changing business domains mid-project can be cumbersome, especially if deduplication projects, rules, or streaming configurations have already been applied. Filtering golden records . Business domain acts as the default filter for many operations in CluedIn. Selecting the right business domain allows you to target groups of golden records that logically belong together. Producing primary identifier . Business domain forms part of the primary identifier value. This structure enforces that records can only merge if they share the same business domain. ",
    "url": "/key-terms-and-features/entity-type#business-domain-usage",
    
    "relUrl": "/key-terms-and-features/entity-type#business-domain-usage"
  },"1512": {
    "doc": "Business domain",
    "title": "Business domain properties and characteristics",
    "content": "Business domain code . Business domains have a unique code, represented as a simple string prefixed by a slash (/). The code uniquely identifies the business domain. That is why you see a slash (/) in front of a business domain. To create a business domain code, use concise, meaningful names and avoid non-alphanumeric characters where possible. Nested business domains . CluedIn supports nested business domains, allowing hierarchical organization of business domains. While not mandatory, this feature can help group business domains of the same nature. Example of nested business domains . In the following hierarchy, Video is a child of Document. /Document /Document/XLS /Document/Video /Document/Audio . Benefits of nested business domains . | Filter grouped entities – nested business domains allow you to filter or stream entities collectively. For example, using a filter that starts with /Document would include all documents, regardless of their specific sub-business domain. | Streamline reporting – nested business domains simplify reporting and analysis across related business domains. | . ",
    "url": "/key-terms-and-features/entity-type#business-domain-properties-and-characteristics",
    
    "relUrl": "/key-terms-and-features/entity-type#business-domain-properties-and-characteristics"
  },"1513": {
    "doc": "Business domain",
    "title": "Useful resources",
    "content": ". | Add or modify a business domain | . ",
    "url": "/key-terms-and-features/entity-type#useful-resources",
    
    "relUrl": "/key-terms-and-features/entity-type#useful-resources"
  },"1514": {
    "doc": "Business domain",
    "title": "Business domain",
    "content": " ",
    "url": "/key-terms-and-features/entity-type",
    
    "relUrl": "/key-terms-and-features/entity-type"
  },"1515": {
    "doc": "Excel Add-in",
    "title": "On this page",
    "content": ". | Add CluedIn Excel Add-in | Connect CluedIn Excel Add-in to CluedIn | Actions in CluedIn group | Work with CluedIn Excel Add-in . | Load data from CluedIn to Excel | Modify loaded data in Excel | Cluster and edit data in Excel | . | . In this article, you will learn how to add the Excel Add-in, connect it to CluedIn, and use it to work with data. The add-in enables you to load the data from CluedIn into Excel, modify the data, and send the changes back to CluedIn. ",
    "url": "/microsoft-integration/excel-add-in#on-this-page",
    
    "relUrl": "/microsoft-integration/excel-add-in#on-this-page"
  },"1516": {
    "doc": "Excel Add-in",
    "title": "Add CluedIn Excel Add-in",
    "content": "The CluedIn Excel Add-in allows you to extend Excel application functionality across multiple platforms including Windows, Mac, and in a browser. The CluedIn Excel Add-in is hosted on the internet. If you have a zero-trust corporate environment that restricts internet access, contact our support team at support@cluedin.com to organize the installation into your tenant. To add CluedIn Excel Add-in . | In Excel, go to the Home tab, and then select Add-ins. | Select More Add-ins. In the search bar, enter CluedIn and then start the search. | Next to the CluedIn Excel Add-in, select Add. | In the License terms and privacy policy dialog, select Continue. A new group called CluedIn appears on the ribbon of the Home tab in Excel. | In the New Office Add-in pane, select Allow and Continue. If you do not see this pane, select Show Taskpane in the CluedIn group. As a result, the CluedIn Excel Add-in is added to your Excel application. Next, connect the add-in to your CluedIn instance. | . ",
    "url": "/microsoft-integration/excel-add-in#add-cluedin-excel-add-in",
    
    "relUrl": "/microsoft-integration/excel-add-in#add-cluedin-excel-add-in"
  },"1517": {
    "doc": "Excel Add-in",
    "title": "Connect CluedIn Excel Add-in to CluedIn",
    "content": "To enable the CluedIn Excel Add-in to communicate with your CluedIn instance, provide the server address and then sign in to your CluedIn instance when prompted. To connect CluedIn Excel Add-in to CluedIn . | In the CluedIn Excel Add-in task pane, select New. | Enter the Description and the Server address of a CluedIn instance that you want to connect to. Then, select Create. | Hover over the newly added connection, and then select the checkbox next to in. Then, select Connect. A new window opens, prompting you to sign in to your CluedIn instance. You can sign in using your username and password, or with SSO if it is enabled for you. After you sign in to CluedIn, you can start working with the CluedIn Excel Add-in. | . You can add multiple connections to the CluedIn Excel Add-in. However, you can connect only to one CluedIn instance at a time. To add another connection . | In the CluedIn Excel Add-in task pane, do one of the following: . | If you have already connected to a CluedIn instance, select Change connection. | If you have not yet connected to a CluedIn instance, select New. | . | Enter the Description and the Server address of a CluedIn instance that you want to connect to. Then, select Create. A new connection is added to the CluedIn Excel Add-in. To connect to a specific CluedIn instance, hover over the needed connection, and then select the checkbox next to in. Then, select Connect. You will be prompted to sign in to the CluedIn instance. | . ",
    "url": "/microsoft-integration/excel-add-in#connect-cluedin-excel-add-in-to-cluedin",
    
    "relUrl": "/microsoft-integration/excel-add-in#connect-cluedin-excel-add-in-to-cluedin"
  },"1518": {
    "doc": "Excel Add-in",
    "title": "Actions in CluedIn group",
    "content": "When the CluedIn Excel Add-in is added, a new group called CluedIn appears on the ribbon of the Home tab in Excel. This group contains several actions for working with the add-in. Note that the actions become available only after you connect to CluedIn. | Show Taskpane – opens the CluedIn Excel Add-in task pane to the right side of the window. | Create Entity Type – opens the business domain (previously entity type) creation pane in CluedIn in your default browser. For more information, see Create business domain. | Merge Entity – initiates the merging process by opening the merging page in CluedIn in your default browser. To define the records for merging, select multiple cells or rows in the worksheet, and then select Merge Entity. In the CluedIn Excel Add-in task pane, select the checkbox next to the target record into which you want to merge other records. and then select Merge. As a result, the merging page opens in CluedIn in your default browser. There, you can select the winning properties for the target record and complete the merging process. | View Entity – opens the golden record page in CluedIn in your default browser. To specify a golden record that you want to open in CluedIn, select the row number or any cell in the row of the record that you want to view. | Publish – publishes the changes made to the data in Excel to CluedIn. | Refresh – refreshes the data on the worksheet to include any new data that has appeared in CluedIn since the last load or refresh. | . ",
    "url": "/microsoft-integration/excel-add-in#actions-in-cluedin-group",
    
    "relUrl": "/microsoft-integration/excel-add-in#actions-in-cluedin-group"
  },"1519": {
    "doc": "Excel Add-in",
    "title": "Work with CluedIn Excel Add-in",
    "content": "Once you connect to an instance of CluedIn, you can start working with the data from CluedIn in your Excel application. Load data from CluedIn to Excel . | In Entity Type, select the business domain (previously entity type) of golden records that you want to load. The dropdown list contains the business domains that are currently used in CluedIn. | Specify the Vocabulary Keys of golden records that you want to load. To add all vocabulary keys associated with the selected business domain, use the Auto-select option. Alternatively, start entering the name of the vocabulary key, and then select the needed option. If you cannot find the needed vocabulary keys, refresh the reference data. If you cannot find the needed vocabulary keys after refreshing the reference data, select the gear icon and clear the Exclude Non-core Keys and Exclude Provider Keys checkboxes. | (Optional) To specify additional filters for loading golden records, use the following options: . | Text Filter – enter the text filter to load only golden records that contain specific text. | Providers – enter the name of the provider to load only golden records that appeared in CluedIn from a specific provider. | Tags – enter the tag to load only golden records that contain a specific tag. | Glossary Terms – enter the name of the glossary term to load only golden records that belong to a specific glossary term. You can enter multiple filters in each field, separated by commas. | . | Select Load. When the data is loaded, it becomes available in the spreadsheet, and you can start modifying it as needed. By default, the rows are presented in alternating light blue and white colors. Note that the sheet name corresponds to the business domain of loaded golden records. If you want to load the data of another business domain, just add a new sheet, edit the configuration on the Load Data tab, and load the data. and You can have as many sheets as you like. You can view the summary on the latest data load on the Overview tab. | . Modify loaded data in Excel . When the data from CluedIn is loaded to Excel, you can modify it by making changes to the cells or adding new rows. These new rows will then become new records once published to CluedIn. To modify uploaded data in Excel . | Make changes to the loaded data. Note that the background of the modified cells is changed to orange. If you want to undo the changes you have just made, select Undo on the Quick Access Toolbar or press Ctrl+Z. | To send your changes to CluedIn, on the Home tab, in the CluedIn group, select Publish. When the changes are published, the background of the modified cells is changed to blue. To make sure the changes have been saved in CluedIn, refresh the loaded data by selecting Refresh in the CluedIn group on the Home ribbon tab. As a result, the background of the cells returns to its default color (light blue or white). You can view the summary on the latest published data load on the Overview tab. | . Cluster and edit data in Excel . You can identify groups of different cell values that may represent the same thing. Additionally, you can merge these values into one, thereby performing data cleaning. To cluster and edit data . | In the CluedIn Excel Add-in task pane, go to the Cluster tab. | Select the Column Name that you want to cluster, and then select View Clusters. The clustering results page opens in a new window where you can view the clusters identified per the selected column. | If you want to merge the values in a cluster: . | Select the corresponding checkbox and enter the new cell value. | Select Merge &amp; Update Worksheet. The modified values are highlighted in orange. To send these changes to CluedIn, select Publish in the CluedIn group on the Home ribbon tab. Note that after applying a new cell value, you cannot revert this change using the standard Excel undo functionality. If you want to return the data to its original state, go to the Load Data tab of the CluedIn Excel Add-in task pane, and select Load. You will see a message informing you that all unpublished changes will be lost. If you agree to this, select Confirm. | . | . ",
    "url": "/microsoft-integration/excel-add-in#work-with-cluedin-excel-add-in",
    
    "relUrl": "/microsoft-integration/excel-add-in#work-with-cluedin-excel-add-in"
  },"1520": {
    "doc": "Excel Add-in",
    "title": "Excel Add-in",
    "content": " ",
    "url": "/microsoft-integration/excel-add-in",
    
    "relUrl": "/microsoft-integration/excel-add-in"
  },"1521": {
    "doc": "Gleif",
    "title": "On this page",
    "content": ". | Add Gleif enricher | Properties from Gleif enricher | . This article explains how to add the Gleif enricher. The purpose of this enricher is to provide a wide range of information about an organization (for example, legal name, address, entity status, and so on). More details are provided in Properties from Gleif enricher. The Gleif enricher supports the following endpoint: . | https://api.gleif.org/api/v1/lei-records?page[size]=1&amp;page[number]=1&amp;filter[lei]={leicode}, where {leicode} is the LEI code of the company. | . ",
    "url": "/preparation/enricher/gleif#on-this-page",
    
    "relUrl": "/preparation/enricher/gleif#on-this-page"
  },"1522": {
    "doc": "Gleif",
    "title": "Add Gleif enricher",
    "content": "The enricher uses the Legal Entity Identifier (LEI) code to search for information about companies. To add the Gleif enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Gleif, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Lei Code Vocabulary Key – enter the vocabulary key that contains LEI codes of companies that you want to enrich. | Skip Entity Code Creation (Lei Code) – turn on the toggle if you don’t want to add new entity codes that come from the source system to the enriched golden records. Otherwise, new entity codes containing LEI codes will be added to the enriched golden records. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Gleif enricher is added and has an active status. This means that it will enrich relevant golden records during processing or when you trigger external enrichment. | . After the Gleif enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/gleif#add-gleif-enricher",
    
    "relUrl": "/preparation/enricher/gleif#add-gleif-enricher"
  },"1523": {
    "doc": "Gleif",
    "title": "Properties from Gleif enricher",
    "content": "To quickly find the properties added to golden records from the Gleif enricher, use the integrations filter on the Properties page. For a more detailed information about the changes made to a golden record by the Gleif enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Gleif enricher. | Display name | Vocabulary key | . | Lei Code | gleif.organization.leiCode | . | Legal Name | gleif.organization.legalName | . | Address | gleif.organization.legalAddress+address | . | Number | gleif.organization.legalAddress+number | . | Number Within Building | gleif.organization.legalAddress+numberWithinBuilding | . | Mail Routing | gleif.organization.legalAddress+mailRouting | . | Additional Address | gleif.organization.legalAddress+additionalAddress | . | Region | gleif.organization.legalAddress+region | . | City | gleif.organization.legalAddress+city | . | Postal Code | gleif.organization.legalAddress+postalCode | . | Country Code | gleif.organization.legalAddress+countryCode | . | Legal Jurisdiction | gleif.organization.legalJurisdiction | . | Legal Form Code | gleif.organization.legalFormCode | . | Legal Form Type | gleif.organization.legalFormType | . | Address | gleif.organization.headquartersAddress+address | . | Number | gleif.organization.headquartersAddress+number | . | Number Within Building | gleif.organization.headquartersAddress+numberWithinBuilding | . | Mail Routing | gleif.organization.headquartersAddress+mailRouting | . | Additional Address | gleif.organization.headquartersAddress+additionalAddress | . | Region | gleif.organization.headquartersAddress+region | . | City | gleif.organization.headquartersAddress+city | . | Postal Code | gleif.organization.headquartersAddress+postalCode | . | Country Code | gleif.organization.headquartersAddress+countryCode | . | Other Entity Names | gleif.organization.otherEntityNames | . | Entity Status | gleif.organization.entityStatus | . | Initial Registration Date | gleif.organization.initialRegistrationDate | . | Last Update Date | gleif.organization.lastUpdateDate | . | Registration Status | gleif.organization.registrationStatus | . | Next Renewal Date | gleif.organization.nextRenewalDate | . | Managing LOU | gleif.organization.managingLOU | . | Entity Category | gleif.organization.entityCategory | . ",
    "url": "/preparation/enricher/gleif#properties-from-gleif-enricher",
    
    "relUrl": "/preparation/enricher/gleif#properties-from-gleif-enricher"
  },"1524": {
    "doc": "Gleif",
    "title": "Gleif",
    "content": " ",
    "url": "/preparation/enricher/gleif",
    
    "relUrl": "/preparation/enricher/gleif"
  },"1525": {
    "doc": "Add relations between records",
    "title": "On this page",
    "content": ". | Add edge relations | View relations between golden records | Results &amp; next steps | . Relations are built between source (“to”) and target (“from”) records by using edges of a certain type (for example, /WorksFor). You can create relations either before or after processing the data. In this article, you will learn how to add and view relations between golden records. Creating relations between golden records consists of editing the mapping of the data set that you will use as a source in order to add the edge that will link the source to the target. Files for practice . | File 1: training-company.csv . | File 2: training-employee.csv . | . Prerequisites . Before proceeding with relations between golden records, ensure that you have completed the following tasks: . | Ingested (uploaded, mapped, and processed) the data to which you will be linking the records. You can use file 1 above. See Ingest data guide. | Uploaded and mapped the data that you will be linking to already existing records. You can use file 2 above. See Import file and Create mapping. | . ",
    "url": "/getting-started/relations#on-this-page",
    
    "relUrl": "/getting-started/relations#on-this-page"
  },"1526": {
    "doc": "Add relations between records",
    "title": "Add edge relations",
    "content": "After you imported and mapped the data that you will be linking to already existing records, edit the mapping configuration. To add edge relations . | On the navigation pane, go to Ingestion &gt; Sources. | Find and select the needed data set (for example, file 2 “training-employee.csv”). | Go to the Map tab, and then select Edit mapping. | Go to the Add edge relations tab, and then select Add relation. The Add Edge pane opens on the right side of the page. | On the Edge Mode tab, do the following: . | Select the property to which you want to add edge relation. | Select the edge mode. | In the lower-right corner, select Next. | . | On the Configuration tab, do the following: . | Specify the edge type to define the nature of relation between objects. You can select the existing edge type or create a new one. To create a new edge type, enter a slash (/) and then enter a name. | Find and select the target business domain to which you will link the records from the current data set. | Define the origin of the target data set. It will displayed after you process the data. | In the lower-right corner, select Next. | . | On the Edge Properties tab, in the lower-right corner, select Add Edge. | Select Finish to save your changes. You added edge relations. To view relations between golden records, process the data and stream the records. | . ",
    "url": "/getting-started/relations#add-edge-relations",
    
    "relUrl": "/getting-started/relations#add-edge-relations"
  },"1527": {
    "doc": "Add relations between records",
    "title": "View relations between golden records",
    "content": "After you processed the data and streamed the records, you can view the relations between golden records in the following places: . | In CluedIn: on the Relations tab of the golden record details page. To view the edge relation details, select the arrow that connects the entities. | In the database: in the tables named dbo.xyzOutgoingEdges and dbo.xyzIncomingEdges, where xyz is the target name. If you add more edge relations between the records, CluedIn will automatically identify the changes and update the stream with new edge relations. | . ",
    "url": "/getting-started/relations#view-relations-between-golden-records",
    
    "relUrl": "/getting-started/relations#view-relations-between-golden-records"
  },"1528": {
    "doc": "Add relations between records",
    "title": "Results &amp; next steps",
    "content": "After completing all steps outlined in this guide, you learned how to add edges to build relations between golden records in CluedIn. You’ve reached the final part of the Getting Started section. Now might be a great time to dive deeper into the key terms and features of CluedIn: . | Golden records . | Identifiers . | Origin . | . ",
    "url": "/getting-started/relations#results--next-steps",
    
    "relUrl": "/getting-started/relations#results--next-steps"
  },"1529": {
    "doc": "Add relations between records",
    "title": "Add relations between records",
    "content": " ",
    "url": "/getting-started/relations",
    
    "relUrl": "/getting-started/relations"
  },"1530": {
    "doc": "How to write rules that check conditions across different business domains",
    "title": "Writing Rules That Span Across Business Domains",
    "content": "CluedIn Rules don’t just operate within a single entity. In many use cases, you’ll need to span across business domains — for example, applying a Customer rule that depends on their Orders, or a Supplier rule that looks at related Products. This is similar to performing a join in a traditional database. CluedIn provides functions and entity graph navigation features that allow rules to traverse relationships between entities and operate on connected data. ",
    "url": "/kb/how-to-write-rules-cross-domain#writing-rules-that-span-across-business-domains",
    
    "relUrl": "/kb/how-to-write-rules-cross-domain#writing-rules-that-span-across-business-domains"
  },"1531": {
    "doc": "How to write rules that check conditions across different business domains",
    "title": "Key Functions and Concepts",
    "content": "1. LoadByEntityCode . | Loads another entity into context using its unique EntityCode. | Useful when you already have the identifier of a related entity and want to access its data directly. | Example: ```powerfx LoadByEntityCode(“Customer”, CustomerEntityCode) | . ",
    "url": "/kb/how-to-write-rules-cross-domain#key-functions-and-concepts",
    
    "relUrl": "/kb/how-to-write-rules-cross-domain#key-functions-and-concepts"
  },"1532": {
    "doc": "How to write rules that check conditions across different business domains",
    "title": "How to write rules that check conditions across different business domains",
    "content": " ",
    "url": "/kb/how-to-write-rules-cross-domain",
    
    "relUrl": "/kb/how-to-write-rules-cross-domain"
  },"1533": {
    "doc": "SQL Server connector",
    "title": "On this page",
    "content": ". | Configure SQL Server connector | SQL Server Authentication | . This article outlines how to configure the SQL Server connector to publish data from CluedIn to SQL databases. Prerequisites: The authentication method for the SQL Server must be SQL Server Authentication (not Windows Authentication). If you do not use SQL Server Authentication, you need to enable it as described here. ",
    "url": "/consume/export-targets/sql-server-connector#on-this-page",
    
    "relUrl": "/consume/export-targets/sql-server-connector#on-this-page"
  },"1534": {
    "doc": "SQL Server connector",
    "title": "Configure SQL Server connector",
    "content": ". | On the navigation pane, go to Consume &gt; Export Targets. Then, select Add Export Target. | On the Choose Target tab, select SQL Server Connector. Then, select Next. | On the Configure tab, enter the connection details: . | Name – user-friendly name of the export target that will be displayed on the Export Target page in CluedIn. | Host – server where your database is located. | Database Name – name of a particular database where you want to store the data from CluedIn. | Username – username that you use to access the database. | Password – password associated with the username that grants access to the database. | (Optional) Port Number – network port on which your database server is listening for connections. | (Optional) Schema – container within a database where you want to store the data from CluedIn. | (Optional) Connection Pool Size – number of database connections that are maintained in a pool for reuse. | . | Test the connection to make sure it works, and then select Add. Now, you can select the SQL Server connector in a stream and start exporting golden records. | . ",
    "url": "/consume/export-targets/sql-server-connector#configure-sql-server-connector",
    
    "relUrl": "/consume/export-targets/sql-server-connector#configure-sql-server-connector"
  },"1535": {
    "doc": "SQL Server connector",
    "title": "SQL Server Authentication",
    "content": "When integrating with CluedIn, you typically need to use SQL Server Authentication (not Windows Authentication) to allow the platform to connect to your SQL Server. This is because CluedIn generally requires a consistent, non-interactive authentication method to securely access your database. | Windows Authentication is generally tied to the local user context of the machine you’re using, and it requires a session with specific privileges. This can create issues when trying to connect from an external service like CluedIn, especially when the service doesn’t have access to your machine’s Windows authentication context. | SQL Server Authentication is independent of the Windows environment and uses a specific SQL Server login and password that can be configured to have the appropriate access to your SQL database. This makes it easier to set up and more compatible with external systems. | . To configure SQL Server Authentication . | Ensure that your SQL Server is configured to accept SQL Server Authentication. This is a setting you can enable in the SQL Server Management Studio (SSMS) under the server properties. | To enable SQL Server Authentication: . | Open SQL Server Management Studio (SSMS). | Right-click on the server instance and select Properties. | In the Server Properties window, go to the Security tab. | Choose SQL Server and Windows Authentication mode. | Restart SQL Server after making this change. | . | Create a dedicated SQL Server login (with a username and password) that CluedIn can use to connect. You can do this via SSMS: . | Under the Security node in SSMS, go to Logins. | Right-click on Logins and select New Login. | Choose SQL Server authentication and set the password. | Assign the appropriate roles or permissions for the login to access the necessary databases. | . Once you’ve set up SQL Server Authentication and created a login for CluedIn, you can provide the login credentials (username and password) to connect CluedIn to your SQL Server instance. | . ",
    "url": "/consume/export-targets/sql-server-connector#sql-server-authentication",
    
    "relUrl": "/consume/export-targets/sql-server-connector#sql-server-authentication"
  },"1536": {
    "doc": "SQL Server connector",
    "title": "SQL Server connector",
    "content": " ",
    "url": "/consume/export-targets/sql-server-connector",
    
    "relUrl": "/consume/export-targets/sql-server-connector"
  },"1537": {
    "doc": "Sync streams to Purview",
    "title": "On this page",
    "content": ". | Preparation in CluedIn | Feature demonstration | . In this article, you will learn how to sync CluedIn streams to Purview assets. ",
    "url": "/microsoft-integration/purview/sync-streams-to-purview#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-streams-to-purview#on-this-page"
  },"1538": {
    "doc": "Sync streams to Purview",
    "title": "Preparation in CluedIn",
    "content": ". | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync CluedIn Streams. | Select Save. | Make sure you have a configured stream in place. | . ",
    "url": "/microsoft-integration/purview/sync-streams-to-purview#preparation-in-cluedin",
    
    "relUrl": "/microsoft-integration/purview/sync-streams-to-purview#preparation-in-cluedin"
  },"1539": {
    "doc": "Sync streams to Purview",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of deduplication projects to Purview, you will receive a notification when the project is synced. To find the asset in Purview . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. | In your default domain, select the collection that stores the assets from CluedIn. | Select the assets card. | In the list of assets, find and select the asset with the same name as the stream in CluedIn. | On the asset details page, go to Lineage. Here, you can view a visual representation of how streams are applied to the data set within the CluedIn processing pipeline. The following screenshot shows the application of a stream within the CluedIn processing pipeline. The stream is executed on processed entities. | . ",
    "url": "/microsoft-integration/purview/sync-streams-to-purview#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-streams-to-purview#feature-demonstration"
  },"1540": {
    "doc": "Sync streams to Purview",
    "title": "Sync streams to Purview",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-streams-to-purview",
    
    "relUrl": "/microsoft-integration/purview/sync-streams-to-purview"
  },"1541": {
    "doc": "User access",
    "title": "User access",
    "content": "In this section, you will learn about the main aspects that define user access in CluedIn. Every authorized user in CluedIn has an assigned role that defines the user’s permission to interact with CluedIn modules: Integrations, Preparation, Management, Governance, Consume, Engine Room, and Administration. Every module contains features that enable you to perform specific tasks. For example, in the Preparation module, you can create and work with clean projects. Access to those features is managed by CluedIn roles. For more information, see Feature access. Roles do not affect data access in CluedIn. For example, if you have access to the glossary, you still won’t be able to view golden records from a glossary term without specific data access. To configure secure and reliable access to the data in CluedIn, you need to set up source control and access control permissions. For more information, see Data access. ",
    "url": "/administration/user-access",
    
    "relUrl": "/administration/user-access"
  },"1542": {
    "doc": "Validations",
    "title": "On this page",
    "content": ". | Perform validations . | Auto-validation | Manual validation | Advanced validation | . | Manage validations | Process validation results . | Review validation results | Fix invalid values | . | Validation methods | . In this article, you will learn how to check the source records for errors, inconsistencies, and missing values, as well as how to fix invalid values with the help of validations. As a result, you can improve the quality of source records and prevent incorrect records from becoming golden records or aggregating to the existing golden records. ",
    "url": "/integration/additional-operations-on-records/validations#on-this-page",
    
    "relUrl": "/integration/additional-operations-on-records/validations#on-this-page"
  },"1543": {
    "doc": "Validations",
    "title": "Perform validations",
    "content": "To perform source record validations, you need to meet the following prerequisites: . | The data set should be mapped to standard fields. This way CluedIn can analyze your data and suggest appropriate validation methods. Learn more in Create mapping and Review mapping. | The data set should be in the edit mode. This way you can add validations and modify source records. Learn more in Preview. | . To access validations . | On the data set page, select Validations. The pane containing Validations opens to the right side of the page. | . When you access validations for the first time or when you reset validations, you will see the initial validation setup options: . | Auto-validation – CluedIn will analyze the fields and suggest appropriate validation methods for some fields. | Manual setup – you need to select appropriate validation methods for the fields that you want to validate. | . You can start with auto-validation and then add manual validations for the needed fields. Auto-validation . Auto-validation is a good starting point for finding invalid values. To run auto-validation . | On the Validations pane, in the Initial Validation Setup section, make sure that the Auto-validation option is selected. | In the Validation Preview section, review the fields along with validation methods suggested by CluedIn. | Select Validate. CluedIn will run validations on the specific fields. When the validations are complete, you will see the validation results for each field. The fields that failed the validation check are highlighted in red. Now, you can process the results of auto-validation and fix invalid values. | . Manual validation . If auto-validation is not sufficient for you or if you want to apply different validation methods, use manual validation. To add manual validation . | Depending on whether you have already run auto-validation, do one of the following: . | If you have already run auto-validation, then expand the filter dropdown, and select Show all fields. | If you have not run auto-validation, then in the Initial Validation Setup section, select the Manual Setup option, and then select Validate. | . | From the list of all fields, find the field for which you want to add manual validation. To quickly find the needed field, start entering its name in the search field. | In the Validation Method dropdown, select the validation method that you want to apply to the field. Depending on the selected validation method, you might need to provide additional configuration details. For example, the following screenshot shows the validation method that checks the Job Title field for empty values. The Inverse toggle is turned on, indicating that if the field contains an empty string, it will be marked as invalid. | When the validation for a field is configured, select Validate. CluedIn will run validation on the specific field. When the validation is complete, you will see the validation results for the field. | To add manual validations for other fields, repeat steps 2–4. The fields that failed the validation check are highlighted in red. Now, you can process the results of manual validation and fix invalid values. | . Advanced validation . The previous validation options—auto-validation and manual validation—focus on the field-level validation. If you need to implement complex business logic to check for invalid records, use advanced validation. To add advanced validation . | On the Validations tab, select Advanced Validation. | Select Run to load all clues that were created from the data set. The clues appear on the right side of the page. | On the left side of the page, write the code to create source record validation logic. You can write any JavaScript code. In this example, we wrote the code that checks the Job Title and Department values, and if the Job Title is Accountant and Department is Marketing, then the whole record will be marked as invalid. | To check if the code is applied as intended, select Run. As a result, if the record contains the specified combination of values, the isValid property is set to false. | If you are satisfied with the result, select Save. To return to the data set, select Back. The source records that failed the advanced validation check are highlighted in red. Now, you can fix invalid values. | . ",
    "url": "/integration/additional-operations-on-records/validations#perform-validations",
    
    "relUrl": "/integration/additional-operations-on-records/validations#perform-validations"
  },"1544": {
    "doc": "Validations",
    "title": "Manage validations",
    "content": "Once you have added validations, there are a number of actions you can take to manage these validations: . | Refresh – re-run the validation check for a field. | Remove – remove the validation check for a field. | Edit – edit the configuration of the validation for a field. You can select another validation method and modify the configuration details. After you finish, select Save &amp; Validate. As a result, the validation check for the field is run again. | Filter fields – select between all fields, validated fields, or non-validated fields. | Sort fields – sort the fields displayed on the pane: by name, by newest, by oldest. | Reset field filters – resets the filters to show all fields. | Reset validation methods – removes all validation methods so that you can start adding them from scratch. | . ",
    "url": "/integration/additional-operations-on-records/validations#manage-validations",
    
    "relUrl": "/integration/additional-operations-on-records/validations#manage-validations"
  },"1545": {
    "doc": "Validations",
    "title": "Process validation results",
    "content": "After performing validations, you can start reviewing validation results and fixing invalid values. Review validation results . You can review validation results in two places: on the Validations pane and on the data set page. Validation results on the Validations pane . On the Validations pane, you can view validations results for each field. These results include the validation method as well as the total number of values, the number of valid values, and the number of invalid values. You can select the number of invalid values for a field to filter the records displayed on the page. Additionally, the status bar shows the percentage of valid values for a field. Validation results on the data set page . On the data set page, you can view validation results for each column. Hover over the status bar at the bottom of the column header and you will see the validation results for that column (a). These results are the same as on the Validations pane. Additionally, the status bar under the column header shows the correlation between valid (green) and invalid (red) values in the column. You can also view the general statistics of valid values in the data set (b). Fix invalid values . After running the validation checks, you can start reviewing and fixing invalid values. You are not required to fix invalid values—when you process the data set, the records containing invalid values will be processed in the same way as all the other records and they will not be automatically sent to the quarantine or for approval area unless there are specific rules. To find and fix invalid values . | To find invalid values, use one of the following options: . | In the Validations pane, locate the field for which you want to view invalid values, and then select the number of invalid values. | On the data set page, locate the column for which you want to view invalid values. Then, open the three-dot menu in the column header, and select Filter &gt; Invalid Fields. | . Regardless of the option that you use, the filter for invalid values is added to the Filters pane. As a result, the data set page displays the records containing invalid values. These invalid values are highlighted in red. | To fix invalid values, click on the cell containing the invalid value and modify it accordingly. The modified value is marked in bold. Once the value is correct, it becomes highlighted in green. | Select Save. The records containing fixed values disappear from the data set page because they no longer meet the filter criteria for showing invalid values. The modifications that you make to the source records are added to the Operations pane every time you save changes. If you want to revert changes, select the delete icon next to the operation name or the revert icon in the upper-right corner. Note that changes can only be reverted consecutively, one by one, and not selectively. | . ",
    "url": "/integration/additional-operations-on-records/validations#process-validation-results",
    
    "relUrl": "/integration/additional-operations-on-records/validations#process-validation-results"
  },"1546": {
    "doc": "Validations",
    "title": "Validation methods",
    "content": "| Method | Description | . | Range (&lt;%=min&gt; to &lt;%=max%&gt;) | Check if a value falls within a specified range. You need to provide the min and max values. | . | Is valid email address | Check if an email address is correctly formatted and potentially valid. This validation method ensures the email address follows standard formatting rules, such as having an “@” symbol and a valid domain name (e.g., example@domain.com). | . | Is empty string | Check if a value is either completely empty or contains no characters. | . | Is equal to | Check if a value is equal to a specific value. | . | Is equal to Multiple (AND) | Check if a value meets all of the specified values using the logical AND operator. | . | Is equal to Multiple (OR) | Check if a value meets at least one of multiple specified values using the logical OR operator. | . | Is a number | Check if a value is a valid number. | . | Regex - matches/&lt;%=pattern%&gt; | Check if a value matches a regex pattern. You need to provide the regex pattern. | . | Is URL | Check if a value is a valid URL (uniform resource locator). | . | Ip Address V4 | Check if a value is a valid IPv4 address. An IPv4 address consists of four numerical segments separated by dots, with each segment ranging from 0 to 255 (for example, 192.168.1.1). | . | Ip Address V6 | Check if a value is a valid IPv6 address. An IPv6 address consists of eight groups of four hexadecimal digits, separated by colons (for example, 2001:0db8:85a3:0000:0000:8a2e:0370:7334). | . | Is UUID | Check if a value is a valid UUID (universally unique identifier). | . | Is Credit Card | Check if a value is a valid credit card number. | . | Is Boolean | Check if value is a valid Boolean (either true or false). | . | Is Currency | Check if a value is a valid ISO 4217 currency code. | . | Is ISO31661 Alpha2 | Check if a value is a valid ISO 3166-1 alpha-2 country code. ISO 3166-1 alpha-2 codes are two-letter codes used to represent countries, dependent territories, and special areas of geographical interest. | . | Is ISO31661 Alpha3 | Check if a value is a valid ISO 3166-1 alpha-3 country code. ISO 3166-1 alpha-3 codes are three-letter codes used to represent countries, dependent territories, and special areas of geographical interest. | . | Is gender Facebook | Check if a gender value falls within the list of gender options available on Facebook. | . | Is Bank Card Type | Check if a value corresponds to a recognized bank card type, such as American Express, Bankcard, China UnionPay, Diners Club Carte Blanche, Diners Club enRoute, Diners Club International, Diners Club United States &amp; Canada, InstaPayment, JCB, Laser, Maestro, Mastercard, Solo, Switch, Visa, Visa Electron. | . | Is gender Abbreviation | Check if a given value is a valid gender abbreviation, such as “M” for Male, “F” for Female, and “X” for non-binary or other gender identities. | . | Is Integer | Check if a value is a valid integer. | . | Is Age | Check if a value is a valid age between 0 and 199. | . Regardless of the validation method, the validation for a field contains two additional settings: . | Required – this setting defines whether a field is required to have a value. This setting is particularly useful when you are checking for empty fields. If this is your case, make sure you select the checkbox. If you are using other validation methods, it does not matter if the checkbox is selected or not because the validation will be performed based on the existing values, and empty fields will not be marked as invalid. | Inverse – this setting defines whether a field is marked valid or invalid based on the validation method. When the toggle is turned off, the value is marked as invalid if it does not meet the condition expressed in the validation method; if the value meets the condition expressed in the validation method, then it is marked as valid. When the toggle is turned on, the value is marked as invalid if it meets the condition expressed in the validation method; if the value does not meet the condition expressed in the validation method, it is marked as valid. For example, if you want to mark all empty fields as invalid, use the Is empty string validation method, select the Required checkbox, and turn on the Inverse toggle. | . ",
    "url": "/integration/additional-operations-on-records/validations#validation-methods",
    
    "relUrl": "/integration/additional-operations-on-records/validations#validation-methods"
  },"1547": {
    "doc": "Validations",
    "title": "Validations",
    "content": " ",
    "url": "/integration/additional-operations-on-records/validations",
    
    "relUrl": "/integration/additional-operations-on-records/validations"
  },"1548": {
    "doc": "Upgrade guide",
    "title": "Upgrade guide",
    "content": "This guide provides instructions for upgrading CluedIn after the initial installation. Regular upgrades are recommended to ensure you benefit from the latest features, improvements, and fixes. For details about the updates available in a specific release, see Release notes. The upgrade process typically involves the steps outlined below. If issues occur during the upgrade, see Resolve common upgrade issues for troubleshooting instructions. Prerequisites for upgrade . Before starting an upgrade, make sure you have the following in place: . | Access to live CluedIn application. | Access to AKS cluster. | Access to the kubeconfig file – this must be provided by your Azure administrator. | A machine or a virtual machine (VM). | All the required tools installed on the machine/VM. | . Stage 1 – Plan your upgrade . | Get familiar with the versioning scheme. See instructions. | Schedule the upgrade window. See instructions. | Inform the stakeholders. See instructions. | Perform a full backup. See instructions. | Review upgrade-related documentation. See instructions. | Prepare and test custom packages. See instructions. | . Stage 2 – Prepare for the upgrade . | Get access to CluedIn application. See instructions. | Connect Helm and kubectl to the CluedIn AKS cluster. See instructions. | Configure kubectl. See instructions. | Configure Helm. See instructions. | (Optional) Connect Lens or Freelens to your cluster. See instructions. | . Stage 3 – Perform the upgrade . | Get current Helm user values. See instructions. | Prepare new Helm user values. See instructions. | Perform system pre-checks. See instructions. | Perform Helm upgrade. See instructions. | Verify the upgrade. See instructions. | Notify about upgrade completion. See instructions. | . ",
    "url": "/paas-operations/upgrade/guide",
    
    "relUrl": "/paas-operations/upgrade/guide"
  },"1549": {
    "doc": "Firewall",
    "title": "On this page",
    "content": ". | AKS and CluedIn resources | Jumpbox &amp; bastion resources | Enricher examples | Power Apps and Power Automate | . Your Azure Firewall should cover the following: . | Default AKS functionality – logs and pods should be able to see Kubernetes API Server (as recommended in Outbound network and FQDN rules for AKS clusters). | CluedIn resource access – resources needed for the CluedIn installation. | CluedIn and custom enrichers - external web endpoints to enrich your data. | . Additionally, if you want to use Power Automate integration for workflows in CluedIn and/or Power Apps integration, you need to add specific URLs to your Azure Firewall as described here . ",
    "url": "/deployment/infra-how-tos/configure-firewall#on-this-page",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall#on-this-page"
  },"1550": {
    "doc": "Firewall",
    "title": "AKS and CluedIn resources",
    "content": "Below are the required endpoints for CluedIn to be functional out of the box. | Rule address | Port | Description | . | cluedinprod.azurecr.io | 443 | CluedIn container registry | . | *.blob.core.windows.net | 443 | storage account endpoint provided by the registry | . | api.nuget.org | 443 | NuGet packages | . | github.com | 443 | GitHub artifacts | . | objects.githubusercontent.com | 443 | GitHub artifacts | . | billing.cluedin.com | 443 | CluedIn licensing server | . | *.grafana.com | 443 | Grafana chart content | . | acme-v02.api.letsencrypt.org | 443 | (Optional) Let’s Encrypt service. Only required if not supplying own certificate | . | quay.io/jetstack | 443 | (Optional) The cert-manager Let’s Encrypt ACME service uses images. Not required if using own certificate | . | *.file.core.windows.net | 445 | The Azure File CSI driver mounts some shares via NFS/SMB using this port | . | pkgs.dev.azure.com | 443 | download the nuget dlls from cluedin public | . | *.hcp.&lt;location&gt;.azmk8s.io | 443 | (For public clusters only) Required for Node &lt;-&gt; API server communication. Replace with the region where your AKS cluster is deployed. Can skip if Private cluster. | . | mcr.microsoft.com | 443 | Microsoft container registry | . | *.data.mcr.microsoft.com | 443 | MCR storage backed by the Azure content delivery network | . | management.azure.com | 443 | Required for Kubernetes operations against the Azure API. | . | login.microsoftonline.com | 443 | Required for Microsoft Entra authentication. | . | packages.microsoft.com | 443 | This address is the Microsoft packages repository used for cached apt-get operations. Example packages include Moby, PowerShell, and Azure CLI. | . | acs-mirror.azureedge.net | 443 | This address is for the repository required to download and install required binaries like kubenet and Azure CNI. | . | packages.aks.azure.com | 443 | This address will be replacing acs-mirror.azureedge.net in the future and will be used to download and install required Kubernetes and Azure CNI binaries. | . ",
    "url": "/deployment/infra-how-tos/configure-firewall#aks-and-cluedin-resources",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall#aks-and-cluedin-resources"
  },"1551": {
    "doc": "Firewall",
    "title": "Jumpbox &amp; bastion resources",
    "content": "Below are the required endpoints for Jumpbox/bastion access . | Rule address | Port | Description | . | cluedin-io.github.io | 443 | Cluedin helm repository | . | community.chocolatey.org | 443 | community.chocolatey.org | . | get.helm.sh | 443 | Helm binary downloads | . | chocolatey.org | 443 | Chocolatey package repository | . | aka.ms | 443 | Microsoft redirection service (Azure CLI) | . | packages.microsoft.com | 443 | Azure CLI and Kubernetes CLI packages | . | dl.k8s.io | 443 | Kubernetes CLI downloads | . | openssl.org | 443 | OpenSSL downloads | . ",
    "url": "/deployment/infra-how-tos/configure-firewall#jumpbox--bastion-resources",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall#jumpbox--bastion-resources"
  },"1552": {
    "doc": "Firewall",
    "title": "Enricher examples",
    "content": "Below are optional additions to the above and are only required if you use enrichers. Below are two of our common enrichers, but each enricher will have its own endpoint configured. If you require assistance with what endpoint is used for each CluedIn enricher, please reach out to CluedIn support who will be happy to assist. | Enricher name | Port | Description | . | CompanyHouse | 443 | Our companies house enricher will call the endpoint https://api.companieshouse.gov.uk to validate UK based businesses | . | GoogleMaps | 443 | The endpoint https://maps.googleapis.com/maps/api is called to query an address for correct address formatting and other metadata | . Because both enrichers call external addresses, this traffic will leave the Kubernetes cluster and will need to be whitelisted if using CluedIn enrichers or developing your own enrichers that require external endpoints. If the rules have not been added, the installation may fail. ",
    "url": "/deployment/infra-how-tos/configure-firewall#enricher-examples",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall#enricher-examples"
  },"1553": {
    "doc": "Firewall",
    "title": "Power Apps and Power Automate",
    "content": "If you want to use Power Automate integration for workflows in CluedIn and/or Power Apps integration, you need to add the following rules to your Azure Firewall: . | https://api.flow.microsoft.com | https://&lt;env-name&gt;.api.crm4.dynamics.com – for example, https://org7bfc52cb.api.crm4.dynamics.com | https://&lt;env-name&gt;.crm4.dynamics.com – for example, https://org7bfc52cb.crm4.dynamics.com | https://graph.microsoft.com | https://api.powerapps.com | https://*.&lt;region&gt;.logic.azure.com – for example, https://prod-251.westeurope.logic.azure.com | . Additionally, you need to add the following domains, which are hosts for incoming requests from Power Automate: . | https://&lt;name&gt;.consent.azure-apihub.net – for example, https://europe002-002.consent.azure-apihub.net | https://*.azure-apihub.net – for example, https://europe002-002.consent.azure-apihub.net and  https://europe002-002.azure-apihub.net | . Instead of using the domains above, you may choose to whitelist them by service tags: . | AzureConnectors | LogicApps | . There are options to further narrow down by region, for example, AzureConnectors.NorthEurope. This would depend on your Power Platform region. For example, if Power Platform is created in the Europe region, it is not clear whether it is North or West Europe, so you would add both regions in Europe: . | AzureConnectors.NorthEurope | AzureConnectors.WestEurope | . ",
    "url": "/deployment/infra-how-tos/configure-firewall#power-apps-and-power-automate",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall#power-apps-and-power-automate"
  },"1554": {
    "doc": "Firewall",
    "title": "Firewall",
    "content": " ",
    "url": "/deployment/infra-how-tos/configure-firewall",
    
    "relUrl": "/deployment/infra-how-tos/configure-firewall"
  },"1555": {
    "doc": "How to connect CluedIn to Slack for alerts",
    "title": "CluedIn → Slack Alerts: End-to-End Setup Guide",
    "content": "This walkthrough shows two supported patterns for getting data quality and master data (MDM) alerts from CluedIn into Slack channels: . | Option A (Fastest): Post alerts to Slack with an Incoming Webhook | Option B (Flexible/Recommended): Use a Slack App (bot) with fine-grained permissions, per-channel routing, threads, and richer formatting | . Both options include examples for Data Quality (DQ) and Master Data (MDM) events, plus testing, routing, and troubleshooting. ",
    "url": "/kb/connect-to-slack#cluedin--slack-alerts-end-to-end-setup-guide",
    
    "relUrl": "/kb/connect-to-slack#cluedin--slack-alerts-end-to-end-setup-guide"
  },"1556": {
    "doc": "How to connect CluedIn to Slack for alerts",
    "title": "Prerequisites",
    "content": ". | CluedIn: Access to Alerts/Workflows (or Webhooks/Integrations), and permissions to configure environment variables/secret store. | Slack: Workspace admin rights (to create/manage apps) and permission to install apps in target channels. | Network/Security: Allow outbound HTTPS from CluedIn to Slack endpoints. | . ",
    "url": "/kb/connect-to-slack#prerequisites",
    
    "relUrl": "/kb/connect-to-slack#prerequisites"
  },"1557": {
    "doc": "How to connect CluedIn to Slack for alerts",
    "title": "Terminology",
    "content": ". | DQ Alert: Triggers when rules/thresholds fail (e.g., completeness &lt; 98%, schema drift, dedupe conflicts, pipeline errors). | MDM Alert: Triggers on golden record lifecycle events (create/update/merge/survivorship changes, source conflict). | Routing: Logic that picks a Slack channel per alert (by domain, severity, or entity type). | Block Kit: Slack’s JSON format for rich messages. | . ",
    "url": "/kb/connect-to-slack#terminology",
    
    "relUrl": "/kb/connect-to-slack#terminology"
  },"1558": {
    "doc": "How to connect CluedIn to Slack for alerts",
    "title": "Option A — Incoming Webhook (Fastest Path)",
    "content": "Great for simple “send this alert to #data-quality” use cases. 1) Create a Slack Incoming Webhook . | Go to https://api.slack.com/apps → Create New App → “From scratch”. | App name: CluedIn Alerts, Workspace: your workspace. | In Incoming Webhooks, Activate webhooks. | Add New Webhook to Workspace, choose a default channel (e.g., #data-quality), then copy the Webhook URL (looks like https://hooks.slack.com/services/T...). | . 2) Store the Webhook URL in CluedIn . | Put the webhook URL in CluedIn Secrets/Key Vault: SLACK_WEBHOOK_DQ (for DQ) and/or SLACK_WEBHOOK_MDM (for MDM). You can reuse one URL or keep per-channel secrets. | . 3) Create/Adjust CluedIn Alert Rules . | DQ Examples: . | Completeness &lt; 98% for Customer | New Schema Drift detected for Product (new column, type change) | Deduplication conflict rate &gt; threshold | Pipeline job failure (ingestion/enrichment/curation) | . | MDM Examples: . | Golden Record created (Customer) | Survivorship rule changed attribute owner/priority | Merge/Split events | Upstream conflict on key attribute (email, legal name, VAT) | . | . 4) Add a “Send to Slack (Webhook)” Workflow Step . | For each alert rule, add an action that POSTs to the webhook URL with a Block Kit payload. | . Minimal payload (JSON): ```json { “text”: “CluedIn Alert”, “blocks”: [ { “type”: “header”, “text”: { “type”: “plain_text”, “text”: “🚨 Data Quality Alert” } }, { “type”: “section”, “fields”: [ { “type”: “mrkdwn”, “text”: “Domain:\\nCustomer” }, { “type”: “mrkdwn”, “text”: “Severity:\\nHigh” } ]}, { “type”: “section”, “text”: { “type”: “mrkdwn”, “text”: “Rule: Completeness &lt; 98%\\nScore: 96.2%\\nRecords impacted: 1,283” } }, { “type”: “context”, “elements”: [ { “type”: “mrkdwn”, “text”: “Run ID: 2025-08-24T03:45Z • Env: prod” } ] } ] } . ",
    "url": "/kb/connect-to-slack#option-a--incoming-webhook-fastest-path",
    
    "relUrl": "/kb/connect-to-slack#option-a--incoming-webhook-fastest-path"
  },"1559": {
    "doc": "How to connect CluedIn to Slack for alerts",
    "title": "How to connect CluedIn to Slack for alerts",
    "content": " ",
    "url": "/kb/connect-to-slack",
    
    "relUrl": "/kb/connect-to-slack"
  },"1560": {
    "doc": "Create a service principal",
    "title": "Create a service principal",
    "content": "This article outlines how to register an application and create a service principal. When you register a new application in Microsoft Entra ID, a service principal is automatically created for the app registration. Following the steps in this article will provide you with the Client ID, Tenant ID, and Client Secret required to configure integration with CluedIn. Prerequisites: Make sure you have access to Microsoft Azure and Microsoft Entra ID. To register and application and create a service principal . | Go to the Azure portal. | Select Microsoft Entra ID. | On the left-hand navigation pane, under Manage, select App registrations. | Select New registration. | Enter the Name of service principal, select Supported account types, and then select Register. After successful registration, you can find Client ID (a) and Tenant ID (b) on the overview page of your service principal (app registration). | To create Client Secret, select the Add a certificate or secret link on the overview page of your service principal (app registration). | Select New client secret. | In the Description field, enter the name of the client secret. In the Expires field, select an expiration for the secret. Finally, select Add. Now, you can find the client secret value under Certificates &amp; secrets. | . ",
    "url": "/consume/export-targets/create-service-principal",
    
    "relUrl": "/consume/export-targets/create-service-principal"
  },"1561": {
    "doc": "Data engineering playbook",
    "title": "Data engineering playbook",
    "content": ". ",
    "url": "/playbooks/data-engineering-playbook",
    
    "relUrl": "/playbooks/data-engineering-playbook"
  },"1562": {
    "doc": "Identifiers",
    "title": "On this page",
    "content": ". | Primary identifier . | Empty value in primary identifier | Auto-generated key in primary identifier | Compound key (MDM code) in primary identifier | Lack of options to define uniqueness | . | Identifiers | FAQ | Useful resources | . An identifier (previously known as code) is a mechanism that CluedIn uses to define the uniqueness of a golden record. During processing, if two clues share the same identifier, they are merged into a single golden record. This ensures that data from different sources is unified under a consistent, unique identifier. Example . Let’s explore the concept of identifiers in CluedIn through an example. We have a golden record—John Smith—that originates from the HR system. One of the identifiers for this golden record is created using the ID. Now, a new record from the CRM system appears in CluedIn, and one of its identifiers matches the code of the golden record from the HR system. As a result, the new CRM record is merged with the existing HR record, integrating any new properties from the CRM record into the existing golden record. To find all the identifiers that uniquely represent a golden record in the system, go to the golden record page, and select View Identifiers. The identifiers are divided into two sections: . | Primary identifier – this is the primary unique identifier of a golden record in CluedIn. | Identifiers – this section contains all identifiers associated with a golden record. | . For more information, see the Identifiers section in our Review mapping article. ",
    "url": "/key-terms-and-features/entity-codes#on-this-page",
    
    "relUrl": "/key-terms-and-features/entity-codes#on-this-page"
  },"1563": {
    "doc": "Identifiers",
    "title": "Primary identifier",
    "content": "A primary identifier (previously knows as entity origin code) is a primary unique identifier of a record in CluedIn. The required details for producing the primary identifier are established when the mapping for a data set is created. To find these details, go to the Map tab of the data set and select Edit mapping. On the Map entity tab, you’ll find the Primary Identifier section, which contains the required details for producing the primary identifier. The primary identifier is made up from the business domain (1), the origin (2), and the value of the property that was selected for producing the primary identifier (3). This combination allows achieving absolute uniqueness across any data source that you interact with. There might be cases when the property for producing the primary identifier is empty or you don’t have any property suitable for defining uniqueness. In the following sections, we’ll explore different ways for defining uniqueness. Empty value in primary identifier . If the property that you selected in your mapping for producing the primary identifier has an empty value, then this empty value will be converted to a hash code that will try to represent uniqueness. Please note that even if the hash code is a good fallback option, you need to consider if it is viable for your source of data. For data sets that have many empty values or are incomplete, it can lead to unwanted merges. Consider the following example, where the hash code will be e7c4d00573302d3b1432fd14d89e5dd0dc68a0ea. { firstName: \"Robert\", lastName: \"Smith\" } . Hash codes are case sensitive. So, with the same properties as in the example above, but all values in lower case, the hash code will be 479b8ebe1612297996532b9abeeb9feee4ed4569. { firstName: \"robert\", lastName: \"smith\" } . Auto-generated key in primary identifier . If your records do not have a property suitable for defining uniqueness, you can select the auto-generated option for producing the primary identifier in mapping. As a result, CluedIn will generate unique primary identifiers for the records using hash codes as documented above. When not to use auto-generated keys? . If you are using hash codes, remember that when the record is changed, the value of the identifier will change as well. It means that you should avoid using auto-generated keys when you edit a data set. In CluedIn, we offer you the possibility to edit the source data. This is a great option as it can lead to much faster and better results in your golden records. However, if you use the auto-generated key to produce the primary identifier, each time you change the value, it will generate a different identifier. For example, let’s say you have some rules to capitalize firstName and lastName. Let’s assume we have 2 records. [{ firstName: \"Robert\", lastName: \"Smith\" }, { firstName: \"robert\", lastName: \"smith\" }] . If we add a rule to capitalize firstName and lastName, the records will be changed as in the example below. [{ firstName: \"Robert\", lastName: \"Smith\" }, { firstName: \"Robert\", lastName: \"Smith\" }] . If you use the auto-generated key to produce the primary identifier, these two records will use the same hash code e7c4d00573302d3b1432fd14d89e5dd0dc68a0ea, so they will merge. However, if you have already processed the data, it can lead to duplication. Let’s trace this process step-by-step. | Upload the following JSON: . [{ firstName: \"Robert\", lastName: \"Smith\" }, { firstName: \"robert\", lastName: \"smith\" }] . | Map the data with Auto-generated key to produce the primary identifier. | Process the data. | Switch to edit mode for the data set. | Apply changes to the data set, such as capitalize firstName and lastName. | Re-process the data. | . As a result, you will have 2 golden records because you have changed the primary identifier of the golden record that had lowercase values. When you process the records for the first time in step 3, you send 2 different codes: . 1. e7c4d00573302d3b1432fd14d89e5dd0dc68a0ea, the hash code with values capitalized. 2. 479b8ebe1612297996532b9abeeb9feee4ed4569, the hash code with values in lowercase. When you process the records for the second time in step 6, you send the same code 2 times: . 3. e7c4d00573302d3b1432fd14d89e5dd0dc68a0ea, the hash code with values capitalized. 4. e7c4d00573302d3b1432fd14d89e5dd0dc68a0ea, identical hash code for the record that initially was in lowercase but has been capitalized in edit mode. The records with codes 1, 3, and 4 will merge together. And the record with code 2 will remain as a separate golden record. If you want to edit your records in the source, do not use auto-generated key for producing the primary identifier. Compound key (MDM code) in primary identifier . If you do not want to use an auto-generated key to produce the primary identifier, you can use a compound key. A compound key is built by concatenating different attributes to ensure uniqueness. It is commonly referred to as the MMDM code. For example, an MDM code can combine multiple attributes for a customer. - firstName - lastName - line 1 - city - country - date of birth . If you go with the MDM code, make sure you normalize the values by either creating a computed column for your data set or by adding a bit of glue code in using advanced mapping. Our CluedIn experts can assist you with this task. Normalizing the MDM code is important because it will prevent scenarios where editing values changes the primary identifier, leading to undesired effects. Lack of options to define uniqueness . What to do if there is no way to define uniqueness? . If you have no way to define uniqueness for your records, generate a GUID using advanced mapping or a pre-process rule. However, each time you process the records, duplicates will be created. So, use this option for one-time only ingestion. What to if there is no way to define uniqueness and you need to re-process data multiple times? . If this is your case, the only way is to fix the issue on the source level. You can modify the source of data to set up some kind of uniqueness. For example, if you have a SQL table, you can add a unique identifier for each row. ",
    "url": "/key-terms-and-features/entity-codes#primary-identifier",
    
    "relUrl": "/key-terms-and-features/entity-codes#primary-identifier"
  },"1564": {
    "doc": "Identifiers",
    "title": "Identifiers",
    "content": "In addition to primary identifier, identifiers (previously known as entity codes) can uniquely represents a record in CluedIn. The required details for producing the identifiers are established when the mapping for a data set is created. To find these details, go to the Map tab of the data set and select Edit mapping. On the Map entity tab, you’ll find the Identifiers section, which contains the required details for producing the identifiers. If a data set contains additional columns that can serve as unique identifiers besides the ones used for producing the primary identifier, then these columns can also be used to produce additional identifiers. For example, if the primary identifier is produced using the ID, then the additional identifier could be produced using the email. The identifiers are made up from the business domain, origin, and the value from the column that was selected for producing the identifiers. In the Identifiers section, you can instruct CluedIn to produce additional codes: . | Provider name identifiers – identifiers that are built form the business domain, provider name (for example, File Data Source), and the value from the column that was selected for producing the primary identifier. | Strict edge identifiers – identifiers that are built from the business domain, data source group ID/data source ID/data set ID, and the value from the column that was selected for producing the primary identifier. | . What happens if the value of am identifier is empty? . The value will be ignored and no identifier will be added. An identifier is not a required element and using a hash code would be unnecessary as you have already defined uniqueness with the primary identifier. Is it bad if I have no identifiers defined? . No, it can happen regularly, generally when the source records cannot be trusted or are unknown. When in doubt, it is better not to add extra identifier and rely on deduplication projects to find duplicates. ",
    "url": "/key-terms-and-features/entity-codes",
    
    "relUrl": "/key-terms-and-features/entity-codes"
  },"1565": {
    "doc": "Identifiers",
    "title": "FAQ",
    "content": "How to make sure that the identifiers will blend across different data sources? . Since an identifier will only merge with another identifier if they are identical, how can you merge records across different systems if the origin is different? One of the ways to achieve it is through the GUID. If a record has an identifier that is a GUID/UUID, you can set the origin as CluedIn because no matter the system, the identifier should be unique. However, this is not applicable if you are using deterministic GUIDS. If you’re wondering whether you use deterministic GUIDs, conducting preliminary analysis on the data can help. Check if many GUIDs overlap in a certain sequence, such as the first chunk of the GUID being replicated many times. This is a strong indicator that you are using deterministic GUIDs. Random GUIDs are so unique that the chance of them being the same is close to impossible. You could even determine that the business domain can be generic as well. You will have to craft these special identifiers in your clues (for example, something like /Generic#CluedIn:&lt;GUID&gt;). You will need to make sure your edges support the same mechanism. In doing this, you are instructing CluedIn that no matter the business domain, no matter the origin of the data, this record can be uniquely identified by just the GUID. What if a record doesn’t have a unique reference to construct an identifier? . Often you will find that you need to merge or link records across systems that don’t have IDs but rather require fuzzy merging to be able to link records. In this case, we often suggest creating a composite identifier constructed from a combination of column or property values that guarantee uniqueness. For example, if you have a Transaction record, you might find that a combination of the Transaction Date, Product, Location, and Store will guarantee uniqueness. It is best to calculate a “Hash” of these values combined, which means that we can calculate an identifier from this. What if a key is not ready for producing an identifier? . Sometimes keys for identifiers are not ready to be made into a unique primary identifier. For example, your data might include default or fallback values when a real value is not present. Imagine you have an EmployeeId column, and when a value is missing, placeholders like “NONE”, “”, or “N/A” are used. These are not valid identifiers for the EmployeeId. However, the important aspect is that you cannot handle all permutations of these placeholders upfront. Therefore, you should create identifiers with the intention that these values are unique. You can fix and clean up such values later. ",
    "url": "/key-terms-and-features/entity-codes#faq",
    
    "relUrl": "/key-terms-and-features/entity-codes#faq"
  },"1566": {
    "doc": "Identifiers",
    "title": "Useful resources",
    "content": ". | Origin | . ",
    "url": "/key-terms-and-features/entity-codes#useful-resources",
    
    "relUrl": "/key-terms-and-features/entity-codes#useful-resources"
  },"1567": {
    "doc": "Install Integrations",
    "title": "Install Integrations",
    "content": "Installing integrations in Kubernetes . Via the helm chart . In a production environment, using Kubernetes, you can add the components you want to install through the values.yml file. You can specify your own packages and the versions to be installed. You can also provide your own package feeds, authentication, and even an alternative installer image. cluedin: components: image: '' # name of the container to use as an installer - will default to 'cluedin # nuget-installer' packages: [] # list of extra Nuget Packages to install in server in name, or name/version pairs # version should be a supported nuget version format. sources: {} # Nuget sources to use . At pod startup time, the packages will be passed from an init container to the CluedIn container. Configuring Packages . Packages should be listed using their full package id and an optional version. When supplying the version, you may use floating versions to allow the version to be resolved at startup time. cluedin: components: packages: - name: CluedIn.Crawling.HubSpot - name: CluedIn.Provider.HubSpot version: 3.0.0-* . In this example the latest version of CluedIn.Crawling.HubSpot will be installed, while the latest 3.0.0 pre-release, or full-release version of CluedIn.Provider.HubSpot will be installed. Configuring Sources . The packages to install may be resolved from one or more nuget feeds. Each feed can be configured under sources with optional authentication details . cluedin: components: sources: nuget: url: https://api.nuget.org/v3/index.json custom: url: https://myorg.myget.org/F/customfeed/api/v3/index.json user: OrgUser pass: OrgPass . Via Custom Image . You can build your own docker image to contain the integration packages. This has the advantage that the packages and assets are already contained in the init container and do not need to be downloaded. | Create a packages.txt file with the integrations to be installed. Versions can be specified after the package name CluedIn.Crawling.HubSpot CluedIn.Provider.HubSpot 3.0.0-* . | Create a nuget.config with the feeds to be used. This is a standard nuget.config. &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;configuration&gt; &lt;packageSources&gt; &lt;add key=\"custom\" value=\"https://myorg.myget.org/F/customfeed/api/v3/index.json\" /&gt; &lt;/packageSources&gt; &lt;/configuration&gt; . The public nuget.org feed is already included by default . | Use the following Dockerfile to build your image FROM cluedin/nuget-installer as base # If your feed requires passing credentials ARG CUSTOMCRED ENV NUGETCRED_CUSTOM=$CUSTOMCRED COPY ./packages RUN ./Install-Packages.ps1 FROM alpine as final COPY --from=base ./components ./tmp/components ENTRYPOINT [\"sh\", \"-c\", \"false | cp $(ls -d /tmp/components/*/) -r ./components 2&gt;/dev/null\"] . If one or more feeds requires credentials, you’ll need to expose an ARG and an ENV named NUGETCRED_&lt;feedname&gt; to allow passing to the install script. | Validate your custom image # Build the image &gt; docker build -t my/cluedin-installer . # OR build the image with creds - pass aregument as pipe delimited user|password &gt; docker build -t my/cluedin-installer --build-arg CUSTOMCRED='user|pass' . # Mount components folder which is where the integration components would be placed &gt; docker run --rm -it -v '.:/components' my/cluedin-installer . | In the values.yal configure your custome image for the components install cluedin: components: image: 'my/cluedin-installer' . | . ",
    "url": "/integration/install-integrations",
    
    "relUrl": "/integration/install-integrations"
  },"1568": {
    "doc": "Removal of records",
    "title": "Removal of records",
    "content": "In this article, you will learn how to remove records (also referred to as data parts) that were created from a specific data source. For more information about records, data parts, and golden records, see Data life cycle. If you no longer need records from a specific data source, you can easily remove them. If a record is not merged during processing, it becomes a golden record. Removing such a record also removes the golden record. However, if a record is merged into the existing golden record, removing it doesn’t remove the entire golden record; only the specific data part represented by that record is removed. Keep in mind that when you remove records from CluedIn, the count of billable records will decrease when it is recalculated. After removing records, you’ll notice that the Data tab disappears from the data set. Additionally, the Process tab won’t contain any information about previous processing. However, the incoming records on the Preview tab remain intact; and the mapping configuration on the Map tab is preserved, so you can edit, reset, or reuse it for other data sets. Removing records can be useful in the following cases: . | If you notice an error in the mapping configuration. Although the mapping configuration is preserved after removing records, you can reset the mapping and start again. | If you processed a subset of data to test the mapping configuration and you don’t need the test data anymore. Since the mapping configuration is preserved, you can reuse it for other data sets. | . What happens if you remove records from one of the data sources that form a golden record? . Consider a scenario where a golden record is composed of data parts from three sources. If you remove records from one of these sources, the corresponding data part will be removed from the golden record. This means that all properties originating from the data part are removed from the golden record, and the data part itself is deleted from the history of the golden record. Since the records from the other sources remain intact, the golden record still exists. However, if you remove records from all sources that form a golden record, then the golden record will be deleted. What is the difference between removing records and deleting golden records using the GraphQL tool? . Removing records from a specific data source is designed to remove the corresponding data parts from all relevant golden records. If these golden records contain other data parts, they will continue to exist. On the other hand, deleting a golden record using the GraphQL tool will result in the complete deletion of the entire golden record. To remove records . | On the navigation pane, go to Integrations &gt; Data Sources. | Find and open the data source that contains records that you want to remove. | Near the upper-right corner of the data source details page, select Remove records. The Remove processed records dialog opens where you can view the number of records that will be removed as well as the data set containing those records. | Confirm that you want to remove records by entering REMOVE. Then, select Remove records. | . The process of removing records might take some time. During or after the process of removing records, you can make changes in the data set (edit mapping, add or remove property or pre-process rules) or process the data set again. If you are working with an ingestion endpoint data source, you can also remove records that were produced from a specific request send to that ingestion endpoint. For more details, see Ingestion reports. ",
    "url": "/integration/additional-operations-on-records/remove-records",
    
    "relUrl": "/integration/additional-operations-on-records/remove-records"
  },"1569": {
    "doc": "Sync Purview glossaries to CluedIn glossaries",
    "title": "On this page",
    "content": ". | Preparation . | Preparation in Purview | Preparation in CluedIn | . | Feature demonstration | . In this article, you will learn how to sync Purview glossaries to CluedIn glossaries. ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#on-this-page"
  },"1570": {
    "doc": "Sync Purview glossaries to CluedIn glossaries",
    "title": "Preparation",
    "content": "This section contains the steps required to prepare for syncing glossaries from Purview to CluedIn glossary categories and terms. Preparation in Purview . Make sure you have the existing classic type glossaries that contain some terms. In the following screenshot, we have 2 glossaries: CluedIn and Data Synchronization. Preparation in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Purview glossaries to CluedIn glossaries. | Select Save. | . ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#preparation",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#preparation"
  },"1571": {
    "doc": "Sync Purview glossaries to CluedIn glossaries",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of Purview glossaries to CluedIn glossaries, you will receive a notification when the glossary term is created in CluedIn. To find terms in CluedIn . | On the navigation pane, go to Management &gt; Glossary. On the terms page, you can find the categories (a) and terms (b). The category in CluedIn corresponds to the glossary in Purview. | . ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries#feature-demonstration"
  },"1572": {
    "doc": "Sync Purview glossaries to CluedIn glossaries",
    "title": "Sync Purview glossaries to CluedIn glossaries",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-glossaries"
  },"1573": {
    "doc": "User interface",
    "title": "How to Generate an API Token in CluedIn",
    "content": "API Tokens allow external systems and applications to authenticate and interact with CluedIn programmatically. You can create and manage API tokens directly from the Administration section of the CluedIn Portal. ",
    "url": "/administration/api-token#how-to-generate-an-api-token-in-cluedin",
    
    "relUrl": "/administration/api-token#how-to-generate-an-api-token-in-cluedin"
  },"1574": {
    "doc": "User interface",
    "title": "Prerequisites",
    "content": ". | You must be logged into the CluedIn Portal. | You need appropriate administrative permissions (only Administrators can generate or revoke API tokens). | . ",
    "url": "/administration/api-token#prerequisites",
    
    "relUrl": "/administration/api-token#prerequisites"
  },"1575": {
    "doc": "User interface",
    "title": "Steps to Generate an API Token",
    "content": ". | Log in to the CluedIn Portal Open your CluedIn environment in your browser and log in with an account that has administrative privileges. | Navigate to Administration . | From the left-hand navigation menu, click on Administration. | Select API Tokens from the options. | . | Create a New Token . | Click the New Token or + Generate Token button. | Enter a descriptive name for the token (e.g., Integration_Snowflake_Export or MarketingAutomationSync). | (Optional) Specify a description to clarify its purpose. | . | Set Token Permissions/Scope . | Select the scope or permissions you want the token to have. | Choose the minimum necessary permissions to follow security best practices (e.g., read-only vs full write). | . | Set Expiry (Optional) . | If supported in your version, define an expiry date/time for the token. | Tokens without an expiry remain valid until explicitly revoked. | . | Generate the Token . | Click Generate. | CluedIn will display the newly created token. | . | Copy and Store the Token Securely . | Copy the token value immediately and store it in a secure location (e.g., a password vault). | Important: You will not be able to see the token value again once you leave the page. | . | . ",
    "url": "/administration/api-token#steps-to-generate-an-api-token",
    
    "relUrl": "/administration/api-token#steps-to-generate-an-api-token"
  },"1576": {
    "doc": "User interface",
    "title": "Managing API Tokens",
    "content": ". | View Active Tokens: The Administration → API Tokens page lists all currently active tokens. | Revoke a Token: To disable access, select the token and choose Revoke/Delete. Revocation is immediate. | Audit Usage: Some environments log usage details (last used timestamp, created by). Review periodically to keep tokens up to date. | . ",
    "url": "/administration/api-token#managing-api-tokens",
    
    "relUrl": "/administration/api-token#managing-api-tokens"
  },"1577": {
    "doc": "User interface",
    "title": "Best Practices",
    "content": ". | Generate tokens per integration (not shared across multiple apps). | Store tokens securely and never embed them in plain text or version control. | Use least privilege when assigning scopes. | Revoke tokens when they are no longer needed. | Rotate long-lived tokens regularly as part of your security policy. | . ",
    "url": "/administration/api-token#best-practices",
    
    "relUrl": "/administration/api-token#best-practices"
  },"1578": {
    "doc": "User interface",
    "title": "Summary",
    "content": "You can generate an API token in CluedIn by navigating to Administration → API Tokens, creating a new token with a descriptive name, setting permissions, and securely storing the generated value. API tokens allow safe, authenticated access to CluedIn’s APIs for integrations and automation. ",
    "url": "/administration/api-token#summary",
    
    "relUrl": "/administration/api-token#summary"
  },"1579": {
    "doc": "User interface",
    "title": "User interface",
    "content": " ",
    "url": "/administration/api-token",
    
    "relUrl": "/administration/api-token"
  },"1580": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Connecting Fivetran to CluedIn: A Complete Guide",
    "content": "This guide explains how to integrate Fivetran with CluedIn so that the data pipelines and transformations managed in Fivetran flow directly into CluedIn’s data quality and master data management (MDM) framework. ",
    "url": "/kb/connect-to-fivetran#connecting-fivetran-to-cluedin-a-complete-guide",
    
    "relUrl": "/kb/connect-to-fivetran#connecting-fivetran-to-cluedin-a-complete-guide"
  },"1581": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Why Connect Fivetran to CluedIn?",
    "content": ". | Seamless Ingestion: Automatically land data from SaaS, databases, and applications into CluedIn via your warehouse. | Data Quality Monitoring: Allow CluedIn to validate and score the data streamed by Fivetran. | Master Data Governance: Feed harmonized data into CluedIn golden records for entity resolution and survivorship rules. | Operational Efficiency: Use CluedIn’s alerts (Slack/Teams/Email) to monitor Fivetran pipelines and resulting data quality. | . ",
    "url": "/kb/connect-to-fivetran#why-connect-fivetran-to-cluedin",
    
    "relUrl": "/kb/connect-to-fivetran#why-connect-fivetran-to-cluedin"
  },"1582": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Integration Architecture",
    "content": ". | Fivetran extracts and loads data into a supported warehouse. | CluedIn connects to the same warehouse (via JDBC/ODBC) or consumes Fivetran outputs. | CluedIn applies enrichment, data quality rules, and master data processing. | . ",
    "url": "/kb/connect-to-fivetran#integration-architecture",
    
    "relUrl": "/kb/connect-to-fivetran#integration-architecture"
  },"1583": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Prerequisites",
    "content": ". | Fivetran: Active account with connectors configured to land data in your warehouse. | CluedIn: Access to ingestion pipelines and entity mappings. | Warehouse: Supported target (Snowflake, BigQuery, Redshift, Azure Synapse, SQL Server, etc.). | Credentials: Database/service accounts with read access for CluedIn. | . ",
    "url": "/kb/connect-to-fivetran#prerequisites",
    
    "relUrl": "/kb/connect-to-fivetran#prerequisites"
  },"1584": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 1 — Set Up Fivetran Connector",
    "content": ". | Log in to Fivetran dashboard. | Choose a source connector (e.g., Salesforce, HubSpot, MySQL). | Configure destination schema (e.g., fivetran_salesforce, fivetran_hubspot). | Verify sync is active and data is landing in the warehouse. | . ",
    "url": "/kb/connect-to-fivetran#step-1--set-up-fivetran-connector",
    
    "relUrl": "/kb/connect-to-fivetran#step-1--set-up-fivetran-connector"
  },"1585": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 2 — Prepare Data for CluedIn",
    "content": ". | Align schema names and tables with business domains (e.g., customer_raw, product_raw). | Optionally use Fivetran Transformations (dbt integrated) to clean data before CluedIn ingestion. | Ensure consistent primary keys (id, email, sku) for CluedIn entity resolution. | . ",
    "url": "/kb/connect-to-fivetran#step-2--prepare-data-for-cluedin",
    
    "relUrl": "/kb/connect-to-fivetran#step-2--prepare-data-for-cluedin"
  },"1586": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 3 — Connect CluedIn to the Warehouse",
    "content": ". | In CluedIn, go to Data Sources → Add Connector. | Select your warehouse type (Snowflake, BigQuery, SQL Server, etc.). | Enter JDBC/ODBC connection details: . | Host, Port, Database | User &amp; Password / Key | Schema (e.g., fivetran_salesforce) | . | Test the connection and save. | . ",
    "url": "/kb/connect-to-fivetran#step-3--connect-cluedin-to-the-warehouse",
    
    "relUrl": "/kb/connect-to-fivetran#step-3--connect-cluedin-to-the-warehouse"
  },"1587": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 4 — Map Fivetran Tables to CluedIn Entities",
    "content": ". | Example mapping: . | fivetran_salesforce.accounts → Customer entity | fivetran_hubspot.contacts → Lead/Customer entity | fivetran_erp.products → Product entity | . | Define attributes (columns) and link them to CluedIn’s schema. | Enable deduplication and survivorship rules. | . ",
    "url": "/kb/connect-to-fivetran#step-4--map-fivetran-tables-to-cluedin-entities",
    
    "relUrl": "/kb/connect-to-fivetran#step-4--map-fivetran-tables-to-cluedin-entities"
  },"1588": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 5 — Automate Data Quality Monitoring",
    "content": ". | Create CluedIn Data Quality rules: . | Completeness (e.g., Customer Email required). | Validity (e.g., VAT number format). | Consistency (e.g., SKU in ERP matches SKU in CRM). | . | Apply rules to ingested Fivetran tables. | Enable alerts (Slack, Teams, Email) to monitor DQ scores in real time. | . ",
    "url": "/kb/connect-to-fivetran#step-5--automate-data-quality-monitoring",
    
    "relUrl": "/kb/connect-to-fivetran#step-5--automate-data-quality-monitoring"
  },"1589": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 6 — Enable Master Data Workflows",
    "content": ". | As Fivetran loads new/updated data, CluedIn: . | Merges records into golden entities. | Resolves duplicates across systems. | Applies survivorship rules (e.g., “email from CRM overrides ERP”). | . | Configure downstream syncs to push golden records back to target systems if needed. | . ",
    "url": "/kb/connect-to-fivetran#step-6--enable-master-data-workflows",
    
    "relUrl": "/kb/connect-to-fivetran#step-6--enable-master-data-workflows"
  },"1590": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Step 7 — Automate via Scheduling",
    "content": ". | Fivetran handles incremental syncs automatically. | CluedIn ingestion jobs can be scheduled to run after each Fivetran sync. | Optionally integrate via webhook/trigger: when Fivetran finishes sync, it triggers CluedIn ingestion. | . ",
    "url": "/kb/connect-to-fivetran#step-7--automate-via-scheduling",
    
    "relUrl": "/kb/connect-to-fivetran#step-7--automate-via-scheduling"
  },"1591": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Best Practices",
    "content": ". | Schema Naming: Keep Fivetran schemas clear (fivetran_&lt;source&gt;). | Entity Alignment: Decide early how sources map to CluedIn entities (Customer, Product, Supplier). | Transform Before or After? . | Use dbt/Fivetran transformations for light cleaning. | Use CluedIn for entity resolution, survivorship, and golden records. | . | Monitoring: Configure Slack alerts for pipeline failures in Fivetran and DQ failures in CluedIn. | Security: Use vaults/secret stores for credentials, never hardcode them. | . ",
    "url": "/kb/connect-to-fivetran#best-practices",
    
    "relUrl": "/kb/connect-to-fivetran#best-practices"
  },"1592": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Troubleshooting",
    "content": ". | No data in CluedIn: Check schema permissions and ensure warehouse sync is complete. | Duplicate records: Verify unique keys are consistent across Fivetran sources. | DQ rules failing unexpectedly: Inspect dbt/Fivetran transformations vs. CluedIn expectations. | Slow ingestion: Partition large tables or enable incremental loading. | . ",
    "url": "/kb/connect-to-fivetran#troubleshooting",
    
    "relUrl": "/kb/connect-to-fivetran#troubleshooting"
  },"1593": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Summary",
    "content": "By connecting Fivetran to CluedIn, you can: . | Ingest data from hundreds of SaaS apps and databases. | Monitor and improve data quality across sources. | Build and maintain golden records through entity resolution and MDM. | Keep teams informed with real-time alerts when data quality or pipeline issues arise. | . This integration turns your Fivetran pipelines into a governed, quality-assured data supply chain managed inside CluedIn. ",
    "url": "/kb/connect-to-fivetran#summary",
    
    "relUrl": "/kb/connect-to-fivetran#summary"
  },"1594": {
    "doc": "Connect FiveTran to CluedIn",
    "title": "Connect FiveTran to CluedIn",
    "content": " ",
    "url": "/kb/connect-to-fivetran",
    
    "relUrl": "/kb/connect-to-fivetran"
  },"1595": {
    "doc": "Create security role for Dataverse connector",
    "title": "On this page",
    "content": ". | Create security role | Reference table | . This article outlines how to create a security role in Power Apps in order to use it for Dataverse connector. Prerequisites: Make sure you have an existing Power Apps account. ",
    "url": "/consume/export-targets/create-security-role#on-this-page",
    
    "relUrl": "/consume/export-targets/create-security-role#on-this-page"
  },"1596": {
    "doc": "Create security role for Dataverse connector",
    "title": "Create security role",
    "content": ". | Sign in to your Power Apps account and make sure you are in the intended environment. | Select Power Platform &gt; Power Platform Admin Center. | Select Environments, and then select your intended environment. | At the top of the page, select Settings. | Expand the Users + permissions dropdown, and then select Security roles. | At the top of the page, select New role. Enter the Role Name and select the Business unit of your organization. If needed, change the Member’s privilege inheritance according to your preference. Finally, select Create. | In the list of all security roles, find and select the role that you’ve just created. | Edit the security role’s privileges according to the reference table. To open the edit mode, select the three-dot button next to the table that you want to edit. | Once you’ve updated the security role’s privileges according to the reference table, select Save. | . ",
    "url": "/consume/export-targets/create-security-role#create-security-role",
    
    "relUrl": "/consume/export-targets/create-security-role#create-security-role"
  },"1597": {
    "doc": "Create security role for Dataverse connector",
    "title": "Reference table",
    "content": "| Table | Create | Read | Write | Delete | . | Customization |   |   |   |   | . | Solution | Organization | Organization | Organization | Organization | . | Publisher | Organization | Organization | Organization | Organization | . | Entity | Organization | Organization | Organization | Organization | . | Entity Key | Organization | Organization |   | Organization | . | Attribute | Organization | Organization | Organization | Organization | . | System Form | Organization | Organization | Organization | Organization | . | View | Organization | Organization | Organization | Organization | . | Custom Control Default Config | Organization |   | Organization | Organization | . | Process | Organization | Organization | Organization | Organization | . | Custom Tables |   |   |   |   | . | Connection Reference | Organization | Organization | Organization | Organization | . | Connector | Organization | Organization | Organization | Organization | . | Dataflow | Organization | Organization | Organization | Organization | . ",
    "url": "/consume/export-targets/create-security-role#reference-table",
    
    "relUrl": "/consume/export-targets/create-security-role#reference-table"
  },"1598": {
    "doc": "Create security role for Dataverse connector",
    "title": "Create security role for Dataverse connector",
    "content": " ",
    "url": "/consume/export-targets/create-security-role",
    
    "relUrl": "/consume/export-targets/create-security-role"
  },"1599": {
    "doc": "Google Maps",
    "title": "On this page",
    "content": ". | Add Google Maps enricher | Properties from Google Maps enricher | . This article outlines how to configure the Google Maps enricher. The purpose of this enricher is to provide address-related information. More details can be found in Properties from Google Maps enricher. The Google Maps enricher supports the following endpoints: . | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationName}{organizationAddress}{organizationCity}{organizationZip}{organizationState}{organizationCountry} – if organization name, address, city, zip, state, and country are provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationName} – if organization name is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={organizationAddress} – if organization address is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={location} – if one of the addresses other than organization address is provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query={location}&amp;location={latitude}{longitude} – if one of the addresses other than organization address and latitude and longitude are provided. | https://maps.googleapis.com/maps/api/place/textsearch/json?query=location={latitude}{longitude} – if latitude and longitude are provided. | https://maps.googleapis.com/maps/api/place/details/json?placeid={id} – the ID is taken from the previous text search API (the result of https://maps.googleapis.com/maps/api/place/textsearch/json). | . ",
    "url": "/preparation/enricher/google-maps#on-this-page",
    
    "relUrl": "/preparation/enricher/google-maps#on-this-page"
  },"1600": {
    "doc": "Google Maps",
    "title": "Add Google Maps enricher",
    "content": "To use the Google Maps enricher, you must provide the API key. To get the API key, follow the instructions here. To enrich golden records using the Google Maps enricher, ensure they have a specific property (for example, Enrich) set to True. Golden records without this property or with a value other than True will be skipped during enrichment. The Google Maps enricher can use a variety of attributes for searching the Google Maps Platform: . | For Organization: . | Organization Name . | Organization Address . | A combination of Organization Name, Address, Zip, State, City, Country . | A combination of Organization Name and Address. | A combination of Organization Name, Address, Latitude, and Longitude. | . | For Location: . | Location Address . | A combination of Location Address, Latitude, and Longitude. | . | For Person: . | Person Address . | A combination of Person Address and Person Address City . | A combination of Person Address, Latitude, and Longitude. | . | For User: . | User Address . | A combination of User Address, Latitude, and Longitude. | . | . When you’re configuring the Google Maps enricher for a specific business domain, make sure you fill in the relevant fields for that business domain. To add the Google Maps enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Google Maps, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API key for retrieving information from the Google Maps Platform. | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Vocabulary Key used to control whether it should be enriched – enter the vocabulary key that indicates if the golden record should be enriched. If the value is true, then the golden record will be enriched. Otherwise, the golden record will not be enriched. | Organization Name Vocabulary Key – enter the vocabulary key that contains company names that will be used for searching the Google Maps Platform. | Organization Address Vocabulary Key – enter the vocabulary key that contains company addresses that will be used for searching the Google Maps Platform. | Organization City Vocabulary Key – enter the vocabulary key that contains cities that will be used for searching the Google Maps Platform. | Organization Zip Vocabulary Key – enter the vocabulary key that contains company ZIP Codes that will be used for searching the Google Maps Platform. | Organization State Vocabulary Key – enter the vocabulary key that contains states that will be used for searching the Google Maps Platform. | Organization Country Vocabulary Key – enter the vocabulary key that contains countries that will be used for searching the Google Maps Platform. | Location Address Vocabulary Key – enter the vocabulary key that contains location addresses that will be used for searching the Google Maps Platform. | User Address Vocabulary Key – enter the vocabulary key that contains user addresses that will be used for searching the Google Maps Platform. | Person Address Vocabulary Key – enter the vocabulary key that contains person addresses that will be used for searching the Google Maps Platform. | Person Address City Vocabulary Key – enter the vocabulary key that contains person cities that will be used for searching the Google Maps Platform. | Latitude Vocabulary Key – enter the vocabulary key that contains the latitude of golden records that you want to enrich. This latitude will be used for searching the Google Maps Platform only if the corresponding address vocabulary key is provided in the configuration (Organization Address Vocab Key, Location Address Vocab Key, User Address Vocab Key, or Person Address Vocab Key). | Longitude Vocabulary Key – enter the vocabulary key that contains the longitude of golden records that you want to enrich. This longitude will be used for searching the Google Maps Platform only if the corresponding address vocabulary key is provided in the configuration (Organization Address Vocab Key, Location Address Vocab Key, User Address Vocab Key, or Person Address Vocab Key). | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Google Maps enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Google Maps enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/google-maps#add-google-maps-enricher",
    
    "relUrl": "/preparation/enricher/google-maps#add-google-maps-enricher"
  },"1601": {
    "doc": "Google Maps",
    "title": "Properties from Google Maps enricher",
    "content": "You can find the properties added to golden records from the Google Maps enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Google Maps enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Google Maps enricher. | Display name | Vocabulary key | . | Address Components | googleMaps.Organization.AddressComponents | . | Administrative Area Level 1 | googleMaps.Organization.AdministrativeAreaLevel1 | . | Administrative Area Level 2 | googleMaps.Organization.AdministrativeAreaLevel2 | . | Adr Address | googleMaps.Organization.AdrAddress | . | Business Status | googleMaps.Organization.BusinessStatus | . | City Name | googleMaps.Organization.CityName | . | Country Code | googleMaps.Organization.CountryCode | . | Formatted Address | googleMaps.Organization.FormattedAddress | . | Formatted PhoneNumber | googleMaps.Organization.FormattedPhoneNumber | . | Icon | googleMaps.Organization.Icon | . | Id | googleMaps.Organization.Id | . | International Phone Number | googleMaps.Organization.InternationalPhoneNumber | . | Latitude | googleMaps.Organization.Latitude | . | Longitude | googleMaps.Organization.Longitude | . | Name | googleMaps.Organization.Name | . | Neighborhood | googleMaps.Organization.Neighborhood | . | Opening Hours | googleMaps.Organization.OpeningHours | . | Place Id | googleMaps.Organization.PlaceId | . | Plus Code | googleMaps.Organization.PlusCode | . | Postal Code | googleMaps.Organization.PostalCode | . | Rating | googleMaps.Organization.Rating | . | Reference | googleMaps.Organization.Reference | . | Reviews | googleMaps.Organization.Reviews | . | Scope | googleMaps.Organization.Scope | . | Street Number | googleMaps.Organization.StreetNumber | . | Sub Premise | googleMaps.Organization.SubPremise | . | Types | googleMaps.Organization.Types | . | Url | googleMaps.Organization.Url | . | User Ratings Total | googleMaps.Organization.UserRatingsTotal | . | Utc Offset | googleMaps.Organization.UtcOffset | . | Vicinity | googleMaps.Organization.Vicinity | . | Website | googleMaps.Organization.Website | . | AdministrativeArea | googleMaps.Location.AdministrativeArea | . | Code Country | googleMaps.Location.CodeCountry | . | Code Postal | googleMaps.Location.CodePostal | . | Components Address | googleMaps.Location.componentsAddress | . | Formatted Address | googleMaps.Location.formattedAddress | . | Geometry | googleMaps.Location.Geometry | . | Latitude | googleMaps.Location.Latitude | . | Longitude | googleMaps.Location.Longitude | . | Name | googleMaps.Location.Name | . | Name City | googleMaps.Location.NameCity | . | Name Street | googleMaps.Location.NameStreet | . | Number Street | googleMaps.Location.NumberStreet | . ",
    "url": "/preparation/enricher/google-maps#properties-from-google-maps-enricher",
    
    "relUrl": "/preparation/enricher/google-maps#properties-from-google-maps-enricher"
  },"1602": {
    "doc": "Google Maps",
    "title": "Google Maps",
    "content": " ",
    "url": "/preparation/enricher/google-maps",
    
    "relUrl": "/preparation/enricher/google-maps"
  },"1603": {
    "doc": "Integration and the CluedIn UI",
    "title": "Integration and the CluedIn UI",
    "content": "Introduction . To be able to let someone adding and managing an Integration from the user interface, you need to add some information in your provider. If you don’t, the provider will be valid but you will deteriorate the User experience of the product. IExtendedProviderMetadata interface . To provide the information to the CluedIn.UI, you will need to implement the IExtendedProviderMetadata interface. Please read the sections below to understand what each property of that interface is used for. A concrete example: . | HelloWorldProvider.cs | . The code: . public interface IExtendedProviderMetadata { string Icon { get; } // see notes below string Domain { get; } // The Url to your application string About { get; } // A sentence describing the purpose of your application string AuthMethods { get; } // A serialised JSON array of Authentication Methods supports (TODO Obtain list of ) string Properties { get; } // TODO find out how this is used by UI string ServiceType { get; } // IE CRMType etc (TODO get full list) string Aliases { get; } // TODO find out how used in the UI Guide Guide { get; set; } // Instructions on how to configure your Provider in the UI string Details { get; set; } // Details of how this provider will interact with your application string Category { get; set; } // The category your provider fall under to allow filtering in the UI string Type { get; set; } // Available options [cloud, on-premise] } . Icon . The Icon is used by the CluedIn.UI to quickly show to all users what the provider is. The ‘ideal’ size of the image is 128x128px. | For your icon to be found, you must add it as an Embedded Resource via the Build Action property in your Provider project. See Build actions. The convention we are using is to place the icon image file under a Resources folder. The Icon property above must point to this file using ‘.’ notation rather than ‘'. For example: | . - Provider.HellowWorld.csproj \\Resources \\cluedin.png . would be represented as Resources.cluedin.png . Here is an example of how the Provider Icon is used in the application: . Domain . This is the URL of your application. Leave it empty if the integration does not have a website. The value is used by the CluedIn.UI to redirect to the integration’s website if he needs more information. Example: . | if you build a Slack integration, you would have: https://slack.com/ assigned to the Domain property. | . About . About is the description of Integration. Example: . | For a Zendesk integration, you would write: Zendesk makes better experiences for agents, admins, and customers. As employees, we encourage each other to grow and innovate. | . AuthMethods . The authentication methods is a JSON object used to explain to the CluedIn.UI how the user needs to authenticate towards the integration. Oauth . CluedIn provides you a mechanism to get permission on the integration that requires an ‘Oauth’ dance, never the less, you still need to add ‘some’ URL for the UI to know where he should redirect correctly. Example: . \"authMethods\": { \"oauth\": { \"oauthCallbackKey\": \"office365azureactivedirectory\", \"initial\": \"api/office365azureactivedirectory?authError=none\", \"callback\": \"api/office365azureactivedirectory?authError\", }, }, . NOTE: For future version, CluedIn will work to remove the needs of those value by creating a generic controllers, but due to some ‘exceptions’ we have encounter with some Oauth mechanism, it is still required to mention those values. Credentials . Credentials is generally use for the system that requires a BASIC authentication. This will be used by the integration to pull the data. Example: . \"credentials\": [{ \"displayName\": \"User Name\", \"type\": \"input\", \"isRequired\": true, \"name\": \"username\", }, { \"displayName\": \"Password\", \"name\": \"password\", \"type\": \"password\", \"isRequired\": true, }], . API Token . Some integrations require sometimes an API token to be passed along the request. Example: . \"token\": [{ \"displayName\": \"Api Token\", \"type\": \"input\", \"isRequired\": true, \"name\": \"apiToken\", }], . Custom . If you need a ‘custom’ field to be sent to your Integration, you can use the Credentials object with extra fields. \"credentials\": [{ \"type\": \"subdomain\", \"name\": \"websiteName\", \"displayName\": \"Website Name\", \"protocol\": \"https://\", \"domain\": \"zendesk.com\", \"isRequired\": true, }, { \"type\": \"input\", \"name\": \"username\", \"displayName\": \"Username\", \"isRequired\": true, }, { \"type\": \"password\", \"name\": \"password\", \"displayName\": \"Password\", \"isRequired\": true, }], . Properties . The properties used to setup more precisely what you want to crawl from that integration. If you want the integration to get ALL data, leave it empty but from time to time, you want the User adding the integration to pick a specific ‘project’ or ‘folder’ or any other kind of segmentation that your integration might have. Example: List of Projects . [{ \"displayName\": \"Projects to include\", \"type\": \"list\", \"isRequired\": true, \"name\": \"projects\", \"options\": { \"key\": \"Id\", \"displayName\": \"Name\", \"stateField\": \"State\", \"states\": [\"ACTIVE\", \"INACTIVE\"], } }] . | displayName: the label that would be displayed in the UI once the integration is rendered. | type: The type of data that would be returned list or tree. | isRequired: mentioned if it is needed for the User to setup this information. | name: The name of the field to setup (taken from the HelperConfiguration). | options: The ‘value’ that should be set for each value selected by the user. | . Example: Tree of folders . [{ \"displayName\": \"Folders to include\", \"name\": \"folders\", \"type\": \"tree\", \"isRequired\": true, }] . NOTE: In the case you more options, please contact us. Type . A list of type for the integration. Useful when you have hundreds of integration installed. Values can be: . | “Cloudfile” | “Support” | “CRM” | “Social” | “Code” | “Task” | “Communication” | . Example: . type: [\"Task\", \"Support\"] . In the UI: . ",
    "url": "/integration/integration-compatible-with-ui",
    
    "relUrl": "/integration/integration-compatible-with-ui"
  },"1604": {
    "doc": "Reset tool",
    "title": "On this page",
    "content": ". | Reset tool components | Run reset tool | . In this article, you will learn about the reset tool that you can use to restore your CluedIn cluster to its original installation state, retaining only the essential organization and user data. You may want to reset your CluedIn instance if you’ve been using it for testing and have reached the desired result. Once you no longer need it for testing, you can reset it. The reset tool deletes all data and configurations except for users, roles, organization, and the following organization-related tables: . | Agent | Agent_AgentGroup | AgentGroup | OrganizationProfile | UserProfile | Provider | PageTemplate | EntityType | DefaultPageTemplate | PageTemplateEntityType | LayoutTemplate | NotificationTemplates | Dimension | Metric | __MigrationHistory | __RefactorLog | . ",
    "url": "/paas-operations/reset-tool#on-this-page",
    
    "relUrl": "/paas-operations/reset-tool#on-this-page"
  },"1605": {
    "doc": "Reset tool",
    "title": "Reset tool components",
    "content": "The reset tool is implemented as a console application that runs within a pod in your cluster, seamlessly integrating into your CI/CD pipeline. The main components of the reset tool are the following: . | Automation account | Reset runbook | Storage account | Manifest file | An instance of CluedIn | . The automation account with the reset runbook and the storage account with the manifest file are prepared for you by CluedIn. The only thing you need to do is to run the reset tool. If you would like to use the reset tool, reach out to CluedIn support at support@cluedin.com, and we will install and configure this tool in your environment. The following diagram displays the configuration of the reset tool in your environment. ",
    "url": "/paas-operations/reset-tool#reset-tool-components",
    
    "relUrl": "/paas-operations/reset-tool#reset-tool-components"
  },"1606": {
    "doc": "Reset tool",
    "title": "Run reset tool",
    "content": "Once we set up the reset tool in your environment, you can reset your CluedIn instance any time you want. To reset your CluedIn instance . | In the Azure portal, find the automation account that contains the reset runbook, and then open the reset runbook. | At the top of the page, select Start. | Enter the name of the CluedIn cluster that you want to reset, and then select OK. It takes up to 30 minutes to reset your CluedIn instance. While the reset tool is running, you can view the detailed logs. Once the process is completed, your CluedIn instance will become available again. | . ",
    "url": "/paas-operations/reset-tool#run-reset-tool",
    
    "relUrl": "/paas-operations/reset-tool#run-reset-tool"
  },"1607": {
    "doc": "Reset tool",
    "title": "Reset tool",
    "content": " ",
    "url": "/paas-operations/reset-tool",
    
    "relUrl": "/paas-operations/reset-tool"
  },"1608": {
    "doc": "Resources for Data Steward",
    "title": "On this page",
    "content": ". | Before you start | Data ingestion | Data transformation . | Cleaning | Enrichment | Deduplication | . | Data automation | Data export | Additional data management activities | . | Audience | Time to read | . | Data Steward | 3 min | . This page includes links to relevant documentation and videos to help Data Stewards perform their tasks in the data quality virtuous cycle. ",
    "url": "/playbooks/resources-for-data-steward#on-this-page",
    
    "relUrl": "/playbooks/resources-for-data-steward#on-this-page"
  },"1609": {
    "doc": "Resources for Data Steward",
    "title": "Before you start",
    "content": "Learn about the fundamental concepts and features in CluedIn. | Activity | Resources | . | Learn about golden records in CluedIn | Golden records | . | Learn how to search for golden records in CluedIn | Search | . | Learn how to work with filters in CluedIn | Filters | . ",
    "url": "/playbooks/resources-for-data-steward#before-you-start",
    
    "relUrl": "/playbooks/resources-for-data-steward#before-you-start"
  },"1610": {
    "doc": "Resources for Data Steward",
    "title": "Data ingestion",
    "content": "Data Stewards can review the records in quarantine and approve or reject them as needed. Approved records will go into processing and aggregate to the existing golden records or create new golden records. | Activity | Resources | . | Learn about the quarantine tool in CluedIn | Quarantine | . | Learn how to make changes to data that has been quarantined before submitting it to CluedIn | Video | . ",
    "url": "/playbooks/resources-for-data-steward#data-ingestion",
    
    "relUrl": "/playbooks/resources-for-data-steward#data-ingestion"
  },"1611": {
    "doc": "Resources for Data Steward",
    "title": "Data transformation",
    "content": "Data Stewards can identify and correct any data quality issues with the help of clean projects, enhance golden records with information from third-party sources, and establish criteria for finding and eliminating duplicates. Cleaning . | Activity | Resources | . | Get acquainted with the cleaning process in CluedIn | How to get started with data cleaning | . | Learn about different ways to create a clean project | Create a clean project | . | Fix data quality issues in the clean application | Manage a clean project | . | Build automated rules based on data cleaning activities | Video | . | Understand the statuses of the clean project | Clean project reference | . | Use profiling to find data quality issues | Video | . Enrichment . | Activity | Resources | . | Get acquainted with the enrichment process | Concept of enricher | . | Learn about different enrichers and find instructions on how to configure each enricher | Enricher reference | . | Enrich data from third-party services in CluedIn | Video | . Deduplication . | Activity | Resources | . | Get acquainted with the deduplication process in CluedIn | How to get started with deduplication | . | Watch step-by-step video on the basic deduplication of data | Video | . | Learn about the strategies on how to efficiently deduplicate large data sets in CluedIn | Deduplication in practice | . | Create a deduplication project and configure matching rules for detecting duplicates | Create a deduplication project | . | Learn how to use probabilistic matching rules to find duplicates | Video | . | Learn how to process and merge groups of duplicates | Manage groups of duplicates | . | Learn how to split merged records | Video | . ",
    "url": "/playbooks/resources-for-data-steward#data-transformation",
    
    "relUrl": "/playbooks/resources-for-data-steward#data-transformation"
  },"1612": {
    "doc": "Resources for Data Steward",
    "title": "Data automation",
    "content": "Data Stewards can create business rules to apply data transformations, capture data quality issues, and determine operational values. | Activity | Resources | . | Get acquainted with high-level rule creation process | How to get started with rules | . | Get acquainted with different types of rules in CluedIn | Data part rules, survivorship rules, golden record rules | . | Learn how to create a business rule | Create a rule | . | Use OpenAI for data automation in CluedIn | Video | . | Use OpenAI to explain automation in rules | Video | . ",
    "url": "/playbooks/resources-for-data-steward#data-automation",
    
    "relUrl": "/playbooks/resources-for-data-steward#data-automation"
  },"1613": {
    "doc": "Resources for Data Steward",
    "title": "Data export",
    "content": "Data Stewards can configure the stream and define which golden records should be sent to the external systems. | Activity | Resources | . | Get acquainted with the high-level streaming process | How to get started with streaming | . | Learn how to create and configure a stream | Create a stream | . | Watch a video about creating a stream proactively without having the data yet | Video | . | Watch a video about synchronized and event log stream modes | Video | . ",
    "url": "/playbooks/resources-for-data-steward#data-export",
    
    "relUrl": "/playbooks/resources-for-data-steward#data-export"
  },"1614": {
    "doc": "Resources for Data Steward",
    "title": "Additional data management activities",
    "content": "Data Stewards can use hierarchies to visualize relations between golden records, create glossary categories and terms to group golden records, and use other tools in CluedIn to facilitate data management. | Activity | Resources | . | Visualize relations between golden records with the help of hierarchies | How to get started with hierarchies | . | Build manual hierarchies | Video | . | Build automated hierarchies | Video | . | Create glossary categories and terms | How to get started with glossary | . | Use glossary terms in streams | Video | . | Using Copilot to help in the data management process | Video | . ",
    "url": "/playbooks/resources-for-data-steward#additional-data-management-activities",
    
    "relUrl": "/playbooks/resources-for-data-steward#additional-data-management-activities"
  },"1615": {
    "doc": "Resources for Data Steward",
    "title": "Resources for Data Steward",
    "content": " ",
    "url": "/playbooks/resources-for-data-steward",
    
    "relUrl": "/playbooks/resources-for-data-steward"
  },"1616": {
    "doc": "Sync Purview glossaries to CluedIn vocabularies",
    "title": "On this page",
    "content": ". | Preparation . | Preparation in Purview | Preparation in CluedIn | . | Feature demonstration | . In this article, you will learn how to sync Purview glossaries to CluedIn vocabularies. ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#on-this-page"
  },"1617": {
    "doc": "Sync Purview glossaries to CluedIn vocabularies",
    "title": "Preparation",
    "content": "To sync glossaries from Purview to CluedIn vocabularies and vocabulary keys, complete 2 preparation steps: . | Prepare glossary terms in Purview – create a new term template with an attribute to indicate that glossary terms can be synced to CluedIn vocabulary keys and prepare glossary terms for syncing. | Configure settings in CluedIn – enable the sync Purview glossaries to CluedIn vocabularies feature and provide the attribute to identify the glossary terms for syncing. | . Preparation in Purview . Make sure you have the existing classic type glossary that contains some glossary terms along with child glossary terms. The child glossary terms in Purview must be assigned to the schema of the asset that is synced with CluedIn. Purview glossary terms will be synced to CluedIn vocabularies and Purview child glossary terms will be synced to CluedIn vocabulary keys. On the following screenshot, we have the CluedIn glossary that contains 7 glossary terms. The Contact glossary term contains 2 child glossary terms: Email and FullName. In this section, you will find an instruction on how to prepare these two child glossary terms for synchronization with CluedIn. Additionally, the glossary terms should contain an attribute to indicate that they can be synced to CluedIn vocabulary keys. The name of this attribute should be the same as in the Glossary to Vocabulary Attribute Filter field from CluedIn settings. The default value is CluedInVocab. You can create this attribute in Purview with the help of a new term template. To create a new term template . | In the Microsoft Purview portal, navigate to Unified Catalog &gt; Catalog management &gt; Classic types, and then select the glossary that contains the term you want to sync to CluedIn. | On the glossary terms details page, select New term. | Select New term template. | Enter the Template name. | Select New attributes. | Enter the Attribute name. It should be the same as in the Glossary to Vocabulary Attribute Filter field from CluedIn settings. The default value is CluedInVocab. | In Field type, select Boolean. | Select Apply, and then select Create. Now, when you create a new glossary term that you want to sync to CluedIn vocabulary key, set the attribute to True. | . To prepare Purview child glossary terms . | In the Microsoft Purview portal, navigate to Data Map &gt; Domains. In your default domain, select the collection that stores the assets from Azure Data Lake Storage. | Select the assets card. | Find and select the asset that is synced with CluedIn. | On the asset details page, select Schema. | Find and select the column that should be associated with the child glossary term that you want to sync. | On the column details pane, select Edit. | In Glossary terms, find and select the child glossary term that you want to assign to the column. | Select Save. The child glossary term is now associated with the column. | Repeat steps 1–8 for all child glossary terms that you want to sync to CluedIn vocabularies. | . Preparation in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Purview glossaries to CluedIn glossaries. | You can leave the other settings for this feature at their default values. | Select Save. | . ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#preparation",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#preparation"
  },"1618": {
    "doc": "Sync Purview glossaries to CluedIn vocabularies",
    "title": "Feature demonstration",
    "content": "Once you enable synchronization of Purview glossaries to CluedIn vocabularies, you will receive a notification when the vocabulary key is created in CluedIn. If there are matching CluedIn vocabularies, they will be updated; otherwise, new CluedIn vocabularies are created for the incoming Purview glossaries. To find synced vocabulary keys in CluedIn . | On the navigation pane, go to Management &gt; Data Catalog &gt; View All Vocabularies. | Find and expand the vocabulary that is named as the Purview glossary term. The vocabulary (a) corresponds to the glossary term in Purview. It contains vocabulary keys (b) that correspond to child glossary terms in Purview. According to default settings, the vocabulary is automatically associated with the Purview/Contact entity type, and the vocabulary key prefix is “purview”. However, you can change these settings if needed. | . ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#feature-demonstration",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies#feature-demonstration"
  },"1619": {
    "doc": "Sync Purview glossaries to CluedIn vocabularies",
    "title": "Sync Purview glossaries to CluedIn vocabularies",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies",
    
    "relUrl": "/microsoft-integration/purview/sync-purview-glossaries-to-cluedin-vocabularies"
  },"1620": {
    "doc": "Vocabulary (Schema)",
    "title": "On this page",
    "content": ". | Vocabulary characteristics . | Vocabulary prefix | Vocabulary keys (attributes) | Vocabulary groups | . | Vocabulary usage . | One vocabulary per source | Business domain vs. vocabulary | . | Core vocabularies | Useful resources | . A vocabulary (schema) is a framework that defines how metadata is stored and organized within the system. A well-defined vocabulary is essential for maintaining data consistency, accuracy, and usability across an organization. It ensures that all stakeholders are working with consistent and reliable master data definitions and structures. ",
    "url": "/key-terms-and-features/vocabularies#on-this-page",
    
    "relUrl": "/key-terms-and-features/vocabularies#on-this-page"
  },"1621": {
    "doc": "Vocabulary (Schema)",
    "title": "Vocabulary characteristics",
    "content": "The primary purpose of vocabulary is to hold vocabulary keys. Vocabulary keys are your way to be able to describe properties that are coming in from data sources. A vocabulary is composed of multiple vocabulary groups and multiple vocabulary keys. Vocabulary prefix . The vocabulary prefix is a part of what we call the full vocabulary key. It is added before vocabulary key names for consistent naming, efficient searching, and data filtering. The prefix is stored in our databases and is given automatically when adding a vocabulary key to a given vocabulary. Changing the prefix of an existing vocabulary could influence many records and would require their re-processing. For example, let’s take the prefix contact_person and two vocabulary keys firstName and lastName. If you want to rename contact_person to contactPerson, it means that all golden records using contact_person.firstName and contact_person.lastName will need to be changed. Applying those changes on millions of golden records might require a bit of time. Vocabulary keys (attributes) . To maintain proper data lineage, it is recommended that the key name is set to the exact same value of the property name coming in from the source system. Vocabulary groups . A vocabulary group is an optional group for organizing vocabulary keys in a logical collection. It is used for aesthetic purposes and has no influence on your records. For example, you could have the Social group for the vocabulary Contact. This group would have the following vocabulary keys: . | LinkedIn Profile | X.com username | Website | Blog | . If you do not provide any group for your vocabulary keys, they will be located in a group called Ungrouped Keys. ",
    "url": "/key-terms-and-features/vocabularies#vocabulary-characteristics",
    
    "relUrl": "/key-terms-and-features/vocabularies#vocabulary-characteristics"
  },"1622": {
    "doc": "Vocabulary (Schema)",
    "title": "Vocabulary usage",
    "content": "One vocabulary per source . Even if it is tempting to reuse the vocabulary across multiple sources, you should keep the vocabulary close to your sources for the lineage purposes. This gives you better flexibility when working with clean projects, rules, or deduplication projects because you’ll know where a specific vocabulary key is coming from. To start using what we call shared vocabulary, you need to map one vocabulary key to another. For example, let’s take a field called Email from the CRM source. You would use a vocabulary key called CRM.contact.email. The CRM.contact would be the prefix of a vocabulary (which is probably called CRM Contact), and email would be a child vocabulary key of CRM Contact. So, the full vocabulary key is CRM.contact.email. Now, you add a field called Email from the ERP source. In this case, you would use a vocabulary key called ERP.contact.email. The ERP.contact would be the prefix of a vocabulary (which is probably called ERP Contact), and email would be a child vocabulary key of ERP Contact. So, the full vocabulary key is ERP.contact.email. Obviously, those 2 keys—CRM.contact.email and ERP.contact.email—represent the same meaning, so you would want to have a single shared vocabulary key called contact.email to be used in your golden records. This is possible to achieve by mapping those 2 keys to the shared vocabulary key called contact.email. This means that the data will be flowing towards the shared vocabulary key, and the values of CRM.contact.email and ERP.contact.email will now all be located in contact.email (as shown in the table below). | Source | Vocabulary | Vocabulary key | Maps to | . | CRM | CRM.contact | email | contact.email | . | ERP | ERP.contact | email | contact.email | . By applying this principle, you can keep your lineage and have better flexibility and agility. However, it is up to you to decide when and if you want to map vocabulary keys with the same meaning to a shared vocabulary key or keep them separate. Business domain vs. vocabulary . When you map your data in CluedIn, you can map it to one business domain and one vocabulary. However, a vocabulary can be shared among different business domains and can represent only a partial aspect of the golden record. So, the following statements are true when describing a golden record: . | You can assign only one business domain to a golden record. | You can use multiple vocabularies for a golden record. | . By distinguishing between business domain and vocabulary, we decouple the value aspect of records from their modeling aspect. This approach provides greater flexibility in modeling and allows for evolution with changing use cases. ",
    "url": "/key-terms-and-features/vocabularies#vocabulary-usage",
    
    "relUrl": "/key-terms-and-features/vocabularies#vocabulary-usage"
  },"1623": {
    "doc": "Vocabulary (Schema)",
    "title": "Core vocabularies",
    "content": "Core vocabularies are where a lot of the “smarts” in CluedIn exist. If you map your custom integration sources’ keys into core vocabulary keys, there is a high likelihood that something will be done on the data to your advantage. For example, if you map a phone number into a core vocabulary Phone Number Key, then CluedIn will, amongst other things: . | Create new normalized representations of the phone number in industry standards e.g. E164. | Will see if we can confidentially identify or merge records based off this phone number. | . The role of core vocabularies is to merge records from different systems based on knowing that these clues will have a different origin. As you have already learnt about entity codes, they need to be exact for a merge to occur and simply the change of the origin could cause records not to merge. Although you can add your own core vocabularies, we suggest that you do not. The reason is mainly due to upgrade support and making sure your upgrades are as seamless and automated as possible. Core vocabularies do not include the namespace for a source system. It will typically have the business domain, the name of the key - or if it is a nested key like and address, it will have the nesting shown in the key name: . | organization.industry | organization.address.city | organization.address.countryName | person.age | person.gender | . Vocabularies are allowed (and designed) to be hierarchical in nature. This means that you can have many (max 10) levels of hierarchy in your vocabularies. This is to allow you to be able to map source systems into a Company, Department, Team, or other levels. Here is a simple example of how a vocabulary key could map from source to core. hubspot.contact.contactName - Provider Level lego.sales.contact.fullName - Department Level lego.marketing.contact.leadName - Department Level lego.contact.Name - Company Level person.fullName - CluedIn Core Vocabulary . ",
    "url": "/key-terms-and-features/vocabularies#core-vocabularies",
    
    "relUrl": "/key-terms-and-features/vocabularies#core-vocabularies"
  },"1624": {
    "doc": "Vocabulary (Schema)",
    "title": "Useful resources",
    "content": ". | Modeling approaches . | Create and manage a vocabulary . | Create and manage vocabulary keys . | . ",
    "url": "/key-terms-and-features/vocabularies#useful-resources",
    
    "relUrl": "/key-terms-and-features/vocabularies#useful-resources"
  },"1625": {
    "doc": "Vocabulary (Schema)",
    "title": "Vocabulary (Schema)",
    "content": " ",
    "url": "/key-terms-and-features/vocabularies",
    
    "relUrl": "/key-terms-and-features/vocabularies"
  },"1626": {
    "doc": "Workflow",
    "title": "Workflow",
    "content": "The Workflow module is designed to help you streamline and track approvals and notifications for specific activities in CluedIn with the help of Power Automate workflows. This functionality is intended only for SSO users. To use the Workflow module in CluedIn, you need to configure Power Automate integration. This section covers the following areas: . | Concept of workflows – learn about the approval workflow and its participants. | Creating and managing workflows – learn how to create a workflow for automating a specific approval process and how to find the details about the actions that triggered workflow runs. | Manage approval requests – learn how to view, approve, or reject the approval requests. | . ",
    "url": "/workflow",
    
    "relUrl": "/workflow"
  },"1627": {
    "doc": "AKS upgrade",
    "title": "On this page",
    "content": ". | Prerequisites . | Tools | Azure checks | . | Upgrade setup . | Upgrade with Azure checks passed | Upgrade with Azure checks NOT passed | . | Post-upgrade | . In this article, you will learn how to upgrade your Azure Kubernetes Service (AKS) to a supported version. This is included as part of the AMA agreement with us, and we’ll normally facilitate these upgrades on your behalf. However, there may be times where we are unable to do this upgrade due to missing levels of permission or other factors. Before starting the AKS upgrade, make sure that you have read the prerequisites section. ",
    "url": "/paas-operations/upgrade/aks#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/aks#on-this-page"
  },"1628": {
    "doc": "AKS upgrade",
    "title": "Prerequisites",
    "content": "Tools . | az cli and a shell. This guide will primarily use az cli along with PowerShell to manage the AKS cluster. It is recommended to follow along, but not necessarily required. | . Azure checks . When doing a Kubernetes upgrade, nodes will surge to try minimise downtime of the application. This means that additional nodes in the Node Pool will be spun up at upgrade time and may cause issues if preparation isn’t done beforehand. | Quota – each node will eventually spin up a new replica of the same SKU. It’s important to make sure you have additional allocated quota so that these nodes can be spun up without any problems. | IP address allocation – if your environment is using Azure CNI without Dynamic Allocation, your nodes will reserve IP addresses for each potential pod. By default, this is between 30-50 pods per node. As a result, each node will use the same amount of IP addresses. This can mean that when additional nodes are spun up during surge, there’s not enough available space to allocate. Please see if there’s enough space before going ahead. | . If you do not meet the prerequisites, please scroll down to the here to follow the steps to get around this. For reference information, see Microsoft documentation. ",
    "url": "/paas-operations/upgrade/aks#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/aks#prerequisites"
  },"1629": {
    "doc": "AKS upgrade",
    "title": "Upgrade setup",
    "content": "Before proceeding, please check the prerequisites as it will determine which upgrade path to take. Before any Kubernetes upgrades take place, we highly recommend you scale down your instance to avoid any potential data loss. Whilst upgrading without scaling down is possible, it’s best to be safe here. To scale down your instance . | Open up pwsh | Run the following commands: . $aksParams = @( '--name', ${aksClusterName} # This is the name of the AKS cluster '--resource-group', ${resourceGroup} # This is the resource group name the AKS cluster resides in '--subscription', ${subscription} # This is the subscription the AKS cluster resides in ) $command = 'kubectl scale statefulset -n cluedin --replicas=0 --all --timeout 5m' az aks command invoke @aksParams --command $command $command = 'kubectl scale deploy -n cluedin --replicas=0 --all --timeout 5m' az aks command invoke @aksParams --command $command . The above may take a couple minutes to complete. But once done, it should be safe to upgrade the AKS cluster without any potential data loss. | Run the following commands to get the list of availble version to upgrade . $params = @( '--name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} ) az aks get-upgrades @params -o table . Pick the version to upgrade from the output list . | . With the above steps completed, it’s time to upgrade. Depending on if you pass or fail the prerequisites, please follow the appropriate path below: . | Passed | Failed | . Upgrade with Azure checks passed . Because everything checks out from the Azure side, you should be able to upgrade the Kubernetes cluster without any issues. Please note that Kubernetes follows an upgrade path and it’s not always possible to upgrade from a lower version to a much higher version. You may need to perform this operation a number of times. To upgrade the cluster . | Determine what version of Kubernetes you will need to get to, and then perform the below steps until you are successfully running that version. | Open up pwsh | Run the following commands: . $params = @( '--name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} '--kubernetes-version', '1.nn.n' # This will vary depending on what CluedIn version you are running. '--yes' ) az aks upgrade @params . | It will take between 5-15 minutes to complete each upgrade. Once you have completed the upgrade path to the necessary version, scale up your environment again following the steps here. | . If you are unsure about any of the steps above, please reach out to CluedIn support. Upgrade with Azure checks NOT passed . When you do not have the correct setup to perform a smooth upgrade, additional steps are going to be required to get your environment upgraded to the desired supported version. If at any point in this section you are not comfortable with the steps, please reach out to CluedIn support. Make notes of the current node pools as these will be deleted as part of the process. This includes taints, labels, and any other potential configurations. | Open up pwsh | Run the following commands: . $aksClusterParams = @( '--name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} ) $aksCluster = az aks show @aksClusterParams | ConvertFrom-Json $aksNodePools = $aksCluster.agentPoolProfiles . | The above will return back an object that contains the agent pool configuration. This can be queried by running $aksNodePools in your shell and making notes of the configuration. It is unlikely you need to touch the System node pool, so chances are you only need to make notes of the User node pools which is what CluedIn runs on. Make notes of the following configuration for each node you need to delete: . | name | nodeCount | nodeVmSize | maxPods | vnetSubnetId | labels | taints | . Note: As long as the PowerShell session doesn’t get closed, this object will always exist to reference. | In PowerShell, run the following commands: . $params = @( '--nodepool-name', ${nodeName} # This is the name of the node pool you are attempting to delete '--cluster-name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} ) az aks nodepool delete @params . Note: You need to repeat this for all node pools that are affected by either the IP address allocation or quota (based on the VM SKU it is using). | Because all affected node pools have now been deleted, you should perform the upgrade steps next until your desired version. Follow the steps above here and return back here once you have hit your version. | With Kubernetes control plane now updated, it’s time to recreate the node pools before scaling the application back up. | In pwsh run the following commands: . $params = @( '--cluster-name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} '--name', ${nodeName} '--mode', 'User' '--node-count', ${nodeCount} '--node-vm-size', ${nodeVmSku} '--max-pods', ${nodeMaxPods} '--vnet-subnet-id', ${nodeVnetSubnetId} # Optionals # '--node-taints', ${nodeTaints} key=value format # '--labels', ${nodeLabels} # key=value format # '--enable-cluster-autoscaler' ) az aks nodepool add @params . Note: You need to repeat this for all node pools that were affected by either the IP address allocation or quota (based on the VM SKU it is using). | All nodepools should now be added back. You can validate in the Azure portal by navigating to the AKS cluster resource and selecting Node Pools on the left hand blade. If everything has the status Succeeded you can resume down below and scale the application back up. | . ",
    "url": "/paas-operations/upgrade/aks#upgrade-setup",
    
    "relUrl": "/paas-operations/upgrade/aks#upgrade-setup"
  },"1630": {
    "doc": "AKS upgrade",
    "title": "Post-upgrade",
    "content": "With the upgrade successfully completed, it’s time to bring the application back up into a running state and validate access. | Open up pwsh | Run the following commands: . $aksParams = @( '--name', ${aksClusterName} '--resource-group', ${resourceGroup} '--subscription', ${subscription} ) $command = 'kubectl scale statefulset -n cluedin --replicas=1 --all --timeout 5m' az aks command invoke @aksParams --command $command $command = 'kubectl scale deploy -n cluedin --replicas=1 --all --timeout 5m' az aks command invoke @aksParams --command $command . | The commands will complete before the application is technically ready. Please wait another 5-15 minutes after running the above and validating access to the environment. | . With CluedIn now back up and running, please navigate to the front end you would normally use and ensure you can sign in. If successful, you can now conclude that the AKS has been upgraded to the correct version. ",
    "url": "/paas-operations/upgrade/aks#post-upgrade",
    
    "relUrl": "/paas-operations/upgrade/aks#post-upgrade"
  },"1631": {
    "doc": "AKS upgrade",
    "title": "AKS upgrade",
    "content": " ",
    "url": "/paas-operations/upgrade/aks",
    
    "relUrl": "/paas-operations/upgrade/aks"
  },"1632": {
    "doc": "SQL Server",
    "title": "On this page",
    "content": ". | Inside the cluster | Custom SQL server (Azure SQL) | . To run SQL Server in production environment, you should have a valid Microsoft SQL Server license. Learn more about licensing here. ",
    "url": "/deployment/kubernetes/sql#on-this-page",
    
    "relUrl": "/deployment/kubernetes/sql#on-this-page"
  },"1633": {
    "doc": "SQL Server",
    "title": "Inside the cluster",
    "content": "By default, SQL Server is deployed within the cluster. This is currently the recommended approach. However, it is possible to deploy to Azure SQL instead. When deployed inside the cluster, two secrets will get created. One will contain the password for the SQL Server (which will be a randomly generated password), the other will contain the connection strings that will be consumed by various deployments. ",
    "url": "/deployment/kubernetes/sql#inside-the-cluster",
    
    "relUrl": "/deployment/kubernetes/sql#inside-the-cluster"
  },"1634": {
    "doc": "SQL Server",
    "title": "Custom SQL server (Azure SQL)",
    "content": "If you are using your own SQL installation, like Azure SQL, you will need to do the following: . | Install the database definitions (DACPACs) to your SQL instance. This can be done from the command line using SqlPackage.exe. | Create a secret with the connection strings for each database. The secret should have the following keys: apiVersion: v1 kind: Secret metadata: name: my-connection-string-secret type: Opaque data: AuthenticationStore: &lt;connection-string&gt; BlobStorage: &lt;connection-string&gt; ConfigurationStore: &lt;connection-string&gt; CluedInEntities: &lt;connection-string&gt; TokenStore: &lt;connection-string&gt; Training: &lt;connection-string&gt; ExternalSearch: &lt;connection-string&gt; ML-Logging: &lt;connection-string&gt; Metrics: &lt;connection-string&gt; . | Pass the name of the secret in the values.yaml override file: application: sqlserver: connectionsSecretName: my-connection-string-secret . | . ",
    "url": "/deployment/kubernetes/sql#custom-sql-server-azure-sql",
    
    "relUrl": "/deployment/kubernetes/sql#custom-sql-server-azure-sql"
  },"1635": {
    "doc": "SQL Server",
    "title": "SQL Server",
    "content": " ",
    "url": "/deployment/kubernetes/sql",
    
    "relUrl": "/deployment/kubernetes/sql"
  },"1636": {
    "doc": "Modify web response headers",
    "title": "Modify web response headers",
    "content": "By default, CluedIn is configured with web security in mind and out of the box is quite secure. In certain cases, security teams may require additional configuration on the response headers to comply with policies. This guide explains how you can modify these headers to comply with your organization. Prerequisites . | Access to your Kubernetes cluster | Helm installed | . In order to modify the headers, we’ll recommend updating the configMap of HAProxy Ingress Controller by amending your values.yaml file to add the supported key:value pairs. To modify web response headers . | Export your current values file from a given environment. | Edit the now exported values file in the following section: . infrastructure: controller: config: # This is additions to the configMap config-backend: | # This refers to: https://haproxy-ingress.github.io/docs/configuration/keys/#configuration-snippet http-response add-header Cache-Control no-store http-response add-header X-Content-Type-Options nosniff # The above will add two additional response header entries for Cache-Control and X-Content-Type. # This can be updated to something more suitable to your organization. | Run helm update with the new values. After a few minutes, the config map cluedin-haproxy-ingress should now have a new entry for config-backend | To validate that the headers are being passed, go to your front end in the browser, open up the developer tools (In Chrome, this is F12), and check the response headers from the front end. You should notice that the added responses are now part of the overall response. | . For any further assistance or an edge case, reach out to CluedIn support. ",
    "url": "/paas-operations/config/modify-response-headers",
    
    "relUrl": "/paas-operations/config/modify-response-headers"
  },"1637": {
    "doc": "Create dedicated stream processing pod",
    "title": "On this page",
    "content": ". | Config map | Update existing config map | Deployment | . In this article, you’ll find instructions for creating a dedicated pod that only processes streams. Prerequisites . | Access to your Kubernetes cluster | Helm installed | . ",
    "url": "/paas-operations/config/dedicated-stream-processing-pod#on-this-page",
    
    "relUrl": "/paas-operations/config/dedicated-stream-processing-pod#on-this-page"
  },"1638": {
    "doc": "Create dedicated stream processing pod",
    "title": "Config map",
    "content": "Create the following config map. Note the important environment variable CLUEDIN_appSettings__Streams_Processing_Enabled: \"true\". apiVersion: v1 kind: ConfigMap data: ASPNETCORE_ENVIRONMENT: verbose CLUEDIN_appSettings__Agent_Enabled: \"false\" CLUEDIN_appSettings__ClueProcessing_FuzzyEntityMatching_Enabled: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ArchiveMetricsValues: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ClueProcessing: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_IProcessing_Clues: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_MergeEntities: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_MeshData: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_Metrics_ArchiveMetricsValuesCommand: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ParentsProcessing: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ParentsProcessing_ParentIds: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ProcessEdges: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ProcessEntityMetrics: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_ProcessGlobalMetrics: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_SaveEntity: \"false\" CLUEDIN_appSettings__ClueProcessing_Subscribe_SendMail: \"false\" CLUEDIN_appSettings__ClueProcessing_SubscribeSeperateQueues: \"true\" CLUEDIN_appSettings__JobServer_Enabled: \"true\" CLUEDIN_appSettings__Logging_Targets_Exceptions: \"false\" CLUEDIN_appSettings__Metrics_Enabled: \"false\" CLUEDIN_appSettings__Metrics_Accuracy_Enabled: \"false\" CLUEDIN_appSettings__Metrics_Completeness_Enabled: \"false\" CLUEDIN_appSettings__Metrics_Connectivity_Enabled: \"false\" CLUEDIN_appSettings__Metrics_DataClassificationType_Enabled: \"false\" CLUEDIN_appSettings__Metrics_Relevance_Enabled: \"false\" CLUEDIN_appSettings__SeqUrl: \"\" CLUEDIN_appSettings__Streams_ExportShadowEntities: \"false\" CLUEDIN_appSettings__Streams_Processing_Enabled: \"true\" metadata: labels: app: cluedin release: cluedin-platform name: cluedin-server-processing-streams namespace: cluedin . ",
    "url": "/paas-operations/config/dedicated-stream-processing-pod#config-map",
    
    "relUrl": "/paas-operations/config/dedicated-stream-processing-pod#config-map"
  },"1639": {
    "doc": "Create dedicated stream processing pod",
    "title": "Update existing config map",
    "content": "Update the following config map cluedin-server-processing. Note the important environment variable CLUEDIN_appSettings__Streams_Processing_Enabled: 'false'. apiVersion: v1 kind: ConfigMap metadata: name: cluedin-server-processing namespace: cluedin labels: app: cluedin release: cluedin-platform data: CLUEDIN_APPSETTINGS__AGENT_ENABLED: 'false' CLUEDIN_APPSETTINGS__CLUEPROCESSING_SUBSCRIBE_SENDMAIL: 'true' CLUEDIN_APPSETTINGS__JOBSERVER_DASHBOARDVISIBLE: localOnly CLUEDIN_APPSETTINGS__JOBSERVER_ENABLED: 'true' CLUEDIN_APPSETTINGS__METEREDBILLING_ENABLED: 'false' CLUEDIN_appSettings__Streams_Processing_Enabled: 'false' . ",
    "url": "/paas-operations/config/dedicated-stream-processing-pod#update-existing-config-map",
    
    "relUrl": "/paas-operations/config/dedicated-stream-processing-pod#update-existing-config-map"
  },"1640": {
    "doc": "Create dedicated stream processing pod",
    "title": "Deployment",
    "content": "Take the current deployment for cluedin-server-processing as shown below. | Change name to cluedin-server-processing-streams. | Replace the config map reference cluedin-server-processing with cluedin-server-processing-streams | Deploy. | . apiVersion: apps/v1 kind: Deployment metadata: name: cluedin-server-processing-streams namespace: cluedin labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.3.0 release: cluedin-platform role: processing annotations: deployment.kubernetes.io/revision: '42' meta.helm.sh/release-name: cluedin-platform meta.helm.sh/release-namespace: cluedin spec: replicas: 1 selector: matchLabels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/name: application role: processing template: metadata: labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.3.0 release: cluedin-platform role: processing spec: volumes: - name: components persistentVolumeClaim: claimName: cluedin-init-data initContainers: - name: wait-cluedin-sqlserver image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - service - cluedin-sqlserver - '-n' - cluedin imagePullPolicy: IfNotPresent - name: wait-init-sqlserver-job image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - job - init-sqlserver-job - '-n' - cluedin imagePullPolicy: IfNotPresent - name: wait-init-neo4j-job image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - job - init-neo4j-job - '-n' - cluedin imagePullPolicy: IfNotPresent - name: wait-init-cluedin-job image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - job - init-cluedin-job - '-n' - cluedin imagePullPolicy: IfNotPresent - name: wait-cluedin-redis-master image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - service - cluedin-redis-master - '-n' - cluedin imagePullPolicy: IfNotPresent - name: wait-cluedin-elasticsearch image: cluedinprod.azurecr.io/groundnuty/k8s-wait-for:v1.3 args: - service - cluedin-elasticsearch - '-n' - cluedin imagePullPolicy: IfNotPresent containers: - name: cluedin-processing image: cluedinprod.azurecr.io/cluedin/cluedin-server:2024.07 ports: - containerPort: 9000 protocol: TCP - containerPort: 9001 protocol: TCP - containerPort: 9003 protocol: TCP - containerPort: 9006 protocol: TCP - containerPort: 9007 protocol: TCP envFrom: - configMapRef: name: cluedin-server - configMapRef: name: cluedin-server-processing-streams - configMapRef: name: cluedin-server-crawling-disable env: - name: MSSQL_HOST value: cluedin-sqlserver - name: CLUEDIN_APPSETTINGS__MESSAGING_CONSUMER_PREFETCHCOUNT value: '4' - name: MSSQL_PORT value: '1433' - name: MSSQL_TIMEOUT value: '150' - name: MSSQL_CLIENTUSER_PASSWORD valueFrom: secretKeyRef: name: cluedin-sqlserver-clientuser-secret key: password - name: CLUEDIN_CONNECTIONSTRINGS__AUTHENTICATIONSTORE value: &gt;- Initial Catalog=DataStore.Db.Authentication;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;;MultipleActiveResultSets=True; - name: CLUEDIN_CONNECTIONSTRINGS__AUDITLOG value: &gt;- Initial Catalog=DataStore.Db.AuditLog;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__BLOBSTORAGE value: &gt;- Initial Catalog=DataStore.Db.BlobStorage;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__CONFIGURATIONSTORE value: &gt;- Initial Catalog=DataStore.Db.Configuration;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__CLUEDINENTITIES value: &gt;- Initial Catalog=DataStore.Db.OpenCommunication;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__TOKENSTORE value: &gt;- Initial Catalog=DataStore.Db.TokenStore;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__TRAINING value: &gt;- Initial Catalog=DataStore.Db.Training;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__EXTERNALSEARCH value: &gt;- Initial Catalog=DataStore.Db.ExternalSearch;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__LOCKING value: &gt;- Initial Catalog=DataStore.Db.OpenCommunication;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__ML_LOGGING value: &gt;- Initial Catalog=DataStore.Db.ML-Logging;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__METRICS value: &gt;- Initial Catalog=DataStore.Db.Metrics;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__MICROSERVICES value: &gt;- Initial Catalog=DataStore.Db.MicroServices;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__CLEANCACHE value: &gt;- Initial Catalog=DataStore.Db.CleanCache;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: MSSQL_SA_PASSWORD valueFrom: secretKeyRef: name: cluedin-sqlserver-secret key: sapassword - name: &gt;- CLUEDIN_CONNECTIONSTRINGS__STREAMS_COMMON_SQLCACHECONNECTIONSTRING value: &gt;- Initial Catalog=DataStore.Db.OpenCommunication;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=sa;Password=$(MSSQL_SA_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__INVITATIONSTORE value: &gt;- Initial Catalog=DataStore.Db.WebApp;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=sa;Password=$(MSSQL_SA_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__STREAMLOG value: &gt;- Initial Catalog=DataStore.Db.StreamLog;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: CLUEDIN_CONNECTIONSTRINGS__STREAMCACHE value: &gt;- Initial Catalog=DataStore.Db.StreamCache;Data Source=$(MSSQL_HOST),$(MSSQL_PORT);Encrypt=false;User Id=clientUser;Password=$(MSSQL_CLIENTUSER_PASSWORD);connection timeout=$(MSSQL_TIMEOUT);Max Pool Size=200;Pooling=True;; - name: RABBITMQ_CLUEDIN_PASSWORD valueFrom: secretKeyRef: name: cluedin-rabbitmq key: rabbitmq-password - name: CLUEDIN_CONNECTIONSTRINGS__MESSAGEBUS value: &gt;- amqp://cluedin:$(RABBITMQ_CLUEDIN_PASSWORD)@cluedin-rabbitmq:5672 - name: CLUEDIN_CONNECTIONSTRINGS__MESSAGEBUSMANAGEMENT value: &gt;- host=cluedin-rabbitmq:15672;username=cluedin;password=$(RABBITMQ_CLUEDIN_PASSWORD) - name: CLUEDIN_CONNECTIONSTRINGS__SIGNALRSCALEOUT value: &gt;- amqp://cluedin:$(RABBITMQ_CLUEDIN_PASSWORD)@cluedin-rabbitmq:5672 - name: NEO4J_NEO4J_PASSWORD valueFrom: secretKeyRef: name: cluedin-neo4j-secrets key: neo4j-password-encoded - name: CLUEDIN_CONNECTIONSTRINGS__GRAPHSTORE_READ value: http://neo4j:$(NEO4J_NEO4J_PASSWORD)@cluedin-neo4j:7474 - name: CLUEDIN_CONNECTIONSTRINGS__GRAPHSTORE_WRITE value: http://neo4j:$(NEO4J_NEO4J_PASSWORD)@cluedin-neo4j:7474 - name: REDIS_REDIS_PASSWORD valueFrom: secretKeyRef: name: cluedin-redis key: redis-password - name: CLUEDIN_CONNECTIONSTRINGS__CACHESTORE value: &gt;- cluedin-redis-master:6379,password=$(REDIS_REDIS_PASSWORD),ssl=false,abortConnect=false,connectRetry=0,connectTimeout=20000,syncTimeout=20000,keepAlive=30 - name: CLUEDIN_CONNECTIONSTRINGS__ETAGSTORE value: &gt;- cluedin-redis-master:6379,password=$(REDIS_REDIS_PASSWORD),ssl=false,abortConnect=false,connectRetry=0,connectTimeout=20000,syncTimeout=20000,keepAlive=30 - name: CLUEDIN_CONNECTIONSTRINGS__JOBSTORE value: &gt;- cluedin-redis-master:6379,password=$(REDIS_REDIS_PASSWORD),ssl=false,abortConnect=false,connectRetry=0,connectTimeout=20000,syncTimeout=20000,keepAlive=30 - name: CLUEDIN_CONNECTIONSTRINGS__DATAPROTECTIONPERSISTENCE value: &gt;- cluedin-redis-master:6379,password=$(REDIS_REDIS_PASSWORD),ssl=false,abortConnect=false,connectRetry=0,connectTimeout=20000,syncTimeout=20000,keepAlive=30 - name: ELASTICSEARCH_ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elasticsearch-credentials key: password - name: CLUEDIN_CONNECTIONSTRINGS__SEARCHSTORE value: &gt;- http://elastic:$(ELASTICSEARCH_ELASTIC_PASSWORD)@cluedin-elasticsearch:9200 - name: CLUEDIN_APPSETTINGS__EMAILUSERNAME valueFrom: secretKeyRef: name: cluedin-email key: EmailUserName - name: CLUEDIN_APPSETTINGS__EMAILPASSWORD valueFrom: secretKeyRef: name: cluedin-email key: EmailPassword - name: CLUEDIN_APPSETTINGS__EMAILSERVER valueFrom: secretKeyRef: name: cluedin-email key: EmailHost - name: CLUEDIN_APPSETTINGS__EMAILPORT valueFrom: secretKeyRef: name: cluedin-email key: EmailHostPort - name: CLUEDIN_APPSETTINGS__EMAILSERVERSSL valueFrom: secretKeyRef: name: cluedin-email key: EmailEnableSsl resources: limits: cpu: '7' memory: 25G requests: cpu: '1' memory: 2Gi volumeMounts: - name: components mountPath: /components livenessProbe: httpGet: path: /health/liveness port: 9000 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 30 periodSeconds: 20 successThreshold: 1 failureThreshold: 30 readinessProbe: httpGet: path: /health/readiness port: 9000 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 30 periodSeconds: 20 successThreshold: 1 failureThreshold: 30 imagePullPolicy: IfNotPresent securityContext: capabilities: add: - SYS_PTRACE runAsUser: 0 runAsNonRoot: false allowPrivilegeEscalation: true restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst nodeSelector: kubernetes.cluedin.com/pooltype: processing serviceAccountName: cluedin-serviceaccount serviceAccount: cluedin-serviceaccount securityContext: {} imagePullSecrets: - name: acr-registry-key schedulerName: default-scheduler tolerations: - key: kubernetes.cluedin.com/pool operator: Equal value: processing effect: NoSchedule strategy: type: Recreate revisionHistoryLimit: 10 progressDeadlineSeconds: 600 . ",
    "url": "/paas-operations/config/dedicated-stream-processing-pod#deployment",
    
    "relUrl": "/paas-operations/config/dedicated-stream-processing-pod#deployment"
  },"1641": {
    "doc": "Create dedicated stream processing pod",
    "title": "Create dedicated stream processing pod",
    "content": " ",
    "url": "/paas-operations/config/dedicated-stream-processing-pod",
    
    "relUrl": "/paas-operations/config/dedicated-stream-processing-pod"
  },"1642": {
    "doc": "Connect DBT to CluedIn",
    "title": "Connecting dbt to CluedIn: A Step-by-Step Guide",
    "content": "This guide explains how to integrate dbt (Data Build Tool) with CluedIn so that the transformations, tests, and models in dbt flow seamlessly into CluedIn’s data quality and master data management (MDM) framework. ",
    "url": "/kb/dbt-to-cluedin#connecting-dbt-to-cluedin-a-step-by-step-guide",
    
    "relUrl": "/kb/dbt-to-cluedin#connecting-dbt-to-cluedin-a-step-by-step-guide"
  },"1643": {
    "doc": "Connect DBT to CluedIn",
    "title": "Why Connect dbt to CluedIn?",
    "content": ". | Centralized Data Quality: Push dbt test results into CluedIn to trigger alerts, dashboards, and Slack notifications. | Metadata Enrichment: CluedIn can ingest dbt lineage, schema, and model information for entity resolution and cataloging. | Golden Records Context: Enrich CluedIn golden records with dbt’s transformed data. | Closed Feedback Loop: Use CluedIn quality insights to inform dbt models, enabling proactive improvements. | . ",
    "url": "/kb/dbt-to-cluedin#why-connect-dbt-to-cluedin",
    
    "relUrl": "/kb/dbt-to-cluedin#why-connect-dbt-to-cluedin"
  },"1644": {
    "doc": "Connect DBT to CluedIn",
    "title": "Integration Architecture",
    "content": ". | dbt runs in your analytics stack (Snowflake, BigQuery, Redshift, etc.). | dbt artifacts (manifest.json, run_results.json, sources.json) are generated. | CluedIn ingests these artifacts or results via APIs, Connectors, or scheduled jobs. | . ",
    "url": "/kb/dbt-to-cluedin#integration-architecture",
    
    "relUrl": "/kb/dbt-to-cluedin#integration-architecture"
  },"1645": {
    "doc": "Connect DBT to CluedIn",
    "title": "Prerequisites",
    "content": ". | CluedIn: Access to ingestion pipelines and APIs. | dbt: Installed and configured with your warehouse. | Warehouse: Supported by both dbt and CluedIn (e.g., Snowflake, BigQuery, SQL Server). | Credentials: Service accounts/tokens with read access to dbt artifacts and write access to CluedIn. | . ",
    "url": "/kb/dbt-to-cluedin#prerequisites",
    
    "relUrl": "/kb/dbt-to-cluedin#prerequisites"
  },"1646": {
    "doc": "Connect DBT to CluedIn",
    "title": "Methods of Integration",
    "content": "1. Ingest dbt Models into CluedIn . | Configure dbt models to materialize into a schema that CluedIn can connect to. Example in dbt_project.yml: ```yaml models: my_project: marts: schema: cluedin_ready | . ",
    "url": "/kb/dbt-to-cluedin#methods-of-integration",
    
    "relUrl": "/kb/dbt-to-cluedin#methods-of-integration"
  },"1647": {
    "doc": "Connect DBT to CluedIn",
    "title": "Connect DBT to CluedIn",
    "content": " ",
    "url": "/kb/dbt-to-cluedin",
    
    "relUrl": "/kb/dbt-to-cluedin"
  },"1648": {
    "doc": "Create application user for Dataverse connector",
    "title": "Create application user for Dataverse connector",
    "content": "This article outlines how to create an application user in Power Apps in order to use it for Dataverse connector. Prerequisites: Make sure you have an existing Power Apps account. To create an application user . | Sign in to your Power Apps account and make sure you are in the intended environment. | Select Power Platform &gt; Power Platform Admin Center. | Select Environments, and then select your intended environment. | At the top of the page, select Settings. | Expand the Users + permissions dropdown, and then select Application users. | At the top of the page, select New app user. | In the Create a new app user pane, do the following: . | Select Add an app, and then select the app registration (service principal) created earlier. This must be the app registration that you use to get Client ID, Tenant ID, and Client Secret for Dataverse connector configuration. | Enter the Business unit. | In the Security roles field, select the pencil icon, and then enter the security role created earlier. Also, you need to enter the System Administrator role to the new app user. The Security roles field must contain two roles. | Select Create. | . As a result, the new application user is created. | . ",
    "url": "/consume/export-targets/create-application-user",
    
    "relUrl": "/consume/export-targets/create-application-user"
  },"1649": {
    "doc": "Knowledge Graph",
    "title": "On this page",
    "content": ". | Add Knowledge Graph enricher | Properties from Knowledge Graph enricher | . This article outlines how to configure the Knowledge Graph enricher. The purpose of this enricher is to provide a description of an organization. More details can be found in Properties from Knowledge Graph enricher. The Knowledge Graph enricher supports the following endpoint: . | https://kgsearch.googleapis.com?query={nameOrUri}&amp;key={apiKey}&amp;limit=10&amp;indent=true, where {nameOrUri} is the name or website of the organization and {apiKey} is your API key for accessing Google’s Knowledge Graph database. | . ",
    "url": "/preparation/enricher/knowledge-graph#on-this-page",
    
    "relUrl": "/preparation/enricher/knowledge-graph#on-this-page"
  },"1650": {
    "doc": "Knowledge Graph",
    "title": "Add Knowledge Graph enricher",
    "content": "To use the Knowledge Graph enricher, you must provide the API key. To get the API key, follow the instructions here. The enricher uses the organization name or website to retrieve information from Google’s Knowledge Graph database. To add Knowledge Graph enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Knowledge Graph, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API key for accessing Google’s Knowledge Graph database. | Accepted Business Domain – enter the business domain to define which golden records will be enriched using the Knowledge Graph enricher. | Organization Name Vocabulary Key – enter the vocabulary key that contains the names of organizations that will be used for searching the Knowledge Graph database. | Website Vocabulary Key – enter the vocabulary key that contains the websites of organizations that will be used for searching the Knowledge Graph database. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Knowledge Graph enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Knowledge Graph enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided to configure the enricher. | . ",
    "url": "/preparation/enricher/knowledge-graph#add-knowledge-graph-enricher",
    
    "relUrl": "/preparation/enricher/knowledge-graph#add-knowledge-graph-enricher"
  },"1651": {
    "doc": "Knowledge Graph",
    "title": "Properties from Knowledge Graph enricher",
    "content": "You can find the properties added to golden records from the Knowledge Graph enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Knowledge Graph enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Knowledge Graph enricher. | Display name | Vocabulary key | . | Description | knowledgeGraph.organization.description | . | Description Body | knowledgeGraph.organization.detailedDescriptionBody | . | Detailed Description License | knowledgeGraph.organization.detailedDescriptionLicense | . | Detailed Description Url | knowledgeGraph.organization.detailedDescriptionUrl | . | Url | knowledgeGraph.organization.url | . ",
    "url": "/preparation/enricher/knowledge-graph#properties-from-knowledge-graph-enricher",
    
    "relUrl": "/preparation/enricher/knowledge-graph#properties-from-knowledge-graph-enricher"
  },"1652": {
    "doc": "Knowledge Graph",
    "title": "Knowledge Graph",
    "content": " ",
    "url": "/preparation/enricher/knowledge-graph",
    
    "relUrl": "/preparation/enricher/knowledge-graph"
  },"1653": {
    "doc": "Review logs in Log Analytics workspace",
    "title": "On this page",
    "content": ". | Retrieve logs | Example of retrieving logs for cluedin-server pod | Service name to pod name mapping | . In this article, you will learn how to review logs from all CluedIn services in your Log Analytics workspace. ",
    "url": "/paas-operations/review-logs-in-log-analytics-workspace#on-this-page",
    
    "relUrl": "/paas-operations/review-logs-in-log-analytics-workspace#on-this-page"
  },"1654": {
    "doc": "Review logs in Log Analytics workspace",
    "title": "Retrieve logs",
    "content": ". | In the Azure portal, go to your Log Analytics workspace. | On the left navigation pane of the Log Analytics workspace, select Logs. Then, close the Queries hub pop-up window. This will open the query editor where you can run queries to retrieve logs. | Run the following query in the Log Analytics workspace. Replace {name of service} with the appropriate service name from the mapping reference table. KubePodInventory | where ServiceName == {name of service} | distinct ContainerID | join( ContainerLog | project ContainerID, LogEntry, TimeGenerated ) on ContainerID | order by TimeGenerated asc | project TimeGenerated, LogEntry . This query retrieves logs for the specified service, orders them chronologically, and displays the log entries along with their timestamps. | . ",
    "url": "/paas-operations/review-logs-in-log-analytics-workspace#retrieve-logs",
    
    "relUrl": "/paas-operations/review-logs-in-log-analytics-workspace#retrieve-logs"
  },"1655": {
    "doc": "Review logs in Log Analytics workspace",
    "title": "Example of retrieving logs for cluedin-server pod",
    "content": "If you want to retrieve logs for the cluedin-server pod, do the following: . | Identify the corresponding service name from the mapping reference table: cluedin-clean-auth. | Run the following query in the Log Analytics workspace: . KubePodInventory | where ServiceName == \"cluedin-clean-auth\" | distinct ContainerID | join( ContainerLog | project ContainerID, LogEntry, TimeGenerated ) on ContainerID | order by TimeGenerated asc | project TimeGenerated, LogEntry . This query will return logs related to the cluedin-server pod. | . ",
    "url": "/paas-operations/review-logs-in-log-analytics-workspace#example-of-retrieving-logs-for-cluedin-server-pod",
    
    "relUrl": "/paas-operations/review-logs-in-log-analytics-workspace#example-of-retrieving-logs-for-cluedin-server-pod"
  },"1656": {
    "doc": "Review logs in Log Analytics workspace",
    "title": "Service name to pod name mapping",
    "content": "| Service name | Pod name | . | cluedin-haproxy-ingress | cluedin-haproxy-ingress | . | cluedin-libpostal | cluedin-libpostal | . | cluedin-clean-auth | cluedin-server | . | cluedin-server-processing | cluedin-server-processing | . | cluedin-datasource | cluedin-datasource | . | cluedin-datasource-processing | cluedin-datasource-processing | . | cluedin-datasource-submitter | cluedin-datasource-submitter | . | cluedin-gql | cluedin-gql | . | cluedin-elasticsearch | cluedin-elasticsearch | . | cluedin-rabbitmq | cluedin-rabbitmq | . | cluedin-controller | cluedin-controller | . | cluedin-neo4j | cluedin-neo4j | . | cluedin-sqlserver | cluedin-sqlserver | . | cluedin-openrefine | cluedin-openrefine | . ",
    "url": "/paas-operations/review-logs-in-log-analytics-workspace#service-name-to-pod-name-mapping",
    
    "relUrl": "/paas-operations/review-logs-in-log-analytics-workspace#service-name-to-pod-name-mapping"
  },"1657": {
    "doc": "Review logs in Log Analytics workspace",
    "title": "Review logs in Log Analytics workspace",
    "content": " ",
    "url": "/paas-operations/review-logs-in-log-analytics-workspace",
    
    "relUrl": "/paas-operations/review-logs-in-log-analytics-workspace"
  },"1658": {
    "doc": "Modify an Integration",
    "title": "Modify an Integration",
    "content": "There will be many times where you will need to change an integration. This may be due to changes in the source, changes in versions or fixing mistakes. There are some situations where you may need to cleanup changes. These include: . | You change the name of a Vocabulary Key | You need to remove edges | You need to change existing edges | You need to remove Vocabularies | . Due to CluedIn being a append-only system (with support for deleting if necessary) it means that certain changes require cleanup. For removing or changing edges, you can use the “ObsoleteSince” extension method to instruct to CluedIn that since a particular Version if your Crawlers, you had an edge, and after the case you need to remove or change the data in the edge. If you do this, this CluedIn will do the cleanup for you. You can also perform this using Post Processors. If you decide to do it in your CluedIn Crawlers then it means that your Crawlers might become a bit harder to manage. If you solve it in Post Processing, then you make sure that your crawlers always stay business-logic-agnostic. For changing or removing Vocabulary Key Names, the same method applies. There is the “ObsoleteSince” extension methods available on the VocabulayKey class which will allow you to instruct CluedIn to cleanup the mistakes. ",
    "url": "/integration/modify-integrations",
    
    "relUrl": "/integration/modify-integrations"
  },"1659": {
    "doc": "Origin",
    "title": "On this page",
    "content": ". | Merging records by identifiers | Linking golden records | Useful resources | . Generally, the origin determines the source of a golden record. So, when you map your data, the origin will be automatically set to the name of the data source, for example, MicrosoftDynamics, Oracle, Hubspot, or MsSQLDatabase5651651. However, you can change the origin during mapping if needed. As mentioned in our Identifiers reference article, the origin is used in the primary identifier and the identifiers. In this article, we’ll explain the usage of the origin in two important processes in CluedIn: . | Merging records by identifiers . | Linking golden records . | . ",
    "url": "/key-terms-and-features/origin#on-this-page",
    
    "relUrl": "/key-terms-and-features/origin#on-this-page"
  },"1660": {
    "doc": "Origin",
    "title": "Merging records by identifiers",
    "content": "Since the origin is used in the primary identifier and the identifiers, it plays a role in merging—when 2 identifiers are identical, the records will merge together. To understand the role of origin in merging, suppose you have an attribute that you can safely rely on to merge records across source systems. Let’s say this attribute is a SerialNumber that is used in your CRM, ERP, and Support systems. As the serial number is unique and cross-system, you can use it to merge together all golden records that have the same serial number. Of course, you can achieve this using our UI; however, there is a faster way to do this via merging by identifiers. Let’s consider the example of three records, each coming from a different source system—CRM, ERP, and Support. For each data source, we select the Serial Number to produce the primary identifier. The following table shows the identifiers that will be produced by default. | Source | Business domain | Origin | Primary identifier | . | CRM | Product | crm | /Product#crm:[SERIAL NUMBER VALUE] | . | ERP | Product | erp | /Product#erp:[SERIAL NUMBER VALUE] | . | Support System | Product | support | /Product#support:[SERIAL NUMBER VALUE] | . Even if the serial number is the same, the records will not merge together as the origin of each record is different. So, how would you use the serial number to merge records together? The answer is by producing an identifier that shares the same origin, for example, PRODUCT-SERIALNUMBER. As a result, the identifier for each record will share the same business domain, origin, and the value of serial number as shown in the following table. | Source | Business domain | Origin | Primary identifier | . | CRM | Product | PRODUCT-SERIALNUMBER | /Product#PRODUCT-SERIALNUMBER:[SERIAL NUMBER VALUE] | . | ERP | Product | PRODUCT-SERIALNUMBER | /Product#PRODUCT-SERIALNUMBER:[SERIAL NUMBER VALUE] | . | Support System | Product | PRODUCT-SERIALNUMBER | /Product#PRODUCT-SERIALNUMBER:[SERIAL NUMBER VALUE] | . Since the origin is shared among different sources, each time the same serial number for a product is sent to CluedIn, it will be merged. ",
    "url": "/key-terms-and-features/origin#merging-records-by-identifiers",
    
    "relUrl": "/key-terms-and-features/origin#merging-records-by-identifiers"
  },"1661": {
    "doc": "Origin",
    "title": "Linking golden records",
    "content": "Origin can be used to link golden records together to create relationship. You can link golden records using identifiers, rules, or manually in the UI. To create a relationship using identifiers, you need to know the origin of target golden records. These are the golden records to which you want to link current records. Suppose you have Contact records that contain the companyID property, and you know that you have Company records with this ID. To establish a link between Contact and Company, you need to define the “to” relationship by setting up the following: . | Business domain: /Company | Origin: [ORIGIN-OF-COMPANY-RECORDS] | Value: Company ID | . The combination of those 3 values needs to match one of the identifiers of target records. To make the process of linking golden records easier, you can use the recommendation for defining the origin that we provided in Merging by identifiers. Essentially, the method of shared origin that you use for merging by identifiers can also be used to facilitate the process of linking golden records. This way you do not have to rely on the source system and instead use the origin that you defined for related data. ",
    "url": "/key-terms-and-features/origin#linking-golden-records",
    
    "relUrl": "/key-terms-and-features/origin#linking-golden-records"
  },"1662": {
    "doc": "Origin",
    "title": "Useful resources",
    "content": ". | Business domain . | Identifiers . | Review mapping . | . ",
    "url": "/key-terms-and-features/origin#useful-resources",
    
    "relUrl": "/key-terms-and-features/origin#useful-resources"
  },"1663": {
    "doc": "Origin",
    "title": "Origin",
    "content": " ",
    "url": "/key-terms-and-features/origin",
    
    "relUrl": "/key-terms-and-features/origin"
  },"1664": {
    "doc": "Sync data products",
    "title": "On this page",
    "content": ". | Preparation . | Preparation in Purview | Preparation in CluedIn | . | Feature overview | . In this article, you will learn how to sync Purview data products and data assets into CluedIn data sources. This feature works only if Azure Data Factory (ADF) automation is enabled and configured. For more information, see Azure Data Factory pipeline automation. ",
    "url": "/microsoft-integration/purview/sync-data-products#on-this-page",
    
    "relUrl": "/microsoft-integration/purview/sync-data-products#on-this-page"
  },"1665": {
    "doc": "Sync data products",
    "title": "Preparation",
    "content": "To sync Purview data products and data assets into CluedIn data sources, complete 2 preparation steps: . | Prepare a data product in Purview – create a governance domain and a glossary term to act a filter for the data products you want to sync, create a data product and add data assets and a glossary term to it, publish the prepared resources, and finally assign the appropriate roles to Purview service principal in the governance domain. | Configure settings in CluedIn – enable the sync data products feature in Purview settings and provide the glossary term to identify the data products for syncing. | . Preparation in Purview . To create a governance domain . | In the Microsoft Purview portal, navigate to Unified Catalog &gt; Catalog management &gt; Governance domain. | Select new governance domain. | Enter the Name of the governance domain. | Enter the Description of the governance domain. | Select the Type of the governance domain. | Select Save. | . The governance domain should have a dedicated glossary term that acts as a filter for the data products you want to sync. To create a glossary term for a governance domain . | On the governance domain details page, in the glossary terms card, select View all. | Select New term. | Enter the Name and Description of the term. | Select Create. | . When the glossary term is created, note two methods for obtaining its identification: . | The name of the glossary term. | The ID of the glossary term, which can be found in the URL. | . To create data products . | On the governance domain details page, in the data products card, select Go to data products. | Select New data product. | Enter the Name and Description of the data product. | In Type, select Master data and reference data. | Select Next. | Enter the Use cases for the data product. | In Next steps, select Add data assets. | Select Done. | . To add data assets to the data product . | On the data product details page, expand the Add data assets dropdown list, and then select Find and select. | Find and select the data asset that you want to add to the data product. | Select Add. As a result, the data assets are added to the data product. | . To add a glossary term to the data product . | On the data product details page, in the Glossary terms section, select Add. | Find and select the glossary term that you want to add to the data product. | Select Add. As a result, the glossary term is added to the data product. | . To publish a governance domain . | On the governance domain details page, select Publish. After successful publishing, the status of the governance domain changes to Successful. | . To publish a glossary term . | On the governance domain details page, in the glossary terms card, select View all. | Select the glossary term that you created before. | On the glossary term detail page, select Publish. | . To publish a data product . | On the governance domain details page, in the data products card, select Go to data products. | Select the data product that you created before. | On the data product detail page, select Publish. | . To assign roles to service principal . | On the governance domain details page, go to the Roles tab. | Find the Data Catalog Reader role, and then select the icon next to the role name. | Find and select the Purview service principal. | Select Save. | Find the Data Product Owners role, and then select the icon next to the role name. | Find and select the Purview service principal. | Select Save. | . Preparation in CluedIn . | In CluedIn, go to Administration &gt; Settings, and then scroll down to find the Purview section. | Turn on the toggle in Sync Data Products DataSources. | In Sync Data Products Term Pattern, enter the identification of the glossary term that is associated with the governance domain that you want to sync. | If you want to automatically add the asset that has been already synced to CluedIn to the list of data assets of a specific data product, turn on the toggle in Append Asset to Data Product. | Select Save. Once you save the changes, synchronization begins. | . ",
    "url": "/microsoft-integration/purview/sync-data-products#preparation",
    
    "relUrl": "/microsoft-integration/purview/sync-data-products#preparation"
  },"1666": {
    "doc": "Sync data products",
    "title": "Feature overview",
    "content": "Once you enable synchronization of data products, you will receive a notification when the data product is synced. Additionally, you will receive notifications about the execution of ADF automation pipelines, which create ingestion endpoints in CluedIn and ingest the data from data assets. How to check the ingested data in CluedIn? . As a result of pipeline run, a new data source group is created in CluedIn. The data source group corresponds to Purview data product, and the data sources within the group correspond to data assets within the data product. After you create the mapping and process the data set, it will be synced to Purview. As a result, you can view a visual representation of an asset within the CluedIn processing pipeline. Since we enabled Append Asset to Data Product, a new asset is created in the data product to represent the entity type that was created in CluedIn. ",
    "url": "/microsoft-integration/purview/sync-data-products#feature-overview",
    
    "relUrl": "/microsoft-integration/purview/sync-data-products#feature-overview"
  },"1667": {
    "doc": "Sync data products",
    "title": "Sync data products",
    "content": " ",
    "url": "/microsoft-integration/purview/sync-data-products",
    
    "relUrl": "/microsoft-integration/purview/sync-data-products"
  },"1668": {
    "doc": "Common questions",
    "title": "On this page",
    "content": ". | Cluster overview | Security | Pod and container management | Networking | . In this article, we address some of the most common questions related to CluedIn PaaS installation and configuration. ",
    "url": "/paas-operations/common-questions#on-this-page",
    
    "relUrl": "/paas-operations/common-questions#on-this-page"
  },"1669": {
    "doc": "Common questions",
    "title": "Cluster overview",
    "content": "How is the AKS cluster configured? . CluedIn is shipped with a pre-defined configuration based on the selected plan. Since this configuration has been thoroughly tested for the number of records associated with each plan, auto-scaling in not enabled by default. However, CluedIn does support horizontal auto-scaling. So, if you need more power to process faster, we can enable this. If you have any questions about cluster sizing, we recommend discussing them with one of our cloud architects. They can provide better advice and help you avoid unnecessary costs. How many pods and containers are deployed for CluedIn? . The number of pods depends on the size of your instance and whether auto-scaling is enabled. A typical production instance runs around 100 pods. CluedIn is composed by 5 main containers: . | UI | GQL | Data Source | Data Source Processing | Submitter | Server | . The server can operate across multiple modes. For example, it can be setup to be a Web API, a processing box, or a streaming box. By default, your configuration will have 2 pods running as a Web API and 2 pods running as a processing box. Generally, this configuration is sufficient for 90% of the use cases we face. The following screenshot shows an example of nodes running in production. What are the key workloads running in CluedIn? . The main workload in CluedIn is the server processing that requires large number of CPU and memory. ",
    "url": "/paas-operations/common-questions#cluster-overview",
    
    "relUrl": "/paas-operations/common-questions#cluster-overview"
  },"1670": {
    "doc": "Common questions",
    "title": "Security",
    "content": "Is RBAC enabled, and how is access to the cluster managed? . By default, RBAC is not configured. However, Just-In-Time access or RBAC can be implemented if requested before the installation process. This needs to be discussed with our Operations team. We have been installing CluedIn in a variety of setups, so customizing access to the cluster is something we do regularly. However, this needs to be done with caution, as less access can lead to less managed instance. Are containers running with least privilege? . Yes, the containers are running with the non-root user privilege. How are secrets and sensitive data managed? . Secrets and sensitive data are managed using Azure Key Vault integration, which ensures secure storage and access control. ",
    "url": "/paas-operations/common-questions#security",
    
    "relUrl": "/paas-operations/common-questions#security"
  },"1671": {
    "doc": "Common questions",
    "title": "Pod and container management",
    "content": "Are resource requests and limits (CPU/memory) configured for pods? . Yes, the CPU and memory of the pods are limited by our Helm charts to ensure a pod does not exceed its allocated resources. However, please do not modify these values without consulting us first, as they are critical for the stable operation of the cluster. Are the containers sourced from a trusted registry and scanned for vulnerabilities? . We have a non-public ACR registry that you will have access to, and we use Blackduck to scan all our containers on a release-by-release basis. ",
    "url": "/paas-operations/common-questions#pod-and-container-management",
    
    "relUrl": "/paas-operations/common-questions#pod-and-container-management"
  },"1672": {
    "doc": "Common questions",
    "title": "Networking",
    "content": "Are network policies in place to control pod-to-pod communication? . CluedIn does not provide any policy for pod-to-pod communication out of the box. However, we have applied HTTPS communications between services for some customers. We do not advise setting this up, as the cluster should run within its own network, meaning that traffic from one pod to another would not be accessible outside the cluster. ",
    "url": "/paas-operations/common-questions#networking",
    
    "relUrl": "/paas-operations/common-questions#networking"
  },"1673": {
    "doc": "Common questions",
    "title": "Common questions",
    "content": " ",
    "url": "/paas-operations/common-questions",
    
    "relUrl": "/paas-operations/common-questions"
  },"1674": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Push data from Snowflake to CluedIn Ingestion Endpoints",
    "content": "This guide shows practical patterns for pushing data directly from Snowflake into CluedIn Ingestion Endpoints using either a Snowpark Python stored procedure (batch) or a Snowflake External Function (event/row-oriented). It follows CluedIn’s recommended practices around payload shape, batching, idempotency, and operational monitoring. ",
    "url": "/kb/snowflake-to-cluedin#push-data-from-snowflake-to-cluedin-ingestion-endpoints",
    
    "relUrl": "/kb/snowflake-to-cluedin#push-data-from-snowflake-to-cluedin-ingestion-endpoints"
  },"1675": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "When to use this",
    "content": "Use this guide if you already have mastered data in Snowflake (curated views/tables) and want to push it into CluedIn for entity creation/enrichment, rather than having CluedIn pull from Snowflake. ",
    "url": "/kb/snowflake-to-cluedin#when-to-use-this",
    
    "relUrl": "/kb/snowflake-to-cluedin#when-to-use-this"
  },"1676": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Prerequisites",
    "content": ". | CluedIn . | An Ingestion Endpoint created (from Ingestion → Endpoints in your CluedIn UI). | The endpoint URL and an API token (or tenant key) with rights to ingest. | . | Snowflake . | A warehouse you can use for procedures/functions. | Role privileges to create procedures/functions, tasks, integrations. | (For outbound HTTP) External Network Access is enabled and allowed to reach your CluedIn domain. | Optional but recommended: Snowflake Secrets to store the CluedIn token. | . | . Note Every CluedIn environment exposes an HTTPS ingestion URL. The exact path format can differ by version/tenant. In the examples below, replace https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt; and CLUE_TOKEN with your actual values from the CluedIn UI. ",
    "url": "/kb/snowflake-to-cluedin#prerequisites",
    
    "relUrl": "/kb/snowflake-to-cluedin#prerequisites"
  },"1677": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Data model &amp; payloads",
    "content": "CluedIn endpoints accept JSON (and typically NDJSON for high-volume). Send one JSON object per record. Include a stable identifier and a last-modified timestamp whenever possible. Minimal recommended shape (per line for NDJSON): . { \"externalId\": \"CUST-000123\", \"type\": \"Customer\", \"timestamp\": \"2025-08-20T03:14:07Z\", \"properties\": { \"name\": \"Acme Pty Ltd\", \"country\": \"AU\", \"email\": \"ap@acme.example\", \"isActive\": true, \"updatedAt\": \"2025-08-20T03:14:07Z\" } } . Tips . | Idempotency: CluedIn deduplicates/merges best when externalId is stable for the same logical entity. | Upserts: Always send your system-of-record “last changed” timestamp so CluedIn can reason about freshness. | Batching: Prefer 500–5,000 records per POST (NDJSON), gzip-compressed. | Headers: Authorization: Bearer &lt;token&gt;, Content-Type: application/x-ndjson, Content-Encoding: gzip (if gzipping). | . ",
    "url": "/kb/snowflake-to-cluedin#data-model--payloads",
    
    "relUrl": "/kb/snowflake-to-cluedin#data-model--payloads"
  },"1678": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Option A — Batch push with Snowpark Python (stored procedure)",
    "content": "This pattern is ideal for scheduled loads or micro-batching (minutes). 1) (Once) Allow Snowflake to call CluedIn over HTTPS . Ask your Snowflake admin to: . | Create a NETWORK RULE for your CluedIn hostname. | Create an EXTERNAL ACCESS INTEGRATION that references that rule. | (Recommended) Create a SECRET that stores the CluedIn API token. | . If you can’t use Secrets yet, you can pass the token as a procedure argument for initial testing, then switch to Secrets. 2) Create the stored procedure . CREATE OR REPLACE PROCEDURE CLUEDIN_PUSH( endpoint STRING, -- e.g. 'https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt;' api_token STRING, -- for production, read from a Snowflake Secret sql_query STRING, -- SELECT ... that returns the rows to push batch_size NUMBER DEFAULT 1000 ) RETURNS VARCHAR LANGUAGE PYTHON RUNTIME_VERSION = 3.10 PACKAGES = ('snowflake-snowpark-python','requests') -- If your account requires it, include the integration name below: EXTERNAL_ACCESS_INTEGRATIONS = ('&lt;YOUR_EXTERNAL_ACCESS_INTEGRATION&gt;') HANDLER = 'run' AS $$ import io, json, gzip, time import requests from snowflake.snowpark import Session def _iter_rows(session, query): # Streams rows without collecting the full DataFrame in memory for row in session.sql(query).to_local_iterator(): yield row.asDict() def _batched(iterable, size): batch = [] for item in iterable: batch.append(item) if len(batch) &gt;= size: yield batch batch = [] if batch: yield batch def _post_ndjson(endpoint, token, records, timeout=30): lines = [json.dumps(r, default=str) for r in records] raw = (\"\\n\".join(lines)).encode(\"utf-8\") gz = io.BytesIO() with gzip.GzipFile(fileobj=gz, mode=\"wb\") as f: f.write(raw) headers = { \"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/x-ndjson\", \"Content-Encoding\": \"gzip\", \"Accept\": \"application/json\", } resp = requests.post(endpoint, data=gz.getvalue(), headers=headers, timeout=timeout) if resp.status_code &gt;= 300: raise Exception(f\"CluedIn ingest failed: {resp.status_code} {resp.text[:500]}\") return resp def run(session: Session, endpoint: str, api_token: str, sql_query: str, batch_size: int = 1000) -&gt; str: total = 0 for batch in _batched(_iter_rows(session, sql_query), int(batch_size)): # Basic, polite retry on 429/5xx for attempt in range(5): try: r = _post_ndjson(endpoint, api_token, batch) break except Exception as ex: if attempt == 4: raise time.sleep(2 ** attempt) # backoff total += len(batch) return f\"Pushed {total} records to CluedIn\" $$; . Quick test . CALL CLUEDIN_PUSH( 'https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt;', '&lt;CLUE_TOKEN&gt;', $$SELECT CUSTOMER_ID AS externalId, 'Customer' AS type, UPDATED_AT AS timestamp, OBJECT_CONSTRUCT( 'name', NAME, 'country', COUNTRY, 'email', EMAIL, 'isActive', IS_ACTIVE, 'updatedAt', UPDATED_AT ) AS properties FROM ANALYTICS.DIM_CUSTOMER WHERE UPDATED_AT &gt;= DATEADD(day,-1,current_timestamp())$$, 1000 ); . 3) Schedule it with a Snowflake Task (example daily 01:00 AEST) . CREATE OR REPLACE TASK CLUEDIN_PUSH_DAILY WAREHOUSE = &lt;YOUR_WH&gt; SCHEDULE = 'USING CRON 0 1 * * * Australia/Brisbane' AS CALL CLUEDIN_PUSH( 'https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt;', '&lt;CLUE_TOKEN&gt;', $$SELECT /* your incremental extract */ * FROM ... $$, 2000 ); -- Enable the task ALTER TASK CLUEDIN_PUSH_DAILY RESUME; . Production hardening Replace the literal token with a Snowflake Secret and read it in the procedure (or pass it via a secure UDF argument). Keep batch sizes modest, enable backoff/retry, and consider watermarking with a control table to ensure exactly-once semantics for each run. ",
    "url": "/kb/snowflake-to-cluedin#option-a--batch-push-with-snowpark-python-stored-procedure",
    
    "relUrl": "/kb/snowflake-to-cluedin#option-a--batch-push-with-snowpark-python-stored-procedure"
  },"1679": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Option B — Row-oriented push with External Function",
    "content": "Use this when you want to call CluedIn inline from SQL (e.g., as part of a pipeline or upon changes). Because Snowflake External Functions call through a managed API gateway, we proxy the request via a lightweight serverless function. Architecture . Snowflake SQL → External Function → (API Gateway + Lambda/Azure Function) → CluedIn Ingestion Endpoint . 1) Create a tiny relay function (example: Node.js) . AWS Lambda (sketch): . // Sends JSON payload received from Snowflake to CluedIn export const handler = async (event) =&gt; { const payload = JSON.parse(event.body || \"{}\"); // Snowflake sends JSON const resp = await fetch(process.env.CLUE_ENDPOINT, { method: \"POST\", headers: { \"Authorization\": `Bearer ${process.env.CLUE_TOKEN}`, \"Content-Type\": \"application/json\", \"Accept\": \"application/json\" }, body: JSON.stringify(payload) }); const text = await resp.text(); return { statusCode: resp.status, body: text }; }; . Store CLUE_TOKEN and CLUE_ENDPOINT as function secrets. In Azure, create the same with an HTTP-triggered Function App. 2) Register the External Function in Snowflake . Create an API INTEGRATION (one-time) and then: . CREATE OR REPLACE EXTERNAL FUNCTION PUSH_TO_CLUEDIN(PAYLOAD VARIANT) RETURNS VARIANT API_INTEGRATION = &lt;YOUR_API_INTEGRATION&gt; AS 'https://&lt;your-api-gateway-endpoint&gt;'; . 3) Call it from SQL . -- Push single object per call SELECT PUSH_TO_CLUEDIN( OBJECT_CONSTRUCT( 'externalId', CUSTOMER_ID, 'type', 'Customer', 'timestamp', UPDATED_AT, 'properties', OBJECT_CONSTRUCT( 'name', NAME, 'country', COUNTRY, 'email', EMAIL, 'isActive', IS_ACTIVE ) ) ) FROM ANALYTICS.DIM_CUSTOMER WHERE UPDATED_AT &gt;= DATEADD(minute, -15, CURRENT_TIMESTAMP()); . Tip For higher throughput, wrap multiple rows into an array and let your relay function POST NDJSON to CluedIn in batches. ",
    "url": "/kb/snowflake-to-cluedin#option-b--row-oriented-push-with-external-function",
    
    "relUrl": "/kb/snowflake-to-cluedin#option-b--row-oriented-push-with-external-function"
  },"1680": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Validate in CluedIn",
    "content": "After a push: . | Open Ingestion → Endpoints and select your endpoint. | Check the recent requests, throughput, and errors (4xx/5xx). | Navigate to Ingestion → Processing (or your entity overview) to verify created/updated entities. | Review any schema mapping or business rule outcomes relevant to your entities. | . ",
    "url": "/kb/snowflake-to-cluedin#validate-in-cluedin",
    
    "relUrl": "/kb/snowflake-to-cluedin#validate-in-cluedin"
  },"1681": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Operational guidance",
    "content": ". | Batch sizing: Start with 1,000 records per batch. Increase while monitoring latency and server CPU. | Compression: Gzip NDJSON bodies; it drastically reduces egress cost and improves throughput. | Retries: Retry on HTTP 429/5xx with exponential backoff. Do not retry on 4xx without fixing the payload. | Watermarking: Track the max UPDATED_AT pushed in a control table to ensure incremental loads are correct. | Observability: Log run IDs, batch counts, response codes, and the CluedIn request correlation ID if provided. | Security: Never hard-code tokens. Prefer Snowflake Secrets or your cloud’s secret manager with short-lived tokens. | Schema drift: Add defensive guards (e.g., COALESCE/type casts) so payloads remain valid if upstream changes. | . ",
    "url": "/kb/snowflake-to-cluedin#operational-guidance",
    
    "relUrl": "/kb/snowflake-to-cluedin#operational-guidance"
  },"1682": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Troubleshooting",
    "content": "| Symptom | Likely cause | Fix | . | 403 Forbidden | Token missing/invalid | Ensure Authorization: Bearer &lt;token&gt; and token scope includes ingest. Rotate if expired. | . | 413 Payload Too Large | Batch too big | Reduce batch size or gzip; verify CluedIn’s configured max body size. | . | 429 Too Many Requests | Rate limiting | Implement backoff; add jitter; schedule during off-peak. | . | 5xx server errors | Temporary downstream issue | Retry with backoff; contact CluedIn support if persistent. | . | Entities not appearing | Wrong externalId/type or mapping | Verify payload shape and mapping rules; check processing logs in CluedIn. | . ",
    "url": "/kb/snowflake-to-cluedin#troubleshooting",
    
    "relUrl": "/kb/snowflake-to-cluedin#troubleshooting"
  },"1683": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Copy-paste snippets",
    "content": "Transform a table into NDJSON-friendly rows . SELECT CUSTOMER_ID AS externalId, 'Customer' AS type, UPDATED_AT AS timestamp, OBJECT_CONSTRUCT( 'name', NAME, 'country', COUNTRY, 'email', EMAIL, 'isActive', IS_ACTIVE ) AS properties FROM ANALYTICS.DIM_CUSTOMER; . Call the stored procedure with an ad-hoc filter . CALL CLUEDIN_PUSH( 'https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt;', '&lt;CLUE_TOKEN&gt;', $$SELECT * FROM ANALYTICS.DIM_CUSTOMER WHERE UPDATED_AT &gt;= '2025-08-01'$$, 1000 ); . Cron schedule (every 15 minutes) . CREATE OR REPLACE TASK CLUEDIN_PUSH_Q15 WAREHOUSE = &lt;YOUR_WH&gt; SCHEDULE = 'USING CRON */15 * * * * Australia/Brisbane' AS CALL CLUEDIN_PUSH('https://&lt;cluedin-host&gt;/&lt;ingestion-path&gt;', '&lt;CLUE_TOKEN&gt;', $$SELECT * FROM ANALYTICS.DIM_CUSTOMER WHERE UPDATED_AT &gt;= DATEADD(minute,-15,current_timestamp())$$, 2000); ALTER TASK CLUEDIN_PUSH_Q15 RESUME; . ",
    "url": "/kb/snowflake-to-cluedin#copy-paste-snippets",
    
    "relUrl": "/kb/snowflake-to-cluedin#copy-paste-snippets"
  },"1684": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "FAQs",
    "content": "Can I send CSV? Yes, but JSON/NDJSON is strongly recommended—JSON preserves types and nested structures that CluedIn can use for mapping and enrichment. What about deletes? Emit a tombstone signal (e.g., {\"externalId\": \"...\", \"type\": \"Customer\", \"deleted\": true, \"timestamp\": \"...\"}) and configure your downstream processing rules in CluedIn to interpret deletes. How do I handle nested relations? Send separate entity payloads (e.g., Customer, Address) with stable externalIds. If your CluedIn build supports edge creation via ingestion, include reference keys; otherwise, model relationships in subsequent enrichment steps. ",
    "url": "/kb/snowflake-to-cluedin#faqs",
    
    "relUrl": "/kb/snowflake-to-cluedin#faqs"
  },"1685": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Summary",
    "content": ". | For scheduled or micro-batch loads, use the Snowpark Python stored procedure to POST NDJSON to CluedIn with gzip, backoff, and watermarking. | For inline/row-oriented flows, use a Snowflake External Function that relays to CluedIn via a small serverless function. | Always send stable IDs, timestamps, and typed properties to make CluedIn’s unification and lineage work in your favor. | . ",
    "url": "/kb/snowflake-to-cluedin#summary",
    
    "relUrl": "/kb/snowflake-to-cluedin#summary"
  },"1686": {
    "doc": "Connect SnowFlake to CluedIn",
    "title": "Connect SnowFlake to CluedIn",
    "content": " ",
    "url": "/kb/snowflake-to-cluedin",
    
    "relUrl": "/kb/snowflake-to-cluedin"
  },"1687": {
    "doc": "Define data to ingest",
    "title": "Define data to ingest",
    "content": "In this article, you will explore various methods available for ingesting the data into CluedIn. You can ingest the data into CluedIn using the following methods: . | File – you can upload files in CSV, JSON, XLS, XLSX, and Parquet formats. You can upload up to 5 files at once, and the total file size should not exceed 1 GB. | Ingestion endpoint – you can push a JSON array to an HTTP endpoint created by CluedIn. | Database – you can set up a connection with your database and add the needed database tables to CluedIn. | Connector (also referred to as a provider or crawler) – you can configure a software component to enable seamless interaction and data exchange between different software applications or systems and CluedIn. Currently, the ability to use connectors in CluedIn is different than using the above-mentioned options. | Out-of-the-box integration – CluedIn supports out-of-the-box integration, but it is not covered in this section. | . Regardless of the data ingestion method you choose, the processing of data is the same across all sources. ",
    "url": "/integration/define-data-to-ingest",
    
    "relUrl": "/integration/define-data-to-ingest"
  },"1688": {
    "doc": "Edges",
    "title": "Edges",
    "content": "An edge is your way to be able to create relations between records. This relation will consist of a source and a target, or a “to” and a “from”. In addition to an edge, you will have an edge type. This edge type will explain the relation (for example, /WorksFor, /LivesWith). Each edge can also contain properties including a weight and a general property bag. Weight is used to indicate the strength of the relation between two golden records. This weight is used in the processing pipeline to help evaluate decisions. How can you pass data to another record through an edge? . There are many cases where you will denormalize references to other entities, but you do want to have some friendly values to reference these records. When you are creating your edges, you will notice that you can pass in extra properties on either the “from” reference or the “to” reference. Placing properties on these references will propagate these values onto the appropriate entity. For example, you created a clue for a company and it had a reference to a country via an ID. In this case, you would want to reference that country, but that country should bring back friendly values to the company clue so that it is easy to look at in the user interface. In this way, if you update the values in the country clue, then CluedIn will automatically update all references to this country. The properties are used only for passing vocabularies. By default, it will only copy these properties to the target reference when that target reference exists. What are the edge types? . Edges types are a way to determine the relations between data. This is typically in the structure of Object - Verb - Object. For example, John - works at - Lego. In CluedIn, edges can store properties such as weights and general metadata, but the main idea behind these edges is to describe the relation of two nodes for processing and querying purposes. In CluedIn, there are static edge types and dynamic edge types. Static edge types are your way to set an edge type based on known rules that will not change. All other edge types should be dynamic. It is always recommended to leave edges in crawlers as generic as possible and introduce new processors in the processing server to dynamically resolve generic edge types into specific ones. Imagine you have an edge type of “Works At” that you set statically in your crawlers - you can see that it has a temporal factor to it, in that you have no guarantee that this will always be “Works At”. Due to this, you can introduce new processors that would check other values e.g. A Job start and end date, and use this to dynamically change the edge type to “Worked At” if this person was ever to leave. ",
    "url": "/key-terms-and-features/edges",
    
    "relUrl": "/key-terms-and-features/edges"
  },"1689": {
    "doc": "File name patterns",
    "title": "File name patterns",
    "content": "In this article, you will learn about the file name patterns that you can use in the export target configuration to customize the output file names. CluedIn supports the following file name patterns: . | {DataTime} and {DataTime:formatString}. DataTime is in UTC, and formatString accepts formats available to the DateTime.ToString() method in C#. For more information, see Microsoft documentation: Standard date and time format strings and Custom date and time format strings. When using {DataTime} without formatString, it defaults to the o format (for example, 2024-07-05T03:02:57.2612933Z). | {StreamId} and {DataTime:formatString}. formatString accepts formats available to the Guid.ToString() method in C#. For more information, see Guid.ToString Method. When using {StreamId} without formatString, it defaults to D format (for example, ba4afc12-f6dc-4394-b9d5-68f6dacf3b3b). | {OutputFormat} and {OutputFormat:formatString}. formatString accepts the following methods: . | ToUpperInvariant | ToUpper (equivalent to ToUpperInvariant) | ToLower | ToUpperInvariant (equivalent to ToLowerInvariant) | . When using {OutputFormat} without formatString, no extra formatting is performed (for example, csv, parquet, json). | {ContainerName} and {ContainerName:formatString}. {ContainerName} uses the value in the Target Name of the stream. formatString accepts the following methods: . | ToUpperInvariant | ToUpper (equivalent to ToUpperInvariant) | ToLower | ToLowerInvariant (equivalent to ToLowerInvariant) | . | . Example filename patterns . | File name pattern | Example output | . | {StreamId:N}_{DataTime:yyyyMMddHHmmss}.{OutputFormat} | ba4afc12f6dc4394b9d568f6dacf3b3b_20240705030355.parquet | . | {StreamId}_{DataTime}.{OutputFormat:ToUpper} | ba4afc12-f6dc-4394-b9d5-68f6dacf3b3b_2024-07-05T03:02:57.2612933Z.PARQUET | . | {ContainerName}_{DataTime:yyyyMMddHHmmss}.{OutputFormat} | CustomerRecord_20240705030355.parquet | . Default file name patterns . | Connector | Default file name pattern | . | Azure Data Lake connector | {StreamId:D}_{DataTime:yyyyMMddHHmmss}.{OutputFormat} | . | OneLake connector | {StreamId:N}_{DataTime:yyyyMMddHHmmss}.{OutputFormat} | . ",
    "url": "/consume/export-targets/file-name-patterns",
    
    "relUrl": "/consume/export-targets/file-name-patterns"
  },"1690": {
    "doc": "Libpostal",
    "title": "On this page",
    "content": ". | Add Libpostal enricher | Properties from Libpostal enricher | . This article outlines how to configure the Libpostal enricher. The purpose of this enricher is to parse and normalize street addresses from around the world using statistical NLP and open data. More details can be found in Properties from Libpostal enricher. The Libpostal enricher supports the following endpoint: . | http://&lt;host&gt;:&lt;port&gt;/parser body = {query: {address}} | . ",
    "url": "/preparation/enricher/libpostal#on-this-page",
    
    "relUrl": "/preparation/enricher/libpostal#on-this-page"
  },"1691": {
    "doc": "Libpostal",
    "title": "Add Libpostal enricher",
    "content": "The Libpostal enricher uses the address as input to parse and normalize the street address used in a golden record. You can use this enricher to parse and normalize street addresses for organizations, users, persons, and locations. Depending on the business domain you specify in the enricher configuration, you will need to provide the appropriate vocabulary key that contains the address. If you don’t provide the vocabulary key, CluedIn will use the following vocabulary keys by default: . | Person Address Vocab Key - person.home.address . | Organization Address Vocab Key - organization.address . | User Address Vocab Key - user.home.address . | Location Address Vocab Key - location.fullAddress . | . To add the Libpostal enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Libpostal, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched using the Libpostal enricher. Depending on the business domain that you provide, you need to fill out one more field to define the vocabulary key that contains addresses of golden records that you want to enrich. | Person Address Vocabulary Key – if you entered /Person as the accepted business domain, enter the vocabulary key that contains the home addresses of persons that you want to enrich. | Organization Address Vocabulary Key – if you entered /Organization as the accepted business domain, enter the vocabulary key that contains the addresses of organizations that you want to enrich. | User Address Vocabulary Key – if you entered /User as the accepted business domain, enter the vocabulary key that contains the addresses of users that you want to enrich. | Location Address Vocabulary Key – if you entered /Location as the accepted business domain, enter the vocabulary key that contains the addresses of locations that you want to enrich. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Libpostal enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Libpostal enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/libpostal#add-libpostal-enricher",
    
    "relUrl": "/preparation/enricher/libpostal#add-libpostal-enricher"
  },"1692": {
    "doc": "Libpostal",
    "title": "Properties from Libpostal enricher",
    "content": "You can find the properties added to golden records from the Libpostal enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Libpostal enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Libpostal enricher. | Display name | Vocabulary key | . | Category | libpostal.location.Category | . | City | libpostal.location.City | . | City_district | libpostal.location.City_district | . | Country | libpostal.location.Country | . | Country_region | libpostal.location.Country_region | . | Entrance | libpostal.location.Entrance | . | House | libpostal.location.House | . | House_number | libpostal.location.House_number | . | Island | libpostal.location.Island | . | Level | libpostal.location.Level | . | Near | libpostal.location.Near | . | Po_box | libpostal.location.Po_box | . | Postcode | libpostal.location.Postcode | . | Road | libpostal.location.Road | . | Staircase | libpostal.location.Staircase | . | State | libpostal.location.State | . | State_district | libpostal.location.State_district | . | Suburb | libpostal.location.Suburb | . | Unit | libpostal.location.Unit | . | World_region | libpostal.location.World_region | . ",
    "url": "/preparation/enricher/libpostal#properties-from-libpostal-enricher",
    
    "relUrl": "/preparation/enricher/libpostal#properties-from-libpostal-enricher"
  },"1693": {
    "doc": "Libpostal",
    "title": "Libpostal",
    "content": " ",
    "url": "/preparation/enricher/libpostal",
    
    "relUrl": "/preparation/enricher/libpostal"
  },"1694": {
    "doc": "Using Agents",
    "title": "Deploying an agent",
    "content": "Agents can be deployed by downloading the Agent binaries and decompressing it onto an operating system of your choice. The folder ships with binaries for many different operating systems. We will use the example below of installing on a Windows machine. Please ask your partner or direct CluedIn contact to give you access to these binaries. Configuring the agent . The Agent can be configured by visiting the container.config that is inside your &lt;agent-root&gt;/Agent folder. You should see a file that looks like this configuration file: . &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;configuration xmlns:urn=\"urn:schemas-microsoft-com:asm.v1\" xmlns:xdt=\"http://schemas.microsoft.com/XML-Document-Transform\"&gt; &lt;xdt:Import assembly=\"ComponentHost\" namespace=\"ComponentHost.Transforms\"/&gt; &lt;startup xdt:Transform=\"InsertIfMissing\"&gt; &lt;supportedRuntime version=\"v4.0\" sku=\".NETFramework,Version=v4.8\"/&gt; &lt;/startup&gt; &lt;appSettings xdt:Transform=\"InsertIfMissing\"&gt; &lt;add key=\"ServerUrl\" value=\"https://localhost:9000/\" xdt:Locator=\"Condition(@key='ServerUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"AgentServerUrl\" value=\"https://localhost:9000/\" xdt:Locator=\"Condition(@key='AgentServerUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"ServerBlobUrl\" value=\"https://localhost:9000/\" xdt:Locator=\"Condition(@key='ServerBlobUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"WebhookUrl\" value=\"https://localhost:9006/\" xdt:Locator=\"Condition(@key='WebhookUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"WebhookServerUrl\" value=\"https://localhost:9006/\" xdt:Locator=\"Condition(@key='WebhookServerUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"ServerStatusUrl\" value=\"http://localhost:9004/\" xdt:Locator=\"Condition(@key='ServerStatusUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"ServerLoggingUrl\" value=\"http://localhost:9005/\" xdt:Locator=\"Condition(@key='ServerLoggingUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"ServerStatusDefaultRedirect\" value=\"https://www.cluedin.net/\" xdt:Locator=\"Condition(@key='ServerStatusDefaultRedirect')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"AuthServerUrl\" value=\"https://localhost:9001/\" xdt:Locator=\"Condition(@key='AuthServerUrl')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Domain\" value=\"localhost\" xdt:Locator=\"Condition(@key='Domain')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailServer\" value=\"\" xdt:Locator=\"Condition(@key='EmailServer')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailPort\" value=\"\" xdt:Locator=\"Condition(@key='EmailPort')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailDisplayName\" value=\"\" xdt:Locator=\"Condition(@key='EmailDisplayName')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailSender\" value=\"\" xdt:Locator=\"Condition(@key='EmailSender')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailUserName\" value=\"\" xdt:Locator=\"Condition(@key='EmailUserName')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"EmailPassword\" value=\"\" xdt:Locator=\"Condition(@key='EmailPassword')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Logging.Targets.Exceptions\" value=\"false\" xdt:Locator=\"Condition(@key='Logging.Targets.Exceptions')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Health.TemporaryDirectory.Enabled\" value=\"true\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.Enabled')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Health.TemporaryDirectory.FreeSpacePctThreshold\" value=\"0.0\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.FreeSpacePctThreshold')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Health.TemporaryDirectory.FreeSpaceBytesThreshold\" value=\"209715200\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.FreeSpaceBytesThreshold')\" xdt:Transform=\"Replace\" /&gt; &lt;!-- Agent --&gt; &lt;add key=\"Agent.Enabled\" value=\"true\" xdt:Locator=\"Condition(@key='Agent.Enabled')\" xdt:Transform=\"Replace\"/&gt; &lt;add key=\"Agent.ErrorLogging.Project\" value=\"6\" xdt:Locator=\"Condition(@key='Agent.ErrorLogging.Project')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Agent.APIKey\" value=\"!2qwaszx12346\" xdt:Locator=\"Condition(@key='Agent.APIKey')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Agent.TaskScheduler\" value=\"Default\" xdt:Locator=\"Condition(@key='Agent.TaskScheduler')\" xdt:Transform=\"Replace\" /&gt; &lt;add key=\"Agent.Queue.MaximumJobsToQueuePerCpu\" value=\"1.5\" xdt:Locator=\"Condition(@key='Agent.Queue.MaximumJobsToQueuePerCpu')\" xdt:Transform=\"Replace\" /&gt; &lt;/appSettings&gt; &lt;/configuration&gt; . Connecting to Kubernetes master server . You will need to fill out the following configuration to connect to your Kubernetes cluster that is running the CluedIn Server, API and JobServer: . ServerUrl should contain a value that is the route to your CluedIn WebAPI you have installed. Usually it is something like https://app.&lt;hostname&gt;/api/ . AgentServerUrl should contain a value that is the route to your CluedIn WebAPI you have installed. Usually it is something like https://app.&lt;hostname&gt;/api/ . ServerBlobUrl should contain a value to your blob url. By default, it should be the same as your WebApi URL. WebhookUrl should contain your Webhook API Url. By default, it is https://app.&lt;hostname&gt;/webhooks/ . ServerStatusUrl should be https://app.&lt;hostname&gt;/api/status . ServerLoggingUrl should point to your WebApi. By default, it should be https://app.&lt;hostname&gt;/api/ . AuthServerUrl should point to your Authentication API. By default, you can find the Authentication API at ttps://app.&lt;hostname&gt;/auth/ . Agent Authentication . _BY DEFAULT, THE AGENT IS ALREADY SETUP TO RUN ON A DEFAULT ON-PREMISES CRAWLER. CHANGING A TOKEN IS RECOMMENDED BUT NOT NEEDED TO GET THE AGENT COMMUNICATING WITH THE SERVER_ . The Agent that you have downloaded has to authenticate against a registered agent within the CluedIn Server. You can do this by port-forwarding to the CluedIn SQL Server pod within your Kubernetes Cluster (kubectl port-forward -l app=sqlserver 1433 --address 0.0.0.0) and adding an agent registration to DataStore.Db.OpenCommunication &gt; dbo.Agent`, or just fill out the SQL query below and execute against the DataStore.Db.OpenCommunication database: . DECLARE @AgentId varchar(60); SET @AgentId = '6CF17140-0FB0-47C5-AAAA-9A40A0ECF8BA'; DECLARE @AgentGroup varchar(60); SET @AgentGroup = '612ed11a-b1b3-463f-b4a5-7c1bb7bd55a0'; DECLARE @AgentToken varchar(60); SET @AgentToken = '--- INSERT YOUR API TOKEN HERE ---'; DECLARE @OrganizationId varchar(60); SET @OrganizationId = (SELECT Id AS OrganizationId FROM dbo.OrganizationProfile WHERE OrganizationName = '--- INSERT YOUR CLIENT ID HERE ---'); DECLARE @DateTimeMin varchar(60); SET @DateTimeMin = (select cast(-53690 as datetime)); UPDATE dbo.Agent SET AccountId = @OrganizationId, ApiKey = @AgentToken, LastPing = @DateTimeMin WHERE Id = @AgentId; . After you have done this, copy the API Token you entered above, then set the ApiKey value in the container.config of the file you downloaded above. Deploying Crawlers . In order to deploy a crawler into an agent you will need to have the crawler assembly files (Dll’s) either from a Nuget Package or by compiling and building a crawler locally on your developer machine. Assemblies required: . | CluedIn.Crawling.CrawlerName | CluedIn.Crawling.CrawlerName.Core | CluedIn.Crawling.CrawlerName.Infrastructure | . !!! Note, that Provider project is not required in the agent because the Provider is registereted in the cloud WebApi instance. Everything that is contained in the Provider project will be executed from the cluster itself . !! Ensure that all of the dependencies needed by your crawler are also deployed along with the crawler assemblies e.g. Nuget Dependencies. Crawler assemblies needs to be moved into &lt;agent-root&gt;/Agent folder and will be picked up once the Agent is started. On the cluster, you need to deploy all your crawlers packages above plus the Provider project’s NuGet package parts responsible for your crawler that will be executed by the agent. Running the agent . Running the agent is as simple as starting boot.sh file. For this, you may need to install something like Cygwin to be able to run .sh files on Windows. You will see an output where your agent is trying to load the assemblies and connect to the cluster. Make sure there is nothing blocking the call getting to the API server networking-wise. You can now login to CluedIn and add your integration and the actual crawling of data will be then done through the Agent instead of the CluedIn server in the Kubernetes cluster. You can also register this as a Windows Service so that it can be automatically restarted if the Windows VM is to restart. You can use this guide here on how to setup a Windows Service: Here . Enable Verbose Logging . By default the Agent will be running with low logging verbosity. To increase this you can set the $env:ASPNETCORE_ENVIRONMENT = “verbose”. You can also make sure this persists on the machine by setting it as a System Variable. You will need to close and restart your Agent and the session of the bash/command prompt you are using to invoke the boot.sh file to see the changes take effect. ",
    "url": "/crawling/using-agents#deploying-an-agent",
    
    "relUrl": "/crawling/using-agents#deploying-an-agent"
  },"1695": {
    "doc": "Using Agents",
    "title": "Using Agents",
    "content": "Agents are the orchestrators of running integrations. Agents allow you to run crawlers in remote environments, typically on different machines, even in different physical environments. Agents are typically used for running hybrid environments of CluedIn where you may host CluedIn itself in the cloud, but need to run crawls on systems that live within an internal network of a business. The Agents are responsible for running scheduled crawls and the robustness of making sure that the crawlers can survive times where they crash. For running an Agent, you will need to register an Agent API key within the CluedIn datastore and then the Agents will need matching API keys in their configuration files on the remote machines. CluedIn will use Websockets to communicate between the Agents and the CluedIn Server. When deploying your Agents, they will need to have the Agent API key match one of the API Keys that are registered in the Agents Database within CluedIn. The API key must be associated with the Organization ID of the account that is running the Agent. The simplest way to setup an agent is to remove the ServerComponent folder from CluedIn, leaving only the Agent folder. In container.config, you have to make sure that the URLs are correct (e.g. AgentServerUrl should have the value of the CluedIn’s API endpoint). For communication, Agents cannot receive incoming messages but rather uses a polling mechanism to talk with the CluedIn Server. In this way, other systems cannot instruct the Agents with a Job to run. The Agents will post data, logs and health statistics back to the CluedIn server so that CluedIn has knowledge of what is running within the Agents and any possible issues that could be happening. Executes Agent Jobs from the CluedIn System against a 3rd party / provider api . Job results (clues) is sent back to CluedIn as payloads . Agents can be deployed: . | Within the CluedIn cluster (cloud) | As a separate isolated component (onprem) | . Cloud (within the CluedIn cluster) . | Directly connected to the backend | Communicates with Agent Controller via direct reference from the container | . Onprem (outside of our control) . | Deployed as VM’s within customers own environment | Enables access to customers environments that is not accessible from the CluedIn Cluster Directly | Communicates with Agent Controller over HTTP, TLS | No access to CluedIn databases, Message Bus etc. | Deployed with ComponentHost + individual components | Ie. Smaller deployment package than the full CluedIn | Processing, WebApi, DataStores is not available | Agent API key is used for “Authentication” | . Payload . | Binary Format | Multiple Records | Compressed | . Types of Payloads . | Clue Payloads | Clues produced from Crawlers | Agent Job Log Payloads | Logs produced from the job/crawler execution | (Log shipping from the Agent back to the CluedIn cluster) | (CompressedRecord Payload) | . Job Types . | Normal Execute job, finishes when crawling is done . | Continuous Does not finish Used to monitor as system and produce clues when changes happen Ie. File system monitoring, Kafka queue,…. | . Jobs have statistics of Start / stop dates Current number of tasks Number of completed tasks Number of failed tasks Number of clues produced Number of payloads submitted . Jobs can be restricted to only run on A specific agent A specific group of agents Any agent with type Cloud SharedProcessor (shared between multiple tenants) Onprem (A single tenant) . Orchestration Server Agents automatically download updates from the server (Zip file deployed centrally) Enables updates of Agent deployed in scenarios where we do not have access to the machines they are running on. ",
    "url": "/crawling/using-agents",
    
    "relUrl": "/crawling/using-agents"
  },"1696": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Best practices to integrate hundreds of data sources through CluedIn",
    "content": "Integrating 100+ sources isn’t a modeling marathon you finish upfront—it’s an incremental journey you steer with evidence from the data itself. CluedIn’s graph-first architecture, schema-on-read ingestion, and late-binding mapping make this practical at scale. This article distills field-proven practices to help you land sources quickly, let the model evolve, and still end up with trustworthy, connected master data. ",
    "url": "/kb/connect-lots-of-systems#best-practices-to-integrate-hundreds-of-data-sources-through-cluedin",
    
    "relUrl": "/kb/connect-lots-of-systems#best-practices-to-integrate-hundreds-of-data-sources-through-cluedin"
  },"1697": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Core principles (TL;DR)",
    "content": ". | Don’t model upfront — evolve the model one dataset at a time with schema-on-read. | Many models can represent the same business object — let each source speak its native shape, then unify. | Prefer Edges over Strict Edges in mapping — enable eventual connectivity across unordered loads. | Order doesn’t matter — if your identities &amp; edges are right, the graph will converge as sources arrive. | Build natural models from data — don’t force-fit sources into preconceived structures. | . ",
    "url": "/kb/connect-lots-of-systems#core-principles-tldr",
    
    "relUrl": "/kb/connect-lots-of-systems#core-principles-tldr"
  },"1698": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "1) Evolve the model one dataset at a time",
    "content": "Why . Upfront “big design” collapses under heterogeneous, fast-changing sources. The winning pattern is progressive modeling: ingest → observe → map → harden → repeat. How . | Ingest raw, lossless payloads as-is (JSON/NDJSON). Preserve lineage (source, system, timestamps). | Inspect emergent fields and candidate identifiers (emails, account numbers, GUIDs, URLs). | Map minimally to start: external IDs, core attributes, and a couple of high-value edges. | Add survivorship rules and standardization after you see real conflicts. | Iterate when the next source arrives—never block integration on a “final” schema. | . Tip: Keep a “New Source Intake” checklist per source: identifiers, key properties, candidate edges, expected volume/frequency, and known quality issues. ",
    "url": "/kb/connect-lots-of-systems#1-evolve-the-model-one-dataset-at-a-time",
    
    "relUrl": "/kb/connect-lots-of-systems#1-evolve-the-model-one-dataset-at-a-time"
  },"1699": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "2) Embrace many models for the same business object",
    "content": "Why . A “Customer” in CRM ≠ “Customer” in Billing ≠ “Customer” in Support. Let each source keep its natural model and unify through identities and edges, not by flattening everything into one canonical record upfront. How . | Allow multiple shapes (facets) to coexist: CRM.Person, Billing.Account, Support.Requester. | Unify through identity (emails, account numbers, tax IDs, domain names) plus similarity (names, addresses, phones). | Use property namespaces (e.g., crm.name, billing.name) and define survivorship rules (trusted source by field, recency, completeness). | Compute curated views (golden records) downstream of the raw merged graph—don’t make sources pretend they’re identical at ingestion. | . Example: parallel models for the same customer . // CRM facet { \"type\": \"Person\", \"externalId\": \"crm:123\", \"properties\": { \"name\": \"Alex Lee\", \"email\": \"alex@acme.com\" } } // Billing facet { \"type\": \"Account\", \"externalId\": \"bill:9981\", \"properties\": { \"billingEmail\": \"alex@acme.com\", \"status\": \"Active\" } } // Support facet { \"type\": \"Requester\", \"externalId\": \"sup:730\", \"properties\": { \"email\": \"alex@acme.com\", \"vip\": true } } . These different models refer to the same human and will unify via shared identities (email), then contribute fields to the mastered view. ",
    "url": "/kb/connect-lots-of-systems#2-embrace-many-models-for-the-same-business-object",
    
    "relUrl": "/kb/connect-lots-of-systems#2-embrace-many-models-for-the-same-business-object"
  },"1700": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "3) Prefer Edges over Strict Edges in mapping",
    "content": "Concepts . | Edges: relationships that can be created even if the target doesn’t exist yet. The link resolves when the target appears or is recognized later (by identity). | Strict Edges: relationships that require the target to exist at mapping time (and usually a direct key match). | . Why prefer Edges . | Order-agnostic ingestion — you don’t care which source lands first. | Resilience to late-arriving keys — links snap into place when the target becomes known. | Fewer brittle dependencies — no blocking on cross-system referential integrity on day one. | . When to consider Strict Edges . | You truly have strong referential guarantees and want validation failures when targets are absent. | Tiny, tightly controlled domains where strictness prevents incorrect links. | . Example: edge-centric mapping . { \"type\": \"Invoice\", \"externalId\": \"inv:2025-000045\", \"properties\": { \"total\": 1200.50, \"accountNumber\": \"ACME-001\" }, \"edges\": [ { \"rel\": \"billTo\", \"to\": { \"hint\": { \"accountNumber\": \"ACME-001\" } } } ] } . Mapping “billTo” as an Edge lets CluedIn resolve it later to the correct Organization/Account once that node is seen or matched. ",
    "url": "/kb/connect-lots-of-systems#3-prefer-edges-over-strict-edges-in-mapping",
    
    "relUrl": "/kb/connect-lots-of-systems#3-prefer-edges-over-strict-edges-in-mapping"
  },"1701": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "4) Don’t worry about load order (if mapping is right)",
    "content": "Why . In a graph, identity + edges provide eventual connectivity. You can load Support today, Billing tomorrow, CRM next week—links will converge as identities and edges accumulate. How . | Always include a stable externalId and a last-changed timestamp. | Emit edges with hints (email, account number, domain) so relationships can resolve later. | Retry &amp; reprocess are safe when ingestion is idempotent (same externalId = upsert). | Backfills: you can land historical data whenever it’s ready; the graph will merge it. | . Anti-pattern: sequencing complex multi-system pipelines to “guarantee” order. Prefer independent, frequent micro-loads with reconciling edges. ",
    "url": "/kb/connect-lots-of-systems#4-dont-worry-about-load-order-if-mapping-is-right",
    
    "relUrl": "/kb/connect-lots-of-systems#4-dont-worry-about-load-order-if-mapping-is-right"
  },"1702": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "5) Build natural models from the data (don’t force-fit)",
    "content": "Why . Forcing sources into rigid, predefined structures leads to data loss and brittle mappings. Natural models preserve signals you’ll later use for matching, survivorship, and quality rules. How . | Keep nested structures and arrays if the source has them (addresses, contacts, SKUs). | Preserve original field names (with source namespaces) alongside standardized ones. | Promote to standardized fields after you confirm semantic equivalence across sources. | Capture lineage (source, file, system, timestamp) on every property to enable trust and audit. | . Example: natural → standardized (progressive) . // Natural (from e-commerce) { \"type\": \"Order\", \"externalId\": \"web:O-5571\", \"properties\": { \"buyerEmail\": \"alex@acme.com\", \"items\": [ {\"sku\":\"A1\",\"qty\":2}, {\"sku\":\"B4\",\"qty\":1} ] } } // Later: add standardized fields (don’t delete natural) { \"type\": \"Order\", \"externalId\": \"web:O-5571\", \"properties\": { \"customer.email\": \"alex@acme.com\", \"lineItems\": [ {\"sku\":\"A1\",\"quantity\":2}, {\"sku\":\"B4\",\"quantity\":1} ] } } . ",
    "url": "/kb/connect-lots-of-systems#5-build-natural-models-from-the-data-dont-force-fit",
    
    "relUrl": "/kb/connect-lots-of-systems#5-build-natural-models-from-the-data-dont-force-fit"
  },"1703": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "A repeatable workflow for each new source",
    "content": ". | Ingest the source as-is (lossless JSON/NDJSON). | Identify robust identifiers (stable keys, emails, domains) and add them to identity resolution. | Map a minimal set of properties + a couple of Edges (non-strict) to high-value neighbors. | Unify and inspect collisions; add survivorship &amp; standardization rules where needed. | Harden: data quality checks, formats, code lists; keep the raw signals. | Iterate when the next source arrives (don’t refactor the world—add the next facet). | . ",
    "url": "/kb/connect-lots-of-systems#a-repeatable-workflow-for-each-new-source",
    
    "relUrl": "/kb/connect-lots-of-systems#a-repeatable-workflow-for-each-new-source"
  },"1704": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Do / Don’t",
    "content": "| Do | Don’t | . | Add sources fast with minimal mapping, then evolve | Wait for a “final” enterprise model before ingesting | . | Use Edges (non-strict) to enable eventual linking | Depend on Strict Edges that require load order | . | Keep natural source fields + lineage | Flatten away nested structure you’ll need for matching | . | Define survivorship per field (trust, recency, completeness) | Declare one source “always wins” across the board | . | Backfill anytime; idempotent upserts | Build brittle orchestration to enforce source order | . ",
    "url": "/kb/connect-lots-of-systems#do--dont",
    
    "relUrl": "/kb/connect-lots-of-systems#do--dont"
  },"1705": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Practical mapping tips",
    "content": ". | Edge hints: include multiple hints (email, accountNumber, domain) to raise link success. | Identity bundles: combine deterministic (IDs, emails) with probabilistic (name + address). | Confidence scoring: use higher confidence to auto-link; route lower scores to review. | Property provenance: store sourceSystem &amp; lastSeen per property for audit and rollbacks. | Deletes: model as tombstones (soft-delete flags) unless you truly want to erase history. | . ",
    "url": "/kb/connect-lots-of-systems#practical-mapping-tips",
    
    "relUrl": "/kb/connect-lots-of-systems#practical-mapping-tips"
  },"1706": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Anti-patterns to avoid",
    "content": ". | Massive canonical model upfront: slows onboarding and guarantees rework. | Global strict referential integrity across heterogeneous sources. | One-size-fits-all survivorship: trust varies by attribute and by source. | Over-normalization early: removes signals needed for matching and quality scoring. | . ",
    "url": "/kb/connect-lots-of-systems#anti-patterns-to-avoid",
    
    "relUrl": "/kb/connect-lots-of-systems#anti-patterns-to-avoid"
  },"1707": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Example: making unordered loads converge",
    "content": ". | Day 1: Load Support tickets (Requester.email). | Day 3: Load Billing accounts (billingEmail, accountNumber). | Day 7: Load CRM persons (email, phone). | . Because tickets have an edge reportedBy → { email }, and billing has an edge billTo → { accountNumber | email }, once CRM lands, identity unification snaps edges together—no reruns required. ",
    "url": "/kb/connect-lots-of-systems#example-making-unordered-loads-converge",
    
    "relUrl": "/kb/connect-lots-of-systems#example-making-unordered-loads-converge"
  },"1708": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Checklist for “ready to scale to 100+ sources”",
    "content": ". | Every payload has stable externalId and timestamp. | Mappings use Edges (non-strict) wherever feasible. | Identity rules combine deterministic and probabilistic signals. | Survivorship is per-field and transparent. | Natural source fields are retained with lineage. | Backfill and reprocessing are idempotent. | Monitoring covers volume, link-rate, identity collisions, and edge resolution lag. | . ",
    "url": "/kb/connect-lots-of-systems#checklist-for-ready-to-scale-to-100-sources",
    
    "relUrl": "/kb/connect-lots-of-systems#checklist-for-ready-to-scale-to-100-sources"
  },"1709": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "FAQ",
    "content": "Q: Won’t multiple models make reporting harder? A: Raw models stay natural; curated views and golden records provide stable shapes for analytics. You get flexibility and standardization—just at the right stage. Q: When would I choose a Strict Edge? A: Small, controlled domains with guaranteed referential integrity where a missing target must fail fast (e.g., reference tax codes). Q: How do I control “who wins” when fields disagree? A: Use survivorship rules: source trust ranking, last-updated recency, field completeness, or quality scores; never blanket “system X always wins”. Summary . To integrate hundreds of sources with CluedIn, optimize for speed-to-land and correctness-over-time. Let models emerge from real data, connect nodes with Edges that resolve as identities converge, and postpone strictness until you have evidence. Your graph will become richer, more accurate, and easier to maintain—without the drag of upfront over-modeling. ",
    "url": "/kb/connect-lots-of-systems#faq",
    
    "relUrl": "/kb/connect-lots-of-systems#faq"
  },"1710": {
    "doc": "Connect lots of systems into CluedIn",
    "title": "Connect lots of systems into CluedIn",
    "content": " ",
    "url": "/kb/connect-lots-of-systems",
    
    "relUrl": "/kb/connect-lots-of-systems"
  },"1711": {
    "doc": "Crawler Validation Framework",
    "title": "Crawler Validation Framework",
    "content": "While building new integrations for CluedIn, you will want to make sure that you are building your integrations in the recommended way. The crawler validation framework will help guide a developer to produce a Clue that is of the highest readiness for processing. The validation framework will only run during Debug / Developer mode and will not run once deployed to production. The validation framework acts as a guide to warn a developer if they may have forgotten to do something important such as setting a Uri, setting a Name. It might be the case that you don’t haave these properties (which is fine and normal) but it is also one of those pieces that is easy to forget - hence the framework. A developer can supress the validators at a Clue Producer level in the cases where you can confirm that you won’t be able to produce what is expected for a Clue. ",
    "url": "/crawling/crawler-validation-framework",
    
    "relUrl": "/crawling/crawler-validation-framework"
  },"1712": {
    "doc": "File",
    "title": "On this page",
    "content": ". | Overview of the data ingestion process for files | Upload a file | . In this article, you will learn how to ingest data from files into CluedIn. You can upload files in CSV, JSON, XLS, XLSX, and Parquet formats. You can upload up to 5 files at once, and the total file size should not exceed 1 GB. Note that uploading Parquet files is a beta feature. To be able to upload Parquet files, go to Administration &gt; Feature Flags, and enable the Parquet file support feature. The maximum number of columns accepted for data ingestion is 490.Before loading data into CluedIn, ensure that it does not contain unsupported characters. Refer to our knowledge base article for information about supported and unsupported characters. ",
    "url": "/integration/file#on-this-page",
    
    "relUrl": "/integration/file#on-this-page"
  },"1713": {
    "doc": "File",
    "title": "Overview of the data ingestion process for files",
    "content": "When you need to upload a large set of records to CluedIn, we recommend that you start by uploading a small, representative subset of your data in CSV or JSON format. This approach allows you to verify the accuracy of mapping and relations before dealing with a large set of records. Once the golden records generated from this subset align with your expectations, you can safely remove those records from the system, while keeping the mapping configuration intact. After that you can upload a large set of records and use the existing mapping to generate golden records in an efficient way. CluedIn file uploader accommodates structured data, so you can upload the files in CSV, JSON, or basic Excel formats. After you upload a JSON or XLS/XLSX file, we recommend that you download an example file provided by the system. This example serves as a reference to confirm the expected data format. If your file deviates from this format, adjust it according to the example to ensure a smooth data ingestion process. The data ingestion process for files consists of three stages: uploading, parsing, and loading. When your file adheres to the required data format and each stage executes without interruption, the entire process runs seamlessly. However, to efficiently address issues that might arise during the data ingestion process, get acquainted with the potential reasons for failure at each stage: . | If the upload fails, the recovery of the file is not possible. Such a situation may arise if the file upload was initiated but the browser tab was closed while the upload was in progress. To resolve this, remove the data source and upload the file again. | If the parsing fails—for example, due to the file being corrupted or having invalid data format—you will see a corresponding error message from the parser. To resolve this, remove the data source, fix the file and make sure it conforms to the required data format, and upload the file again. | If the loading fails, you will see an error message with the number of chunks that could not be loaded. To resolve this, retry to load the data or remove the data source altogether and upload the file again. | . ",
    "url": "/integration/file#overview-of-the-data-ingestion-process-for-files",
    
    "relUrl": "/integration/file#overview-of-the-data-ingestion-process-for-files"
  },"1714": {
    "doc": "File",
    "title": "Upload a file",
    "content": "Follow our step-by-step instruction to upload a file. To upload a file . | On CluedIn home page, in the Integrations section, select Import from files. | In the Add files section, either drag the file or choose it from your computer. | Specify the Group where the data source will be stored. You can choose the existing group or create a new group. If you upload several files, they will be stored as separate data sources within a group. | In the lower-right corner, select Upload. The data has been ingested to CluedIn, and you can view it on the Preview tab of the data set. Next, check the logs to make sure all your records are valid. To turn your data into golden records, create a mapping and process the data. | . ",
    "url": "/integration/file#upload-a-file",
    
    "relUrl": "/integration/file#upload-a-file"
  },"1715": {
    "doc": "File",
    "title": "File",
    "content": " ",
    "url": "/integration/file",
    
    "relUrl": "/integration/file"
  },"1716": {
    "doc": "OpenCorporates",
    "title": "On this page",
    "content": ". | Add OpenCorporates enricher | Properties from OpenCorporates enricher | . This article outlines how to configure the OpenCorporates enricher. The purpose of this enricher is to provide a wide range of information about companies from around the world (for example, creation date, status, officers, and so on). More details can be found in Properties from OpenCorporates enricher. The Open Corporates enricher supports the following endpoints: . | https://api.opencorporates.com/v0.4/companies/search?q={nameLookup}, where {nameLookup} is the name of company – this endpoint is called when no company codes can be found in golden record’s organization.codes.cvr, organization.codes.brreg, organization.codes.companyHouse, or organization.codes.cik . | https://api.opencorporates.com/v0.4companies/{jurisdictionCodeLookup.Jurisdiction}/{jurisdictionCodeLookup.Value}?format=json – this endpoint is called when there are company codes, so the jurisdiction of golden record will be retrieved and used in the API. | . ",
    "url": "/preparation/enricher/open-corporates#on-this-page",
    
    "relUrl": "/preparation/enricher/open-corporates#on-this-page"
  },"1717": {
    "doc": "OpenCorporates",
    "title": "Add OpenCorporates enricher",
    "content": "To use the OpenCorporates enricher, you must provide the API token. To get the API token, follow the instructions on the OpenCorporates website. The OpenCorportes enricher uses the company name to retrieve information from the OpenCorporates website. To add the OpenCorporates enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select OpenCorporates, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API token for retrieving information from the OpenCorporates website. | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Lookup Vocabulary Key – enter the vocabulary key that contains the names of companies that you want to enrich. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The OpenCorporates enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the OpenCorporates enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided to configure the enricher. | . ",
    "url": "/preparation/enricher/open-corporates#add-opencorporates-enricher",
    
    "relUrl": "/preparation/enricher/open-corporates#add-opencorporates-enricher"
  },"1718": {
    "doc": "OpenCorporates",
    "title": "Properties from OpenCorporates enricher",
    "content": "You can find the properties added to golden records from the OpenCorporates enricher on the Properties page. For a more detailed information about the changes made to a golden record by the OpenCorporates enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the OpenCorporates enricher. | Display name | Vocabulary key | . | Agent Name | openCorporates.organization.agentName | . | Agent Address | openCorporates.organization.agentAddress | . | Alternative Names | openCorporates.organization.alternativeNames | . | Branch | openCorporates.organization.branch | . | Branch Status | openCorporates.organization.branchStatus | . | Company Number | openCorporates.organization.companyNumber, mapped to organization.codes.companyNumber | . | Company Type | openCorporates.organization.companyType, mapped to organization.type | . | Controlling Entity | openCorporates.organization.controllingEntity | . | Corporate Groupings | openCorporates.organization.corporateGroupings | . | Created At | openCorporates.organization.createdAt, mapped to date.createdDate | . | Current Status | openCorporates.organization.currentStatus | . | Data | openCorporates.organization.data | . | Dissolution Date | openCorporates.organization.dissolutionDate, mapped to organization.dissolutionDate | . | Filings | openCorporates.organization.filings | . | Incorporation Date | openCorporates.organization.incorporationDate, mapped to organization.foundingDate | . | Identifiers | openCorporates.organization.identifiers | . | Industry Codes | openCorporates.organization.industryCodes | . | Native Company Number | openCorporates.organization.nativeCompanyNumber | . | Registry Url | openCorporates.organization.registryUrl | . | Jurisdiction Code | openCorporates.organization.jurisdictionCode, mapped to organization.jurisdictionCode | . | Address Country Code | openCorporates.organization.addressCountryCode, mapped to organization.address.countryCode | . | In Active | openCorporates.organization.inActive | . | Home Company | openCorporates.organization.homeCompany | . | Open Corporates Url | openCorporates.organization.openCorporatesUrl | . | Officers Name | openCorporates.organization.officersName | . | Officers | openCorporates.organization.officers | . | Number Of Employees | openCorporates.organization.numberOfEmployees, mapped to organization.employeeCount | . | Source Publisher | openCorporates.organization.sourcePublisher | . | Source Url | openCorporates.organization.sourceUrl | . | Previous Names | openCorporates.organization.previousNames | . | Registered Address | openCorporates.organization.registeredAddress, mapped to organization.address | . | Updated At | openCorporates.organization.updatedAt, mapped to date.modifiedDate | . | Trademark Registration | openCorporates.organization.trademarkRegistration | . | Website | openCorporates.organization.website, mapped to organization.website | . | Telephone Number | openCorporates.organization.telephoneNumber, mapped to organization.phoneNumber | . | Official Register Entry | openCorporates.organization.officialRegisterEntry | . | Date | openCorporates.filing.date, mapped to date.createdDate | . | Filing Code | openCorporates.filing.filingCode | . | Filing Type | openCorporates.filing.filingType | . | Open Corporates Url | openCorporates.filing.openCorporatesUrl | . | Score | openCorporates.filing.score | . | Uid | openCorporates.filing.uid | . ",
    "url": "/preparation/enricher/open-corporates#properties-from-opencorporates-enricher",
    
    "relUrl": "/preparation/enricher/open-corporates#properties-from-opencorporates-enricher"
  },"1719": {
    "doc": "OpenCorporates",
    "title": "OpenCorporates",
    "content": " ",
    "url": "/preparation/enricher/open-corporates",
    
    "relUrl": "/preparation/enricher/open-corporates"
  },"1720": {
    "doc": "Queues in CluedIn",
    "title": "On this page",
    "content": ". | Introduction | Source queues . | General ingestion queues | Dedicated ingestion queues | . | Data pipeline queues . | Clue processing queues | Edge processing queue | Metrics processing queue | Commands queue | Enricher queue | Processing retry handling | Processing error handling queues | . | Stream queues . | Stream error handling queues | . | Distributed jobs subsystem queues | Robust callback subsystem queue | Remote events | . In this article, you will learn about the queue-based service bus architecture used by CluedIn and find a complete list of queues involved in its operations. ",
    "url": "/paas-operations/queues-in-cluedin#on-this-page",
    
    "relUrl": "/paas-operations/queues-in-cluedin#on-this-page"
  },"1721": {
    "doc": "Queues in CluedIn",
    "title": "Introduction",
    "content": "Queues are central to how CluedIn operates. Each operation or record in CluedIn is represented as a message in a queue. This design means that to truly understand how CluedIn functions under the hood, you need to understand how these queues behave. Some of these queues are visible directly in the CluedIn UI—for example, on the Monitoring tab of a data set or within the Processing Pipeline section in Engine Room. For customers running CluedIn in a self-managed environment, a great deal of insight can be gained simply by observing the state of these queues. You might ask questions like: . | How many messages are currently in a queue? . | How many consumers are connected to a given queue? . | Are there any dead-letter messages? . | . Why CluedIn uses a service bus instead of an event log? . A common question is: why did CluedIn adopt a queue-based service bus architecture (e.g., RabbitMQ) instead of a log-based event-driven system (e.g., Kafka)? The answer lies in a combination of operational simplicity, data safety, and reliability. While both approaches are valid and widely used in modern architectures, event-driven platforms like Kafka come with added complexity—especially in areas like infrastructure (e.g., managing Zookeeper), monitoring, and tracing data lineage. They also follow a different philosophy: Kafka acts more like a distributed commit log where messages are retained for a period and can be replayed, while RabbitMQ uses a work queue model where messages are consumed and removed. CluedIn made an early architectural decision to use a service bus and an actor model. This approach provided a few key advantages: . | Easier traceability – it’s more straightforward to track “who did what and when” in a service bus pattern. | Durability – messages in CluedIn are stored on disk, so even if a service crashes or is restarted (even abruptly), the messages remain intact, virtually eliminating the risk of data loss. | Snapshotting and recovery – the queue-based model makes it easier to pause, inspect, or recover processing flows. | Operational resilience – RabbitMQ offers robust support for acknowledgment, retries, and dead-lettering, which are vital for maintaining a healthy pipeline under load. | . After years of investment in this architecture, we’ve reached a point where CluedIn’s pipeline is stable, performant, and resilient, even under heavy record volumes. This is why we don’t anticipate major architectural changes to this part of the system in the near future. ",
    "url": "/paas-operations/queues-in-cluedin#introduction",
    
    "relUrl": "/paas-operations/queues-in-cluedin#introduction"
  },"1722": {
    "doc": "Queues in CluedIn",
    "title": "Source queues",
    "content": "The source queues are used for data ingestion in CluedIn. General ingestion queues . The following queues are not linked to a specific data source. They are cross-source queues, primarily used for service-to-service communication and to enable asynchronous execution of long-running jobs. | Name | Queue name | Purpose | Error handling | . | Ingestion Data Set | ingestion-data-set | General-purpose queue used to ingest records from all source types (files, endpoints, databases, and so on). | If a failure occurs, the data source is set to an error state, and a message is displayed in the UI. | . | Commit Data Set | commit-data-set | General-purpose queue used to initiate processing of a source of any type. | In case of an error, the message is retried after 30 seconds; errors are logged in the logs. | . | File Recovery Queue | file-recovery | Used when a file upload fails during processing. Upon pod restart, a recovery message is sent to retry parsing/loading the file. | If recovery fails, the data source is set to an error state, and the error is visible in the UI. | . | Cloning Data Set Queue | cloning-data-set | Instructs the system to set a source in edit mode and clone the data; can also revert the data set to its original state. | In case of an error, it is logged in the data set logs and chunk loading may fail. | . | Source Operation Queue | operation-data-set | Stores all jobs related to operations on a data set, especially actions performed in edit mode. | If a failure occurs, the error is logged. | . | Source Anomaly Detection | profiling-anomaly-detection-job | Verifies if ingestions are in a correct state, attempts self-healing, and triggers alerts if needed. | If a failure occurs, the error is logged, and the message is moved to the dead letter queue after 5 attempts. | . | Source Anomaly Detection Dead Letter | profiling-anomaly-detection-job_dead_letter | Stores messages that failed after 5 retry attempts in the anomaly detection queue. | If moved back manually to profiling-anomaly-detection-job, the job will be retried. | . | Source Key Metrics Queue | profiling-ingestion-metrics | Scheduled job that collects key ingestion metrics. | Retries up to 5 times; if a failure occurs, the message is stored in the dedicated dead letter queue; the dead letter queue also serves as a log for critical failures. Messages can be manually retried. | . | Source Key Metrics Dead Letter Queue | profiling-ingestion-metrics_dead_letter | Stores messages that failed after 5 retry attempts in the key metrics queue. | Moving the message back to profiling-ingestion-metrics will retry the job. | . | Duplicate Check | profiling-duplicate-check-job | Scheduled job to detect duplicate records based on identifiers. | After 5 retries, the message is moved to the profiling-duplicate-check-job_dead_letter queue. | . | Duplicate Check Dead-Letter Queue | profiling-duplicate-check-job_dead_letter | Backup queue for messages that failed duplicate checks. | Messages can be manually moved back to profiling-duplicate-check-job to retry. | . | Exporter Queue | exporter | Sends export information to consumers. | If a failure occurs, the message is rejected, and an error is logged. | . Dedicated ingestion queues . The following queues are dedicated to individual data sources. Depending on the source type and the actions performed, a separate set of queues may be created for each source. | Name | Queue name | Purpose | Error handling | . | Source Mapping Queue | clue_datasource_submit_[datasetId] | Receives JSON data and applies mapping to transform it into a clue. | If mapping fails, an error log is added to the data set logs in the UI. | . | Source Processing Queue | clue_datasource_process_[datasetId] | Sends clues to the data processing pipelines. | In case of an error, logs are generated, and a Retry button appears on the Process tab of the data set. | . | Source Error Processing Queue | clue_datasource_process_[datasetId]_error | Stores clues that failed during processing. | Temporarily stores clues that could not be successfully sent. | . | Manual Data Entry Mapping Queue | clue_manual_entry_submit_[manualProjectId] | Receives user-entered data and transforms it into clues. | If mapping fails, an error log is added to the logs in the UI. | . | Manual Data Entry Processing Queue | clue_manual_entry_process_[manualProjectId] | Sends manually entered clues to the data pipelines. | In case of an error, logs are generated and a Retry button appears on the Process tab of the data set. | . | Manual Data Entry Error Processing Queue | clue_manual_entry_process_[manualProjectId]_error | Stores clues that failed during manual data entry processing. | Temporarily stores clues that could not be successfully sent. | . | Quarantine Mapping Queue | source_mapping_quarantine_[sourceId] | Reverse maps clues for approval validation before resending them to the quarantine processing queue. | In case of an error, a log message is shown in the UI. | . | Quarantine Processing Queue | source_process_quarantine_[sourceId] | Sends approved or quarantined clues to the data processing pipeline. | In case of an error, logs are generated, and a Retry button appears on the Process tab of the data set. | . | Quarantine Error Processing Queue | source_process_quarantine_[sourceId]_error | Stores clues that failed during quarantine processing. | Temporarily stores clues that could not be successfully sent. | . | Source Failed Loading Queue | dataset_failed_elastic_search_bulk_[datasetId] | Stores failed chunks when saving records to Elasticsearch. | If a failure occurs, a Retry button is shown in the UI. | . | Manual Data Entry Failed Loading Queue | manual-data-entry-session_failed_elastic_search_bulk_[manualProjectId] | Stores failed chunks when saving manually entered records to Elasticsearch. | If a failure occurs, a Retry button is shown in the UI. | . ",
    "url": "/paas-operations/queues-in-cluedin#source-queues",
    
    "relUrl": "/paas-operations/queues-in-cluedin#source-queues"
  },"1723": {
    "doc": "Queues in CluedIn",
    "title": "Data pipeline queues",
    "content": "Clue processing queues . | Name | Queue name | Purpose | Error handling | . | Default Clue Processing | CluedIn.Core.Messages.Processing.IProcessingCommand, CluedIn.Core_CluedIn_Clues | General clue data ingestion. | Default processing error handling | . | High Priority Clue Processing | CluedIn.Core.Messages.Processing.ProcessPrioritizedClueCommand, CluedIn.Core_CluedIn | Prioritized clue ingestion. | Default processing error handling | . | Big Clue Processing | CluedIn.Core.Messages.Processing.ProcessBigClueCommand, CluedIn.Core_CluedIn | Clues with raw byte size larger than 500 KB threshold to be executed with less concurrency. | Default processing error handling | . | Low Priority Clue Processing | CluedIn.Core.Messages.Processing.ProcessLowPriorityClueCommand, CluedIn.Core_CluedIn | Clue data ingestion by low priority crawling jobs. | Default processing error handling | . Edge processing queue . | Name | Queue name | Purpose | Error handling | . | Edge Processing Queue | CluedIn.Core.Messages.Processing.ProcessEdgesCommand, CluedIn.Core_CluedIn | Resolves edges and creates shadow entities for missing reference points. | Default processing error handling | . | Parent Processing Queue | CluedIn.Core.Messages.Processing.ParentsProcessingCommand, CluedIn.Core_CluedIn_ParentIds | Old queue, disabled by default. | Default processing error handling | . Metrics processing queue . | Name | Queue name | Purpose | Error handling | . | Entity Metrics Queue | CluedIn.Core.Messages.Processing.Metrics.ProcessEntityMetricsCommand, CluedIn.Core_CluedIn | Calculates entity level metric values. | Default processing error handling | . | Global Metrics Queue | CluedIn.Core.Messages.Processing.Metrics.ProcessGlobalMetricsCommand, CluedIn.Core_CluedIn | Scheduled job to calculate global dimension metric values. | Default processing error handling | . | Archive Metrics Queue | CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand, CluedIn.Core_CluedIn | Creates history of entity level metric values. | Default processing error handling | . Commands queue . | Name | Queue name | Purpose | Error handling | . | Deduplicate Entity | CluedIn.Core.Messages.Processing.DeduplicateEntityCommand, CluedIn.Core_CluedIn | Deduplicates entity by overlapping entity codes or any enabled fuzzy matchers. | Default processing error handling | . | Anonymise | CluedIn.Core.Messages.Processing.AnonymiseDataCommand, CluedIn.Core_CluedIn | Anonymizes entity data processing. | Default processing error handling | . | Mesh Commands | CluedIn.Core.Messages.Processing.MeshDataCommand, CluedIn.Core_CluedIn | Mesh command execution from mesh center. | Default processing error handling | . | Delete Entity | CluedIn.Core.Messages.Processing.DeleteEntityCommand, CluedIn.Core_CluedIn | Deletion of entities. | Default processing error handling | . | Merge Entity | CluedIn.Core.Messages.Processing.MergeEntitiesCommand, CluedIn.Core_CluedIn | Merging of multiple entities and single entity, either by automatic entity deduplication or manual merging. | Default processing error handling | . | Deduplicate | CluedIn.Core.Messages.Processing.DeduplicateCommand, CluedIn.Core_CluedIn | Used for background admin job to execute entity deduplication for all entities or by a specific business domain. | Default processing error handling | . | Remove From Processing | CluedIn.Core.Messages.Processing.RemoveFromProcessingDataCommand, CluedIn.Core_CluedIn | Mesh command to mark entities as removed from processing. | Default processing error handling | . | Split Entity | CluedIn.Core.Messages.Processing.SplitEntityCommand, CluedIn.Core_CluedIn | Re-evaluates entity code overlap and fuzzy matching overlap of records inside an entity and splits entity into several golden records if required. | Default processing error handling | . | Ensure No Entity Code Overlap | CluedIn.Core.Messages.Processing.EnsureNoDuplicatedEntitiesForEntityCodesCommand, CluedIn.Core_CluedIn | Variant of Deduplicate Entity triggered by clue ingestion to validate if deduplication is needed. | Default processing error handling | . | Send Mail | CluedIn.Core.Messages.SendMailCommand, CluedIn.Core_CluedIn | Used to send emails to users. | Default processing error handling | . Enricher queue . | Name | Queue name | Purpose | Error handling | . | Entity Enrichment Queue | CluedIn.ExternalSearch.ExternalSearchCommand, CluedIn.ExternalSearch_CluedIn | Performs enrichment for an entity. | Default processing error handling | . | Public Enricher Queue | CluedIn.PublicApi.PublicApiEnrichmentCommand, CluedIn.PublicApi_CluedIn | Performs async enrichment of data submitted via the public API endpoint to enrich arbitrary data that is not ingested into CluedIn. | Default processing error handling | . Processing retry handling . | Name | Queue name | Purpose | Error handling | . | Dedicated Retry Queue | CluedIn.Core.Messages.Processing.RetryCommand, CluedIn.Core_CluedIn | Used when in-queue retry exceeds threshold (default 2) retry count. Messages will be serialized in fashion to avoid concurrency of processing the same subject at the same time. | In-queue retry until max retry count is reached, then message is sent to dead letter. | . Processing error handling queues . Default processing error handling goes through the following steps: . | In-place “fast” retry with exponential backoff (default 3 attempts). | In-queue retry – commands erroring due to transient error will be re-queued to the same queue (default 10 times for general transient errors, default 30 times for database concurrency errors). | Dedicated retry queue – once in-queue retry attempts have reached threshold (default after 2 retry attempts), the message is routed to dedicated retry queue that will be serialized in a fashion to avoid concurrent processing of the same records. | When all retries have been exhausted, the message is sent to the dead-letter queue. | . | Name | Queue name | Purpose | Error handling | . | Processing Dead-Letter Queue | DeadLetterCommands | Contains processing dead-letter messages when all reties have been exhausted. | Messages can be retried by moving them to CluedIn.Core.Messages.Processing.DeadLetterCommand, CluedIn.Core_CluedIn_Retry. | . | Recoverable Dead-Letter Queue | CluedIn_DeadLetterMessages_Recoverable | Contains messages that were dead-lettered due to identified transient error. | Messages can be retried by moving them to CluedIn_DeadLetterMessages_Retry. | . | Non-Recoverable Dead-Letter Queue | CluedIn_DeadLetterMessages_NonRecoverable | Contains messages that was dead-lettered due to non-transient error. | Messages can be retried by moving them to CluedIn_DeadLetterMessages_Retry. | . | Retry Dead-Letter Processing Command Queue | CluedIn.Core.Messages.Processing.DeadLetterCommand, CluedIn.Core_CluedIn_Retry | Re-queues processing commands to be retried. | N/A | . | Retry Dead-Letter Command Qqueue | CluedIn_DeadLetterMessages_Retry | Re-queues generic commands to be retried. | N/A | . ",
    "url": "/paas-operations/queues-in-cluedin#data-pipeline-queues",
    
    "relUrl": "/paas-operations/queues-in-cluedin#data-pipeline-queues"
  },"1724": {
    "doc": "Queues in CluedIn",
    "title": "Stream queues",
    "content": "| Name | Queue name | Purpose | Error handling | . | StreamControlCommands Queue | StreamControlCommands | Used for stream maintenance and state change. | Default stream error handling | . | Stream Subscribe Queue | StreamSubscriber_[streamId] | Receives messages containing entities to be handled by the stream. | Default stream error handling | . | Stream Events Queue | StreamEvents_[machinename] | Used for managing control events such as start, stop and pause. | Default stream error handling | . Stream error handling queues . Stream error handling is similar to processing error handling and goes through the following steps: . | In-place “fast” retry with exponential backoff (default 3 attempts). | In-queue retry – commands erroring due to transient error will be re-queued to the same queue (default 10 times for general transient errors, default 30 times for database concurrency errors). | When all retries have been exhausted, the message is sent to the dead-letter queue. | . | Name | Queue name | Purpose | Error handling | . | Stream Dead-Letter Queue | StreamSubscriber-deadLetter-[streamid] | Dead-etter queue for stream. | Messages can be moved to retry queue or admin Web API can be used to retry messages. | . ",
    "url": "/paas-operations/queues-in-cluedin#stream-queues",
    
    "relUrl": "/paas-operations/queues-in-cluedin#stream-queues"
  },"1725": {
    "doc": "Queues in CluedIn",
    "title": "Distributed jobs subsystem queues",
    "content": "| Name | Queue name | Purpose | Error handling | . | Distributed Job Control Queue | CluedIn.DistributedJobs.Commands.IDistributedJobsControlCommand, CluedIn.DistributedJobs_DistributedJobQueueManager | Used by distributed jobs subsystem to handle job lifecycle events, | In-place retry; otherwise, always use non-recoverable dead-letter queue (except for completion callback commands). | . | Distributed Job Queue | DistributedJob_[job id] | Created per job and removed when job is done. Used to store and process job-specific work item commands. | Perform in-place retry, then silently drop command if cannot process after all tries. | . ",
    "url": "/paas-operations/queues-in-cluedin#distributed-jobs-subsystem-queues",
    
    "relUrl": "/paas-operations/queues-in-cluedin#distributed-jobs-subsystem-queues"
  },"1726": {
    "doc": "Queues in CluedIn",
    "title": "Robust callback subsystem queue",
    "content": "| Name | Queue name | Purpose | Error handling | . | Robust Callbacks Queue | CluedIn.Processing.RobustCallback.RobustCallbackCommand, CluedIn.Processing_CluedIn | Used to store and process robust callbacks. | Default robust messaging error handling. | . ",
    "url": "/paas-operations/queues-in-cluedin#robust-callback-subsystem-queue",
    
    "relUrl": "/paas-operations/queues-in-cluedin#robust-callback-subsystem-queue"
  },"1727": {
    "doc": "Queues in CluedIn",
    "title": "Remote events",
    "content": "| Name | Queue name | Purpose | Error handling | . | Remove Events Queue | RemoteEvents_[machinename]  | Synchronizes events between machines. | N/A | . ",
    "url": "/paas-operations/queues-in-cluedin#remote-events",
    
    "relUrl": "/paas-operations/queues-in-cluedin#remote-events"
  },"1728": {
    "doc": "Queues in CluedIn",
    "title": "Queues in CluedIn",
    "content": " ",
    "url": "/paas-operations/queues-in-cluedin",
    
    "relUrl": "/paas-operations/queues-in-cluedin"
  },"1729": {
    "doc": "Consume",
    "title": "Consume",
    "content": "The Consume module allows you to set up a connection between CluedIn and external systems where you usually perform tasks with business data. Having this connection, you can establish real-time sharing of ready-to-use data from CluedIn to the required third system (for example, Microsoft SQL Server database). Export targets Set up a connection to an external destination where data from CluedIn can be sent Streams Configure a sequence of actions and transformations for your data before it reaches the destination GraphQL Query your data using the GraphQL endpoint: retrieve entities by ID, vocabulary keys, and more The articles in this section will guide you through the process of connecting to the target and creating a stream to send golden records to the target. ",
    "url": "/consume",
    
    "relUrl": "/consume"
  },"1730": {
    "doc": "Deploying a new Crawler",
    "title": "Deploying a new Crawler",
    "content": "You can deploy new crawlers in two main ways: . 1: Moving files into the ServerComponent directory and running the respective commands to inject the required data into the Relational Store. 2: Use Docker to layer your crawler ontop of the base CluedIn instance. For the first option, there are 3 main components that will need to be deployed: . 1: *.dll files for each of the 4 projects of a Crawler. (You may choose to also take the *.pdb files so that you can debug) . 2: You will need to run the provider.sql file in your Provider project as to instruct CluedIn that there is a new provider that is available. (Make sure the “Active” flag is set to true so that your integration is now available to add). 3: You will need to deploy your additional files for the web application e.g. Logo and some user interface mappings. Note: You will need to reboot the CluedIn host to properly pick up these changes. The second (and recommended) approach is to create a Docker container out of your crawler and then to change your Docker Compose file to reference your new Crawler. This will handle the entire deployment process for you and will even reboot the CluedIn host for you. This approach is recommeneded as it will also help to roll this new Crawler out to test and production environments with full capabilities of roll back and uninterrupted deployments. ",
    "url": "/crawling/deploying-new-crawler",
    
    "relUrl": "/crawling/deploying-new-crawler"
  },"1731": {
    "doc": "Endpoint",
    "title": "On this page",
    "content": ". | Overview of data ingestion using an endpoint | Processing options for data received via an endpoint | Add ingestion endpoint | Send data | . In this article, you will learn how to ingest data into CluedIn using an endpoint. This method is a default solution to push your data easily into CluedIn, especially if you are using tools like Azure Data Factory, Databricks, or Apache NiFi. The process of ingesting data using an endpoint involves two steps: . | Adding an ingestion endpoint . | Sending data in HTTP POST request . | . The maximum number of columns accepted for data ingestion is 490.Before loading data into CluedIn, ensure that it does not contain unsupported characters. Refer to our knowledge base article for information about supported and unsupported characters. ",
    "url": "/integration/endpoint#on-this-page",
    
    "relUrl": "/integration/endpoint#on-this-page"
  },"1732": {
    "doc": "Endpoint",
    "title": "Overview of data ingestion using an endpoint",
    "content": "When you need to push a large set of records into CluedIn, we recommend that you start by pushing a small, representative subset of your data. As with files, this approach allows you to verify the accuracy of mapping and relations before pushing a large set of records. Once the golden records generated from this subset align with your expectations, you can safely remove those records from the system, while keeping the mapping configuration intact. After that you can push a large set of records and use the existing mapping to generate golden records in an efficient way. After you send an HTTP POST request, CluedIn checks if it is correct based on the built-in logic and limitations. If your JSON is completely invalid, CluedIn won’t start ingesting the data, and you’ll receive a response with the status “400 Bad Request”. The response body will include an array of errors that provide context for fixing them. Once you correct the request, you can try sending it again. If your JSON contains issues, such as spaces or dots in property names, CluedIn tries to fix them and store the data. These issues are treated as warnings and they are recorded on the Logs tab of the data set. You’ll receive a response with the status “200 OK”, which means that the data has been successfully sent to CluedIn. However, the response body will include an array of warnings explaining how the issues were fixed. Once CluedIn receives the data, it initially stores it in a temporary storage, accessible through the Preview tab. To turn the received data into golden records, you need to map it to the semantic model and then process it. (For advanced users) To ensure the creation of the expected golden records, you can generate a sample clue and verify its accuracy before processing. This step helps confirm that the resulting golden record aligns with your expectations. For more information, see Clue. ",
    "url": "/integration/endpoint#overview-of-data-ingestion-using-an-endpoint",
    
    "relUrl": "/integration/endpoint#overview-of-data-ingestion-using-an-endpoint"
  },"1733": {
    "doc": "Endpoint",
    "title": "Processing options for data received via an endpoint",
    "content": "CluedIn provides the following processing options for turning your data into golden records: . | Manual processing - when CluedIn receives the data from the endpoint, you are required to process the data manually. You can view the received data in the temporary storage at any time, and you can process the data set as many times as you need. In CluedIn, once a record has been processed, it won’t undergo processing again. When you trigger processing, CluedIn will check for identical records. If identical records are found, they won’t be processed again. However, if you change the origin code for the previously processed records, CluedIn will treat these record as new and process them. | Automatic processing - when CluedIn receives the data from the endpoint, this data is processed automatically. You can view the received data in the temporary storage at any time. | Bridge mode – all your JSON records will be transformed into golden records directly, without being stored in the temporary storage. However, you can rely on rely on data set logs for debugging purposes. Bridge mode allows you to use less storage and memory, resulting in increased performance. Use this mode when your mapping will not change over time and you want to use the ingestion endpoint only as a mapper. | . When you send the data to CluedIn via ingestion endpoint, a separate data set is created. If you want to send more data, add a new ingestion endpoint instead of reusing the existing one. ",
    "url": "/integration/endpoint#processing-options-for-data-received-via-an-endpoint",
    
    "relUrl": "/integration/endpoint#processing-options-for-data-received-via-an-endpoint"
  },"1734": {
    "doc": "Endpoint",
    "title": "Add ingestion endpoint",
    "content": "An ingestion endpoint is a channel through which CluedIn can receive data from external sources. To add an ingestion endpoint . | On CluedIn home page, in the Integrations section, select Import from ingestion endpoint. The Import from ingestion endpoint pane opens, where you can choose the group for storing the data source and define preliminary mapping configuration. | On the Configure tab, do the following: . | Enter the Name of the data source. | Specify the Group where the data source will be stored. You can choose the existing group or create a new group. | In the lower-right corner, select Next. | . | On Add ingestion endpoint tab, do the following: . | Enter the Endpoint name that will be used as the name of the data set. | Select the Mapping configuration option: . | New mapping – you can create a new mapping for the data set. If you choose this option, you need to select the existing business domain or create a new one. If you create a new business domain, select an icon to visually represent the business domain. | Existing mapping – you can reuse the mapping from the data set that has the same structure. If you choose this option, you need to indicate the data set with the required mapping configuration. To do that, choose the following items one by one: a data source group, a data source, and a data set. | . | In the lower-right corner, select Add. | . The ingestion endpoint is added to CluedIn. It has a label No data sent, which indicates that CluedIn has not received data for this ingestion endpoint. Now, you can send data to CluedIn by creating HTTP POST requests. | . ",
    "url": "/integration/endpoint#add-ingestion-endpoint",
    
    "relUrl": "/integration/endpoint#add-ingestion-endpoint"
  },"1735": {
    "doc": "Endpoint",
    "title": "Send data",
    "content": "In this section, Postman is used as a tool to demonstrate how to send an HTTP POST request to CluedIn. Prerequisites . To be accepted by CluedIn, your HTTP POST request should meet the following prerequisites: . | The request’s header must contain Authorization key with the value set to Bearer &lt;Your API token&gt;. It is very important to include the word Bearer followed by a space before pasting the API token. You can find the API token in CluedIn in Administration &gt; API Tokens. | The request’s body should contain raw data in JSON format. | The content-type in the request’s header should be set to application/json. | . To send data to CluedIn . | In CluedIn, open the data set that was created in the previous procedure, and then select View instructions. On the Ingestion endpoint instructions pane, find and copy the POST URL that you can use to send data to CluedIn. | In Postman, paste the URL that you copied to the URL input field of your request. Then, send the request. The data has been sent to CluedIn. You can now view it on the Preview tab of the data set. The next steps involve creating a mapping and processing the data. | . ",
    "url": "/integration/endpoint#send-data",
    
    "relUrl": "/integration/endpoint#send-data"
  },"1736": {
    "doc": "Endpoint",
    "title": "Endpoint",
    "content": " ",
    "url": "/integration/endpoint",
    
    "relUrl": "/integration/endpoint"
  },"1737": {
    "doc": "PermID",
    "title": "On this page",
    "content": ". | Add PermID enricher | Properties from PermID enricher | . This article outlines how to configure the PermID enricher. The purpose of this enricher is to provide a wide range of information about an organization (for example, Legal Entity Identifier (LEI), headquarters location, industry classification, and so on). More details are provided in Properties from PermID enricher. The PermID enricher supports the following endpoints: . | https://api-eit.refinitiv.com/permid – this API is called first. | https://permid.org/api/mdaas/getEntityById/ – this API is called second to get the social data. | . ",
    "url": "/preparation/enricher/perm-id#on-this-page",
    
    "relUrl": "/preparation/enricher/perm-id#on-this-page"
  },"1738": {
    "doc": "PermID",
    "title": "Add PermID enricher",
    "content": "To use the PermID enricher, you must provide the API key. To get the API key, register an account with PermID.org. The enricher uses the organization name to retrieve information from the PermID database. To add the PermID enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select PermID, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API token for accessing the PermID database. | Accepted Business Domain – enter the business domain to define which golden records will be enriched using the PermID enricher. | Organization Name Vocabulary Key – enter the vocabulary key that contains the names of organizations that will be used for retrieving information from the PermID database. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The PermID enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the PermID enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/perm-id#add-permid-enricher",
    
    "relUrl": "/preparation/enricher/perm-id#add-permid-enricher"
  },"1739": {
    "doc": "PermID",
    "title": "Properties from PermID enricher",
    "content": "You can find the properties added to golden records from the OpenCorporates enricher on the Properties page. For a more detailed information about the changes made to a golden record by the PermID enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the PermID enricher. | Display name | Vocabulary key | . | Domiciled In | permId.organization.domiciledIn | . | Entity Type | permId.organization.entityType | . | Incorporated In | permId.organization.incorporatedIn | . | Latest Date Of Incorporation | permId.organization.latestDateOfIncorporation | . | Lei | permId.organization.lei | . | Organization Name | permId.organization.organizationName | . | Public | permId.organization.public | . | Status | permId.organization.status | . | Website | permId.organization.website, mapped to organization.website | . | Hq Address | permId.organization.hqAddress | . | Hq Fax | permId.organization.hqFax | . | Hq Phone | permId.organization.hqPhone | . | Registered Address | permId.organization.registeredAddress | . | Registered Fax | permId.organization.registeredFax | . | Registered Phone | permId.organization.registeredPhone | . | Main Quote Exchange | permId.organization.mainQuote.exchange | . | Main Quote Id | permId.organization.mainQuote.id | . | Main Quote Mic | permId.organization.mainQuote.mic | . | Main Quote Ric | permId.organization.mainQuote.ric | . | Main Quote Ticker | permId.organization.mainQuote.ticker | . | Main Quote Url | permId.organization.mainQuote.url | . | Primary Business Sector | permId.organization.primaryBusinessSector | . | Primary Business Sector Id | permId.organization.primaryBusinessSectorId | . | Primary Economic Sector | permId.organization.primaryEconomicSector | . | Primary Economic Sector Id | permId.organization.primaryEconomicSectorId | . | Primary Industry | permId.organization.primaryIndustry | . | Primary Industry Id | permId.organization.primaryIndustryId | . | Primary Instrument Id | permId.organization.primaryInstrument.id | . | Primary Instrument Name | permId.organization.primaryInstrument.name | . | Primary Instrument Type | permId.organization.primaryInstrument.type | . | Primary Instrument Type Url | permId.organization.primaryInstrument.typeUrl | . | Primary Instrument Url | permId.organization.primaryInstrument.url | . ",
    "url": "/preparation/enricher/perm-id#properties-from-permid-enricher",
    
    "relUrl": "/preparation/enricher/perm-id#properties-from-permid-enricher"
  },"1740": {
    "doc": "PermID",
    "title": "PermID",
    "content": " ",
    "url": "/preparation/enricher/perm-id",
    
    "relUrl": "/preparation/enricher/perm-id"
  },"1741": {
    "doc": "Writing data back to operational systems",
    "title": "How to write clean data back to your operational systems",
    "content": "After CluedIn has unified and cleaned your data, you can write mastered data back to the systems that run your business. This article covers two proven patterns: . | Export Targets to generic data backplanes such as Azure Event Hubs and Azure Service Bus. | Reverse ETL via the CluedIn Power Automate integration, giving you access to 1000+ connectors to write back into SaaS and line‑of‑business apps. | . Use these approaches together: publish an authoritative event stream for broad consumption, and use targeted Reverse ETL flows where you must invoke system‑specific APIs or business logic. ",
    "url": "/kb/writing-back-to-operational-systems#how-to-write-clean-data-back-to-your-operational-systems",
    
    "relUrl": "/kb/writing-back-to-operational-systems#how-to-write-clean-data-back-to-your-operational-systems"
  },"1742": {
    "doc": "Writing data back to operational systems",
    "title": "When to use which pattern",
    "content": "| Pattern | Use when | Strengths | Considerations | . | Event backplane (Event Hubs / Service Bus) | Many consumers need the same change events (analytics, integration, microservices) | High throughput, decoupled, replayable | Consumers must subscribe and apply changes | . | Power Automate (Reverse ETL) | You need to push updates into specific SaaS/LOB systems using native connectors | 1000+ connectors, low/no‑code, retries built‑in | Per‑system flow logic; throughput depends on connector limits | . ",
    "url": "/kb/writing-back-to-operational-systems#when-to-use-which-pattern",
    
    "relUrl": "/kb/writing-back-to-operational-systems#when-to-use-which-pattern"
  },"1743": {
    "doc": "Writing data back to operational systems",
    "title": "Architecture at a glance",
    "content": "CluedIn (Mastered Changes) │ ├─ Export Target → Azure Event Hubs → Subscribers (microservices, Synapse, Databricks, Functions) │ └─ Export Target → Power Automate → 1000+ connectors (Dynamics, Salesforce, ServiceNow, SharePoint, SAP, custom APIs) . Tip: Start by publishing to an event backplane for broad access. Add Power Automate flows for systems that require validated API calls, complex transforms, or approvals. ",
    "url": "/kb/writing-back-to-operational-systems#architecture-at-a-glance",
    
    "relUrl": "/kb/writing-back-to-operational-systems#architecture-at-a-glance"
  },"1744": {
    "doc": "Writing data back to operational systems",
    "title": "Prerequisites",
    "content": ". | A CluedIn environment with Export Targets enabled. | Access to Azure Event Hubs and/or Azure Service Bus (namespace, hub/queue, credentials/Managed Identity). | Access to Microsoft Power Automate with permission to create flows and use target connectors. | A clear definition of which entity types and which changes you want to publish (e.g., Customer upserts, Address deletes, Product price changes). | . ",
    "url": "/kb/writing-back-to-operational-systems#prerequisites",
    
    "relUrl": "/kb/writing-back-to-operational-systems#prerequisites"
  },"1745": {
    "doc": "Writing data back to operational systems",
    "title": "Event backplanes with Azure Event Hubs / Service Bus",
    "content": "CluedIn can publish change events (golden record upserts/deletes, property changes, edge changes) to an Azure messaging fabric. Consumers subscribe and apply those changes to their own stores. 1) Create an Export Target . | In CluedIn, go to Integrations → Export Targets. | Choose Azure Event Hubs (streaming) or Azure Service Bus (queue/topic). | Provide Namespace, Hub/Queue/Topic name, and credentials. Prefer Managed Identity or short‑lived SAS tokens. | Select entity types to publish (e.g., Customer, Organization, Product) and the event kinds (Upsert, Delete). | Set partition key (recommended: stable identity such as entityId or a composite like entityType:entityId). | . 2) Event shape (recommended) . Emit compact, versioned, self‑describing messages. { \"specVersion\": \"cluedin-1.0\", \"eventType\": \"EntityChanged\", // EntityChanged | EntityDeleted \"entityType\": \"Customer\", \"entityId\": \"cluedin:entity:7f2a...\", \"version\": 42, // monotonically increasing \"timestamp\": \"2025-08-24T03:15:21Z\", \"externalIds\": [ { \"system\": \"crm\", \"id\": \"12345\" }, { \"system\": \"billing\", \"id\": \"ACME-001\" } ], \"changes\": { \"attributes\": { \"name\": { \"value\": \"Acme Pty Ltd\", \"provenance\": \"crm\", \"confidence\": 0.98 }, \"email\": { \"value\": \"ap@acme.com\", \"provenance\": \"crm\" } }, \"edges\": [ { \"rel\": \"billTo\", \"to\": { \"externalId\": { \"system\": \"erp\", \"id\": \"ACME-001\" } } } ] }, \"hash\": \"sha256:...\", \"previousHash\": \"sha256:...\" } . Headers (suggested for Service Bus): . | messageId: ${entityType}:${entityId}:${version} (deduplication) | correlationId: CluedIn request or pipeline correlation id | contentType: application/json | subject: ${entityType}.${eventType} | . 3) Delivery &amp; reliability tips . | At‑least‑once: Make consumers idempotent using messageId and version. | Ordering: Choose a partition key that keeps related updates together (e.g., entityId). | Replay: Use Event Hubs consumer groups and checkpoints to reprocess. For Service Bus, use sessions and dead‑letter queues. | Filtering: Publish only fields that downstreams need, or keep full payloads and let consumers select. | PII: Mask/redact fields not required by consumers. Apply data classification tags for governance. | . ",
    "url": "/kb/writing-back-to-operational-systems#event-backplanes-with-azure-event-hubs--service-bus",
    
    "relUrl": "/kb/writing-back-to-operational-systems#event-backplanes-with-azure-event-hubs--service-bus"
  },"1746": {
    "doc": "Writing data back to operational systems",
    "title": "Reverse ETL via Power Automate (CluedIn integration)",
    "content": "Use CluedIn’s integration with Power Automate to call system‑specific connectors (Dynamics 365, Salesforce, ServiceNow, SharePoint, SAP, custom HTTP, etc.). 1) Create a Power Automate Export Target . | In CluedIn, create an Export Target → Power Automate. | Choose which entities and events to send (e.g., Customer upserts). | Copy the Flow endpoint URL or connect using the CluedIn Power Automate connector (if available in your tenant). | . 2) Build the flow in Power Automate . | Trigger: When an HTTP request is received or CluedIn — On entity change. | Parse JSON: Use the sample payload above to generate a schema. | Branching: Use conditions to route by entityType, eventType, or changed fields. | Actions: Use target connectors (e.g., Dynamics 365 — Update a row, Salesforce — Upsert record, ServiceNow — Update record, SharePoint — Create item, HTTP — Invoke API). | Idempotency: Use entityId + version as a natural de‑dup key; many connectors support upsert based on an external ID. | Error handling: Configure retry policies, scope with run after on failure, and send alerts to Teams/Email. | . Example mapping (pseudo): . IF eventType == 'EntityChanged' AND entityType == 'Customer': Dynamics.UpdateRow( table='account', key=externalIds['crm'], name=changes.attributes.name.value, email=changes.attributes.email.value, source='CluedIn', version=version ) . 3) Throughput &amp; limits . | Use concurrency control in the trigger and action steps. | Batch writes where connectors support it; otherwise, shard by entityType or route via Event Hubs → Azure Functions → Connector. | Respect connector API quotas; enable per‑flow and per‑user limits as needed. | . ",
    "url": "/kb/writing-back-to-operational-systems#reverse-etl-via-power-automate-cluedin-integration",
    
    "relUrl": "/kb/writing-back-to-operational-systems#reverse-etl-via-power-automate-cluedin-integration"
  },"1747": {
    "doc": "Writing data back to operational systems",
    "title": "Best practices",
    "content": ". | Publish only “clean” changes: filter on golden status, minimum confidence, or approved stewardship state. | Model deletes as tombstones: send EntityDeleted with entityId, version, and timestamp; let consumers decide physical delete vs. soft‑delete. | Use consistent versions: monotonic version per entity to simplify consumer logic. | Keep provenance: include provenance and lastSeen per attribute for auditability. | Protect sensitive data: apply column‑level masking where not needed downstream; prefer per‑consumer event contracts. | Observability: capture delivery success rate, dead‑letter counts, consumer lag, connector API errors, and average end‑to‑end latency. | Rollout safely: canary by entity type or by subset of records; use shadow topics/queues before switching consumers. | . ",
    "url": "/kb/writing-back-to-operational-systems#best-practices",
    
    "relUrl": "/kb/writing-back-to-operational-systems#best-practices"
  },"1748": {
    "doc": "Writing data back to operational systems",
    "title": "Common patterns",
    "content": "Backplane → Function → System . Use Event Hubs as the single export; Azure Functions subscribe, transform, and write to SAP/CRM/ERP with native SDKs or Power Automate webhooks. Direct Reverse ETL . For business apps with well‑behaved connectors (Dynamics, Salesforce), send directly to Power Automate and upsert into the target object. Keep a control table/log of last pushed version per entity in the target for audit and rollback. ",
    "url": "/kb/writing-back-to-operational-systems#common-patterns",
    
    "relUrl": "/kb/writing-back-to-operational-systems#common-patterns"
  },"1749": {
    "doc": "Writing data back to operational systems",
    "title": "Security &amp; governance",
    "content": ". | Authentication: Use Managed Identity for Event Hubs/Service Bus where possible. Store secrets in Key Vault. | Authorization: Scope connectors and SAS tokens to least privilege. | Network: Restrict namespaces with private endpoints where feasible. | Compliance: Tag messages with data classification; ensure downstreams inherit retention/DSAR policies. | . ",
    "url": "/kb/writing-back-to-operational-systems#security--governance",
    
    "relUrl": "/kb/writing-back-to-operational-systems#security--governance"
  },"1750": {
    "doc": "Writing data back to operational systems",
    "title": "Troubleshooting",
    "content": "| Symptom | Likely cause | Fix | . | Consumers see duplicates | At‑least‑once delivery | Use messageId + version for idempotency; ignore lower versions | . | Lost ordering for the same entity | Partitioning by a non‑stable key | Partition by entityId; enable sessions (Service Bus) | . | Connector throttling (429/5xx) | API limits | Enable retries with backoff, add concurrency caps, batch where supported | . | Flow runs succeed but target doesn’t change | Wrong key mapping | Verify external ID used for upsert; log target response IDs | . | PII leakage to broad consumers | Over‑rich event payload | Publish per‑consumer schemas; mask unnecessary attributes | . ",
    "url": "/kb/writing-back-to-operational-systems#troubleshooting",
    
    "relUrl": "/kb/writing-back-to-operational-systems#troubleshooting"
  },"1751": {
    "doc": "Writing data back to operational systems",
    "title": "FAQ",
    "content": "Can I publish only approved stewarded changes? Yes. Filter on stewardship state or confidence before emitting to the Export Target. What about bi‑directional sync? Treat inbound changes from operational systems as new facts; CluedIn re‑masters and emits a new version. Avoid blind echo loops by checking provenance or version before writing back. Do I need both Event Hubs and Power Automate? Not always. Use Event Hubs for broad distribution and analytics; add Power Automate when you must call specific application connectors or orchestrate approvals. ",
    "url": "/kb/writing-back-to-operational-systems#faq",
    
    "relUrl": "/kb/writing-back-to-operational-systems#faq"
  },"1752": {
    "doc": "Writing data back to operational systems",
    "title": "Quick start checklists",
    "content": "Event Hubs / Service Bus . | Export Target created with correct namespace &amp; hub/queue/topic | Partition key = entityId (or stable composite) | Message headers set (messageId, subject, correlationId) | Consumer checkpoints configured; dead‑letter monitored | . Power Automate . | Flow trigger wired to CluedIn export (HTTP or connector) | Parse JSON step with validated schema | Upsert action mapped to correct external ID | Retry and alerting configured; concurrency tuned | . Summary . | Use Azure Event Hubs / Service Bus Export Targets to publish a high‑quality, replayable stream of mastered changes. | Use Power Automate (Reverse ETL) to push clean data into operational systems via 1000+ connectors with minimal code. | Design for idempotency, ordering, security, and observability so downstream systems remain accurate and auditable. | . ",
    "url": "/kb/writing-back-to-operational-systems#quick-start-checklists",
    
    "relUrl": "/kb/writing-back-to-operational-systems#quick-start-checklists"
  },"1753": {
    "doc": "Writing data back to operational systems",
    "title": "Writing data back to operational systems",
    "content": " ",
    "url": "/kb/writing-back-to-operational-systems",
    
    "relUrl": "/kb/writing-back-to-operational-systems"
  },"1754": {
    "doc": "Database",
    "title": "On this page",
    "content": ". | Add connection to database | Ingest database tables | . In this article, you will learn how to ingest data from a database into CluedIn. You can ingest the database tables to CluedIn if you have the read permission to these database tables. The process of ingesting data from the database involves two steps: . | Adding connection to the database . | Ingesting the database tables . | . The maximum number of columns accepted for data ingestion is 490.Before loading data into CluedIn, ensure that it does not contain unsupported characters. Refer to our knowledge base article for information about supported and unsupported characters. ",
    "url": "/integration/database#on-this-page",
    
    "relUrl": "/integration/database#on-this-page"
  },"1755": {
    "doc": "Database",
    "title": "Add connection to database",
    "content": "To be able to access the database tables in CluedIn, first establish a connection to the database. To add a connection to the database . | On CluedIn home page, in the Integrations section, select Import from database. The Import from database pane opens, where you can provide the database connection details and choose the group for storing the data source. | On the Connection string tab, do the following: . | Choose the SQL database technology to query the data (Microsoft SQL Server, MySQL, or Postgres). | Enter the database connection details such as Host, Database name, Username, and Password. Optionally, you may add Port number. | In the lower-right corner, select Test connection. After you receive a notification that the connection is successful, select Next. | . | On the Configure tab, do the following: . | Enter the Name of the data source. | Specify the Group where the data source will be stored. You can choose the existing group or create a new group. | In the lower-right corner, select Add. | . The database connection is added to CluedIn. Now, you can add database tables to CluedIn. | . ",
    "url": "/integration/database#add-connection-to-database",
    
    "relUrl": "/integration/database#add-connection-to-database"
  },"1756": {
    "doc": "Database",
    "title": "Ingest database tables",
    "content": "With an established connection to the database, you can choose which database tables you want to ingest into CluedIn. To ingest database tables . | Open the data source, and then select Add new table. The Add data table pane opens, where you can view all tables existing in the database. | Select the checkboxes next to the tables you want to ingest into CluedIn. Then, in the lower-right corner, select Add. The tables are added to CluedIn. Each table is added as a separate data set. Next, create mapping and process data. | . ",
    "url": "/integration/database#ingest-database-tables",
    
    "relUrl": "/integration/database#ingest-database-tables"
  },"1757": {
    "doc": "Database",
    "title": "Database",
    "content": " ",
    "url": "/integration/database",
    
    "relUrl": "/integration/database"
  },"1758": {
    "doc": "Vatlayer",
    "title": "On this page",
    "content": ". | Add Vatlayer enricher | Properties from Vatlayer enricher | . This article outlines how to configure the Vatlayer enricher. The purpose of this enricher is to validate EU VAT numbers and retrieve company information (for example, address, CVR number, full VAT, and so on). More details can be found in Properties from Vatlayer enricher. The Vatlayer enricher does not support UK VAT numbers. The Vatlayer enricher supports the following endpoint: . | http://www.apilayer.net/api/validate?access_key={apiToken}&amp;vat_number={vat}&amp;format=1, where {apiToken} is your API key for retrieving information from the Vatlayer website and {vat} is the VAT number of the company. | . ",
    "url": "/preparation/enricher/vatlayer#on-this-page",
    
    "relUrl": "/preparation/enricher/vatlayer#on-this-page"
  },"1759": {
    "doc": "Vatlayer",
    "title": "Add Vatlayer enricher",
    "content": "To use the Vatlayer enricher, you will need to provide the API key. To get it, sign up for an account on the Vatlayer website. The enricher uses the VAT number to retrieve VAT-related information. To add the Vatlayer enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Vatlayer, and then select Next. | On the Configure tab, provide the following details: . | API Access Key – enter the API key for retrieving information from the Vatlayer website. | Accepted Business Domain – enter the business domain to define which golden records will be enriched using the Vatlayer enricher. | Accepted Vocabulary Key – enter the vocabulary key that contains the VAT numbers of companies that you want to enrich. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Vatlayer enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Vatlayer enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided to configure the enricher. | . ",
    "url": "/preparation/enricher/vatlayer#add-vatlayer-enricher",
    
    "relUrl": "/preparation/enricher/vatlayer#add-vatlayer-enricher"
  },"1760": {
    "doc": "Vatlayer",
    "title": "Properties from Vatlayer enricher",
    "content": "You can find the properties added to golden records from the Vatlayer enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Vatlayer enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Vatlayer enricher. | Display name | Vocabulary key | . | Address | vatLayer.organization.address | . | Country Code | vatLayer.organization.countryCode | . | Cvr Number | vatLayer.organization.cvrNumber | . | Full Vat | vatLayer.organization.fullVat | . | Name | vatLayer.organization.name | . ",
    "url": "/preparation/enricher/vatlayer#properties-from-vatlayer-enricher",
    
    "relUrl": "/preparation/enricher/vatlayer#properties-from-vatlayer-enricher"
  },"1761": {
    "doc": "Vatlayer",
    "title": "Vatlayer",
    "content": " ",
    "url": "/preparation/enricher/vatlayer",
    
    "relUrl": "/preparation/enricher/vatlayer"
  },"1762": {
    "doc": "Create mapping",
    "title": "On this page",
    "content": ". | Choose mapping type | Configure mapping details . | Manual mapping | Auto mapping | Existing mapping | AI mapping | . | Mapping preview | Reset mapping | . Mapping is a semantic layer that allows CluedIn to understand the nature of data and process it to produce golden records. Mapping encompasses complex tasks, and CluedIn is committed to consistently enhancing the user experience associated with it. Nevertheless, it is crucial to grasp certain mapping concepts to leverage its full potential. The process of creating a mapping consists of two parts: . | Choosing the mapping type . | Configuring the mapping details . | . ",
    "url": "/integration/create-mapping#on-this-page",
    
    "relUrl": "/integration/create-mapping#on-this-page"
  },"1763": {
    "doc": "Create mapping",
    "title": "Choose mapping type",
    "content": "Mapping type defines how you want to configure the mapping of original fields from the data set to the vocabulary keys in CluedIn. You can choose from the following mapping types: . | Auto Mapping – CluedIn automatically creates mapping for your data. This option provides a reliable mapping configuration. You can check the results of auto mapping and make changes if needed. | Manual Mapping – CluedIn creates empty mapping configuration that you can modify as you want. This option is challenging and time-consuming because you have to create the configuration from scratch. | Existing Mapping – CluedIn uses the existing mapping configuration. This option is useful if you want to create mapping for the data set that has the exact same structure as the existing data sets. | AI Mapping – CluedIn uses AI capabilities to interpret your data and create mapping. You can check the results of AI mapping and make changes if needed. | . The AI mapping feature is is not available by default. To use it, you need to fulfill several prerequisites: . | Go to Administration &gt; Feature Flags and enable the AI Mapping feature. | Go to Administration &gt; Settings. Scroll down to the Open AI section and complete the following fields: . | API Key – check the following link to learn where to find your API Key. | Base Url – you can find this value in your Azure OpenAI resource, in Resource Management &gt; Keys and Endpoint, in the Endpoint field. | Resource Key – you can find this value in your Azure OpenAI resource, in Resource Management &gt; Keys and Endpoint. You can use either KEY 1 or KEY 2. | Deployment Name – you can find this value in your Azure OpenAI resource, in Resource Management &gt; Model deployments. For more information about the required variables, see Microsoft documentation. | . | . To choose the mapping type . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set for which you need to create mapping. | Go to the Map tab, and then select Map Data. The Create Mapping pane opens, where you can choose the mapping type and configure the mapping details. | On the Mapping Type tab, choose the mapping type. Then, in the lower-right corner, select Next. The Configure tab opens, where you can provide other mapping details following the instructions from the next section. | . ",
    "url": "/integration/create-mapping#choose-mapping-type",
    
    "relUrl": "/integration/create-mapping#choose-mapping-type"
  },"1764": {
    "doc": "Create mapping",
    "title": "Configure mapping details",
    "content": "Depending on the selected mapping type, the actions required from you on the Configure tab may differ. To find relevant instructions for your chosen mapping type, refer to the needed section: . | Manual mapping . | Auto mapping . | Existing mapping . | AI mapping . | . Manual mapping . Manual mapping gives you full control over how each field for your data set will generate records. To configure manual mapping . | Choose the existing business domain or create a new one. If you create a new business domain, select an icon to visually represent the business domain. | Choose the existing vocabulary or create a new one. With manual mapping, the mapping preview is not available. | Select Create Mapping. An empty mapping configuration is created. Next, map the original fields to the vocabulary keys. You can map all fields at once or map fields one by one. | . To map all fields at once . | On the Map tab, select Map All Fields. | In the pane that opens, review the suggested mapping. You can edit the vocabulary key names if needed. Also, if you change the vocabulary key name, you can edit its data type. | In the lower-right corner, select Add Property to Annotation. The mapping is created. Next, review the mapping details. | . To map fields one by one . | On the Map tab, select Edit Mapping. | In the Maps to vocabulary key column, expand the dropdown list, and then select Add mapping. In the pane that opens, do the following: . | Expand the dropdown list and select the vocabulary key to which the original field should be mapped. | If you want to use the field as a primary unique identifier for the record, turn on the Use as entity code toggle. | If you want to use the field as an alias for the records, turn on the Use as alias toggle. | In the lower-right corner, select Add Property to Annotation. | . | Repeat step 2 for each field. The mapping is created. Next, review the mapping details. | . Auto mapping . Auto mapping tries to detect unique codes and map original columns to the most appropriate vocabulary keys. To configure auto mapping . | Choose the existing business domain or create a new one. If you create a new business domain, select an icon to visually represent the business domain. | Choose the existing vocabulary or create a new one. | In the Mapping Preview section, review how the original fields will be mapped to the vocabulary keys in CluedIn. | In the lower-right corner, select Create Mapping. The mapping is created. Next, review the mapping details. | . Existing mapping . If you’ve already created a mapping for a data set and you have another data set with the same structure, you can save time by reusing the existing mapping. To configure existing mapping . | Select the existing mapping that you want to reuse. The name of the mapping consists of two parts: the mapping type and the data set for which the mapping was generated. | Select the existing data set that contains the same fields as in the current data set. | In the lower-right corner, select Create Mapping. The mapping is created. Next, review the mapping details. | . AI mapping . CluedIn AI recommendation engine helps you create mapping in a quick and efficient manner. To use AI capabilities to create mapping, first complete all the steps described in Azure Open AI Integration. AI mapping analyzes your data set and suggests the following details for your mapping: . | Business domain and vocabulary. | Origin – field used to produce the primary unique identifier. | Codes – fields used to produce additional unique identifiers. | Mapping of original fields to the vocabulary keys. For more details, see Mapping preview. | . You need to review AI suggestions and make changes if needed. ",
    "url": "/integration/create-mapping#configure-mapping-details",
    
    "relUrl": "/integration/create-mapping#configure-mapping-details"
  },"1765": {
    "doc": "Create mapping",
    "title": "Mapping preview",
    "content": "The Mapping Preview section is available when creating mapping with the following types: auto mapping and AI mapping. This section contains a table showing how the original fields will be mapped to the vocabulary keys in CluedIn. The table consists of the following columns: . | The checkboxes column that represents the fields that will be sent to CluedIn during processing. If you do not want to send a specific field, clear the checkbox. The status of such field will be changed to Ignored. | Original Field – contains the names of the columns from the data set. | Will Map To – contains the vocabulary keys to which the original fields will be mapped. If needed, you can edit the names of both new and existing vocabulary keys directly in the table. If you edited the name of the existing vocabulary key, its status becomes New. | Data Type – contains the data type of each vocabulary key for strong typing. If needed, you can change the data type of both new and existing vocabulary keys directly in the table. If you edited the data type of the existing vocabulary key, its status becomes New. | Status – contains the status of each vocabulary key (New, Existing, or Ignored). | Preview – allows you to view the values in the column. | . ",
    "url": "/integration/create-mapping#mapping-preview",
    
    "relUrl": "/integration/create-mapping#mapping-preview"
  },"1766": {
    "doc": "Create mapping",
    "title": "Reset mapping",
    "content": "If you’ve created a mapping and are concerned about its accuracy or correctness, you can reset it and start over. However, note that if the data set has already been processed, the option to reset the mapping is not available. In such cases, you’ll need to perform additional steps before you can reset the mapping for a processed data set. To reset the mapping . | Depending on whether you have already processed the data set, do one of the following: . | If you have already processed the data set: . | Remove records created from the data set following the instruction here. | Remove processing logs of the data set following the instruction here. | On the Map tab, select Reset, and then confirm that you want to reset the mapping configuration. | . | In you have not yet processed the data set: . | Go the Map tab, select Reset, and then confirm that you want to reset the mapping configuration. | . | . Now, you can create the mapping from scratch. | . ",
    "url": "/integration/create-mapping#reset-mapping",
    
    "relUrl": "/integration/create-mapping#reset-mapping"
  },"1767": {
    "doc": "Create mapping",
    "title": "Create mapping",
    "content": " ",
    "url": "/integration/create-mapping",
    
    "relUrl": "/integration/create-mapping"
  },"1768": {
    "doc": "Release overview",
    "title": "On this page",
    "content": ". | Release notes . | Latest release | Previous releases | Releases of additional resources | . | Release plan for 2025–2026 | Release process . | Release stages | Product release versioning | . | . In this article, you will find links to release notes and learn about our release process and product versioning. ",
    "url": "/release-notes#on-this-page",
    
    "relUrl": "/release-notes#on-this-page"
  },"1769": {
    "doc": "Release overview",
    "title": "Release notes",
    "content": "This section includes links to release notes for the CluedIn platform as well as links to the releases of additional resources. Latest release . | Version | Technical version | Release notes | . | 2025.05.02 | 4.5.2 | View release notes | . | 2025.05.01 | 4.5.1 | View release notes | . | 2025.05.00 | 4.5.0 | View release notes | . Previous releases . | Version | Technical version | Release notes | . | 2024.12.02 | 4.4.2 | View release notes | . | 2024.12.01 | 4.4.1 | View release notes | . | 2024.12 | 4.4.0 | View release notes | . | 2024.07 | 4.3 | View release notes | . | 2024.04 | 4.3.0 | View release notes | . | 2024.03 | 4.1.0 | View release notes | . | 2024.01 | 4.0.0 | View release notes | . | 2023.07 |   | View release notes | . | 2023.04 |   | View release notes | . | 2022.10 |   | View release notes | . | 2022.06 |   | View release notes | . | 3.3 |   | View release notes | . | 3.2 |   | View release notes | . Releases of additional resources . | Resource | Description | Release | . | Home repo | Contains resources for the local installation of CluedIn. | View releases | . | Charts repo | Contains installation scripts to install CluedIn in Kubernetes. | View releases | . | Integrations releases | Contains releases of installation packages for enrichers and connectors. | View releases | . ",
    "url": "/release-notes#release-notes",
    
    "relUrl": "/release-notes#release-notes"
  },"1770": {
    "doc": "Release overview",
    "title": "Release plan for 2025–2026",
    "content": "The following table outlines the features, updates, and UX improvements we plan to implement between September 2025 and September 2026. ",
    "url": "/release-notes#release-plan-for-20252026",
    
    "relUrl": "/release-notes#release-plan-for-20252026"
  },"1771": {
    "doc": "Release overview",
    "title": "Release process",
    "content": "In this section, you will learn about our release process and the versioning of product releases. You’ll gain an understanding of the stages that CluedIn features go through before becoming generally available, as well as get to know the versioning scheme that we use to deliver changes. Release stages . In order to prepare new and requested features, enhancements, and fixes, we follow a 6-stage release process. What is more, we are often in multiple release stages simultaneously across several releases. This approach allows us to efficiently prioritize tasks and ensure that features are released in a timely manner. By overlapping stages, we can quickly adapt to changing requirements and deliver continuous improvements. The following table describes each stage of the release process. | Release stage | Description | . | Pre-Planning | This stage is dedicated to the ongoing grooming exercise to keep our backlog up to date.On conclusion of the Planning stage for a given release, we enter the Pre-Planning stage for the subsequent release. The Pre-Planning stage begins with identification of the new top 20 features and improvements and refinement efforts towards them. | . | Planning | During this stage, we assess our groomed backlog items and upcoming customer deadlines. As a result, we create a release manifest detailing expectations and timelines for a given release. | . | Development | This stage is dedicated to active development. This is where the engineering team brings the customer’s ideas to life. It is important for us to stay in communication during this phase to ensure that we are building functionality that brings value to the customer. | . | Beta | This is the first testable version of the release. This is the step where we start regression testing. While this version is available in ACR for customer use, it is not recommended to give access to it without specific relationship management. We recommend using the Beta to demo the functionality and gather feedback.Beta cannot be used in production or with real data. | . | Release Candidate | This is a stable version of the release, with potential for a few outstanding minor issues. We open the Release Candidate for testing by our internal teams and selected customers. If a customer needs specific functionality early, this is the recommended point at which they are upgraded.Even though Release Candidate is a production-ready version of the release, the release assets are not yet ready. | . | Public Availability | This is when our release becomes generally available to customers. The release can be classified as major or minor; this classification affects whether a customer should upgrade. SaaS customers are always on the latest Public Availability release. | . Product release versioning . To support prompt delivery of patches, security fixes, and features, we use a versioning scheme that allows us to deliver changes as they are available rather than waiting for a full platform release. Starting from June 30, 2022, we started using a date-based pattern for versioning. By using dates, we can better communicate to our customers how up to date their instance of CluedIn is. Our date-based version is divided into three parts: Year, Month, and Update. | The Year is always represented as a four-digit year (e.g. 2024). | The Month is always represented as a two-digit month (e.g. 10 for October). | The Update is always represented as at least a two-digit number (e.g. 01 or 10). | . The parts of the version are then separated by . to supply the final version: . | 2024.10.00 – this would be the first release in October 2024. | 2024.10.01 – this would be the first update to the October 2024 release. | 2024.10.14 – this would be the fourteenth update to the October 2024 release. | . Prior to date-based versioning, the CluedIn platform used a semantic versioning scheme. This is an industry standard approach to versioning that we will keep for each of the services and tools that the platform is built on (e.g., Home, Helm, Crawlers, and so on). Each tool or service is its own product and will release changes and updates as they become available. ",
    "url": "/release-notes#release-process",
    
    "relUrl": "/release-notes#release-process"
  },"1772": {
    "doc": "Release overview",
    "title": "Release overview",
    "content": " ",
    "url": "/release-notes",
    
    "relUrl": "/release-notes"
  },"1773": {
    "doc": "Web",
    "title": "On this page",
    "content": ". | Add Web enricher | Properties from Web enricher | . This article outlines how to configure the Web enricher. The purpose of this enricher is to provide information about companies through their websites (for example, description, Facebook and LinkedIn accounts, technologies list, and so on). More details can be found in Properties from Web enricher. The Web enricher supports any website provided in the Website Vocab Key field of the enricher configuration. ",
    "url": "/preparation/enricher/web#on-this-page",
    
    "relUrl": "/preparation/enricher/web#on-this-page"
  },"1774": {
    "doc": "Web",
    "title": "Add Web enricher",
    "content": "The Web enricher uses the company website as an input for retrieving additional information about a golden record from the internet. To add the Web enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Web, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched using the Web enricher. | Website Vocabulary Key – enter the vocabulary key that contains the websites of companies that you want to enrich. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Web enricher is added and has an active status. This means that it will enrich golden records based on the configuration details during processing or when you trigger external enrichment. | . After the Web enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/web#add-web-enricher",
    
    "relUrl": "/preparation/enricher/web#add-web-enricher"
  },"1775": {
    "doc": "Web",
    "title": "Properties from Web enricher",
    "content": "You can find the properties added to golden records from the Web enricher on the Properties page. For a more detailed information about the changes made to a golden record by the Web enricher, check the corresponding data part on the History page. The following table lists the properties that can be added to golden records by the Web enricher. | Display name | Vocabulary key | . | Copyright Entity | website.CopyrightEntity | . | Description | website.Description | . | Title | website.Title | . | Copyright Entity | website.CopyrightEntity | . | Website Description | website.WebsiteDescription | . | Name | website.Name | . | URI | website.URI | . | Phone Number | website.PhoneNumber | . | Fax Number | website.FaxNumber | . | Contact Email | website.ContactEmail | . | Address | website.Address | . | Country | website.Country | . | Technologies List Text | website.TechnologiesListText | . | Address Country | website.AddressCountry | . | Postal Code | website.PostalCode | . | Street Address | website.StreetAddress | . | Founding Date | website.FoundingDate | . | Duns | website.Duns | . | Global Location Number | website.GlobalLocationNumber | . | Isic V4 | website.IsicV4 | . | Lei Code | website.LeiCode | . | Naics | website.Naics | . | Tax Id | website.TaxId | . | Vat Id | website.VatId | . | Ticker Symbol | website.TickerSymbol | . | CVR | website.CVR | . | Google Analytics | website.GoogleAnalytics | . Additionally, the Web enricher sometimes returns social data. The following table displays social links that can be added to golden records by the Web enricher. | Display name | Vocabulary key | . | Facebook | social.facebook | . | Linked In | social.linkedIn | . | Twitter | social.twitter | . | You Tube | social.youTube | . | Google Plus | social.googlePlus | . | Instagram | social.instagram | . | Four Square | social.fourSquare | . | Blogger | social.blogger | . | Flickr | social.flickr | . | Good Reads | social.goodReads | . | TripIt | social.tripIt | . | Tumblr | social.tumblr | . | Vimeo | social.vimeo | . | Word Press | social.wordPress | . | Yahoo | social.yahoo | . | Pinterest | social.pinterest | . | Weibo | social.weibo | . | Xing | social.xing | . | Git Hub | social.gitHub | . | Stackoverflow | social.stackoverflow | . | Klout | social.klout | . | Gravatar | social.gravatar | . | Angel Co | social.angelCo | . | About Me | social.aboutMe | . | Quora | social.quora | . | Foursquare | social.foursquare | . | Picasa | social.picasa | . | Plan Cast | social.planCast | . ",
    "url": "/preparation/enricher/web#properties-from-web-enricher",
    
    "relUrl": "/preparation/enricher/web#properties-from-web-enricher"
  },"1776": {
    "doc": "Web",
    "title": "Web",
    "content": " ",
    "url": "/preparation/enricher/web",
    
    "relUrl": "/preparation/enricher/web"
  },"1777": {
    "doc": "Additional operations",
    "title": "Additional operations",
    "content": "In this section, you will learn how to improve the quality of your data in the Data Sources module in CluedIn. Although normalizing, transforming, and improving the quality of records before processing is optional, we recommend that you do it for several reasons: . | Ensure alignment with your data normalization polices. | Get better matches in deduplication projects. | Reduce the number of records to clean. | Optimize the streaming of records. | . CluedIn provides the following tools that you can use to enhance the quality of your data before processing: . | Preview – analyze the uploaded records and improve their quality before processing. | Validations – check the records for errors, inconsistencies, and missing values and fix these issues to improve the quality of the records. | Property rules – normalize and transform property values of mapped records. | Pre-process rules – improve the overall quality of mapped records. | Advanced mapping code – modify clues programmatically by applying complex conditions. | Quarantine – handle records that do not meet certain conditions set in property rules, pre-process rules, or advanced mapping. | Approval – approve or reject specific records to ensure that only verified records are sent for processing. | . You will learn how to interpret logs and monitoring statistics to get an insight into what is going on with your records. Additionally, you will learn about the removal of records that were created from a specific data source. ",
    "url": "/integration/additional-operations-on-records",
    
    "relUrl": "/integration/additional-operations-on-records"
  },"1778": {
    "doc": "Microsoft Integration",
    "title": "Microsoft Integration",
    "content": "Introduction . As CluedIn is now available in Azure Marketplace, we aim to provide native integration with Microsoft products and services into our system. These products and services include Purview, Data Factory, Data Lake, PowerApps, Power Automate, Dataverse, and more. These integrations will automate most of the features available in Microsoft, make the system easier to use, and give you a native feel between CluedIn and Microsoft platforms. ",
    "url": "/microsoft-integration",
    
    "relUrl": "/microsoft-integration"
  },"1779": {
    "doc": "Review mapping",
    "title": "On this page",
    "content": ". | Properties | Identifiers | Relationships | . After the mapping is created, review the mapping details to make sure that your records will be produced and merged in the most efficient way. This article will guide you through the essential aspects to check when reviewing your mapping details. Before the data is processed, your mapping changes won’t affect the existing records in CluedIn. To open the mapping details, on the Map tab of the data set, select Edit mapping. You’ll see three tabs containing all mapping details: . | Map columns to vocabulary key – here you can check which properties will be sent to CluedIn after processing. | Map entity – here you can check the general details of the records that will be created after processing and the identifiers that will uniquely represent the records. | Add edge relations – here you can create rules for establishing the relationships between golden records. | . Properties . On the Map columns to vocabulary key tab, check how the original fields will be mapped to the vocabulary keys in CluedIn after processing. The following actions are available for you: . | Ignore certain fields if you don’t want to have them in CluedIn after the data is processed. | Map the original field to a different vocabulary key. | Add property rules to improve the quality of mapped records by normalizing and transforming property values. | . Identifiers . On the Map entity tab, review the general mapping details and check the identifiers that will uniquely represent the records in CluedIn—primary identifier and identifiers. What are general details? . | Business domain and vocabulary. | Entity name – name of the records that is displayed on the search results page and on the record details page. | Preview image, description, date created, and date modified – record properties that you can find on the search results page and on the record details page. You can select which column should be used for each of these settings. | . What is a primary identifier? . A primary identifier is a unique identifier of the record in CluedIn. If the primary identifiers are identical, the records will be merged. This merging is faster than creating a deduplication project because it is done on the fly and is based on strict equality matching. The deduplication project, on the other hand, is based on fuzzy matching and requires you to define matching criteria, making it a more time-consuming process. Even if you prefer to merge records by running a deduplication project, merging by primary identifiers produces cleaner, “pre-merged” records. As a result, the deduplication project will generate better results and be more performant. Options for generating the primary identifier . Depending on how unique you consider the records to be, you can choose one of the following options for generating the primary identifier: . | Single key – CluedIn will generate unique primary identifiers for the records based on the selected property. This is the most commonly used option because data often already contains unique identifiers (for example, GUIDs) from the source systems. | Auto-generated key – CluedIn will generate unique primary identifiers for the records. Choosing this option may lead to an increased number of duplicates in the system, but you can mitigate this by running a deduplication project afterwards. | Compound key – CluedIn will generate unique primary identifiers for the records by combining selected properties. Choose this option if you are confident that the data structure won’t change in the future. For example, you can select the following properties to generate a compound key: First Name, Last Name, City, Address Line 1, and Country. | . The following diagram will help you in determining which option to use for generating the primary identifier. Example . We ingested a file with 1,000 records personal data containing the following columns: ID, First Name, Last Name, Email, SSN, and Country Code. To create the mapping, we selected the Auto Mapping type, and CluedIn automatically generated the mapping for the data set. Since our data set included the ‘ID’ column, it was automatically selected as the primary identifier. This is a favorable option because no empty or duplicate values were found during the current data set check. It means that the ID is a reliable value to uniquely represent the record in CluedIn. If we select a column that contains duplicate values (for example, country), the status check will immediately inform us of the number of duplicate values in the data set. By selecting View more details, you can view the number of duplicate values in the data set, which values are duplicates, and the number of times the duplicate value occurs in the data set. Referring to the screenshot below, there are 3 duplicate values in the data set: United States, Canada, and Spain. The value United States occurs in 550 records. If we proceed with this as the primary identifier and process the data, all 550 records will be merged into a single golden record. In this case, the country cannot serve as a unique representation for each record, as it is acceptable for records to share the same country. However, if you are confident that the selected property can uniquely represent the record, you can proceed with processing the data. Records with identical primary identifiers will be automatically merged, eliminating the need for a separate deduplication project. If the primary identifier contains an empty value, it is replaced with a hash. If your data set contains a record that is nearly identical to the one with the hash value, they will not be merged because the hash is a unique value. In such cases, you can initiate a deduplication project as a solution. What are identifiers? . Identifiers can uniquely represent the record in CluedIn, in addition to the primary identifier. If two identifiers are identical, the records will be merged. CluedIn automatically detects properties that can be used as additional identifiers. For example, if the primary identifier is the ID, then the additional identifier could be the email. Even if there are no duplicate values according to the primary identifier, but there are some according to additional identifiers, the records will be merged. The following diagram will help you in determining if you need to add additional identifiers. Relationships . A relationship is a connection or association between two or more golden records that indicates how the records are related or interact with each other. If you want to connect the records from the current data set to other records in CluedIn, you can add relationships between such records. The procedure for adding relationships between records is described in our “Getting started” guide. For more information, see Add relationships between records. You can add a relationship before or after you process the records. If you add a relationship after processing, you need to process the records again. When you start creating a relationship, you have to select a property from the current data set that references another property existing in CluedIn. For instance, in the case of SQL tables, this property could be a foreign key that connects two tables. The relationship will be established based on the selected property. Then, you need to choose the edge mode: . | Edge – CluedIn creates relationships between the records based on the origin. | Strict Edge – CluedIn creates relationships between the records that belong to a specific data set, data source, or data source group. | Fuzzy Edge – CluedIn creates relationships through fuzzy matching based on the Name property of the records. | . After you select the edge mode, you need to choose the edge type to define the nature of relationships between records (for example, /WorksFor, /RequestedBy,/LocatedIn). Example . We have 2 data sets: . | Companies – contains the following columns: Company_ID and Company_Name. | Employees – contains the following columns: EmployeeID, First_Name, Last_Name, and Company_ID. Here, the Company_ID defines the company where an employee works. | . To connect employees to companies in CluedIn, we can create a relationship in the mapping details of the Employees data set. In this case, the Company_ID will serve as the property upon which the relationship is built. We select the Strict Edge mode because we are aware of the data set to which we want to connect the employees. Once the processing is complete, all employees who work for a particular company will be shown on the Relations tab of a company record. ",
    "url": "/integration/review-mapping#on-this-page",
    
    "relUrl": "/integration/review-mapping#on-this-page"
  },"1780": {
    "doc": "Review mapping",
    "title": "Review mapping",
    "content": " ",
    "url": "/integration/review-mapping",
    
    "relUrl": "/integration/review-mapping"
  },"1781": {
    "doc": "Azure OpenAI",
    "title": "On this page",
    "content": ". | Add Azure OpenAI enricher | Properties from Azure OpenAI enricher | . This article explains how to add the Azure OpenAI enricher. The purpose of this enricher is to enhance data quality by providing more complete, current, and detailed information for your golden records. It can automate the process of data research, offering up-to-date intelligence on your records and reducing the need for manual efforts. The Azure OpenAI enricher supports the following endpoints: . | {baseUrl}/openai/deployments/{deploymentName}/completions?api-version=2022-12-01 . | {baseUrl}/openai/deployments/deploymentName}/chat/completions?api-version=2024-06-01 . | . You can instruct the Azure OpenAI enricher to enhance your golden records with the help of prompts. These prompts must contain at least: . | One input property – the vocabulary key that contains the information you provide to an AI model to process and generate a response. | One output property – the vocabulary key where the desired result will be stored. | . Here are some examples of prompts: . | Finding the country based on address: . Using the address in {Vocabulary:organization.address} provide the country of the organization in {output:vocabulary:organization.CountryAI} . | Translating the text from one language to another: . Please get {output:vocabulary:organization.japaneseName} by translating {Vocabulary:organization.name} into Japanese. | Generating the summary or description based on some text: . Generate a brief summary {output:vocabulary:website.SummaryAI} based on {Vocabulary:website.WebsiteDescription} and {Vocabulary:website.Title} . | . ",
    "url": "/preparation/enricher/azure-openai#on-this-page",
    
    "relUrl": "/preparation/enricher/azure-openai#on-this-page"
  },"1782": {
    "doc": "Azure OpenAI",
    "title": "Add Azure OpenAI enricher",
    "content": "To use the Azure OpenAI enricher, you need to have an Azure OpenAI Service resource set up in the Azure portal and provide the necessary credentials for that resource in CluedIn. To configure Azure OpenAI integration in CluedIn . | Go to Administration &gt; Azure Integration &gt; Azure AI Services. | Enter the API Key used to authenticate and authorize access to your Azure OpenAI resource. | Enter the Base URL for your Azure OpenAI resource in the format similar to the following: https://{resource-name}.openai.azure.com/. | Leave the Resource Key field empty. | Enter the Deployment Name assigned to a specific instance of a model when it was deployed. | Specify the number of Maximum Requests that can be sent to the Azure OpenAI deployment. | Select Save. Once the Azure OpenAI integration is configured in CluedIn, proceed to add the Azure OpenAI enricher. | . To add Azure OpenAI enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Azure OpenAI, and then select Next. | On the Configure tab, provide the following details: . | AI Deployment Name – enter the deployment name assigned to a specific instance of a model when it was deployed. This is the same deployment name as in Administration &gt; Azure Integration &gt; Azure AI Services. | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Prompt – enter the instruction for Azure OpenAI to generate results. The prompt requires at least one input (e.g., Vocabulary:XXXX.YYYY) and one output (e.g., output:Vocabulary:PPPP.QQQQ). For example, the following prompt asks Azure OpenAI to translate the company name and store the result in a dedicated vocabulary key: . Translate {Vocabulary:trainingcompany.name} into French and put the output into {output:vocabulary:trainingcompany.frenchName}. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Azure OpenAI enricher is added and has an active status. This means that it will enrich relevant golden records during processing or when you trigger external enrichment. | . After the Azure OpenAI enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/azure-openai#add-azure-openai-enricher",
    
    "relUrl": "/preparation/enricher/azure-openai#add-azure-openai-enricher"
  },"1783": {
    "doc": "Azure OpenAI",
    "title": "Properties from Azure OpenAI enricher",
    "content": "You can find the properties added to golden records from the Azure OpenAI enricher on the Properties page. The vocabulary keys added to golden records by the Azure Open AI enricher are grouped under No Source source type. For a more detailed information about the changes made to a golden record by the Azure OpenAI enricher, check the corresponding data part on the History page. ",
    "url": "/preparation/enricher/azure-openai#properties-from-azure-openai-enricher",
    
    "relUrl": "/preparation/enricher/azure-openai#properties-from-azure-openai-enricher"
  },"1784": {
    "doc": "Azure OpenAI",
    "title": "Azure OpenAI",
    "content": " ",
    "url": "/preparation/enricher/azure-openai",
    
    "relUrl": "/preparation/enricher/azure-openai"
  },"1785": {
    "doc": "Data sources",
    "title": "Data sources",
    "content": "In this section, you will learn how to get your data from external sources into CluedIn. All data you ingest into CluedIn is stored in Integrations &gt; Data Sources. One of the main purposes of the Data Sources module is to provide you with the tools to create a semantic layer for your data so that CluedIn can understand it. Each step is explained in detail in a separate article in this section: . | Define data to ingest – explore all the ways you can ingest data into CluedIn and choose the one that works best for your needs. | Ingest data from a file, an endpoint, or a database - find step-by-step instructions for ingesting data into CluedIn. | Create mapping - create a semantic layer for your data and review the mapping details to ensure that your records are produced and merged in the most effective way. | Process data - turn your data into golden records, making it searchable and ready to be cleaned, deduplicated, and streamed. | . In addition to these fundamental steps, we encourage you to explore additional operations on records to discover tools for normalizing and improving the quality of data. To ingest, map, and process the data, you need to have access to different modules in CluedIn. The following table contains a list of claims required to work with the Data Sources module. | Section | Claim | Access level | . | Integration | All claims | at least Consulted | . | Management | Data Catalog | at least Informed | . | Management | Annotation | at least Informed | . | Consume | Export Targets | at least Informed | . ",
    "url": "/integration/data%20sources",
    
    "relUrl": "/integration/data sources"
  },"1786": {
    "doc": "PaaS operations",
    "title": "PaaS operations",
    "content": "This section is dedicated to IT specialists responsible for managing CluedIn PaaS within their organizations. Supporting documents and references: . | CluedIn: CluedIn Azure costs . | Kubernetes: . | Kubernetes tools . | Introduction to Kubernetes . | . | Microsoft AKS: Configure Azure CNI networking in Azure Kubernetes Service (AKS) . | . CluedIn team performs installation activities on all business days except Friday. Deploying on a Friday carries higher risk because issues may not surface immediately and can escalate into weekend incidents with fewer people available to respond. As a best practice, it is recommended to schedule installations earlier in the week (Tuesday–Thursday) to allow time for monitoring, troubleshooting, and stabilization. ",
    "url": "/paas-operations",
    
    "relUrl": "/paas-operations"
  },"1787": {
    "doc": "Process data",
    "title": "On this page",
    "content": ". | Manual processing | Auto-submission | Bridge mode | Processing logs . | Remove processing logs | Configure retention settings | . | . In this article, you will learn about the processing of data that you ingested into CluedIn. The goal of processing is to turn your records into standalone golden records or to use them to enhance existing golden records. Depending on the type of data source, there are three processing options: . | For file, ingestion endpoint, and database: Manual processing . | For ingestion endpoint only: Auto-submission . | For ingestion endpoint only: Bridge mode . | . You can process the data set as many times as you want. In CluedIn, once a record has been processed, it won’t undergo processing again. When the processing is started, CluedIn checks for identical records. If identical records are found, they won’t be processed again. However, if you change the primary identifier for the previously processed records, CluedIn will treat these records as new and process them. After the processing is completed, the processing log appears in the table. Any records that fail to meet specific conditions outlined in property or pre-process rules will be sent to quarantine. To learn more about managing these records, see Quarantine. Records that were processed successfully are displayed on the Data tab. If the processing takes a long time, go to the Monitoring tab and check the number of messages in the queues. Depending on the type of message queue with a high message count, you can perform specific troubleshooting actions. For further details, see Monitoring. ",
    "url": "/integration/process-data#on-this-page",
    
    "relUrl": "/integration/process-data#on-this-page"
  },"1788": {
    "doc": "Process data",
    "title": "Manual processing",
    "content": "Manual processing is available for the data coming from a file, an ingestion endpoint, or a database. With manual processing, the original data that was initially sent to CluedIn remains in the temporary storage on the Preview tab. After the data has been processed, the resulting golden records appear on the Data tab. To process the data . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set that you need to process. | Go to the Process tab, and then select Process. The confirmation pane opens, where you can do the following: . | View the number of records that will be processed. | Delete the records that are currently in quarantine. This option is useful if you have already processed the data set before and there are some records in quarantine. | View the result of the primary identifier status check along with the field that was selected for producing the primary identifier. | View the result of the identifier status check along with the field that was selected for producing additional identifier. If any status check shows duplicates, the records containing duplicates will be merged to maintain data integrity and consistency. To learn more about unique identifiers, see Identifiers. | . | In the lower-right corner, select Confirm. | . ",
    "url": "/integration/process-data#manual-processing",
    
    "relUrl": "/integration/process-data#manual-processing"
  },"1789": {
    "doc": "Process data",
    "title": "Auto-submission",
    "content": "Auto-submission is available for the data coming from an ingestion endpoint. When auto-submission is enabled, data received from the ingestion endpoint is processed automatically. With auto-submission, the original data that was initially sent to CluedIn remains in the temporary storage on the Preview tab. After the data has been processed, the resulting golden records appear on the Data tab. To enable auto-submission . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set for which you want to enable auto-submission. | Go to the Process tab, and then turn on the toggle next to Auto-submission. | Confirm that you want to enable automatic processing of records once they are received by CluedIn. | . If you no longer want the records to be processed automatically, turn off the toggle next to Auto-submission. ",
    "url": "/integration/process-data#auto-submission",
    
    "relUrl": "/integration/process-data#auto-submission"
  },"1790": {
    "doc": "Process data",
    "title": "Bridge mode",
    "content": "Bridge mode is available for the data coming from an ingestion endpoint. When bridge-mode is enabled, all your JSON records will be transformed into golden records directly, without being stored in the temporary storage on the Preview tab. However, you can rely on data set logs and ingestion receipts for debugging purposes. Bridge mode allows you to use less storage and memory, resulting in increased performance. Use this mode when your mapping will not change over time, and you want to use the ingestion endpoint only as a mapper. To switch to bridge mode . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set that you want to switch to bridge mode. | Go to the Process tab. Open the three dots menu, and then select Switch to bridge mode. | Confirm that you want to switch to bridge mode by entering BRIDGE. Then, select Confirm bridge mode. | . If you no longer want your ingestion endpoint to operate in bridge mode, you can switch it back to the default mode. After switching back to the default mode, the Preview tab will appear. However, it will not contain records received while bridge mode was enabled. To switch back to default mode . | On the navigation pane, go to Integrations &gt; Data Sources. Then, find and open the data set that you want to switch back to default mode. | Go to the Process tab. Open the three dots menu, and then select Switch to default mode. | Confirm that you want to switch back to default mode by entering DEFAULT. Then, select Confirm default mode. | . ",
    "url": "/integration/process-data#bridge-mode",
    
    "relUrl": "/integration/process-data#bridge-mode"
  },"1791": {
    "doc": "Process data",
    "title": "Processing logs",
    "content": "Every time the records are processed, a new processing log appears on the Process tab of the data set. If the number of processing logs is growing, consider removing older logs. You can also configure the retention settings to automatically remove processing logs after a specific period. Remove processing logs . Removing processing logs frees up disk space without impacting the processed records. However, once removed, processing logs cannot be recovered. To remove processing logs . | Near the upper-right corner of the processing logs table, open the three dots menu, and then select Purge processing logs. | Select the statuses of the processing logs that you want to remove. | Confirm that you want to remove processing logs by entering DELETE. Then, select Purge. After processing logs are removed, the Process tab will display information about the user who removed them and the time of removal. | . Configure retention settings . Retention settings allow you to automatically delete processing logs after a specified period. To configure retention settings . | Near the upper-right corner of the processing logs table, open the three dots menu, and then select Retention settings. | Select the checkbox to enable retention. | Select a period to specify which processing logs should be removed. For example, selecting 2 months old means that all processing logs created 2 months ago will be removed. Thus, when a processing log turns 2 months old, it will be automatically removed. | Select Save. | If you want to change the retention period, repeat step 1. Then, select another period and save your changes. | If you want to remove the retention settings, repeat step 1. Then, clear the checkbox to disable retention and save your changes. | . ",
    "url": "/integration/process-data#processing-logs",
    
    "relUrl": "/integration/process-data#processing-logs"
  },"1792": {
    "doc": "Process data",
    "title": "Process data",
    "content": " ",
    "url": "/integration/process-data",
    
    "relUrl": "/integration/process-data"
  },"1793": {
    "doc": "Crawlers",
    "title": "Crawlers",
    "content": " ",
    "url": "/integration/crawlers-and-enrichers",
    
    "relUrl": "/integration/crawlers-and-enrichers"
  },"1794": {
    "doc": "Knowledge base",
    "title": "Knowledge base",
    "content": " ",
    "url": "/kb",
    
    "relUrl": "/kb"
  },"1795": {
    "doc": "REST API",
    "title": "On this page",
    "content": ". | Add REST API enricher | View properties added by REST API enricher | Sample scripts . | Sample process request script | Sample process response script | Clearbit | Azure OpenAI | DuckDuckGo | REST Countries | Melissa . | Configure Melissa REST API enricher | Example of golden record before and after enrichment | Sample response | . | . | . This article explains how to add the REST API enricher. The purpose of this enricher is to retrieve data from a wide variety of endpoints. It provides a flexible way to integrate diverse data sources into your golden records. This enricher is intended for users who are comfortable writing JavaScript code, as scripting is required to customize request and response handling. ",
    "url": "/preparation/enricher/rest-api#on-this-page",
    
    "relUrl": "/preparation/enricher/rest-api#on-this-page"
  },"1796": {
    "doc": "REST API",
    "title": "Add REST API enricher",
    "content": "The REST API enricher requires the URL of an external endpoint to retrieve data. You can use JavaScript to customize request construction and response processing for precise control over data extraction. You can find some examples of JavaScript code in the Sample scripts section of this page. To add REST API enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select REST API, and then select Next. | On the Configure tab, provide the following details: . | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Method – select the HTTP method (GET or POST) that will be used to retrieve data. | URL – enter the URL of the external endpoint. This can be: . | An external API endpoint (for example, DuckDuckGo) that returns data directly to the enricher, where it is processed using the Process Response Script. | A service endpoint (for example, Power Automate, Jupyter Notebook, or a Cloud Function) that acts as a wrapper for external APIs. These endpoints can call external APIs, process the responses internally, and return the final result to the enricher. | . You can use placeholders in URL to dynamically insert values at runtime: . | {APIKey} – this placeholder will be replaced by the actual API key value provided in API Key. | {Vocabulary:vocabularykey} – this placeholder will be replaced with the value of the specified vocabulary key from the golden record. For example, if the URL contains {Vocabulary:trainingcontact.country} and the golden record contains \"trainingcontact.country\": \"Norway\", the placeholder will be replaced with Norway. | . | API Key – enter the API key required to authenticate with the endpoint, if applicable. Provide this only if the API requires a key for access. | Headers – enter any HTTP headers needed to call the endpoint. Enter one header per line in the format Header-Name=value. | Vocabulary and Properties – enter the vocabulary keys and properties to include in the request payload, with one entry per line. These will be passed to the endpoint to retrieve the relevant data. | Process Request Script – provide the JavaScript code used to construct or modify the request before it is sent to the external endpoint. You can find an example of the process request script in the Sample scripts section of this page. | Process Response Script – provide the JavaScript code used to process the response returned from the external endpoint. This script is required to transform the API response into a format that the enricher can understand and use. You can find an example of the process response script in the Sample scripts section of this page. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The REST API enricher is added and has an active status. This means that it will enrich relevant golden records during processing or when you trigger external enrichment. | . After the REST API enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/rest-api#add-rest-api-enricher",
    
    "relUrl": "/preparation/enricher/rest-api#add-rest-api-enricher"
  },"1797": {
    "doc": "REST API",
    "title": "View properties added by REST API enricher",
    "content": "You can find the properties added to golden records from the REST API enricher on the Properties page. The vocabulary keys added to golden records by the REST API enricher are grouped under No Source source type. For a more detailed information about the changes made to a golden record by the REST API enricher, check the corresponding data part on the History page. ",
    "url": "/preparation/enricher/rest-api#view-properties-added-by-rest-api-enricher",
    
    "relUrl": "/preparation/enricher/rest-api#view-properties-added-by-rest-api-enricher"
  },"1798": {
    "doc": "REST API",
    "title": "Sample scripts",
    "content": "This section provides some sample scripts that you can use to configure the RESP API enricher. Sample process request script . // sample request that can be accessed in the script // let request = { // ApiKey: \"testApiKey\", // Url: \"testUrl\", // Header: [{Key:\"TestHeader\", Value: 123}], // Body: { // Properties: [{Key: \"organization.name\", Value: \"CluedIn\"}] // } // }; let detailsArray = []; request.Body.Properties.forEach((x) =&gt; {  detailsArray.push(x.Value) }); request.Body = {   names: detailsArray } //modified request // let request = { // ApiKey: \"testArray\", // Url: \"testUrl\", // Header: [{Key:\"TestHeader\", Value: 123}], // Body: { // name: [\"CluedIn\"] // } // }; . Sample process response script . // Sample response that returned from external api to enricher // const response = { // HttpStatus: \"OK\". // Content: \"{\\\"fullName\\\": \\\"CluedIn ApS\\\"}\", // Headers: [ // { Key: \"Connection\", Value: \"keep-alive\"}, // { Key: \"Content-Type, Value \"application/x-javascript\"}\", // ], // }; let parsedContent = JSON.parse(response.Content); let newContent = {   ‘organization.fullName':parsedContent.fullName }; response.Content = JSON.stringify([{ Data: newContent), Score: 0 }]); // Sample response after executing the script // const response = { // HttpStatus: \"OK\", // Content: \"[{\\\"Data\\\":{\\\"organization.fullName\\\":\\\"CluedIn Aps\\\"}, \\\"Score\\\":0}]\", // ContentType: \"application/x-javascript\", // Headers: [ // { Key: \"Connection\", Value: \"keep-alive\"}, // { Key: \"Content-Type\", Value: \"application/x-javscript\", // ], . Clearbit . This code takes a company name as input and sends it to the Clearbit Autocomplete API. Then, it extracts the domain and logo of the top matching company and returns that data in a structured format usable by the enricher. Method: GET . URL . https://autocomplete.clearbit.com/v1/companies/suggest?query={Vocabulary:organization.name} . Process response script . let parsedContent = JSON.parse(response.Content); let content = parsedContent[0]; let image = { 'clearbit.organization.domain': content.domain, 'clearbit.organization.logo': content.logo, }; response.Content = JSON.stringify([{ Data: image, Score: 0 }]); log(JSON.stringify(response)); . Azure OpenAI . This code takes an organization’s name and sends a request to Azure OpenAI GPT-4 asking it to translate the name into Japanese. Then, it ensures the model responds in a defined JSON format and parses and returns the result in a structure compatible with the enricher. Note that you can use the Azure Open AI enricher for the same task. Method: POST . URL . https://copilot2.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-06-01 . Vocabulary key and properties . organization.name . Headers . api-key={APIKey} . Process request script . let prompt = String.raw`Please get {organization.japaneseNameRestApi} by translating {Vocabulary:organization.name} into Japanese. Response in JSON using the following template ### { \"organization.japaneseNameRestApi\": \"\" } ### `; request.Body.Properties.forEach((x) =&gt; { prompt = prompt.replace('{Vocabulary:' + x.Key + '}', x.Value); }); request.Body = { messages: [{ role: 'user', content: prompt }], temperature: 0, }; . Process response script . let parsedContent = JSON.parse(response.Content); let content = parsedContent.choices[0].message.content.trimEnd(); response.Content = JSON.stringify([{ Data: JSON.parse(content), Score: 0 }]); log(JSON.stringify(response)); . DuckDuckGo . This script handles the JSON response returned by a DuckDuckGo API call. The main purpose is to extract useful information from the API response and transform it into a format usable by the enricher. Method: GET . URL . https://api.duckduckgo.com?q={Vocabulary:organization.name}&amp;format=json . Process response script . function extractStringValues(obj, dict = new Map()) { for (const [key, value] of Object.entries(obj)) { if (value &amp;&amp; typeof value !== 'object' &amp;&amp; !Array.isArray(value)) { dict.set('duckDuckGo.organization.' + key, value.toString()); } } return dict; } function extractInfobox(content, dict = new Map()) { content?.forEach((x) =&gt; { dict.set('duckDuckGo.organization.infobox.' + x.label, x.value); }); } function extractRelatedTopics(relatedTopics, dict = new Map()) { for (let i = 0; i &lt; relatedTopics.length; i++) { dict.set( 'duckDuckGo.organization.relatedTopics' + i + '.firstUrl', relatedTopics[i]?.FirstURL, ); dict.set( 'duckDuckGo.organization.relatedTopics' + i + '.text', relatedTopics[i]?.Text, ); dict.set( 'duckDuckGo.organization.relatedTopics' + i + '.icon', relatedTopics[i]?.Icon?.Url, ); } } try { // Update the content here let parsedContent = JSON.parse(response.Content); let vocabularyKeysWithValue = extractStringValues(parsedContent); extractInfobox(parsedContent.Infobox?.content, vocabularyKeysWithValue); extractRelatedTopics(parsedContent.RelatedTopics, vocabularyKeysWithValue); response.Content = JSON.stringify([ { Data: Object.fromEntries(vocabularyKeysWithValue), Score: 0 }, ]); log(response); } catch (error) { const errorDetails = { name: error?.name, message: error?.message, stack: error?.stack, }; log('Error caught:' + JSON.stringify(errorDetails)); } . REST Countries . This code extracts the official name of a country from the response and formats it into a new JSON structure. Method: GET . URL . https://restcountries.com/v3.1/name/{Vocabulary:country.country}?fullText=true . Process response script . let parsedContent = JSON.parse(response.Content); let content = parsedContent[0]; let officialName = { 'country.officialName': content.name.official, }; response.Content = JSON.stringify([{ Data: officialName, Score: 0 }]); log(JSON.stringify(response)); . Melissa . Melissa is a widely used address validation and data quality service that helps organizations keep address data accurate, standardized, and up to date. CluedIn can connect to Melissa through its REST API, sending address data for validation. The API responds with standardized and enriched results, which CluedIn processes and maps to the appropriate vocabulary keys for storage and further use. In the following example, ingested company data contains missing or incorrect address details. We will use Melissa REST API calls to send address-related data stored in different vocabulary keys and enrich golden records with validated and parsed addresses. Configure Melissa REST API enricher . To add the Melissa REST API enricher, on the Configure tab, provide the following information: . | Accepted Business Domain: In this example, /Address. | Method: GET. | URL: . In this example, the URL references several vocabulary keys that store address-related data. https://address.melissadata.net/V3/WEB/GlobalAddress/doGlobalAddress?id={APIKey}&amp;a1={Vocabulary:company.address.streetAddress}&amp;loc={Vocabulary:company.address.city}&amp;admarea={Vocabulary:company.address.state}&amp;ctry={Vocabulary:company.address.country}&amp;postal={Vocabulary:company.address.zipcode} . | Headers: . Content-Type=application/json . | Process response script: . This script transforms the raw response into a structured format that fits CluedIn’s schema. let parsedContent = JSON.parse(response.Content); let content=parsedContent.Records[0]; let melissaCountryName=content.CountryName; let melissaCountryISO=content.CountryISO3166_1_Alpha3; let melissaZipcode=content.PostalCode; let melissaFullAddress =content.FormattedAddress; let melissaCity=content.Locality; let melissaState=content.AdministrativeArea; let melissaDeliveryIndicator=content.DeliveryIndicator; let newContent = { 'company.address.city':melissaCity, 'company.address.zipcode':melissaZipcode, 'company.address.state':melissaState, 'company.address.country':melissaCountryISO, 'company.address.vocabKey2':melissaCountryName, 'company.address.vocabKey1':melissaFullAddress, 'company.address.type':melissaDeliveryIndicator, }; //apply conditions if required if ( melissaDeliveryIndicator==='B') { newContent[ 'company.address.type']='Business'; } else if( melissaDeliveryIndicator==='R'){ newContent['company.address.type']='Residential'; } else{ newContent['company.address.type']='Unknown'; } response.Content = JSON.stringify([{ Data: newContent, Score: 0 }]); //log(JSON.stringify(response)); . | . Example of golden record before and after enrichment . An example of a golden record before enrichment: . Same record after enrichment: . | The values for the Type and Zipcode properties were updated. | Two new vocabulary keys (Vocab Key 1 and Vocab Key 2 were added). | . Sample response . Provided below is an example of the response you would receive for the same request in Postman. If you need an additional address line in your golden records, select the relevant fields from the response and add them to the process response script provided earlier on this page. For example, you could include the Latitude and Longitude fields. | URL: . https://address.melissadata.net/V3/WEB/GlobalAddress/doGlobalAddress?id=key&amp;org=Walmart Inc.&amp;a1=702 S.W. 8th St.&amp;loc=Bentonville&amp;admarea=AR&amp;ctry=USA&amp;postal=72716&amp;act=check,verify&amp;format=JSON . | Response: . {     \"Version\": \"9.4.1.1228\",     \"TransmissionReference\": \"\",     \"TransmissionResults\": \"\",     \"TotalRecords\": \"1\",     \"Records\": [         {             \"RecordID\": \"1\",             \"Results\": \"AC16,AV24,GS05\",             \"FormattedAddress\": \"Walmart Inc.;702 SW 8th St;Bentonville AR  72716-6299\",             \"Organization\": \"Walmart Inc.\",             \"AddressLine1\": \"702 SW 8th St\",             \"AddressLine2\": \"Bentonville AR  72716-6299\",             \"AddressLine3\": \"\",             \"AddressLine4\": \"\",             \"AddressLine5\": \"\",             \"AddressLine6\": \"\",             \"AddressLine7\": \"\",             \"AddressLine8\": \"\",             \"SubPremises\": \"\",             \"DoubleDependentLocality\": \"\",             \"DependentLocality\": \"\",             \"Locality\": \"Bentonville\",             \"SubAdministrativeArea\": \"Benton\",             \"AdministrativeArea\": \"AR\",             \"PostalCode\": \"72716-6299\",             \"PostalCodeType\": \"U\",             \"AddressType\": \"F\",             \"AddressKey\": \"72716629902\",             \"SubNationalArea\": \"\",             \"CountryName\": \"United States of America\",             \"CountryISO3166_1_Alpha2\": \"US\",             \"CountryISO3166_1_Alpha3\": \"USA\",             \"CountryISO3166_1_Numeric\": \"840\",             \"CountrySubdivisionCode\": \"US-AR\",             \"Thoroughfare\": \"SW 8th St\",             \"ThoroughfarePreDirection\": \"SW\",             \"ThoroughfareLeadingType\": \"\",             \"ThoroughfareName\": \"8th\",             \"ThoroughfareTrailingType\": \"St\",             \"ThoroughfarePostDirection\": \"\",             \"DependentThoroughfare\": \"\",             \"DependentThoroughfarePreDirection\": \"\",             \"DependentThoroughfareLeadingType\": \"\",             \"DependentThoroughfareName\": \"\",             \"DependentThoroughfareTrailingType\": \"\",             \"DependentThoroughfarePostDirection\": \"\",             \"Building\": \"\",             \"PremisesType\": \"\",             \"PremisesNumber\": \"702\",             \"SubPremisesType\": \"\",             \"SubPremisesNumber\": \"\",             \"PostBox\": \"\",             \"Latitude\": \"36.363516\",             \"Longitude\": \"-94.183177\",             \"DeliveryIndicator\": \"U\",             \"MelissaAddressKey\": \"2153275541\",             \"MelissaAddressKeyBase\": \"\",             \"PostOfficeLocation\": \"\",             \"SubPremiseLevel\": \"\",             \"SubPremiseLevelType\": \"\",             \"SubPremiseLevelNumber\": \"\",             \"SubBuilding\": \"\",             \"SubBuildingType\": \"\",             \"SubBuildingNumber\": \"\",             \"UTC\": \"UTC-06:00\",             \"DST\": \"Y\",             \"DeliveryPointSuffix\": \"\",             \"CensusKey\": \"050070205041002\",             \"Extras\": {}         }     ] } . | . ",
    "url": "/preparation/enricher/rest-api#sample-scripts",
    
    "relUrl": "/preparation/enricher/rest-api#sample-scripts"
  },"1799": {
    "doc": "REST API",
    "title": "REST API",
    "content": " ",
    "url": "/preparation/enricher/rest-api",
    
    "relUrl": "/preparation/enricher/rest-api"
  },"1800": {
    "doc": "2024.01",
    "title": "On this page",
    "content": ". | Prerequisites | Guide . | Preparation | Upgrade - Stage 1 | Upgrade - Stage 2 | Upgrade - Stage 3 | . | Troubleshooting | . This document covers the upgrade process from 2023.07 to 2024.01. The upgrade process for this specific release is not a straightforward helm upgrade and does require quite a bit of work to get it into a running state. ",
    "url": "/paas-operations/upgrade/2024-01#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-01#on-this-page"
  },"1801": {
    "doc": "2024.01",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.0.0. You may need to helm repo update to grab the latest. | Download the prerequisite zip artifact here | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-01#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-01#prerequisites"
  },"1802": {
    "doc": "2024.01",
    "title": "Guide",
    "content": "Included in the zipped artifact are a number of .yaml files along with a PowerShell upgrade script. The script is very generic and may not be usable in your environment if your instance of CluedIn is heavily modified. As a result, it should only be used as a reference point rather than something you run. Manual upgrade steps are provided below. Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. Preparation . | Export your current running helm values files. Keep one for this process, and also keep one as a backup. | . Note: You can export by running helm get values cluedin-platform -n cluedin --output yaml &gt; /path/to/file.yaml. This will export the currently connected kubeconfig. | Extract the downloaded artifact to a location where we’ll be working from. | Before the upgrade takes place, you need to apply the data upgrade CRD. You can do this by running kubectl apply -f /path/to/extract/artifact/dataupgrade-crd.yaml -n cluedin. You do not need to modify this file. | Update the platform-custom-values.yaml patch file to include the neo4j password and potentially change the global image tag as it’s shipped with 2024.01.00. This file is part of the extracted artifact. The field is infrastructure.neo4j.neo4j.password. It should be set to xxx in the file, and failing to update this will potentially break the upgrade process. | . Note: You can get the current neo4j password by running kubectl get secret/cluedin-neo4j-secrets -n cluedin -o jsonpath='{.data.neo4j-password}' | base64 --decode . | In the same file, remove any application.cluedin.components.packages which you do not use. This will patch against what is set in your exported values file. | . Now, you are ready to perform the actual upgrade process. It’s done in 3 stages. Upgrade - Stage 1 . Scale down a majority of the running deployments and stateful sets. To do this, run the following: . helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.0.0 \\ --values ${ExportedOriginalValues} \\ --values platform-custom-values.yaml \\ --values platform-upgrade-v2-stage1.yaml \\ --wait \\ --timeout 10m0s . When the above has been ran, you should begin to see a majority of your pods scaling down. It may take 5 minutes, so please be patient. When it has completely scaled down, you’re ready for Stage 2. Upgrade - Stage 2 . Migrate the databases due to a large number of application upgrades. To do this, run the following: . helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.0.0 \\ --values ${ExportedOriginalValues} \\ --values platform-custom-values.yaml \\ --values platform-upgrade-v2-stage2.yaml \\ --wait \\ --timeout 20m0s . This will trigger a neo4j database backup and migration along with a SQL upgrade. It can take up to 20 minutes to complete. It’ll be ready when all init-jobs have completed and the core pods are all in a running state. Please note because of the previous step scale down, only a few pods will be brought up during this stage. When successfully done, it’s ready for the final stage. Upgrade - Stage 3 . Assuming the above has been successful, it’s time to actually bring the environment back up into a running and usable state. To do this, run the final helm command below: . helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.0.0 \\ --values ${ExportedOriginalValues} \\ --values platform-custom-values.yaml \\ --set 'application.system.runDatabaseJobsOnUpgrade=true' \\ --set 'application.system.runNugetFullRestore=true' \\ --wait \\ --timeout 20m0s . After approximately 10-15 minutes, your environment should be 100% completed, and you should be able to access the front end of the UI. ",
    "url": "/paas-operations/upgrade/2024-01#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-01#guide"
  },"1803": {
    "doc": "2024.01",
    "title": "Troubleshooting",
    "content": "If you get stuck at all during the upgrade process, reach out to support@cluedin.com. Below are some common issues due to the upgrade process. | Ensure any custom connectors are updated to .NET 6. This is due to upgrading the libraries within the application to .NET 6. For some connectors, data may not appear. Note that no data should be lost during this transition, it may just be hidden until updated. | CluedIn Developer feed is no longer supported. If your environment connects to the CluedIn Developer nuget feed, it may prevent the environment from standing up correctly. You can validate this by searching for https://pkgs.dev.azure.com/CluedIn-io/_packaging/develop/ in your values files. If a result is returned, it’s best to remove it from values. | . ",
    "url": "/paas-operations/upgrade/2024-01#troubleshooting",
    
    "relUrl": "/paas-operations/upgrade/2024-01#troubleshooting"
  },"1804": {
    "doc": "2024.01",
    "title": "2024.01",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-01",
    
    "relUrl": "/paas-operations/upgrade/2024-01"
  },"1805": {
    "doc": "2024.03",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.01 to 2024.03. We do not support upgrading directly to this version from prior to 2024.01. ",
    "url": "/paas-operations/upgrade/2024-03#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-03#on-this-page"
  },"1806": {
    "doc": "2024.03",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.1.0. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-03#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-03#prerequisites"
  },"1807": {
    "doc": "2024.03",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.03.00\" strategy: type: Recreate application: cluedin: components: packages: - name: CluedIn.EventHub version: 4.1.0 - name: CluedIn.Purview version: 4.1.0 . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.1.0 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-03#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-03#guide"
  },"1808": {
    "doc": "2024.03",
    "title": "2024.03",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-03",
    
    "relUrl": "/paas-operations/upgrade/2024-03"
  },"1809": {
    "doc": "2024.04",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.03 to 2024.04. It is safe to upgrade from 2024.01 directly to 2024.04. We do not support upgrading directly to this version from prior to 2024.01. ",
    "url": "/paas-operations/upgrade/2024-04#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-04#on-this-page"
  },"1810": {
    "doc": "2024.04",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.2.0. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-04#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-04#prerequisites"
  },"1811": {
    "doc": "2024.04",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.04.00\" strategy: type: Recreate application: cluedin: components: packages: - name: CluedIn.EventHub version: 4.2.0 - name: CluedIn.Purview version: 4.2.0 submitter: resources: requests: cpu: 1000m memory: 1G limits: cpu: 1000m memory: 1G . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.2.0 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-04#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-04#guide"
  },"1812": {
    "doc": "2024.04",
    "title": "2024.04",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-04",
    
    "relUrl": "/paas-operations/upgrade/2024-04"
  },"1813": {
    "doc": "2024.07",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.04 to 2024.07. We do not support upgrading directly to this version from prior to 2024.01. ",
    "url": "/paas-operations/upgrade/2024-07#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-07#on-this-page"
  },"1814": {
    "doc": "2024.07",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.3.0. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-07#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-07#prerequisites"
  },"1815": {
    "doc": "2024.07",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.07.00\" strategy: type: Recreate application: bootstrap: dataUpgrade: toVersion: 'Version430' cluedin: components: packages: - name: CluedIn.Connector.AzureDataLake version: 4.3.0 - name: CluedIn.Connector.Dataverse version: 4.3.0 - name: CluedIn.EventHub version: 4.3.0 - name: CluedIn.PowerApps version: 4.3.0 - name: CluedIn.Provider.ExternalSearch.ClearBit version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.CVR version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.GoogleMaps version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.Libpostal version: 4.1.0 - name: CluedIn.Purview version: 4.3.0 - name: CluedIn.Vocabularies.CommonDataModel version: 4.3.0 - name: CluedIn.Connector.SqlServer version: 4.1.0 - name: CluedIn.ExternalSearch.Providers.Web version: 4.1.0 - name: CluedIn.Connector.OneLake version: 4.3.0 roles: main: resources: limits: memory: 2G datasourceProcessing: replicas: 2 resources: requests: cpu: 1000m memory: 512M limits: cpu: 2000m memory: 8G datasource: resources: requests: cpu: 500m memory: 512M limits: cpu: 500m memory: 512M submitter: resources: requests: memory: 512M limits: memory: 512M gql: resources: requests: memory: 512M limits: memory: 4G ui: resources: requests: memory: 256M monitoring: alertManager: licenseKey: &lt;your-license-key&gt; infrastructure: haproxy-ingress: controller: resources: requests: cpu: 125m limits: cpu: 500m cert-manager: cainjector: resources: requests: memory: 128Mi . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.3.0 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-07#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-07#guide"
  },"1816": {
    "doc": "2024.07",
    "title": "2024.07",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-07",
    
    "relUrl": "/paas-operations/upgrade/2024-07"
  },"1817": {
    "doc": "2024.07.01",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.07 to 2024.07.01. We do not support upgrading directly to this version from prior to 2024.01. ",
    "url": "/paas-operations/upgrade/2024-07-01#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-07-01#on-this-page"
  },"1818": {
    "doc": "2024.07.01",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.3.1. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-07-01#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-07-01#prerequisites"
  },"1819": {
    "doc": "2024.07.01",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.07.01\" strategy: type: Recreate application: bootstrap: dataUpgrade: toVersion: 'Version430' cluedin: components: packages: - name: CluedIn.Connector.AzureDataLake version: 4.3.0 - name: CluedIn.Connector.Dataverse version: 4.3.0 - name: CluedIn.EventHub version: 4.3.0 - name: CluedIn.PowerApps version: 4.3.0 - name: CluedIn.Provider.ExternalSearch.ClearBit version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.CVR version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.GoogleMaps version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.Libpostal version: 4.1.0 - name: CluedIn.Purview version: 4.3.0 - name: CluedIn.Vocabularies.CommonDataModel version: 4.3.0 - name: CluedIn.Connector.SqlServer version: 4.1.0 - name: CluedIn.ExternalSearch.Providers.Web version: 4.1.0 - name: CluedIn.Connector.OneLake version: 4.3.0 roles: main: resources: limits: memory: 2G datasourceProcessing: replicas: 2 resources: requests: cpu: 1000m memory: 512M limits: cpu: 2000m memory: 8G datasource: resources: requests: cpu: 500m memory: 512M limits: cpu: 500m memory: 512M submitter: resources: requests: memory: 512M limits: memory: 512M gql: resources: requests: memory: 512M limits: memory: 4G ui: resources: requests: memory: 256M monitoring: alertManager: licenseKey: &lt;your-license-key&gt; infrastructure: haproxy-ingress: controller: resources: requests: cpu: 125m limits: cpu: 500m cert-manager: cainjector: resources: requests: memory: 128Mi . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.3.1 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-07-01#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-07-01#guide"
  },"1820": {
    "doc": "2024.07.01",
    "title": "2024.07.01",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-07-01",
    
    "relUrl": "/paas-operations/upgrade/2024-07-01"
  },"1821": {
    "doc": "2024.07.02",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.07 to 2024.07.02. We do not support upgrading directly to this version from prior to 2024.02. ",
    "url": "/paas-operations/upgrade/2024-07-02#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-07-02#on-this-page"
  },"1822": {
    "doc": "2024.07.02",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.3.1. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-07-02#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-07-02#prerequisites"
  },"1823": {
    "doc": "2024.07.02",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.07.02\" strategy: type: Recreate application: bootstrap: dataUpgrade: toVersion: 'Version430' cluedin: components: packages: - name: CluedIn.Connector.AzureDataLake version: 4.3.0 - name: CluedIn.Connector.Dataverse version: 4.3.0 - name: CluedIn.EventHub version: 4.3.0 - name: CluedIn.PowerApps version: 4.3.0 - name: CluedIn.Provider.ExternalSearch.ClearBit version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.CVR version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.GoogleMaps version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.Libpostal version: 4.1.0 - name: CluedIn.Purview version: 4.3.0 - name: CluedIn.Vocabularies.CommonDataModel version: 4.3.0 - name: CluedIn.Connector.SqlServer version: 4.1.0 - name: CluedIn.ExternalSearch.Providers.Web version: 4.1.0 - name: CluedIn.Connector.OneLake version: 4.3.0 roles: main: resources: limits: memory: 2G datasourceProcessing: replicas: 2 resources: requests: cpu: 1000m memory: 512M limits: cpu: 2000m memory: 8G datasource: resources: requests: cpu: 500m memory: 512M limits: cpu: 500m memory: 512M submitter: resources: requests: memory: 512M limits: memory: 512M gql: resources: requests: memory: 512M limits: memory: 4G ui: resources: requests: memory: 256M monitoring: alertManager: licenseKey: &lt;your-license-key&gt; infrastructure: haproxy-ingress: controller: resources: requests: cpu: 125m limits: cpu: 500m cert-manager: cainjector: resources: requests: memory: 128Mi . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.3.1 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-07-02#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-07-02#guide"
  },"1824": {
    "doc": "2024.07.02",
    "title": "2024.07.02",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-07-02",
    
    "relUrl": "/paas-operations/upgrade/2024-07-02"
  },"1825": {
    "doc": "2024.07.03",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.07 to 2024.07.03. We do not support upgrading directly to this version from prior to 2024.02. ",
    "url": "/paas-operations/upgrade/2024-07-03#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-07-03#on-this-page"
  },"1826": {
    "doc": "2024.07.03",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.3.2. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-07-03#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-07-03#prerequisites"
  },"1827": {
    "doc": "2024.07.03",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.07.03\" strategy: type: Recreate application: bootstrap: dataUpgrade: toVersion: 'Version430' cluedin: components: packages: - name: CluedIn.Connector.AzureDataLake version: 4.3.0 - name: CluedIn.Connector.Dataverse version: 4.3.0 - name: CluedIn.EventHub version: 4.3.0 - name: CluedIn.PowerApps version: 4.3.0 - name: CluedIn.Provider.ExternalSearch.ClearBit version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.CVR version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.GoogleMaps version: 4.1.0 - name: CluedIn.Provider.ExternalSearch.Libpostal version: 4.1.2 - name: CluedIn.Purview version: 4.3.0 - name: CluedIn.Vocabularies.CommonDataModel version: 4.3.0 - name: CluedIn.Connector.SqlServer version: 4.1.0 - name: CluedIn.ExternalSearch.Providers.Web version: 4.1.0 - name: CluedIn.Connector.OneLake version: 4.3.0 roles: main: resources: limits: memory: 2G datasourceProcessing: replicas: 2 resources: requests: cpu: 1000m memory: 512M limits: cpu: 2000m memory: 8G datasource: resources: requests: cpu: 500m memory: 512M limits: cpu: 500m memory: 512M submitter: resources: requests: memory: 1G limits: memory: 1G gql: resources: requests: memory: 512M limits: memory: 4G ui: resources: requests: memory: 256M monitoring: alertManager: licenseKey: &lt;your-license-key&gt; infrastructure: haproxy-ingress: controller: resources: requests: cpu: 125m limits: cpu: 500m cert-manager: cainjector: resources: requests: memory: 128Mi . | Delete existing node exporter daemonsets . kubectl delete daemonsets/cluedin-prometheus-node-exporter -n cluedin . | Delete existing prometheus CRDS. kubectl delete crd alertmanagerconfigs.monitoring.coreos.com kubectl delete crd alertmanagers.monitoring.coreos.com kubectl delete crd podmonitors.monitoring.coreos.com kubectl delete crd probes.monitoring.coreos.com kubectl delete crd prometheusagents.monitoring.coreos.com kubectl delete crd prometheuses.monitoring.coreos.com kubectl delete crd prometheusrules.monitoring.coreos.com kubectl delete crd scrapeconfigs.monitoring.coreos.com kubectl delete crd servicemonitors.monitoring.coreos.com kubectl delete crd thanosrulers.monitoring.coreos.com . | Redeploy prometheus CRDS with compatible version for the new helm chart. kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusagents.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.71.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.3.2 \\ --reuse-values \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-07-03#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-07-03#guide"
  },"1828": {
    "doc": "2024.07.03",
    "title": "2024.07.03",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-07-03",
    
    "relUrl": "/paas-operations/upgrade/2024-07-03"
  },"1829": {
    "doc": "2024.12",
    "title": "On this page",
    "content": ". | Prerequisites | Guide . | Stage 1: Prepare for Upgrade | Stage 2: Run Data Upgrade | Stage 3: Finalize Upgrade | . | . This document covers the upgrade process from 2024.07 to 2024.12. We do not support upgrading directly to this version from prior to 2024.07. ",
    "url": "/paas-operations/upgrade/2024-12#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-12#on-this-page"
  },"1830": {
    "doc": "2024.12",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.4.1. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-12#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-12#prerequisites"
  },"1831": {
    "doc": "2024.12",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. Stage 1: Prepare for Upgrade . | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.12\" strategy: type: Recreate application: cluedin: image: tag: 2024.12 components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.0.1\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.4.0\" - name: \"CluedIn.Connector.Http\" version: \"4.0.0\" - name: \"CluedIn.DataVerse\" version: \"4.2.2\" - name: \"CluedIn.EventHub\" version: \"4.4.0\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.4.0\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.4.0\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.0.2\" - name: \"CluedIn.Connector.OneLake\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.4.0\" - name: \"CluedIn.Purview\" version: \"4.4.0\" - name: \"CluedIn.PowerApps\" version: \"4.4.0\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.4.0\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.1.1\" submitter: resources: requests: memory: 1G limits: memory: 1G . | Use the Elasticsearch custom init container values from elasticsearch-upgrade.yml . | Execute the following Helm command: # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.4.1 \\ --values ${CustomValues} \\ --values elasticsearch-upgrade.yml \\ --set application.system.upgradeMode=true \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true . After a few minutes, it should successfully complete. | Validation: . | ❗ Wait until deployment finishes. Make sure that all pods are healthy and all jobs are completed. | ❗ Don’t proceed further if any of the pods have health issues. Rather investigate the issues. This will apply dynamic mappings to every CluedIn index | If any errors occur whilst running the elastic search upgrade we restart the pod and try to recover. You can find saved logs in /usr/share/elasticsearch/data/ | ⚠ We also store a list of indexes and what stage they last completed on the last run through and some completion files that will stop us reprocessing things that have already completed the upgrade within that folder. | . | . Stage 2: Run Data Upgrade . | Apply data upgrade by running following command: kubectl apply -f - &lt;&lt;EOF apiVersion: api.cluedin.com/v1 kind: DataUpgrade metadata: annotations: meta.helm.sh/release-name: cluedin-platform meta.helm.sh/release-namespace: cluedin labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.4.1 release: cluedin-platform name: data-upgrade-440 namespace: cluedin spec: toVersion: Version440 EOF . | Validation: . | ❗ Confirm the data upgrade completed successfully. | When the data upgrade is completed the cluedin-server pod should have below logs. [#093 12:12:33 INF] Performing Upgrade Scenario Remove legacy DeduplicateEntities jobs for version Version440 [#093 12:12:33 INF] Completed Upgrade Scenario Remove legacy DeduplicateEntities jobs for version Version440 [#093 12:18:33 INF] Performing Upgrade Scenario Migrate clean projects to dedicated hashes table for version Version440 [#124 12:18:33 INF] Completed Upgrade Scenario Migrate clean projects to dedicated hashes table for version Version440 [#124 13:03:17 INF] Performing Upgrade Scenario Elastic Search Repopulate Index for version Version440 [#067 13:03:17 INF] Completed Upgrade Scenario Elastic Search Repopulate Index for version Version440 [#067 13:03:17 INF] HTTP POST /api/upgradeto/Version440 responded 200 in 3044054.6755 ms . | . | . Stage 3: Finalize Upgrade . | Run the final stage of the upgrade to bring the system back online (disable upgrade mode): . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.4.1 \\ --values ${CustomValues} --set application.system.upgradeMode=false \\ --set application.system.runDatabaseJobsOnUpgrade=false \\ --set application.system.runNugetFullRestore=false . | Validation: . | Ensure no errors in the pod logs. | Verify the server starts in normal mode. | Ensure cluedin-ui pod is Running.. | . | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-12#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-12#guide"
  },"1832": {
    "doc": "2024.12",
    "title": "2024.12",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-12",
    
    "relUrl": "/paas-operations/upgrade/2024-12"
  },"1833": {
    "doc": "2024.12.01",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.12 to 2024.12.01. We do not support upgrading directly to this version from prior to 2024.07. ",
    "url": "/paas-operations/upgrade/2024-12-01#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-12-01#on-this-page"
  },"1834": {
    "doc": "2024.12.01",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.4.1. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-12-01#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-12-01#prerequisites"
  },"1835": {
    "doc": "2024.12.01",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.12.01\" strategy: type: Recreate application: cluedin: image: tag: \"2024.12.01\" components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.0.1\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.4.0\" - name: \"CluedIn.Connector.Http\" version: \"4.0.0\" - name: \"CluedIn.DataVerse\" version: \"4.2.2\" - name: \"CluedIn.EventHub\" version: \"4.4.0\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.4.0\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.4.1\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.0.2\" - name: \"CluedIn.Connector.OneLake\" version: \"4.4.1\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.4.0\" - name: \"CluedIn.Purview\" version: \"4.4.0\" - name: \"CluedIn.PowerApps\" version: \"4.4.0\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.4.0\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.1.2\" . | Delete existing node exporter daemonsets . kubectl delete daemonsets/cluedin-prometheus-node-exporter -n cluedin . | Delete existing prometheus CRDS. kubectl delete crd alertmanagerconfigs.monitoring.coreos.com kubectl delete crd alertmanagers.monitoring.coreos.com kubectl delete crd podmonitors.monitoring.coreos.com kubectl delete crd probes.monitoring.coreos.com kubectl delete crd prometheusagents.monitoring.coreos.com kubectl delete crd prometheuses.monitoring.coreos.com kubectl delete crd prometheusrules.monitoring.coreos.com kubectl delete crd scrapeconfigs.monitoring.coreos.com kubectl delete crd servicemonitors.monitoring.coreos.com kubectl delete crd thanosrulers.monitoring.coreos.com . | Redeploy prometheus CRDS with compatible version for the new helm chart. kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusagents.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.4.1 \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-12-01#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-12-01#guide"
  },"1836": {
    "doc": "2024.12.01",
    "title": "2024.12.01",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-12-01",
    
    "relUrl": "/paas-operations/upgrade/2024-12-01"
  },"1837": {
    "doc": "Google Images",
    "title": "On this page",
    "content": ". | Add Google Images enricher | Properties from Google Images enricher | . This article explains how to add the Google Images enricher. The purpose of this enricher is to search for an image based on the provided text—such as a company, organization, or any other name—and add the retrieved image to the golden record. The Google Images enricher supports the following endpoint: . | https://www.googleapis.com/customsearch/v1?key={APIKey}&amp;cx=000950167190857528722:vf0rypkbf0w&amp;q={Name}&amp;searchType=image | . ",
    "url": "/preparation/enricher/google-images#on-this-page",
    
    "relUrl": "/preparation/enricher/google-images#on-this-page"
  },"1838": {
    "doc": "Google Images",
    "title": "Add Google Images enricher",
    "content": "To use the Google Images enricher, you must provide the API key. To get the API key, follow the instructions here. The enricher uses the defined image search text (from a vocabulary key) as the query for retrieving relevant images. To add Google Images enricher . | On the navigation pane, go to Preparation &gt; Enrich. Then, select Add Enricher. | On the Choose Enricher tab, select Google Images, and then select Next. | On the Configure tab, provide the following details: . | API Key – enter the API key required to authenticate with the Google Images endpoint. | Accepted Business Domain – enter the business domain to define which golden records will be enriched. | Image Search text – enter the vocabulary key that contains the text to be used for the image search. | . | Select Test Connection to make sure the enricher is properly configured, and then select Add. The Google Images enricher is added and has an active status. This means that it will enrich relevant golden records during processing or when you trigger external enrichment. | . After the Google Images enricher is added, you can modify its details: . | Settings – add a user-friendly display name, select the description for data coming from the enricher, and define the source quality for determining the winning values. | Authentication – modify the details you provided while configuring the enricher. | . ",
    "url": "/preparation/enricher/google-images#add-google-images-enricher",
    
    "relUrl": "/preparation/enricher/google-images#add-google-images-enricher"
  },"1839": {
    "doc": "Google Images",
    "title": "Properties from Google Images enricher",
    "content": "You can find the image added to the golden record on the golden record Overview page. For a more detailed information about the changes made to a golden record by the Google Images enricher, check the corresponding data part on the History page. ",
    "url": "/preparation/enricher/google-images#properties-from-google-images-enricher",
    
    "relUrl": "/preparation/enricher/google-images#properties-from-google-images-enricher"
  },"1840": {
    "doc": "Google Images",
    "title": "Google Images",
    "content": " ",
    "url": "/preparation/enricher/google-images",
    
    "relUrl": "/preparation/enricher/google-images"
  },"1841": {
    "doc": "Manual data entry",
    "title": "Manual data entry",
    "content": "In addition to loading data from various sources, you can also enter data manually. The Manual data entry module allows you to create records directly in CluedIn. This way, you can capture data that may not be available from other sources but is necessary for your data management purposes. Manual data entry is used to create records in an enforced and structured manner. This means that the Administrator sets up a manual data entry project and defines the schema that will be used to create the records. Data Stewards are then tasked with creating records according to this schema. This section covers the following areas: . | Configuring a manual data entry project – learn how to create and configure a manual data entry project, add form fields, review mapping details, and define the quality rating for the source. | Adding records in a manual data entry project – learn how to add single or multiple the records manually directly to CluedIn. | Managing a manual data entry project – learn how to make changes to the manual data entry project and how to manage access to the manual data entry project and the records created within the project. | . ",
    "url": "/integration/manual-data-entry",
    
    "relUrl": "/integration/manual-data-entry"
  },"1842": {
    "doc": "Playbooks",
    "title": "Playbooks",
    "content": "In this section, you’ll find recommendations and guidelines on how to run a CluedIn project. After purchasing CluedIn, make sure that you get acquainted with our playbooks to ensure smooth and successful implementation of your project. ",
    "url": "/playbooks",
    
    "relUrl": "/playbooks"
  },"1843": {
    "doc": "2024.12.02",
    "title": "On this page",
    "content": ". | Prerequisites | Guide | . This document covers the upgrade process from 2024.12 to 2024.12.02. We do not support upgrading directly to this version from prior to 2024.07. ",
    "url": "/paas-operations/upgrade/2024-12-02#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2024-12-02#on-this-page"
  },"1844": {
    "doc": "2024.12.02",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.4.2. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2024-12-02#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2024-12-02#prerequisites"
  },"1845": {
    "doc": "2024.12.02",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2024.12.02\" strategy: type: Recreate application: cluedin: image: tag: \"2024.12.02\" components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.0.1\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.4.0\" - name: \"CluedIn.Connector.Http\" version: \"4.0.0\" - name: \"CluedIn.DataVerse\" version: \"4.2.2\" - name: \"CluedIn.EventHub\" version: \"4.4.1\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.4.0\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.4.2\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.0.2\" - name: \"CluedIn.Connector.OneLake\" version: \"4.4.1\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.4.0\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.4.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.4.0\" - name: \"CluedIn.Purview\" version: \"4.4.1\" - name: \"CluedIn.PowerApps\" version: \"4.4.3\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.4.2\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.1.2\" . | Delete existing node exporter daemonsets . kubectl delete daemonsets/cluedin-prometheus-node-exporter -n cluedin . | Delete existing prometheus CRDS. kubectl delete crd alertmanagerconfigs.monitoring.coreos.com kubectl delete crd alertmanagers.monitoring.coreos.com kubectl delete crd podmonitors.monitoring.coreos.com kubectl delete crd probes.monitoring.coreos.com kubectl delete crd prometheusagents.monitoring.coreos.com kubectl delete crd prometheuses.monitoring.coreos.com kubectl delete crd prometheusrules.monitoring.coreos.com kubectl delete crd scrapeconfigs.monitoring.coreos.com kubectl delete crd servicemonitors.monitoring.coreos.com kubectl delete crd thanosrulers.monitoring.coreos.com . | Redeploy prometheus CRDS with compatible version for the new helm chart. kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusagents.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml kubectl apply --server-side -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.79.0/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.4.2 \\ --values ${CustomValues} \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2024-12-02#guide",
    
    "relUrl": "/paas-operations/upgrade/2024-12-02#guide"
  },"1846": {
    "doc": "2024.12.02",
    "title": "2024.12.02",
    "content": " ",
    "url": "/paas-operations/upgrade/2024-12-02",
    
    "relUrl": "/paas-operations/upgrade/2024-12-02"
  },"1847": {
    "doc": "2025.05.00",
    "title": "On this page",
    "content": ". | Prerequisites | Guide . | Stage 1: Prepare for Upgrade | Stage 2: Run Data Upgrade | Stage 3: Finalize Upgrade | . | . This document covers the upgrade process from 2024.12 to 2025.05.00. We do not support upgrading directly to this version from prior to 2024.12. ",
    "url": "/paas-operations/upgrade/2025-05-00#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2025-05-00#on-this-page"
  },"1848": {
    "doc": "2025.05.00",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.5.0. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2025-05-00#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2025-05-00#prerequisites"
  },"1849": {
    "doc": "2025.05.00",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. Stage 1: Prepare for Upgrade . | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2025.05.00\" strategy: type: Recreate application: cluedin: components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.5.0\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.5.0\" - name: \"CluedIn.Connector.Http\" version: \"4.5.0\" - name: \"CluedIn.EventHub\" version: \"4.5.0\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.5.0\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.5.0\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.5.0\" - name: \"CluedIn.Connector.OneLake\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.5.0\" - name: \"CluedIn.Purview\" version: \"4.5.0\" - name: \"CluedIn.PowerApps\" version: \"4.5.0\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.5.0\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.5.0\" - name: \"CluedIn.Connector.FabricOpenMirroring\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.AzureOpenAI\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.GoogleImages\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.RestApi\" version: \"4.5.0\" gql: hubSpotToken: &lt;pat-token&gt; . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.5.0 \\ --values ${CustomValues} \\ --set application.system.upgradeMode=true \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | Validation: | . | ❗ Wait until deployment finishes. Make sure that all pods are healthy and all jobs are completed. | ❗ Don’t proceed further if any of the pods have health issues. Rather investigate the issues. This will apply dynamic mappings to every CluedIn index | . | . Stage 2: Run Data Upgrade . | Apply data upgrade by running following command: kubectl apply -f - &lt;&lt;EOF apiVersion: api.cluedin.com/v1 kind: DataUpgrade metadata: annotations: meta.helm.sh/release-name: cluedin-platform meta.helm.sh/release-namespace: cluedin labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.5.0 release: cluedin-platform name: data-upgrade-450 namespace: cluedin spec: toVersion: Version450 EOF . | Validation: . | ❗ Confirm the data upgrade completed successfully. | . | . Stage 3: Finalize Upgrade . | Run the final stage of the upgrade to bring the system back online (disable upgrade mode): . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.5.0 \\ --values ${CustomValues} --set application.system.upgradeMode=false \\ --set application.system.runDatabaseJobsOnUpgrade=false \\ --set application.system.runNugetFullRestore=false . | Validation: . | Ensure no errors in the pod logs. | Verify the server starts in normal mode. | Ensure cluedin-ui pod is Running.. | . | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2025-05-00#guide",
    
    "relUrl": "/paas-operations/upgrade/2025-05-00#guide"
  },"1850": {
    "doc": "2025.05.00",
    "title": "2025.05.00",
    "content": " ",
    "url": "/paas-operations/upgrade/2025-05-00",
    
    "relUrl": "/paas-operations/upgrade/2025-05-00"
  },"1851": {
    "doc": "2025.05.01",
    "title": "On this page",
    "content": ". | Prerequisites | Guide . | Stage 1: Prepare for Upgrade | Stage 2: Run Data Upgrade | Stage 3: Finalize Upgrade | . | . This document covers the upgrade process from 2024.12 to 2025.05.01. We do not support upgrading directly to this version from prior to 2024.12. ",
    "url": "/paas-operations/upgrade/2025-05-01#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2025-05-01#on-this-page"
  },"1852": {
    "doc": "2025.05.01",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.5.0. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2025-05-01#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2025-05-01#prerequisites"
  },"1853": {
    "doc": "2025.05.01",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. Stage 1: Prepare for Upgrade . | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2025.05.01\" strategy: type: Recreate application: cluedin: components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.5.0\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.5.0\" - name: \"CluedIn.Connector.Http\" version: \"4.5.0\" - name: \"CluedIn.EventHub\" version: \"4.5.0\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.5.0\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.5.0\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.5.0\" - name: \"CluedIn.Connector.OneLake\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.5.0\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.5.0\" - name: \"CluedIn.Purview\" version: \"4.5.0\" - name: \"CluedIn.PowerApps\" version: \"4.5.0\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.5.0\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.5.0\" - name: \"CluedIn.Connector.FabricOpenMirroring\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.AzureOpenAI\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.GoogleImages\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.RestApi\" version: \"4.5.0\" gql: hubSpotToken: &lt;pat-token&gt; . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.5.0 \\ --values ${CustomValues} \\ --set application.system.upgradeMode=true \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | Validation: | . | ❗ Wait until deployment finishes. Make sure that all pods are healthy and all jobs are completed. | ❗ Don’t proceed further if any of the pods have health issues. Rather investigate the issues. This will apply dynamic mappings to every CluedIn index | . | . Stage 2: Run Data Upgrade . | Apply data upgrade by running following command: kubectl apply -f - &lt;&lt;EOF apiVersion: api.cluedin.com/v1 kind: DataUpgrade metadata: annotations: meta.helm.sh/release-name: cluedin-platform meta.helm.sh/release-namespace: cluedin labels: app: cluedin app.kubernetes.io/instance: cluedin-platform app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: application helm.sh/chart: application-2.5.0 release: cluedin-platform name: data-upgrade-450 namespace: cluedin spec: toVersion: Version450 EOF . | Validation: . | ❗ Confirm the data upgrade completed successfully. | . | . Stage 3: Finalize Upgrade . | Run the final stage of the upgrade to bring the system back online (disable upgrade mode): . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.5.0 \\ --values ${CustomValues} --set application.system.upgradeMode=false \\ --set application.system.runDatabaseJobsOnUpgrade=false \\ --set application.system.runNugetFullRestore=false . | Validation: . | Ensure no errors in the pod logs. | Verify the server starts in normal mode. | Ensure cluedin-ui pod is Running.. | . | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2025-05-01#guide",
    
    "relUrl": "/paas-operations/upgrade/2025-05-01#guide"
  },"1854": {
    "doc": "2025.05.01",
    "title": "2025.05.01",
    "content": " ",
    "url": "/paas-operations/upgrade/2025-05-01",
    
    "relUrl": "/paas-operations/upgrade/2025-05-01"
  },"1855": {
    "doc": "2025.05.02",
    "title": "On this page",
    "content": ". | Prerequisites | Guide . | Stage 1: Prepare for Upgrade | . | . This document covers the upgrade process from 2025.05.00 to 2025.05.02. We do not support upgrading directly to this version from prior to 2025.05.00 . ",
    "url": "/paas-operations/upgrade/2025-05-02#on-this-page",
    
    "relUrl": "/paas-operations/upgrade/2025-05-02#on-this-page"
  },"1856": {
    "doc": "2025.05.02",
    "title": "Prerequisites",
    "content": ". | Access to the helm chart version 2.5.1. You may need to helm repo update to grab the latest. | kubectl and helm | . ",
    "url": "/paas-operations/upgrade/2025-05-02#prerequisites",
    
    "relUrl": "/paas-operations/upgrade/2025-05-02#prerequisites"
  },"1857": {
    "doc": "2025.05.02",
    "title": "Guide",
    "content": "Before any upgrade takes place, ensure you have taken a snapshot or backup of all your running PVCs in your AKS cluster. This is very important as databases may be upgraded during this process. Stage 1: Prepare for Upgrade . | Connect to your cluster via the kubeconfig. | Export your current running helm values file. Keep one for this process, and also keep one as a backup. | In the values file we’ll be working with, update the following properties: . global: image: tag: \"2025.05.02\" strategy: type: Recreate application: cluedin: components: packages: - name: \"CluedIn.Connector.AzureEventHub\" version: \"4.5.1\" - name: \"CluedIn.Connector.Dataverse\" version: \"4.5.1\" - name: \"CluedIn.Connector.Http\" version: \"4.5.0\" - name: \"CluedIn.EventHub\" version: \"4.5.1\" - name: \"CluedIn.Vocabularies.CommonDataModel\" version: \"4.5.1\" - name: \"CluedIn.Connector.AzureDataLake\" version: \"4.5.1\" - name: \"CluedIn.Connector.AzureDedicatedSqlPool\" version: \"4.0.2\" - name: \"CluedIn.Connector.AzureServiceBus\" version: \"4.5.0\" - name: \"CluedIn.Connector.OneLake\" version: \"4.5.1\" - name: \"CluedIn.ExternalSearch.Providers.DuckDuckGo.Provider\" version: \"4.5.1\" - name: \"CluedIn.ExternalSearch.Providers.PermId.Provider\" version: \"4.5.1\" - name: \"CluedIn.ExternalSearch.Providers.Web\" version: \"4.4.2\" - name: \"CluedIn.Provider.ExternalSearch.Bregg\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.ClearBit\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.CompanyHouse\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.CVR\" version: \"4.4.2\" - name: \"CluedIn.Provider.ExternalSearch.Gleif\" version: \"4.4.2\" - name: \"CluedIn.Provider.ExternalSearch.GoogleMaps\" version: \"4.5.1\" - name: \"CluedIn.Provider.ExternalSearch.KnowledgeGraph\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Libpostal\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.OpenCorporates\" version: \"4.5.0\" - name: \"CluedIn.Provider.ExternalSearch.Providers.VatLayer\" version: \"4.5.1\" - name: \"CluedIn.Purview\" version: \"4.5.1\" - name: \"CluedIn.PowerApps\" version: \"4.5.1\" - name: \"CluedIn.EnterpriseFlows.PowerAutomate\" version: \"4.5.0\" - name: \"CluedIn.Connector.SqlServer\" version: \"4.5.0\" - name: \"CluedIn.Connector.FabricOpenMirroring\" version: \"4.5.1\" - name: \"CluedIn.Provider.ExternalSearch.Providers.AzureOpenAI\" version: \"4.4.1\" - name: \"CluedIn.Provider.ExternalSearch.GoogleImages\" version: \"4.4.2\" - name: \"CluedIn.Provider.ExternalSearch.RestApi\" version: \"4.5.0\" gql: hubSpotToken: &lt;pat-token&gt; . | Apply the values file from your shell by running the following: . # ${CustomValues} refers to the values file you have amended with the above changes. Please type the full path here. helm upgrade cluedin-platform -n cluedin cluedin/cluedin-platform \\ --version 2.5.1 \\ --values ${CustomValues} \\ --set application.system.runDatabaseJobsOnUpgrade=true \\ --set application.system.runNugetFullRestore=true \\ --wait \\ --timeout 10m0s . After a few minutes, it should successfully complete. | Validation: | . | ❗ Wait until deployment finishes. Make sure that all pods are healthy and all jobs are completed. | ❗ Don’t proceed further if any of the pods have health issues. Rather investigate the issues. This will apply dynamic mappings to every CluedIn index | Ensure no errors in the pod logs. | Verify the server starts in normal mode. | Ensure cluedin-ui pod is Running.. | . | . If there are any issues during the upgrade process, please do not hesitate to reach out to CluedIn support. ",
    "url": "/paas-operations/upgrade/2025-05-02#guide",
    
    "relUrl": "/paas-operations/upgrade/2025-05-02#guide"
  },"1858": {
    "doc": "2025.05.02",
    "title": "2025.05.02",
    "content": " ",
    "url": "/paas-operations/upgrade/2025-05-02",
    
    "relUrl": "/paas-operations/upgrade/2025-05-02"
  },"1859": {
    "doc": "Training",
    "title": "Training",
    "content": "This training will introduce you to CluedIn, a modern master data management platform built to simplify, automate, and connect enterprise data. It is designed for anyone who works with data—shaping data strategy, overseeing governance, or analyzing business trends. In the course of this training, you will learn how to use CluedIn’s tools to work with trusted, connected data without the rigid setup and complexity of traditional MDM systems. Presented by: Pierre Derval, Chief Product Officer at CluedIn . Presentation: Download PPT . To get your CluedIn instance, refer to our article How to get CluedIn. ",
    "url": "/training",
    
    "relUrl": "/training"
  },"1860": {
    "doc": "Usecases",
    "title": "Getting Started with CluedIn: Use Case Walkthroughs",
    "content": "As a new user of CluedIn, one of the most effective ways to learn the platform is by walking through real-world use cases. These examples will help you understand not only the features of CluedIn, but also the outcomes you can achieve by putting them into practice. Each walkthrough highlights a typical business challenge and demonstrates how CluedIn solves it end-to-end. ",
    "url": "/usecases#getting-started-with-cluedin-use-case-walkthroughs",
    
    "relUrl": "/usecases#getting-started-with-cluedin-use-case-walkthroughs"
  },"1861": {
    "doc": "Usecases",
    "title": "1. Customer 360: Building a Unified Customer Profile",
    "content": ". | Challenge: Customer data is fragmented across multiple systems (CRM, ERP, support tools, marketing automation). | CluedIn Walkthrough: . | Ingest data from multiple sources. | Automatically detect duplicates, inconsistencies, and missing values. | Link records into a single “golden record” for each customer. | . | Outcome: A reliable and complete view of each customer, enabling better sales insights, personalized marketing, and improved service. | . ",
    "url": "/usecases#1-customer-360-building-a-unified-customer-profile",
    
    "relUrl": "/usecases#1-customer-360-building-a-unified-customer-profile"
  },"1862": {
    "doc": "Usecases",
    "title": "2. Data Quality Management",
    "content": ". | Challenge: Inconsistent, incomplete, or poor-quality data is slowing down reporting and analytics. | CluedIn Walkthrough: . | Run automated data profiling to identify quality issues. | Apply CluedIn’s cleaning and enrichment rules to standardize and validate data. | Track improvements through CluedIn’s quality dashboards. | . | Outcome: Data you can trust, with measurable improvements in accuracy and completeness. | . ",
    "url": "/usecases#2-data-quality-management",
    
    "relUrl": "/usecases#2-data-quality-management"
  },"1863": {
    "doc": "Usecases",
    "title": "3. Master Data Management for Products",
    "content": ". | Challenge: Product data is scattered across e-commerce platforms, ERP, and supplier feeds, causing mismatched records. | CluedIn Walkthrough: . | Consolidate all product records into a central view. | Resolve conflicts in product attributes (e.g., price, category, descriptions). | Publish “golden” product data back to consuming systems. | . | Outcome: Consistent and trusted product information across all channels, reducing errors and improving customer experience. | . ",
    "url": "/usecases#3-master-data-management-for-products",
    
    "relUrl": "/usecases#3-master-data-management-for-products"
  },"1864": {
    "doc": "Usecases",
    "title": "4. Data Integration for Analytics",
    "content": ". | Challenge: Business intelligence (BI) teams struggle to prepare clean datasets for reporting and machine learning. | CluedIn Walkthrough: . | Use CluedIn to unify data from structured and unstructured sources. | Create business-ready datasets by applying rules, transformations, and governance. | Export to analytics tools (e.g., Power BI, Tableau, Snowflake). | . | Outcome: High-quality, analysis-ready datasets delivered faster and with less manual effort. | . ",
    "url": "/usecases#4-data-integration-for-analytics",
    
    "relUrl": "/usecases#4-data-integration-for-analytics"
  },"1865": {
    "doc": "Usecases",
    "title": "5. Regulatory Compliance (GDPR/CCPA)",
    "content": ". | Challenge: Compliance teams need a complete and auditable view of personal data across the organization. | CluedIn Walkthrough: . | Automatically discover personal data across systems. | Link and classify records to individuals. | Generate compliance-ready reports and respond to Subject Access Requests (SARs). | . | Outcome: Reduced risk and effort in meeting regulatory requirements, with confidence in auditability. | . ",
    "url": "/usecases#5-regulatory-compliance-gdprccpa",
    
    "relUrl": "/usecases#5-regulatory-compliance-gdprccpa"
  },"1866": {
    "doc": "Usecases",
    "title": "6. Customer Onboarding Acceleration",
    "content": ". | Challenge: New customers expect fast, accurate, and seamless onboarding. | CluedIn Walkthrough: . | Ingest customer information from forms, contracts, and external systems. | Validate, enrich, and standardize the data. | Deliver complete onboarding records into CRM and workflow systems. | . | Outcome: Faster onboarding cycles, fewer errors, and a better first impression for new customers. | . ",
    "url": "/usecases#6-customer-onboarding-acceleration",
    
    "relUrl": "/usecases#6-customer-onboarding-acceleration"
  },"1867": {
    "doc": "Usecases",
    "title": "7. Helping with ERP Migration",
    "content": ". | Challenge: It is often the case that you will need to migrate an existing ERP system to a new one, or even many ERP systems into a new ERP system. | CluedIn Walkthrough: . | Ingest materials information from SAP, Navision and other systems. | Validate, enrich, and standardize the data. | Deliver complete onboarding records into ERP and workflow systems. | . | Outcome: Faster onboarding cycles, fewer errors, and a better migration process. | . ",
    "url": "/usecases#7-helping-with-erp-migration",
    
    "relUrl": "/usecases#7-helping-with-erp-migration"
  },"1868": {
    "doc": "Usecases",
    "title": "Next Steps",
    "content": "Each of these walkthroughs will be explored in detail with guided exercises. As you go through them, you’ll learn how to: . | Connect data sources. | Apply CluedIn’s enrichment and matching capabilities. | Build golden records. | Deliver data back to the systems where it creates the most value. | . By the end of these use cases, you’ll have hands-on experience with CluedIn’s core functionality and a clear understanding of how to apply it to your own business challenges. ",
    "url": "/usecases#next-steps",
    
    "relUrl": "/usecases#next-steps"
  },"1869": {
    "doc": "Usecases",
    "title": "Usecases",
    "content": " ",
    "url": "/usecases",
    
    "relUrl": "/usecases"
  },"1870": {
    "doc": "Terms of service",
    "title": "Terms of service",
    "content": "Welcome to CluedIn’s Terms of Service. These terms set out the legal framework for using our platform and related services. For full details, please refer to our official Terms and Conditions. These outline your rights and responsibilities as a customer, as well as CluedIn’s commitments as a service provider. CluedIn provides a dedicated Service Level Agreement (SLA) for our SaaS customers, which specifies our commitments to service availability, uptime, and support response times. Download the CluedIn SaaS SLA . ",
    "url": "/terms-of-service",
    
    "relUrl": "/terms-of-service"
  },"1871": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "On this page",
    "content": ". | Update: CVE-2021-45046 | Neo4j . | Actions Required | . | Elasticsearch . | Actions Required | . | OpenRefine . | Actions Required | . | Related links | . On December 9th 2021 a high severity vulnerability was disclosed in the Apache Log4j2 library for all versions between 2.0-beta9 and 2.14.1. This library is used by a significant amount of Java based applications and services. At CluedIn, we build .NET and Node.js services that are not impacted by the vulnerability. However, we do utilise some third-party services that are Java based. This announcement details the usage of those services and their exposure to CVE-2021-44228. ",
    "url": "/kb/log4j#on-this-page",
    
    "relUrl": "/kb/log4j#on-this-page"
  },"1872": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "Update: CVE-2021-45046",
    "content": "After the intial wave of fixes an additonal issue was discovered (CVE-2021-45-46) which highlighted that in some cases the proposed fixes for CVE-2021-44228 were insufficient. In all cases, no actions are required for CluedIn services. Notably Elasticsearch may have false positives, however Elastic reccomends the same changes as for the orginal CVE. Therefore, there are no additonal requirments for fixes with CluedIn. ",
    "url": "/kb/log4j#update-cve-2021-45046",
    
    "relUrl": "/kb/log4j#update-cve-2021-45046"
  },"1873": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "Neo4j",
    "content": "All versions of CluedIn use the 3.5.x (currently 3.5.29) version of the community edition of Neo4j or an earlier version. As identified in the Neo4j repository, versions prior to 4.2 used a custom logging framework and are not affected by the vulnerability. Additionally – Neo4j is not exposed to the public internet so user crafted requests that could expose the vulnerability cannot be targeted to the service. Actions Required . No action is required for the current nor any previous version of CluedIn. ",
    "url": "/kb/log4j#neo4j",
    
    "relUrl": "/kb/log4j#neo4j"
  },"1874": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "Elasticsearch",
    "content": "CluedIn 3.1.0-3.2.5 uses the 7.8.0 version of Elasticsearch. As identified by Elastic, Elasticsearch version 7.8+ running on JDK9+ are not susceptible. CluedIn 3.0.0 uses 7.6.2 and will require setting a JVM option to prevent exposure. Additionally – Elasticsearch is not exposed to the public internet so user crafted requests that could expose the vulnerability cannot be targeted to the service. Actions Required . CluedIn Version 3.0.0 . • In your values.yml set the following . elasticsearch: esJavaOpts: “-Dlog4j2.formatMsgNoLookups=true” . • Perform a helm upgrade with the new values.yml . No action is required for version 3.1+ of CluedIn. ",
    "url": "/kb/log4j#elasticsearch",
    
    "relUrl": "/kb/log4j#elasticsearch"
  },"1875": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "OpenRefine",
    "content": "CluedIn 3.2.2-3.2.5 uses the 3.4.1 version of OpenRefine. OpenRefine uses version 1.x of Log4j which does not contain the features which enable exposure to the vulnerability. All earlier versions of CluedIn use the 3.1 version of OpenRefine. Again, version 1.x of Log4j is used, which does not contain the features which enable exposure to the vulnerability. Actions Required . No action is required. ",
    "url": "/kb/log4j#openrefine",
    
    "relUrl": "/kb/log4j#openrefine"
  },"1876": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "Related links",
    "content": ". | https://www.randori.com/blog/cve-2021-44228/ | https://github.com/mubix/CVE-2021-44228-Log4Shell-Hashes | https://msrc-blog.microsoft.com/2021/12/11/microsofts-response-to-cve-2021-44228-apache-log4j2/ | https://www.synopsys.com/blogs/software-security/zero-day-exploit-log4j-analysis/ | https://blog.sonatype.com/a-new-0-day-log4j-vulnerability-discovered-in-the-wild | https://securelist.com/cve-2021-44228-vulnerability-in-apache-log4j-library/105210/ | https://nvd.nist.gov/vuln/detail/CVE-2021-44228 | https://blog.cloudflare.com/inside-the-log4j2-vulnerability-cve-2021-44228/ | https://github.com/neo4j-graphql/neo4j-graphql-java/issues/260 | https://github.com/neo4j/neo4j/issues/12796 | https://www.elastic.co/blog/detecting-log4j2-with-elastic-security | https://discuss.elastic.co/t/apache-log4j2-remote-code-execution-rce-vulnerability-cve-2021-44228-esa-2021-31/291476 | https://blog.cloudflare.com/cve-2021-44228-log4j-rce-0-day-mitigation/ | . ",
    "url": "/kb/log4j#related-links",
    
    "relUrl": "/kb/log4j#related-links"
  },"1877": {
    "doc": "CluedIn Security - Log4j2 CVE-2021-44228",
    "title": "CluedIn Security - Log4j2 CVE-2021-44228",
    "content": " ",
    "url": "/kb/log4j",
    
    "relUrl": "/kb/log4j"
  },"1878": {
    "doc": "Frequently asked questions",
    "title": "On this page",
    "content": ". | What happens if I change the default configuration installed using the Azure Marketplace offering? | What is different between CluedIn and other data integration platforms? | What happens with the data ingested that is terrible quality, has non-allowed values, or is noise or mess? | What do you do about sensitive and personal data? | If CluedIn has four different databases, all storing my raw data - isn’t that going to be a lot of storage? | Once the data is in CluedIn, how do I get it out? | Considering that CluedIn uses OTS databases (off-the-shelf), can I access data directly from there? | When CluedIn says it has prebuilt integrations, what about when we have customized our system so much? | If CluedIn is connecting the data for us, won’t that end up in a data model that is a mess? | The Admin screen of CluedIn is great, but I want to automate things! How do I do this? | I have made a change in CluedIn that requires me to re-process all the data. What do I do? | Does CluedIn index everything? | With four databases, how does CluedIn guarantee transactions? | Why is CluedIn written in C#? Isn’t that slow? | I am not a C# developer. Does that mean I can’t extend CluedIn? | What type of data is CluedIn not suited for? | . ",
    "url": "/kb/faq#on-this-page",
    
    "relUrl": "/kb/faq#on-this-page"
  },"1879": {
    "doc": "Frequently asked questions",
    "title": "What happens if I change the default configuration installed using the Azure Marketplace offering?",
    "content": "Any modifications made to the default configuration, except those explicitly documented as post-install adjustments on the Documentation portal, will result in the termination of support. This includes Premium Support, Azure Managed Application, and any other customer support service. The reason for this is the high probability that the cluster may not operate correctly. Changes outside the documented scope are considered out of support, and CluedIn will not provide assistance or troubleshooting for configurations that deviate from the originally installed setup. Here is a list of allowed changes: . | Adjusting requests and limits on CluedIn operating pods for both CPU and Memory | Setting up and receiving assistance with the CluedIn Backup Solution | Configuring Single Sign-On | Setting up SMTP details for Welcome Emails | Configuring Extended alerts via CluedIn (Note: CluedIn-specific alerts only, not customer-specific alerts) | Troubleshooting ad-hoc cluster failures, if determined not to be caused by the customer | Upgrading environments | Installing the product using our default configuration | Usage of Azure Key Vault to provide secrets to the cluster | General AKS maintenance on the default configuration | . Any deviation from this list will be considered out of support, and assistance for the issue is not guaranteed and may incur charges. ",
    "url": "/kb/faq#what-happens-if-i-change-the-default-configuration-installed-using-the-azure-marketplace-offering",
    
    "relUrl": "/kb/faq#what-happens-if-i-change-the-default-configuration-installed-using-the-azure-marketplace-offering"
  },"1880": {
    "doc": "Frequently asked questions",
    "title": "What is different between CluedIn and other data integration platforms?",
    "content": "Whether you are using SAP Data Services, Azure Data Factory, SSIS, Talend, Python, SAS Institute, Informatica PowerCenter, or other ETL-based tools, they all hold something in common - they require you as the engineer, architecture or business user to map different systems together. However, at CluedIn, we have found that the process of finding out what IDs need to be connected between systems as to unify or connect the records is actually the hardest part of the entire process - in fact, it gets to a point where it is just impossible to do. CluedIn uses a schema-less based integration pattern that doesn’t require a developer, architect, or business user to instruct our platform on how different systems connect - instead, we ask for a much simpler and streamlined mapping, and CluedIn will do all the work to find the relationships between the systems. As a result, we will often find connections that you could never see yourself. ",
    "url": "/kb/faq#what-is-different-between-cluedin-and-other-data-integration-platforms",
    
    "relUrl": "/kb/faq#what-is-different-between-cluedin-and-other-data-integration-platforms"
  },"1881": {
    "doc": "Frequently asked questions",
    "title": "What happens with the data ingested that is terrible quality, has non-allowed values, or is noise or mess?",
    "content": "CluedIn is an ELT type of integration platform where we want to take the data in as it is. No transforms. No cleaning. Nothing. That is our job! Insufficient quality data will happen all the time, and hence this is one of the big reasons enterprise companies take so long to get access to ready-to-use data. There are so many places in CluedIn that will catch and distribute the work involved in fixing and addressing the data quality issues that stream through the CluedIn processing engine. Firstly, it starts with CluedIn having many prebuilt processing steps to automatically clean and normalize data into the highest-fidelity format. Secondly, you can build up lists of “black-listed” values and either remove them or automatically transform them into another value on processing. Finally, you can set up rules that dictate good versus wrong values. It means that data keeps flowing for data that meet the standards - CluedIn will put everything else into a quarantine section for you to resolve so that less and fewer data has issues. ",
    "url": "/kb/faq#what-happens-with-the-data-ingested-that-is-terrible-quality-has-non-allowed-values-or-is-noise-or-mess",
    
    "relUrl": "/kb/faq#what-happens-with-the-data-ingested-that-is-terrible-quality-has-non-allowed-values-or-is-noise-or-mess"
  },"1882": {
    "doc": "Frequently asked questions",
    "title": "What do you do about sensitive and personal data?",
    "content": "CluedIn will natively detect personal data and will start by simply letting you know where it is. Then, depending on your requirements, we allow simple actions to either mask, anonymize or remove the data. As for sensitive data, this is often open to interpretation. What is sensitive for you might not be for another business, and hence CluedIn allows you to flag records as sensitive. This will build up a data model specifically for your account that will be trained to look for records with similar traits and flag them automatically over time. ",
    "url": "/kb/faq#what-do-you-do-about-sensitive-and-personal-data",
    
    "relUrl": "/kb/faq#what-do-you-do-about-sensitive-and-personal-data"
  },"1883": {
    "doc": "Frequently asked questions",
    "title": "If CluedIn has four different databases, all storing my raw data - isn’t that going to be a lot of storage?",
    "content": "The first thing to answer here is that you have complete control over what data you ingest into CluedIn. There are many cases where you will not want to bring in all the data. For the data that we do ingest, it is also worth mentioning that for certain types of data, we are not taking a full copy of the data but rather extracting what we need e.g., files only extract content and metadata. CluedIn also compresses all the data stored in the databases, meaning that the storage size is often much smaller than the source data itself. To put this in perspective, we have customers that have processed 100’s of TB’s of raw data, but in CluedIn, the databases will collectively be approximately 2 to 3 TB’s. ",
    "url": "/kb/faq#if-cluedin-has-four-different-databases-all-storing-my-raw-data---isnt-that-going-to-be-a-lot-of-storage",
    
    "relUrl": "/kb/faq#if-cluedin-has-four-different-databases-all-storing-my-raw-data---isnt-that-going-to-be-a-lot-of-storage"
  },"1884": {
    "doc": "Frequently asked questions",
    "title": "Once the data is in CluedIn, how do I get it out?",
    "content": "CluedIn is designed to sit in-between source and target systems, and hence it is in our interest to make it extremely easy to get the data out of us. There are two main ways that we recommend this, but ask that in production, you don’t use the CSV or File options - after all, this is typically the cause for siloed data in the first place! Firstly, our GraphQL API allows you to search through all the data in your CluedIn account and will return the results in JSON format. In addition, you can choose what properties you would like to select or project from the query to limit the data to only what you need. It is useful when you have other systems that want to “pull” data from CluedIn. Quite honestly, you won’t find many systems that support making GraphQL queries or REST queries, and hence we offer another option that we think is more suitable. Secondly, we allow you to create data streams directly out of CluedIn (Push) to a target. This target could be a SQL database, an Apache Spark stream, direct to Power BI, or your Data Warehouse’s dimension tables. ",
    "url": "/kb/faq#once-the-data-is-in-cluedin-how-do-i-get-it-out",
    
    "relUrl": "/kb/faq#once-the-data-is-in-cluedin-how-do-i-get-it-out"
  },"1885": {
    "doc": "Frequently asked questions",
    "title": "Considering that CluedIn uses OTS databases (off-the-shelf), can I access data directly from there?",
    "content": "We will not stop you from doing this, but we recommend that you only do this in a developer environment. If you want to talk directly to the databases, CluedIn has built you GraphQL interfaces to do this in a way that guarantees that you have remote access to only the data from your account. CluedIn ships with tools to administrate the different datastores, and hence we are influencing you to look in these datastores as a developer. The recommendation is to turn off native access to these datastores when you deploy into your production environment - in fact, by default, our Kubernetes setup has it disabled by default. ",
    "url": "/kb/faq#considering-that-cluedin-uses-ots-databases-off-the-shelf-can-i-access-data-directly-from-there",
    
    "relUrl": "/kb/faq#considering-that-cluedin-uses-ots-databases-off-the-shelf-can-i-access-data-directly-from-there"
  },"1886": {
    "doc": "Frequently asked questions",
    "title": "When CluedIn says it has prebuilt integrations, what about when we have customized our system so much?",
    "content": "The majority of integrations that CluedIn has been working with the assumption that you can change the underlying model of the source system. Hence, most of our integration will still require you to map the source objects into CluedIn to get the best out of your data. The integrations handle all the connection, delivery, and scheduling, paging, and sourcing of data. You, as a Data Architect, will need to map this into the Clue object. Fortunately, our data integration pattern makes this a simple task, and unlike other data integration platforms, we don’t need you to tell us how everything wires together - that’s our job! . ",
    "url": "/kb/faq#when-cluedin-says-it-has-prebuilt-integrations-what-about-when-we-have-customized-our-system-so-much",
    
    "relUrl": "/kb/faq#when-cluedin-says-it-has-prebuilt-integrations-what-about-when-we-have-customized-our-system-so-much"
  },"1887": {
    "doc": "Frequently asked questions",
    "title": "If CluedIn is connecting the data for us, won’t that end up in a data model that is a mess?",
    "content": "Yes! Because guess what? Your data landscape is typically a mess! The goal of CluedIn is not to form the perfect model of your data for a particular use case - it is to find how the data is naturally connected and then allow you to project your ideal model for ANY use case you can think of. You will find that the model that we build (we use the Graph for this part) will match the model that your data finds to find links between your different systems. Then, using the same model, we give intuitive and simple-to-use tools to mutate this natural model to a more rigid model for the use cases you want in that particular model. The good news is that the Graph was designed for running these types of projects. They are as fast as an index lookup. It means that projecting a “dirty” model into a clean one is something that scales and performs really well. ",
    "url": "/kb/faq#if-cluedin-is-connecting-the-data-for-us-wont-that-end-up-in-a-data-model-that-is-a-mess",
    
    "relUrl": "/kb/faq#if-cluedin-is-connecting-the-data-for-us-wont-that-end-up-in-a-data-model-that-is-a-mess"
  },"1888": {
    "doc": "Frequently asked questions",
    "title": "The Admin screen of CluedIn is great, but I want to automate things! How do I do this?",
    "content": "The good news is that the native CluedIn administration screen is built on top of the same REST API that you have direct access to as well. This means that you can do everything in the REST API that you can do in the user interface (and more!). It also means that you can script anything in CluedIn via our REST API, PowerShell, or other scripting languages. ",
    "url": "/kb/faq#the-admin-screen-of-cluedin-is-great-but-i-want-to-automate-things-how-do-i-do-this",
    
    "relUrl": "/kb/faq#the-admin-screen-of-cluedin-is-great-but-i-want-to-automate-things-how-do-i-do-this"
  },"1889": {
    "doc": "Frequently asked questions",
    "title": "I have made a change in CluedIn that requires me to re-process all the data. What do I do?",
    "content": "It will happen many times with your CluedIn account! It is highly typical and expected. CluedIn has been designed with the idea in mind that we will re-process the data all the time. Once you have made your change, you have quite a lot of control over the level of re-processing. You can re-process at a source level, globally, record level, or even something a little more custom, e.g., business domain level. ",
    "url": "/kb/faq#i-have-made-a-change-in-cluedin-that-requires-me-to-re-process-all-the-data-what-do-i-do",
    
    "relUrl": "/kb/faq#i-have-made-a-change-in-cluedin-that-requires-me-to-re-process-all-the-data-what-do-i-do"
  },"1890": {
    "doc": "Frequently asked questions",
    "title": "Does CluedIn index everything?",
    "content": "Yes. Yes, we do. You can, however, influence the CluedIn engine to store data in a more optimum way. When you are setting up your Vocabularies, you have the chance to set a Vocabulary Key Data Type. It will influence how indexes are built, and we would strongly recommend doing this. Even if you do not do this, all your content will be searchable, but you can always make things faster and more efficient if you set up your Vocabularies correctly. ",
    "url": "/kb/faq#does-cluedin-index-everything",
    
    "relUrl": "/kb/faq#does-cluedin-index-everything"
  },"1891": {
    "doc": "Frequently asked questions",
    "title": "With four databases, how does CluedIn guarantee transactions?",
    "content": "CluedIn ingests data into an enterprise service bus. It helps us guarantee the delivery of data. CluedIn utilizes “Eventual Consistency” for the different data stores. It means that CluedIn takes all records off the bus into a transactional log. The various databases will then read off the transaction log as fast as they can. It guarantees transactional support on your data but “eventual consistency” in the different data stores. With this setup and the fact that the different datastores will consume off the log in batches, it lessens the likelihood that the database will have issues with having a 100% consistency across the datastores simultaneously. ",
    "url": "/kb/faq#with-four-databases-how-does-cluedin-guarantee-transactions",
    
    "relUrl": "/kb/faq#with-four-databases-how-does-cluedin-guarantee-transactions"
  },"1892": {
    "doc": "Frequently asked questions",
    "title": "Why is CluedIn written in C#? Isn’t that slow?",
    "content": "For many reasons, the team behind CluedIn purposefully chose .net core and C# as the language for our processing engine. 1: Our core development team has decades of combined experience in building large, scalable, and performant .net applications. 2: The .NET environment has jumped leaps and bounds and has continued support for Microsoft and the community. The fact that it is open-source helped in the decision as well. 3: .NET is designed to write very large applications and scale in maintaining, separation of concern, and ease of deployment. 4: .net and C# is very much not slow. Compared to the majority of popular languages today, it is very fast. Some could argue that it is not as memory-efficient, but we think that the trade-offs to deliver solutions to you faster is well worth it. ",
    "url": "/kb/faq#why-is-cluedin-written-in-c-isnt-that-slow",
    
    "relUrl": "/kb/faq#why-is-cluedin-written-in-c-isnt-that-slow"
  },"1893": {
    "doc": "Frequently asked questions",
    "title": "I am not a C# developer. Does that mean I can’t extend CluedIn?",
    "content": "The interfaces to communicate with CluedIn are actually HTTP, i.e. REST. Our C# layer essentially wraps this and talks REST. It means that you can actually use any language that you want to talk to CluedIn, however, there are some restrictions or limitations if you do this: . | You cannot inject custom logic into the same application domain as CluedIn. | You will have to host your custom code in another host that is not CluedIn. | You cannot currently use the crawler templates that are available only in C#. | . Let’s go with some examples for you. What if you are a Python, R, SQL, or GO developer? Let’s say you want to build a custom integration that can be added through the user interface, but the code itself is written in Python. ",
    "url": "/kb/faq#i-am-not-a-c-developer-does-that-mean-i-cant-extend-cluedin",
    
    "relUrl": "/kb/faq#i-am-not-a-c-developer-does-that-mean-i-cant-extend-cluedin"
  },"1894": {
    "doc": "Frequently asked questions",
    "title": "What type of data is CluedIn not suited for?",
    "content": "There are many use-cases where CluedIn could be considered overkill or not fit for purpose. There are many use-cases that do not need all of the data preparation and cleaning pieces of CluedIn. It could very much be argued that IoT data doesn’t suffer from the same problems or require the same data preparation than other data such as operation business data. Hence here is a list of use-cases or data that we don’t think is necessarily suitable for CluedIn: . 1: Processing Signal/IoT data that doesn’t have data quality issues or doesn’t need cataloging, governance - but instead needs to be made available in a Dashboard as fast as humanly possible. 2: If you have a limited number of systems to integrate and there are no problems determining how the records are connected between the different systems. A good example would be if you just wanted to integrate your Dropbox account with your CRM. 3: Where you do not want to move data. CluedIn copies data from sources to target. It won’t necessarily take all the data, but it is an ingestion engine. You have to be very careful with this use case as many problems cannot be properly solved without copying the data. ",
    "url": "/kb/faq#what-type-of-data-is-cluedin-not-suited-for",
    
    "relUrl": "/kb/faq#what-type-of-data-is-cluedin-not-suited-for"
  },"1895": {
    "doc": "Frequently asked questions",
    "title": "Frequently asked questions",
    "content": " ",
    "url": "/kb/faq",
    
    "relUrl": "/kb/faq"
  },"1896": {
    "doc": "Rules are running slowly",
    "title": "Why Are My Rules Processing Slowly in CluedIn?",
    "content": "Rules in CluedIn are a powerful way to clean, standardize, and enrich your data as it flows through the platform. They allow you to automate transformations and enforce business logic at scale. However, in some cases, you may notice that your Rules take longer than expected to process. This article explains the key factors that influence Rule performance, common reasons why execution may be slow, and steps you can take to optimize and troubleshoot. ",
    "url": "/kb/rules-slow#why-are-my-rules-processing-slowly-in-cluedin",
    
    "relUrl": "/kb/rules-slow#why-are-my-rules-processing-slowly-in-cluedin"
  },"1897": {
    "doc": "Rules are running slowly",
    "title": "How Rule Processing Works",
    "content": "Before diving into performance issues, it’s important to understand how Rules are executed in CluedIn: . | Trigger – A Rule is triggered when new data is ingested or when a Rule is explicitly re-run on existing data. | Evaluation – The Rule engine evaluates each record against the configured conditions. | Transformation – For matching records, the Rule applies the transformation or enrichment logic. | Propagation – Updates are written back to the affected entities and, if configured, published to downstream systems. | . The time it takes depends on the volume of records, the complexity of the logic, and the resources available in your CluedIn environment. ",
    "url": "/kb/rules-slow#how-rule-processing-works",
    
    "relUrl": "/kb/rules-slow#how-rule-processing-works"
  },"1898": {
    "doc": "Rules are running slowly",
    "title": "Common Reasons Rules Run Slowly",
    "content": "1. Large Data Volumes . | Symptom: Rules that work quickly on small datasets suddenly take much longer on production-scale data. | Explanation: Each Rule must scan through entities to find matches. Processing millions of records inevitably takes more time. | . Tip: Test Rules on smaller sample datasets before applying them to production. 2. Complex Rule Conditions . | Symptom: Rules with multiple conditions, regex patterns, or nested logic perform slower. | Explanation: Each additional condition increases the number of evaluations per record. Regex-based conditions are especially costly. | . Tip: Simplify Rule logic where possible. Split complex Rules into multiple smaller, focused Rules. 3. High Number of Active Rules . | Symptom: Processing slows down as more Rules are added. | Explanation: CluedIn must evaluate every active Rule against incoming records. Even if most Rules don’t apply, they still need to be checked. | . Tip: Review your Rule library and disable Rules that are not actively needed. 4. Data Distribution &amp; Entity Linking . | Symptom: Rules that depend on entity relationships (e.g., linked customers, orders, or products) take longer. | Explanation: Linking requires lookups across multiple entities. Large or poorly matched datasets increase lookup overhead. | . Tip: Ensure entity matching configurations are optimized before running Rules that depend on them. 5. Environment &amp; Resource Constraints . | Symptom: Rules slow down during periods of heavy ingestion or other intensive processing. | Explanation: The Rule engine shares compute resources with ingestion, matching, and enrichment jobs. Resource contention can delay execution. | . Tip: Schedule large Rule runs during off-peak hours or scale your CluedIn deployment to handle higher throughput. 6. Reprocessing vs. Incremental Runs . | Symptom: A Rule re-run on all historical data takes significantly longer than on new incremental data. | Explanation: Full reprocessing forces CluedIn to re-check all records, while incremental runs only apply to newly ingested or updated data. | . Tip: Use incremental runs where possible. Reserve full re-runs for cases where underlying Rule logic has changed. ",
    "url": "/kb/rules-slow#common-reasons-rules-run-slowly",
    
    "relUrl": "/kb/rules-slow#common-reasons-rules-run-slowly"
  },"1899": {
    "doc": "Rules are running slowly",
    "title": "How to Troubleshoot Slow Rules",
    "content": ". | Check Rule Execution Logs . | Navigate to Rules &gt; Execution History. | Look for patterns (e.g., long-running conditions, high record counts). | . | Measure Impact of Individual Rules . | Disable all Rules. | Re-enable them one by one to isolate performance bottlenecks. | . | Profile the Data . | Use Data Quality Profiling to identify large or inconsistent fields that may slow down evaluation (e.g., very long text fields, improperly formatted values). | . | Check System Resources . | Review CluedIn environment metrics (CPU, memory, queue depth). | If resource constraints are observed, consider scaling the environment. | . | Engage with CluedIn Support . | If performance is consistently poor despite optimization, raise a support ticket with logs and execution metrics. | . | . ",
    "url": "/kb/rules-slow#how-to-troubleshoot-slow-rules",
    
    "relUrl": "/kb/rules-slow#how-to-troubleshoot-slow-rules"
  },"1900": {
    "doc": "Rules are running slowly",
    "title": "Best Practices for Optimizing Rule Performance",
    "content": ". | Keep Rules Simple: Favor straightforward conditions over complex regex or nested logic. | Use Targeted Scopes: Apply Rules only to relevant entity types, not globally. | Batch Complex Logic: Break down large transformations into multiple smaller Rules. | Monitor &amp; Review: Regularly audit your Rules and disable ones that are no longer needed. | Test at Scale: Validate Rule performance in staging with production-like volumes. | Leverage Scheduling: Run heavy Rules during low-traffic times. | . ",
    "url": "/kb/rules-slow#best-practices-for-optimizing-rule-performance",
    
    "relUrl": "/kb/rules-slow#best-practices-for-optimizing-rule-performance"
  },"1901": {
    "doc": "Rules are running slowly",
    "title": "Summary",
    "content": "Rules may process slowly for a variety of reasons, most often due to large data volumes, complex conditions, or resource contention. By understanding how Rule execution works and following best practices for optimization, you can ensure that your Rules run efficiently and continue delivering value without bottlenecks. If you’ve optimized your setup and still see performance issues, contact CluedIn Support for advanced troubleshooting. ",
    "url": "/kb/rules-slow#summary",
    
    "relUrl": "/kb/rules-slow#summary"
  },"1902": {
    "doc": "Rules are running slowly",
    "title": "Rules are running slowly",
    "content": " ",
    "url": "/kb/rules-slow",
    
    "relUrl": "/kb/rules-slow"
  },"1903": {
    "doc": "Rules are not working",
    "title": "Why Are My CluedIn Rules Not Working as Expected?",
    "content": "CluedIn Rules are designed to help you automate transformations, validations, and enrichment of your data. When they don’t behave as expected, it can be frustrating and may lead to incorrect data outcomes. This article explains the most common reasons Rules may not work, how to troubleshoot them, and best practices to ensure they deliver the results you want. ",
    "url": "/kb/rules-not-working#why-are-my-cluedin-rules-not-working-as-expected",
    
    "relUrl": "/kb/rules-not-working#why-are-my-cluedin-rules-not-working-as-expected"
  },"1904": {
    "doc": "Rules are not working",
    "title": "Understanding How Rules Work",
    "content": "At a high level, a Rule in CluedIn consists of: . | Scope: The entities or attributes it applies to. | Conditions: The logic that determines whether the Rule applies. | Actions: The transformation or update to apply when conditions are met. | . If any of these are misconfigured, the Rule may not execute as you expect. ",
    "url": "/kb/rules-not-working#understanding-how-rules-work",
    
    "relUrl": "/kb/rules-not-working#understanding-how-rules-work"
  },"1905": {
    "doc": "Rules are not working",
    "title": "Common Reasons Rules Don’t Work",
    "content": "1. Incorrect Scope or Target . | Symptom: Rule never seems to run. | Cause: The Rule is applied to the wrong entity type or attribute. For example, applying a Customer Rule on Contact entities. | Fix: Double-check the entity type and field mappings in the Rule configuration. | . 2. Conditions That Never Match . | Symptom: Rule exists, but no records are updated. | Cause: Condition logic is too strict or incorrectly defined. | Example: Condition is Country = US, but data uses USA or United States. | Example: Regex pattern doesn’t align with the data format. | . | Fix: Validate the condition logic against sample data. Loosen overly strict conditions where necessary. | . 3. Action Misconfiguration . | Symptom: Rule triggers but doesn’t update fields as intended. | Cause: The action is pointing to the wrong field or uses the wrong transformation. | Fix: Review the action configuration to ensure it references the correct attributes and expected values. | . 4. Rule Execution Order . | Symptom: A Rule appears to run, but results are overridden. | Cause: Another Rule is executing afterwards and undoing or replacing the changes. | Fix: Review the Rule execution order. Consider merging related logic into a single Rule or adjusting order of operations. | . 5. Data Quality Issues . | Symptom: Rule applies inconsistently. | Cause: Input data does not match expected formats or contains null values. | Fix: Run Data Profiling to check data consistency. Create preprocessing Rules to normalize data before applying main transformations. | . 6. Rule Disabled or Not Published . | Symptom: Rule doesn’t run at all. | Cause: The Rule has been saved but not activated, or changes have not been published. | Fix: Check the Rule’s status. Ensure it is active and published. | . 7. Running Rules on the Wrong Dataset . | Symptom: Rule tests fine on samples but doesn’t work in production. | Cause: Rule is only applied on a test dataset or hasn’t been re-run on historical records. | Fix: Re-run the Rule on the correct dataset and confirm scope matches production data. | . ",
    "url": "/kb/rules-not-working#common-reasons-rules-dont-work",
    
    "relUrl": "/kb/rules-not-working#common-reasons-rules-dont-work"
  },"1906": {
    "doc": "Rules are not working",
    "title": "How to Troubleshoot Non-Working Rules",
    "content": ". | Check Rule Execution Logs . | Navigate to Rules &gt; Execution History. | Verify if the Rule ran, how many records matched, and whether any errors occurred. | . | Test on Sample Data . | Run the Rule against a small test dataset. | Confirm that the expected outcome occurs on a controlled set of records. | . | Validate Conditions Against Real Data . | Inspect a few sample records directly in Entity Explorer. | Confirm they satisfy the Rule conditions. | . | Review Dependencies . | Check if other Rules, enrichments, or transformations might be interfering. | . | Simplify the Rule . | Temporarily remove complex conditions. | Reintroduce them one by one until the failure point is found. | . | . ",
    "url": "/kb/rules-not-working#how-to-troubleshoot-non-working-rules",
    
    "relUrl": "/kb/rules-not-working#how-to-troubleshoot-non-working-rules"
  },"1907": {
    "doc": "Rules are not working",
    "title": "Best Practices to Avoid Rule Misbehavior",
    "content": ". | Be Precise with Conditions: Use clear logic that matches real data values. | Use Standardized Data: Normalize data (e.g., country codes, phone formats) before applying Rules. | Keep Rules Modular: Use smaller, focused Rules instead of one overly complex Rule. | Document Each Rule: Add descriptions so others understand what it is intended to do. | Test in Staging First: Validate new Rules on non-production data before applying broadly. | Monitor Execution: Regularly review Rule logs and dashboards for anomalies. | . ",
    "url": "/kb/rules-not-working#best-practices-to-avoid-rule-misbehavior",
    
    "relUrl": "/kb/rules-not-working#best-practices-to-avoid-rule-misbehavior"
  },"1908": {
    "doc": "Rules are not working",
    "title": "Summary",
    "content": "When Rules don’t work as expected in CluedIn, it is usually due to issues with scope, conditions, actions, or execution order. By systematically troubleshooting these areas and following best practices, you can ensure your Rules deliver consistent, predictable results. If you have confirmed your configuration is correct but Rules still fail to execute properly, contact CluedIn Support with execution logs and example records for further investigation. ",
    "url": "/kb/rules-not-working#summary",
    
    "relUrl": "/kb/rules-not-working#summary"
  },"1909": {
    "doc": "Rules are not working",
    "title": "Rules are not working",
    "content": " ",
    "url": "/kb/rules-not-working",
    
    "relUrl": "/kb/rules-not-working"
  },"1910": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "Troubleshooting: The Product Toolkit Is Failing to Export or Import",
    "content": "The CluedIn Product Toolkit allows you to export and import configurations such as entities, schemas, and rules between environments. This is essential for promoting changes from development to staging and production. Sometimes, you may encounter failures when running exports or imports. This article explains the most common reasons for failures, how to troubleshoot them, and best practices to ensure smooth operation. ",
    "url": "/kb/product-toolkit-issues#troubleshooting-the-product-toolkit-is-failing-to-export-or-import",
    
    "relUrl": "/kb/product-toolkit-issues#troubleshooting-the-product-toolkit-is-failing-to-export-or-import"
  },"1911": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "Common Reasons for Failures",
    "content": "1. Timeouts During Execution . | Symptom: Export or import jobs stall or fail with timeout errors. | Cause: Large datasets or complex configurations can exceed default timeout limits, especially if run on underpowered machines. | Fix: . | Run the Product Toolkit on a dedicated server or cloud VM, not on a personal laptop. | Ensure stable network connectivity with sufficient bandwidth. | For very large jobs, break down exports/imports into smaller, more manageable chunks. | . | . 2. Insufficient Permissions (RACI Access) . | Symptom: Export or import completes partially, or fails with “permission denied” errors. | Cause: The account running the Toolkit does not have Accountable (A) level RACI access to the entities or configurations being moved. | Fix: . | Verify that your account has Accountable permissions for all objects in the scope of the export or import. | If not, request elevated access from your CluedIn administrator. | . | . 3. Missing or Incomplete Dependencies . | Symptom: Import fails due to missing references (e.g., schemas, rules, or entities not found). | Cause: Dependencies were not included in the export package, or were imported out of order. | Fix: . | Ensure you export all dependent configurations together. | Import dependencies (schemas, vocabularies) before higher-level artifacts (rules, pipelines). | . | . 4. Version Mismatches . | Symptom: Import fails due to incompatible format or unexpected fields. | Cause: Export was created in a different CluedIn version than the target environment. | Fix: . | Confirm both environments are running the same version of CluedIn. | If not, upgrade/downgrade to align versions before attempting import. | . | . 5. Environment Resource Constraints . | Symptom: Export/import hangs, crashes, or causes instability. | Cause: The machine running the Toolkit does not have enough memory, CPU, or disk space. | Fix: . | Run the Toolkit on a machine with at least 4 vCPUs and 8GB+ RAM. | Monitor system resources during execution. | Clean up temporary files and ensure adequate disk space is available. | . | . ",
    "url": "/kb/product-toolkit-issues#common-reasons-for-failures",
    
    "relUrl": "/kb/product-toolkit-issues#common-reasons-for-failures"
  },"1912": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "How to Troubleshoot",
    "content": ". | Check Logs . | Review Toolkit log output for specific error messages. | Logs are typically stored in the working directory where you ran the command. | . | Validate Permissions . | Confirm the user has Accountable RACI access for the entire scope of the operation. | . | Test with Smaller Scope . | Try exporting or importing a smaller subset of objects. | If that works, expand gradually until you identify the problematic artifact. | . | Verify Environment Setup . | Ensure the Toolkit is running on a dedicated machine with sufficient resources. | Avoid running from personal laptops or unstable network connections. | . | Check CluedIn Version Compatibility . | Confirm both source and target environments are on the same version. | Align versions if mismatched. | . | Re-run with Debug Mode . | Use debug flags (if available) to capture more detailed logs. | Share logs with CluedIn Support if the issue persists. | . | . ",
    "url": "/kb/product-toolkit-issues#how-to-troubleshoot",
    
    "relUrl": "/kb/product-toolkit-issues#how-to-troubleshoot"
  },"1913": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "Best Practices for Reliable Toolkit Operations",
    "content": ". | Run on a Dedicated Machine: Use a dedicated server or VM for large exports/imports. | Use Accountable RACI Access: Always ensure you have Accountable access before starting. | Export/Import in Logical Order: Move foundational artifacts (schemas, vocabularies) before dependent ones (rules, pipelines). | Monitor Logs and Metrics: Track execution logs and system resource usage. | Align Environments: Keep source and target environments on the same CluedIn version. | Test Before Production: Run exports/imports in staging first before applying to production. | . ",
    "url": "/kb/product-toolkit-issues#best-practices-for-reliable-toolkit-operations",
    
    "relUrl": "/kb/product-toolkit-issues#best-practices-for-reliable-toolkit-operations"
  },"1914": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "Summary",
    "content": "Product Toolkit export/import issues usually arise from timeouts, missing permissions, incomplete dependencies, version mismatches, or resource limitations. By running the Toolkit on a dedicated machine, ensuring Accountable RACI access, and following best practices, you can significantly reduce the risk of failures. If problems persist, collect execution logs and contact CluedIn Support for assistance. ",
    "url": "/kb/product-toolkit-issues#summary",
    
    "relUrl": "/kb/product-toolkit-issues#summary"
  },"1915": {
    "doc": "The Product Toolkit is failing to export or import",
    "title": "The Product Toolkit is failing to export or import",
    "content": " ",
    "url": "/kb/product-toolkit-issues",
    
    "relUrl": "/kb/product-toolkit-issues"
  },"1916": {
    "doc": "Slow Performance",
    "title": "CluedIn Is Processing Slowly and Feels Slow in General",
    "content": "CluedIn is designed to process large volumes of data at scale, but in some cases you may notice that the platform feels “slow” — whether that’s during ingestion, rule execution, data linking, or even while navigating the user interface. This article explains the typical reasons why CluedIn might perform slowly, how to troubleshoot performance bottlenecks, and best practices to keep your environment running efficiently. ",
    "url": "/kb/slow#cluedin-is-processing-slowly-and-feels-slow-in-general",
    
    "relUrl": "/kb/slow#cluedin-is-processing-slowly-and-feels-slow-in-general"
  },"1917": {
    "doc": "Slow Performance",
    "title": "What Does “Slow” Mean?",
    "content": "Performance issues in CluedIn can show up in different ways: . | Ingestion Lag: New data takes longer than expected to appear in the platform. | Rule Processing Delays: Rules take longer to apply or complete. | Entity Linking/Matching: Deduplication or linking jobs run slowly. | Export Latency: Exports to downstream systems take longer to complete. | UI Responsiveness: The CluedIn portal feels sluggish when navigating or opening dashboards. | . Identifying which area feels slow is the first step to diagnosing the cause. ",
    "url": "/kb/slow#what-does-slow-mean",
    
    "relUrl": "/kb/slow#what-does-slow-mean"
  },"1918": {
    "doc": "Slow Performance",
    "title": "Common Causes of Performance Issues",
    "content": "1. High Data Volumes . | Symptom: Processing slows as more records are ingested. | Explanation: Larger datasets mean more work for ingestion, matching, and rules engines. | Fix: Scale your CluedIn environment to handle growth; consider batching ingestion jobs. | . 2. Complex or Excessive Rules . | Symptom: Rules engine takes a long time to run. | Explanation: Multiple active rules or complex regex conditions can increase evaluation overhead. | Fix: Review active rules, simplify logic, and disable rules that are not essential. | . 3. Resource Constraints . | Symptom: The whole platform (processing + UI) feels sluggish. | Explanation: The CluedIn instance does not have enough CPU, memory, or storage IOPS to handle workload. | Fix: . | Run CluedIn on dedicated infrastructure, not a shared environment. | Scale up VM/container resources. | Monitor system utilization to detect bottlenecks. | . | . 4. Network or Connectivity Issues . | Symptom: Ingestion from external sources and exports to downstream systems are slow. | Explanation: Latency or instability in network connections to source/target systems slows down jobs. | Fix: Ensure stable, high-bandwidth connections between CluedIn and data sources/destinations. | . 5. Inefficient Matching Configurations . | Symptom: Linking and deduplication jobs are disproportionately slow. | Explanation: Overly broad or poorly tuned match rules increase the number of comparisons per entity. | Fix: Optimize matching rules to focus on the most reliable identifiers. | . 6. Running Heavy Jobs on Underpowered Machines . | Symptom: Toolkit exports/imports or processing jobs time out. | Explanation: Running CluedIn utilities (e.g., Product Toolkit) on personal laptops or small VMs can cause slowness. | Fix: Use dedicated servers or cloud VMs with sufficient resources for large-scale jobs. | . 7. Competing Workloads . | Symptom: Performance degrades when multiple ingestion, rules, and export jobs run in parallel. | Explanation: Shared resources are consumed by concurrent jobs. | Fix: Schedule heavy workloads during off-peak hours or stagger job execution. | . ",
    "url": "/kb/slow#common-causes-of-performance-issues",
    
    "relUrl": "/kb/slow#common-causes-of-performance-issues"
  },"1919": {
    "doc": "Slow Performance",
    "title": "Troubleshooting Checklist",
    "content": ". | Identify Where It’s Slow . | Is it ingestion, rule execution, linking, exporting, or UI? | . | Check Logs &amp; Dashboards . | Review processing logs for errors or bottlenecks. | Check dashboards for job status and queue lengths. | . | Monitor System Resources . | CPU, memory, and disk usage should be reviewed on the CluedIn host. | . | Audit Rules &amp; Match Configurations . | Disable unnecessary rules. | Simplify complex logic. | Refine match rules. | . | Test Environment Sizing . | Compare resource allocation (CPU/RAM/storage) to recommended CluedIn sizing guidelines. | Scale up if needed. | . | Check Network Performance . | Ensure data source/destination connectivity is reliable and performant. | . | Try Smaller Batches . | Re-run ingestion or exports on smaller subsets to isolate scaling issues. | . | . ",
    "url": "/kb/slow#troubleshooting-checklist",
    
    "relUrl": "/kb/slow#troubleshooting-checklist"
  },"1920": {
    "doc": "Slow Performance",
    "title": "Best Practices to Maintain Good Performance",
    "content": ". | Run CluedIn on Dedicated Infrastructure: Avoid running on shared or undersized machines. | Scale with Data Growth: Allocate more resources as your dataset grows. | Schedule Heavy Jobs Strategically: Run big exports/imports or reprocessing during quiet hours. | Regularly Audit Rules: Keep only the Rules that add value; retire unused ones. | Optimize Matching Configurations: Focus on the most discriminating identifiers. | Monitor Continuously: Use system and CluedIn logs/dashboards to stay ahead of bottlenecks. | . ",
    "url": "/kb/slow#best-practices-to-maintain-good-performance",
    
    "relUrl": "/kb/slow#best-practices-to-maintain-good-performance"
  },"1921": {
    "doc": "Slow Performance",
    "title": "Summary",
    "content": "When CluedIn feels slow, it is usually due to large data volumes, complex rules, insufficient resources, or network constraints. By identifying where the slowness occurs, monitoring resources, and following optimization best practices, you can significantly improve performance. If slowness persists even after optimization, collect logs and environment metrics and contact CluedIn Support for further assistance. ",
    "url": "/kb/slow#summary",
    
    "relUrl": "/kb/slow#summary"
  },"1922": {
    "doc": "Slow Performance",
    "title": "Slow Performance",
    "content": " ",
    "url": "/kb/slow",
    
    "relUrl": "/kb/slow"
  },"1923": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Known Limitations: Objects That Cannot Currently Be Deleted in CluedIn",
    "content": "CluedIn provides extensive capabilities for creating, managing, and governing master data assets. However, there are certain object types that cannot currently be deleted once created. These are known product limitations and are on the roadmap for improvement. This article outlines what those objects are, why deletion is not yet supported, and recommended workarounds or best practices. ",
    "url": "/kb/unable-to-delete#known-limitations-objects-that-cannot-currently-be-deleted-in-cluedin",
    
    "relUrl": "/kb/unable-to-delete#known-limitations-objects-that-cannot-currently-be-deleted-in-cluedin"
  },"1924": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Objects That Cannot Be Deleted",
    "content": "1. Business Domains . | Description: Business Domains define the logical areas of your data model (e.g., Customer, Product, Supplier). | Issue: Once created, Business Domains cannot be deleted. | Impact: Unused domains remain visible in the portal. | Workaround: Mark unused domains as Inactive or rename them with a prefix (e.g., ZZ_Obsolete) to indicate they are not in use. | . 2. Vocabularies . | Description: Vocabularies are controlled lists of business terms and definitions used across CluedIn. | Issue: Vocabularies cannot be deleted. | Impact: Test or unused vocabularies can clutter the governance layer. | Workaround: Mark them as Deprecated or rename with an ARCHIVE_ prefix. | . 3. Vocabulary Keys . | Description: Keys represent individual entries within a Vocabulary. | Issue: Vocabulary Keys cannot be removed once added. | Impact: Erroneous or outdated keys persist in dropdowns and rules. | Workaround: Use tagging or mark keys as Inactive/Obsolete and provide clear descriptions that they should not be used. | . 4. Roles . | Description: Roles control user permissions and responsibilities (e.g., Steward, Analyst, Administrator). | Issue: Roles cannot be deleted once created. | Impact: Experimental or obsolete roles remain in the UI. | Workaround: Remove all user assignments from unused roles and rename them with an archival prefix. | . 5. Golden Record Layouts . | Description: Layouts define how Golden Record data is displayed in the portal. | Issue: Layouts cannot be deleted once created. | Impact: Testing and experimentation can create clutter. | Workaround: Rename layouts with an ARCHIVE_ prefix or move them into a separate grouping for clarity. | . 6. Deduplication Projects . | Description: Deduplication Projects configure and test matching rules for resolving duplicate records. | Issue: Deduplication Projects cannot be deleted. | Impact: Old experiments or tests stay visible in the list. | Workaround: Use descriptive naming (e.g., Test_Project_DO_NOT_USE) or archive notes to separate them from active projects. | . 7. Cleaning Projects . | Description: Cleaning Projects are used to standardize and fix bulk data issues. | Issue: Cleaning Projects cannot be deleted. | Impact: Completed or test cleaning jobs remain in the history. | Workaround: Clearly label completed or obsolete projects; they remain valuable for audit and rollback purposes. | . ",
    "url": "/kb/unable-to-delete#objects-that-cannot-be-deleted",
    
    "relUrl": "/kb/unable-to-delete#objects-that-cannot-be-deleted"
  },"1925": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Why Deletion Is Restricted",
    "content": "These limitations are primarily due to auditability, lineage, and rollback guarantees: . | CluedIn ensures a full historical view of governance objects and transformations. | Removing these objects would create gaps in audit trails and lineage history. | Certain items (like Cleaning Projects) support undo/rollback, which depends on retaining the original record. | . ",
    "url": "/kb/unable-to-delete#why-deletion-is-restricted",
    
    "relUrl": "/kb/unable-to-delete#why-deletion-is-restricted"
  },"1926": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Best Practices",
    "content": ". | Adopt a naming convention: Prefix unused or test objects with ZZ_, ARCHIVE_, or DO_NOT_USE. | Document clearly: Add notes in descriptions indicating the status of the object. | Limit testing clutter: Use a non-production environment (sandbox) for experimenting with Business Domains, Deduplication, or Cleaning Projects. | Govern creation: Restrict who can create these objects to reduce the number of unused items. | . ",
    "url": "/kb/unable-to-delete#best-practices",
    
    "relUrl": "/kb/unable-to-delete#best-practices"
  },"1927": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Roadmap Note",
    "content": "Deletion support for these object types is a known product limitation. The CluedIn team is tracking this and working toward improvements to better support archival and lifecycle management of governance objects. ",
    "url": "/kb/unable-to-delete#roadmap-note",
    
    "relUrl": "/kb/unable-to-delete#roadmap-note"
  },"1928": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Summary",
    "content": "Currently, the following objects cannot be deleted in CluedIn: . | Business Domains | Vocabularies | Vocabulary Keys | Roles | Golden Record Layouts | Deduplication Projects | Cleaning Projects | . Until deletion support is available, use naming conventions, documentation, and governance controls to manage unused items effectively. ",
    "url": "/kb/unable-to-delete#summary",
    
    "relUrl": "/kb/unable-to-delete#summary"
  },"1929": {
    "doc": "Why can I not delete certain things in CluedIn",
    "title": "Why can I not delete certain things in CluedIn",
    "content": " ",
    "url": "/kb/unable-to-delete",
    
    "relUrl": "/kb/unable-to-delete"
  },"1930": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "How to Get Better Matches in Deduplication Projects",
    "content": "Deduplication Projects in CluedIn help identify and merge duplicate records across your data sources. Good results depend not only on the rules you configure, but also on the quality and consistency of the data being matched. This article provides practical advice for improving match quality. ",
    "url": "/kb/better-matches#how-to-get-better-matches-in-deduplication-projects",
    
    "relUrl": "/kb/better-matches#how-to-get-better-matches-in-deduplication-projects"
  },"1931": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "Key Principles",
    "content": "1. Prioritize Deterministic Matching Rules . It’s tempting to design complex rules to cover every possible variation in your data. However, this often increases false positives and makes matching less predictable. | Instead of chasing perfection, create rules that are as deterministic and strict as possible (e.g., “Exact match on Tax ID” or “Exact match on Email Address”). | Use high-confidence identifiers first: IDs, email, phone numbers, domains. | Apply fuzzy matching sparingly, only where no strong identifier exists. | . 2. Use CluedIn Clean to Improve Input Data . Matching accuracy is only as good as the quality of your input data. Use CluedIn Clean to standardize and normalize data before running matching: . | Clean addresses into a structured, comparable format. | Convert phone numbers to E.164 international format. | Normalize company names (remove “Ltd”, “Inc”, punctuation). | Standardize casing, diacritics, and whitespace. | . By cleaning first, you make it easier for deterministic rules to succeed, rather than relying on fuzzy matching to compensate for poor data quality. 3. Apply Normalisers to Ignore Irrelevant Differences . Normalisers allow you to exclude certain aspects of data from the matching process so that records aren’t incorrectly treated as different: . | Case Normalisation: Treat CLUEdin, CluedIn, and cluedin as identical. | Whitespace Normalisation: Ignore leading/trailing or multiple spaces. | Punctuation Normalisation: Strip characters like commas, periods, or dashes where not significant. | Custom Normalisers: Define rules to remove common “noise” (e.g., Ltd, Inc, GmbH). | . ",
    "url": "/kb/better-matches#key-principles",
    
    "relUrl": "/kb/better-matches#key-principles"
  },"1932": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "Recommended Approach",
    "content": ". | Start simple and strict . | Build rules around unique, high-confidence identifiers. | Ensure those identifiers are cleaned and normalised first. | . | Add cleaning steps in CluedIn Clean . | Focus on the data elements used in matching. | Validate that cleaned values align with your rules. | . | Use fuzzy matching only as a fallback . | Apply it after deterministic rules to catch edge cases. | Always review candidates in stewardship before merging. | . | . ",
    "url": "/kb/better-matches#recommended-approach",
    
    "relUrl": "/kb/better-matches#recommended-approach"
  },"1933": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "Example",
    "content": ". | Before Cleaning . | Acme Inc. vs. ACME, Incorporated | Phone: 0044 7700 900123 vs. +44 7700 900123 | . | After CluedIn Clean &amp; Normalisation . | Acme vs. Acme | Phone: +447700900123 vs. +447700900123 | . | . Result: Deterministic matching rules (Exact Name + Exact Phone) succeed with confidence, no need for fuzzy fallback. ",
    "url": "/kb/better-matches#example",
    
    "relUrl": "/kb/better-matches#example"
  },"1934": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "Summary",
    "content": "To get the best matches in Deduplication Projects: . | Keep rules deterministic and simple. | Clean data with CluedIn Clean so it conforms to your matching rules. | Use normalisers to remove irrelevant differences like case, whitespace, and punctuation. | Reserve fuzzy matching for edge cases where no strong identifiers exist. | . This approach ensures higher accuracy, fewer false positives, and faster stewardship. ",
    "url": "/kb/better-matches#summary",
    
    "relUrl": "/kb/better-matches#summary"
  },"1935": {
    "doc": "How can I get better matches of duplicate records in CluedIn",
    "title": "How can I get better matches of duplicate records in CluedIn",
    "content": " ",
    "url": "/kb/better-matches",
    
    "relUrl": "/kb/better-matches"
  },"1936": {
    "doc": "Supported and unsupported characters",
    "title": "On this page",
    "content": ". | Supported characters | Unsupported characters | . In this article, you’ll find reference information about supported and unsupported characters in clues and entities. If your data contains unsupported characters, remove them before submitting data to CluedIn. ",
    "url": "/kb/supported-characters#on-this-page",
    
    "relUrl": "/kb/supported-characters#on-this-page"
  },"1937": {
    "doc": "Supported and unsupported characters",
    "title": "Supported characters",
    "content": "All valid Unicode characters are supported. Exceptions include Unicode surrogate blocks, 0xFFFE, and 0xFFFF. Characters: 0x9, 0xA, 0xD, Character Range: 0x20-0xD7FF Character Range: 0xE000-0xFFFD Character Range: 0x10000-0x10FFFF . Unicode control characters or undefined Unicode characters should not be used but are supported. These are within the following character ranges. 0x7F-0x84 0x86-0x9F 0xFDD0-0xFDEF 0x1FFFE-0x1FFFF 0x2FFFE-0x2FFFF 0x3FFFE-0x3FFFF 0x4FFFE-0x4FFFF 0x5FFFE-0x5FFFF 0x6FFFE-0x6FFFF 0x7FFFE-0x7FFFF 0x8FFFE-0x8FFFF 0x9FFFE-0x9FFFF 0xAFFFE-0xAFFFF 0xBFFFE-0xBFFFF 0xCFFFE-0xCFFFF 0xDFFFE-0xDFFFF 0xEFFFE-0xEFFFF 0xFFFFE-0xFFFFF 0x10FFFE-0x10FFFF . ",
    "url": "/kb/supported-characters#supported-characters",
    
    "relUrl": "/kb/supported-characters#supported-characters"
  },"1938": {
    "doc": "Supported and unsupported characters",
    "title": "Unsupported characters",
    "content": "ASCI control characters are not supported. Exceptions include 0x9, 0xA, and 0xD, which are valid characters. | Character range 0x-0x1F is not supported. | Character 0x7F is technically supported but should not be used. | Unicode surrogate blocks are not supported. | 0xFFFE and 0xFFFF are not supported. | . ",
    "url": "/kb/supported-characters#unsupported-characters",
    
    "relUrl": "/kb/supported-characters#unsupported-characters"
  },"1939": {
    "doc": "Supported and unsupported characters",
    "title": "Supported and unsupported characters",
    "content": " ",
    "url": "/kb/supported-characters",
    
    "relUrl": "/kb/supported-characters"
  },"1940": {
    "doc": "Microsoft Defender for Cloud recommendations",
    "title": "On this page",
    "content": ". | Recommendations . | Container images should be deployed from trusted registries only | . | . This document covers Microsoft Defender for Cloud advisories that you may face when installing the CluedIn MDM PaaS variant of the product into your environemnt. It will explain what each advisory means and how to potentially resolve the issue. If the advisory is not listed below, please do reach out to support@cluedin.com who will be able to advise further. ",
    "url": "/kb/defender-for-cloud-recommendations#on-this-page",
    
    "relUrl": "/kb/defender-for-cloud-recommendations#on-this-page"
  },"1941": {
    "doc": "Microsoft Defender for Cloud recommendations",
    "title": "Recommendations",
    "content": "Container images should be deployed from trusted registries only . The default security policies that are set on Microsoft Defender for Cloud can be quite bare when it comes out of the box. As a result, this particular recommendation gets flagged frequently as a high risk. The CluedIn installation installs an Azure Kubernetes Service (AKS) cluster which pulls images from cluedinprod.azurecr.io with an authorization token. Because cluedinprod.azurecr.io is external to your environment, it gets flagged by this recommendation when using default baseline advisories. Solution . To resolve this issue, please amend the recommendation under remediation steps and add a regex string that can accomodate the CluedIn registry. An example is: ^cluedinprod\\.azurecr\\.io.*$. This will affect all AKS clusters. If you have additional AKS clusters which are for internal services or other products, you may want to use a more loosely defined regex string that can accommodate both CluedIn and other third-party registries. ",
    "url": "/kb/defender-for-cloud-recommendations#recommendations",
    
    "relUrl": "/kb/defender-for-cloud-recommendations#recommendations"
  },"1942": {
    "doc": "Microsoft Defender for Cloud recommendations",
    "title": "Microsoft Defender for Cloud recommendations",
    "content": " ",
    "url": "/kb/defender-for-cloud-recommendations",
    
    "relUrl": "/kb/defender-for-cloud-recommendations"
  },"1943": {
    "doc": "CluedIn Security - [NOT impacted] Ingress NGINX Controller vulnerabilities",
    "title": "CluedIn Security - [NOT impacted] Ingress NGINX Controller vulnerabilities",
    "content": "Wiz Research recently disclosed a series of critical unauthenticated Remote Code Execution (RCE) vulnerabilities affecting the Ingress NGINX Controller in Kubernetes (CVE-2025-1097, CVE-2025-1098, CVE-2025-24514, and CVE-2025-1974), collectively known as IngressNightmare. These vulnerabilities could allow attackers to execute arbitrary code and gain unauthorized access to Kubernetes secrets across namespaces. More information can be found in Infosecurity Magazine and The Hacker News. CluedIn is not impacted by these vulnerabilities, as we do not use NGINX as our ingress controller. Instead, CluedIn relies on HAProxy, which is not affected by these issues. If you have deployed CluedIn using our standard setup, you are not impacted. However, customers who have modified their ingress setup outside of our recommended configuration should verify their infrastructure and apply any necessary security updates. Our security team continuously monitors emerging threats and ensures our platform remains secure. If you have any questions, please contact our support team at support@cluedin.com. ",
    "url": "/kb/ingress-nginx-controller",
    
    "relUrl": "/kb/ingress-nginx-controller"
  }
}
