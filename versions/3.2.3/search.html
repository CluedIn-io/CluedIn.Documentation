<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Search | CluedIn Documentation</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Product documentation template for Jekyll." />
<meta property="og:description" content="Product documentation template for Jekyll." />
<link rel="canonical" href="https://documentation.cluedin.net//http://documentation.cluedin.net/beta-docs/versions/3.2.3/search.html" />
<meta property="og:url" content="https://documentation.cluedin.net//http://documentation.cluedin.net/beta-docs/versions/3.2.3/search.html" />
<meta property="og:site_name" content="CluedIn Documentation" />
<script type="application/ld+json">
{"url":"https://documentation.cluedin.net//http://documentation.cluedin.net/beta-docs/versions/3.2.3/search.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://documentation.cluedin.net//http://documentation.cluedin.net/beta-docs/versions/3.2.3/siteicon.png"}},"description":"Product documentation template for Jekyll.","headline":"Search","@type":"WebPage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="https://documentation.cluedin.net//http://documentation.cluedin.net/beta-docs/versions/3.2.3/feed.xml" title="CluedIn Documentation" />
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400,400italic,700,700italic|Open+Sans:400,400italic,600,600italic,700,700italic|Inconsolata:400,700">
		<link rel="stylesheet" href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/css/main.css">
		<link rel="apple-touch-icon" href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/images/favicon.png">
		<script src="https://kit.fontawesome.com/53f65601e3.js" crossorigin="anonymous"></script>

		
    <link rel="stylesheet" href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/css/prism.css">
    <!-- Start of HubSpot Embed Code -->
  <script type="text/javascript" id="hs-script-loader" async defer src="//js.hs-scripts.com/2770606.js"></script>
<!-- End of HubSpot Embed Code -->
	</head>

	<body>
		<header>
			<h1>
				<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/"><img src="http://documentation.cluedin.net/beta-docs/versions/3.2.3/images/cluedin-logo-c.svg" width="40" height="40" alt="CluedIn Documentation logo"></a>
				CluedIn Documentation
				<button type="button" class="open-nav" id="open-nav"></button>
			</h1>

			<form action="http://documentation.cluedin.net/beta-docs/versions/3.2.3/search.html" method="get">
				<input type="text" name="q" id="search-input" placeholder="Type to search" autofocus>
				<input type="submit" value="Search" style="display: none;">
			</form>

			<nav >
				<ul>
					<li class="nav-item top-level ">
						
						<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/">What is CluedIn</a>
					</li>
				</ul>

				<ul>
					
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/ssl.html">Get Started</a>
							<ul>
								
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/1-default.html">CluedIn Overview</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/30-docker-local.html">CluedIn with Docker</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/40-kubernetes.html">CluedIn with Kubernetes</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/81-home-dashboard.html">Home Dashboard</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/90-frequently-asked-questions.html">Frequently asked questions</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/Running%20CluedIn%20Efficiently.html">Running CluedIn Efficiently for Cost</a></li>
									
								
									
								
									
								
									
								
									
								
									
								
									
								
									
								
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/00-gettingStarted/0-Welcome-to-CluedIn.html">Welcome to CluedIn</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/50-Consume/Generating%20an%20API%20Token.html">Administration</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/50-Consume/Generating%20an%20API%20Token.html">Generating an API Token</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/20-Users/Adding%20Team%20Members.html">Users</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/30-Authentication/index.html">Authentication</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/40-Theme/index.html">Theme</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/60-Accounts_Multitenancy/0-index.html">Multitenancy</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/70-Removing_data/index.html">Removing data</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/80-Upgrading%20CluedIn/index.html">Changing versions</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/90-Backup_Restore/index.html">Backup and Restore</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/05-Administration/10-Profile/index.html">Profile</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/modify-integration.html">Integration</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/modify-integration.html">Modify an Integration</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/Crawling.html">Crawling</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/Help%20with%20building%20robust%20integrations.html">Help with building robust integrations</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/Home%20Screen.html">Home Screen</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/Push%20data%20to%20CluedIn.html">Push Data to CluedIn</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/add-integration.html">How to add an Integration</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/build-enricher.html">Build Enricher</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/build-integration.html">Build Integration</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/delta-crawls.html">Delta Crawls</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/install-integration.html">Install Integration</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/integration-compatible-with-UI.html">Integration and the CluedIn UI</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/10-Integration/0-index.html">What are integrations</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/entity-setup.html">Preparation</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/entity-setup.html">Entity Type and UI</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/CluedIn-Connect/CluedIn%20Connect.html">CluedIn Connect</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Data-Metrics/10-Data-Quality-Metrics.html">Data Quality Metrics</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Data-Metrics/20-Most-Used-Data-Quality-Metrics.html">Most Used Data Quality Metrics</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Data-Metrics/30-How-To-Add-a-Data-Quality-Metric.html">How to add a new Data Quality Metric</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Data-Metrics/40-Data-Quality-Metrics-Explanations.html">Data Quality Metric Explanations</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Engine-Room/Engine%20Room.html">Engine Room</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Entity-Types/Entity%20Type%20Translation.html">Entity Type Translation</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Shadow-Entities/Shadow%20Entities.html">Shadow Entities</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/20-Preparation/Aggregate/Aggregation.html">Aggregation</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/10-Data_Training/CluedIn%20Train.html">Manipulation</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/10-Data_Training/CluedIn%20Train.html">CluedIn Train</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/20-Data_Cleaning/CluedIn%20Clean.html">CluedIn Clean</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/30-Merging_Records/Fuzzy%20Merging%20Explained.html">Fuzzy Merging Explained</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/30-Merging_Records/Merging%20Records.html">Merging Records</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/40-Mesh_Center/Mesh%20Center.html">Mesh Center</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/50-Splitting_Records/Splitting%20Records.html">Splitting Records</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/30-Manipulation/index.html">Transforming Data</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Subject-Access-Requests/Subject%20Access%20Requests.html">Governance</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Subject-Access-Requests/Subject%20Access%20Requests.html">Subject Access Requests</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Consent-Management/Consent%20Management.html">Consent Management</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Breaches/Data%20Breaches.html">Data Breaches</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Catalog/Data%20Catalog.html">Data Catalog</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Glossary/Data%20Glossary.html">Data Glossary</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Policies/Data%20Policies.html">Data Policies</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Privacy/Data%20Privacy.html">Data Privacy</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Data-Retention/Data%20Retention.html">Data Retention</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Governance.html">Data Governance</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Mesh-API/Mesh%20API.html">Mesh API</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/40-Governance/Access-Control/Access%20Control.html">Access Control</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/60-Management/Duplicate%20List.html">Management</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/60-Management/Duplicate%20List.html">Duplicate List</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/60-Management/Rules/Rules.html">Rules</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/DeveloperPortal.html">Developer</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/DeveloperPortal.html">Developer Portal</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/10-Administration_Operations/Administration%20Operations.html">Administration Operations</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/100-Logging/Building%20a%20new%20Logging%20Provider.html">Building a new Logging Provider</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Change%20Verbs.html">Change Verbs</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Clue.html">Clue</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Core%20Vocabularies.html">Core Vocabularies</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Data%20Part.html">Data Part</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Dynamic%20Vocabularies.html">Dynamic Vocabularies</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Edge%20Types.html">Edge Types</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Edges.html">Edges</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity%20Codes.html">Entity Codes</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity%20Type.html">Entity Type</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Infrastructure%20User%20Entity%20Type.html">Infrastructure User Entity Type</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Organization%20Entity%20Type.html">Organization Entity Type</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Person%20Entity%20Type.html">Person Entity Type</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Entity.html">Entity</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Origin.html">Origin</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Vocabularies.html">Vocabularies</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Vocabularies/Organization%20Vocaulary.html">Organization Vocaulary</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Vocabularies/Person%20Vocabulary.html">Person Vocabulary</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Vocabulary%20Groupings.html">Vocabulary Groupings</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/110-Object_Model/Vocabulary%20Values.html">Vocabulary Values</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/120-Other/Common%20Design%20Decisions.html">Common Design Decisions</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/120-Other/Common%20Problems.html">Common Problems</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/130-Parent_Aggregation/Parent%20Aggregation.html">Parent Aggregation</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/140-Preview_Image/Setting%20a%20Preview%20Image%20for%20a%20Clue.html">Setting a Preview Image for a Clue</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Adding%20a%20new%20Processing%20Pipeline.html">Adding a new Processing Pipeline</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/CluedIn%20Performance%20Metrics.html">CluedIn Performance Metrics</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Extending%20an%20Existing%20Pipeline%20Process.html">Extending an Existing Pipeline Process</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Ignoring%20Insignificant%20Changes.html">Ignoring Insignificant Changes</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Jobs/Agent%20Jobs.html">Agent Jobs</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Pipeline%20Overview.html">Processing Pipeline Overview</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20a%20Preprocessor.html">Adding a new PreProcessor</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20a%20new%20IProcessingFilter.html">Adding a new IProcessingFilter</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IDataPartProcessorExecutor.html">Adding an IDataPartProcessor</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IEntityTypeProcessor.html">Adding an IEntityTypeProcessor</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IMergePreProcessor.html">Adding an IMergePreProcessor</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Turning%20Clues%20into%20ProcessedEntityMetadata.html">Turning Clues into ProcessedEntityMetadata</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/150-Processing/Workflows/Workflow%20Event%20Handlers.html">Workflow Event Handlers</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/170-Thumbnail_Extractors/Thumbnail%20Extractors.html">Thumbnail Extractors</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/20-Configuration/Changing%20Configuration%20Settings.html">Changing Configuration Settings</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/30-Content_Extraction/Content%20Extractors.html">Content Extractors</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/30-Content_Extraction/Downloading%20a%20File%20and%20Indexing%20the%20Content.html">Downloading a File and Indexing the Content</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/33-Commands/Commands.html">Commands</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/40-Datastores/Adding%20a%20new%20Datastore%20Provider.html">Adding a new Datastore Provider</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/40-Datastores/Datastores.html">Datastores</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/40-Datastores/Switching%20a%20Datastore%20Provider.html">Switching a Datastore Provider</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/50-Events/Events.html">Events</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/60-External_Search/Deploying%20a%20New%20External%20Search%20Provider.html">Deploying a New External Search Provider</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/60-External_Search/External%20Search%20Providers.html">External Search Providers</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/70-Integration_Pattern/Eventual%20Connectivity.html">Eventual Connectivity</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/80-Integrity_Checks/Validate%20Server%20Heatlh.html">Validate Server Heatlh</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/90-Jobs/One%20Time%20Jobs.html">One Time Jobs</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/90-Jobs/Recurring%20Jobs.html">Recurring Jobs</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/90-Jobs/Scheduled%20Jobs.html">Scheduled Jobs</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/60-Management/Modelling/Modelling.html">Modelling</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/User%20Interface.html">User Interface</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/User%20Interface.html">User Interface</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/180-User_Interface/Customisations/Searching%20for%20data.html">Searching for data</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/180-User_Interface/Customisations/Suggested%20Searches.html">Suggested Searches</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/190-Webhooks/Webhooks.html">Webhooks</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Entity-Page/All%20Properties.html">All Properties</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Entity-Page/Entity%20Page.html">Entity Page</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Entity-Page/History.html">History</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Entity-Page/Pending%20Changes.html">Pending Changes</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Followed-Entities/Followed%20Entities.html">Followed Entities</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/U-User-Interface/Keep-in-the-loop/Keep%20in%20the%20loop.html">Keep in the loop</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/180-User_Interface/Customisations/Adding%20new%20Search%20Filters.html">Adding new Search Filters</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/55-Rules/Developing%20with%20Rules.html">Rules</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/91-Developer-Portal/55-Rules/Developing%20with%20Rules.html">Developing with Rules</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/98-upgrade-path/upgrade-23-to-24.html">Upgrade</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/98-upgrade-path/upgrade-23-to-24.html">From V2.3.x to V.2.4.x</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/99-releaseNotes/0-index.html">Release notes</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/99-releaseNotes/0-index.html">Handy Links</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/Crawler/Agents/Using%20Agents.html">Crawler</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/Crawler/Agents/Using%20Agents.html">Using Agents</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/Crawler/Crawler%20Validation%20Framework.html">Crawler Validation Framework</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/Crawler/Deploying%20a%20New%20Crawler.html">Deploying a New Crawler</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/Crawler/Mapping%20Encryption%20Levels%20and%20Integration%20Location.html">Mapping Encryption Levels and Integration Locations</a></li>
									
								
							</ul>
						</li>
					
						<li class="nav-item top-level ">
							
							<a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Search/Running%20Searches.html">Consume</a>
							<ul>
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Search/Running%20Searches.html">Running Searches</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Data-Marts/Data%20Marts.html">Data Marts</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Export-Targets/Export%20Targets.html">Export Targets</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Graphql/Add%20GraphQL%20Entity%20Type%20Resolvers.html">Add GraphQL Entity Type Resolvers</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Graphql/GraphQL%20Actions.html">GraphQL Actions</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Graphql/GraphQL.html">GraphQL</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Outgoing-Streams/Outgoing%20Streams.html">Streams</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Rest-API/Rest%20API.html">Rest API</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Search/Configuration%20Search%20Relevance.html">Configuration Search Relevance</a></li>
									
								
									
										<li class="nav-item "><a href="http://documentation.cluedin.net/beta-docs/versions/3.2.3/docs/H-Consume/Consume.html">Consume</a></li>
									
								
							</ul>
						</li>
					
				</ul>
			</nav>
		</header>

		<section class="main">
			<div class="page-header">
				<h2>CluedIn Documentation</h2>
				<h3>Search</h3>
			</div>
			<main>
			<article class="content">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					

					"docs-00-gettingstarted-0-welcome-to-cluedin-html": {
						"id": "docs-00-gettingstarted-0-welcome-to-cluedin-html",
						"title": "Welcome to CluedIn",
						"category": "",
						"url": " /docs/00-gettingStarted/0-Welcome-to-CluedIn.html",
						"content": "Why do you need CluedIn? To understand the value of CluedIn, it is important to talk about why the platform exists in the first place. We can all agree that, nowadays, all enterprise companies have many processes and projects which are fueled by data. This data is typically not in a ready-to-use state and hence it has to flow through some processes in order to make it ready for consumption. Whether the use-case is running advanced analytics, machine learning, business intelligence, anti-money laundering, obtaining a single view of your customer or for data privacy - it turns out that all of these use-cases end up requiring very similar data preparation processes and attention. These common processes is what CluedIn encapsulates into the platform. Data management pillars Data projects all start with data discovery. There is always a need to know where your data is and in what systems it lies. After this, there is typically a stage of data integration, where you need to bring multiple data sources together into a unified or connected set of data. During this phase, you need to make sure that you are not putting too much pressure on the source systems, but at the same time would like the data available to the business as soon as possible. Choosing an integration style or pattern can be complex, but there are many choices that you have - whether that is streaming, batch, CDC or others. After this integration has been done, often the next step is to normalise, standardise, harmonise and clean the data so that it can be processed by machines in a uniform, consistent and high-fidelity manner. While this is occurring, you need to make sure you have control over the data by ensuring there is a clear owner, responsible for its maintenance. This leads into the need for full traceabilty and audit trail of where this data is coming from, what is happening to it in this process of preparation and where it is being used. With privacy being a mandated concern, you also need to make sure you have control over personal data and how it is used within the business. Finally, this data needs to be easily and readily available for the business to consume to fuel it operational needs. Whether your use case is Machine Learning or Business Intelligence, you will need to apply these same core pillars of data management detailed above. You might have extra steps in each use case, but these pillars above are at least the common pillars for any use-case involving data. CluedIn is this layer of foundation that encapsulates everything we mentioned above into a coherent, consistent end to end data fabric of source to target data processing. What you get in the end can be described well in this demonstration video. Why did we create CluedIn? The reason why CluedIn exists, is because our team saw project after project failing, because large enterprises were buying platforms that tackled each part of the journey - but failed in stitching these different pillars together into a platform. CluedIn was born with the stitching, meaning that our different pillars were designed to work well with each other - in fact, they really complement each other. Our documentation will help you understand more about the different components of CluedIn and how it all fits together. This will give you the insights you need to work with and extend the platform with your own needs. CluedIn is designed to be extended and hence we offer easy and intuitive ways to inject your own functionality or to make our data available to other services to do what they do best. At a high level, you can conceptualise CluedIn as a streaming platform. We essential handle the constant flow of data and are designed to work in environments where systems don’t stand still and data is never static. CluedIn is an application that runs in Docker containers and uses Kubernetes as the way to host and orchestrate the different pieces of the application. Because of this, CluedIn is designed to work well in elastic environments and can automatically scale to the sizes and infrastructure you need to handle your data workloads. CluedIn runs large customers with 100’s of TB’s of data, and smaller customers with just a few TB’s. Whether you have TB’s of PB’s of data to process - CluedIn can and will process your workloads. It is important to mention that CluedIn is a persistence layer. Unlike Data Virtualisation, CluedIn ingests parts of your source data into its platform. You have full control over what parts of your source data are ingested. The goal of CluedIn is to create the highest fidelity version of your data you will have in your business. This is not only the data itself, but the structure of the data as well. With CluedIn having the highest fidelity version of your data, it means that any business request for data, in any format, in any modelling can be served out of the CluedIn Data Fabric. This will allow you to project data out exactly how the target systems would like as we contain the highest fidelity version of it. We hope you enjoy and get value out of your CluedIn solution."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-1-default-html": {
						"id": "docs-00-gettingstarted-1-default-html",
						"title": "CluedIn Overview",
						"category": "",
						"url": " /docs/00-gettingStarted/1-default.html",
						"content": "Getting started The following diagram depicts the main components of the CluedIn application with a reference architecture on Microsoft Azure. You can also see a reference architecture directly on the Microsoft website. CluedIn Architecture CluedIn is made up of various functional layers, all encapsulated into their own Docker containers. CluedIn Applications We have a combination of microservices running on .Net Core that handle various distinct functions, from handling the UI, to queuing and processing the large streams of data we need to ingest. CluedIn Security This handles how we connect securely to the cluster and handle permissions and grants to the different services the system is made up of. CluedIn Data Neo4j - Manages complex relationships between CluedIn data objects (“Clues”). SqlServer - Manages our relational data storage. Alternatively a SAAS option (like SQL Azure) can be used. ElasticSearch - Indexes and searches the data for flexible querying. This can be scaled out as needed. RabbitMQ - The servicebus that handles queueing across the system. Redis - Used as the cache for the system. Notice all the communication from the browser into the application comes through a set of ingress definitions (i.e. only a single public IP is required). The communication will all run over SSL in a production environment. Installation For development and evaluation purposes using Docker For testing and production environments using Kubernetes Integration Build my first provider"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-30-docker-local-html": {
						"id": "docs-00-gettingstarted-30-docker-local-html",
						"title": "CluedIn with Docker",
						"category": "",
						"url": " /docs/00-gettingStarted/30-docker-local.html",
						"content": "Introduction This will show you how to install CluedIn on your local machine by running it inside Docker. CluedIn is an application with many services, so you will need to ensure you have adequate resources on the machine you intend to run it on. Requirements At least 16Gb of free memory dedicated to docker (It is preferable to run on a machine with 32Gb of RAM) Latest version of Docker for your operating system (Engine: &gt; 19.03) Powershell 7 for your operating system - This is to run helper scripts Access to the private repositories inside the cluedin DockerHub organization. You will require a Docker Hub account and request access from CluedIn; then use this account to do a docker login. NOTE: Only CluedIn certified developers have access to the CluedIn DockerHub. Please contact us on our website if you would like access. Running CluedIn First time preparation CluedIn provides a helper script to streamline the process of getting started. Clone the helper script from the CluedIn Home repo git clone https: github.com CluedIn-io Home Open a powershell console on Windows - ( or pwsh on Mac &amp; Linux ) and run: . cluedin.ps1 check This will check a few things: That you have the needed software installed That you have the ports needed to run CluedIn available That you have logged into docker hub If all these checks are green you are ready to proceed. If ports are in use then you may need to stop any programs locally that may be using them and re-run check again. Pull the latest cluedin images to your local machine . cluedin.ps1 pull You can use this command to refresh update any images at a later date. Starting the application To start up the application use: . cluedin.ps1 up This will start up the various containers in Docker and begin initializing CluedIn for the first time. Checking application status Depending on the speed of the machine it is being installed onto CluedIn can take a moment to start up. You can check the status of this by using: . cluedin.ps1 status CluedIn is ready when all the status checks are green. Open your browser and CluedIn will be available under http: app.127.0.0.1.nip.io:9080. Creating an organization In order to use CluedIn you need to create an organization. You can use the following command to create an account from the command line . cluedin.ps1 createorg -name foobar -pass Foobar23! Stopping the application There are two ways to stop the application: Stopping (without deletion of data) To stop CluedIn but to preserve the data you created while running, use: . cluedin.ps1 stop To start CluedIn back up again, you can simply use up or : . cluedin.ps1 start Stopping (with removal reset of data) To completely remove CluedIn and all of the associated data use: . cluedin.ps1 down This is a destructive action but it is useful for resetting data in CluedIn. Adding extra components (such as crawlers, providers, enrichers, connectors) Add CluedIn NuGet feed In order to satisify the dependances of any additional components to add to CluedIn will require adding the Public CluedIn NuGet feed: . cluedin.ps1 packages -addfeed cluedin -uri https: pkgs.dev.azure.com CluedIn-io Public _packaging Public nuget v3 index.json This results in the following folder tree being created: env default env default .env env default packages env default packages nuget.config env default packages local env default packages packages.txt NOTE: you can inspect nuget.config and packages.txt for troubleshooting purposes. Adding SqlServer Connector example Download respective .nupkg files from https: github.com CluedIn-io CluedIn.Connector.SqlServer releases and save copy into env default packages local Add the package: . cluedin.ps1 packages -add CluedIn.Connector.SqlServer Restore the package to ensure all dependencies are present: . cluedin.ps1 packages -restore Trash remove the cluedin_default_server_1 container using Docker Desktop or docker rm command, then run . cluedin.ps1 up to force this container to be recreated as shown below: . cluedin.ps1 up +-----------------------------+ | CluedIn - Environment =&gt; up | +-----------------------------+ ... Creating cluedin_default_server_1 ... done ... Sql Server Connector is now available to use as an Export Target, setup in the UI. Deep clean extra components In the event you need to remove all packages, it can be useful as part of troubleshooting to “deep clean” the components. Delete the folders: env default components env default packages Use . cluedin.ps1 down or trash remove the cluedin_default_server_1 container using Docker Desktop or docker rm command. Use . cluedin.ps1 up to setup the cluster again. NOTE: Remember to “Add CluedIn NuGet feed” after deleting the packages folder and re-add any components you require. Developing extra components Increasing the log output The environment is configured for Production level logging by default. You can increase the amount of information produced by switching to Development before using up to start CluedIn. The following will update the environment for Development level logging. It will persist for all future runs of CluedIn: . cluedin.ps1 env -set CLUEDIN_ENVIRONMENT=Development Adding extra components You can add extra providers or enrichers in two different ways: Via Nuget packages Add a a file named Packages.txt in the . components folder with the names of the nuget packages for the components you want to install. If the Nuget packages are not available publicly add a nuget.config file in the . components folder. Either pass the password token to the nuget.config or create a KEY environment variable with it. Copy the relevant DLLs for the components in the . components ServerComponent folder. You will also need to load in the deps.json files that are compiled in your C# projects. If you are wanting to debug your additions locally then you will also want to copy in the .PDB files."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-40-kubernetes-html": {
						"id": "docs-00-gettingstarted-40-kubernetes-html",
						"title": "CluedIn with Kubernetes",
						"category": "",
						"url": " /docs/00-gettingStarted/40-kubernetes.html",
						"content": "Introduction CluedIn provides a helm chart to install CluedIn quickly in a Kubernetes cluster. Helm is a package manager which simplifies the installation and management of complex applications in a Kubernetes environment. Charts are stored in a repository and they are invoked using the helm-cli tool from a terminal that has access to a Kubernetes cluster. The purpose of the chart is to install the CluedIn application, this includes the actual CluedIn server, website, and other services required (storage, queues, etc.) Pre-requisites Local install of kubectl configured to talk to the cluster Cluster’s kubeconfig can be fetched using the following commands: az login az aks get-credentials ` --name &lt;clusterName&gt; ` --resource-group &lt;clusterResourceGroup&gt; ` --subscription &lt;subscriptionId&gt; Local install of the cli for Helm Kubernetes Cluster A Kubernetes cluster and kubeconfig access to it is required. Recommended nodepool sizing for an AKS cluster can be found below: Node Pool VM SKU Type Amount Purpose Core Pool Standard_DS2_v2 1 Kubernetes agent internal processes Data Pool Standard_A8_v2 2 Memory Optimized pool for Databases Processing Pool Standard_F16s_v2 1 CPU Optimized for Processing workloads General Pool Standard_F4s_v2 2 General Purpose nodepool to house CluedIn Microservices Additionally, Memory Optimized and CPU Optimized pools can be tainted to only allow Database or Processing workloads. The same size boxes with the amount of RAM and CPU can also be used for deploying into other cloud providers such as Amazon Web Services of Google Cloud. Networking SSL DNS In addition this cluster should have: HAProxy ingress controller installed (it is possible to use a different ingress controller (like NGINX) with extra customization). You can use the following commands to install HAProxy using Helm: kubectl create namespace cluedin helm repo add haproxy-ingress https: haproxy-ingress.github.io charts helm install haproxy-ingress haproxy-ingress haproxy-ingress --namespace=cluedin DNS configuration pointing to the public IP of the ingress controller for the following routes: app.&lt;hostname&gt; (i.e. https: app.cluedin.com ) &lt;accountName&gt;.&lt;hostname&gt; (i.e. https: cluedin.cluedin.com ) External ingress controller’s IP can be found by using kubectl get services -n cluedin Secret with the SSL certificates for the following routes: app.&lt;hostname&gt; (i.e. https: app.cluedin.com ) &lt;accountName&gt;.&lt;hostname&gt; (i.e. https: cluedin.cluedin.com ) Secret can be created using the following command: kubectl create secret tls &lt;secret-name&gt;--key &lt;private-key-path&gt; --cert &lt;public-certificate-path&gt; There is an option to run without SSL, although not recommended Set the following flag in values.yml to disable HTTPS connection: tls: forceHttps: false Installing CluedIn Platform CluedIn Platform can be installed as a whole with the help of Helm. Preparation The helm chart repository containing the CluedIn chart must be registered. helm repo add cluedin https: cluedin-io.github.io Charts helm repo update Secret with the credentials for accessing the CluedIn images from Docker Hub. Secret can be created using the following command: kubectl create secret docker-registry docker-registry-key ` --namespace cluedin ` --docker-server='docker.io' ` --docker-username='&lt;your Dockerhub username&gt;' ` --docker-password='&lt;your Dockerhub password&gt;' ` --docker-email='&lt;your Dockerhub email&gt;' Fetch values.yml configuration file to configure CluedIn Installation helm inspect values cluedin cluedin &gt; values.yml Installation Fill out the values.yaml file, specifically the following objects: bootstrap: organization: name: # Organization Account Name email: # Admin account's Email username: # Admin account's username (should be the same as above) prefix: # Organization prefix used to access the platform (also use in DNS configuration step above) password: # Admin account's password emailDomain: # Admin account's Email domain *Be aware that you cannot use an organization prefix with a hyphen or period in it. tls: ingressCertSecret: # Name of the secret created in SSL certificate step This creates a values.yml which you can modify to tailor how CluedIn will be installed. Once values.yml file has been populated and settings are adjusted to your liking, you can install CluedIn Platform with the following Helm command: helm upgrade &lt;release-name (i.e. cluedin-dev, cluedin-prod)&gt; cluedin cluedin ` -n cluedin ` --install ` --values &lt;path-to-values.yml&gt; Upon running the helm upgrade command, Helm will begin installation of CluedIn platform into your Kubernetes cluster. At the end of the installation process, you will be prompted with configuration of your install, URLs you can use to access your freshly installed platform. All the workloads may take up to 10 minutes to spin up. You can check your status by running kubectl get pods -n cluedin, in a healthy installation scenario all the pods should be in a Ready state. Additionally, you can check the health of the platform by going to https: app.&lt;hostname&gt; api status healthcheck API. You will be able to login to the platform by going to https: app.&lt;hostname&gt; (or http: app.&lt;hostname&gt; if not using SSL). Next Steps After logging in to the platform, you can proceed with enabling single sign on for your users to access the platform, as well as start loading data in via Data Sources or installing some crawlers. Below you will find some useful links on achieving the above: Enabling Single Sign On Install a crawler custom component Optionally, you can also adjust other settings to cater for more complex scenarios: Persistence Using Managed Disks Azure SQL Server Scaling Monitoring and logging CluedIn Custom Resources for Kubernetes !!! Only available in CluedIn v3.2.4 onwards !!! Custom resources are a set of versioned business objects that extend the Kubernetes API. The goal of these objects is to simply performing certain administrative tasks within CluedIn that would normally be quite complex. CluedIn Custom Resources are monitored and actioned by the CluedIn Controller service. Currently, two actions are supported: 1) Creating Deleting a CluedIn organization 2) Enabling Disabling SSO Note: These guides assume you have access to the cluster via kubectl Creating an organization Note: This action would happen instead of the bootstrap mentioned above. We will create a new organization called foobar by creating a Custom Resource (CR) and installing it into the Kubernetes cluster. 1) Create a secret to hold the administrator email and administrator password for the organization. We create a secret as this is sensitive data we wish to protect. kubectl create secret generic \"foobar-admin\" -n cluedin --from-literal=username=\"admin@foobar.com\" --from-literal=password=\"Foobar23!\" .. this will create a resource that would look like: apiVersion: v1 kind: Secret metadata: name: foobar-admin data: username: YWRtaW5AZm9vYmFyLmNvbQ== password: Rm9vYmFyMjMh 2) Create a custom resource for the new foobar organization. You can see we reference the secret name in the adminUserSecret property. # File: foobar-org.yaml apiVersion: \"api.cluedin.com v1\" kind: Organization metadata: name: foobar-organization spec: name: \"foobar\" adminUserSecret: \"foobar-admin\" .. you can apply the resurce to the Kubernetes cluster to begin the creation process using : kubectl apply -n cluedin -f foobar-org.yaml .. this will create a new organization in CluedIn and also create an administrator account with the email and password defined in the secret. Checking Organization status The STATUS field of an organization displays its current progress. You can list the Organization resources in a namespace by using : kubectl get organizations -n cluedin If the STATUS is Organization [foobar] activated then we know the foobar organization has been created successfully. A successfully created organization will also display an Organization ID. Any error messages from the process will be stored in the STATUS field and theController will re-try periodically. Deleting an Organization You can remove entirely remove an organization by deleteing the custom Organization resource that was created previously. It can be deleted by using the file it was created with .. kubectl delete -n cluedin -f foobar-org.yaml .. or by using its name (e.g. foobar-organization) directly .. kubectl delete organization -n cluedin foobar-organization Warning: This action will also remove all data for the organization. Including SQL Server blob data, Neo4J indexes and ElasticSearch indexes. Creating a resource for an existing Organization If you need to create a reference Organization resource for an already existing organization (that was, for example, created by the bootstrap process), you just need to retrieve the Organization ID. This can be found by running a query against the CluedIn databases. SELECT [Id] FROM [DataStore.Db.Authentication].[dbo].[OrganizationAccount] .. you can then create a new resource using that ID .. # File: foobar-ref.yaml apiVersion: \"api.cluedin.com v1\" kind: Organization metadata: name: foobar-organization spec: name: \"foobar\" id: \"d437b952-8bac-4194-ac51-8d23dd219e58\" adminUserSecret: \"foobar-admin\" .. The Controller will not create anything new but will allow deletion of the organization and enable reference by other CluedIn custom resources such as enabling SSO (see below). Enable SSO Note: This action requires an Organization custom resource to exist. CluedIn currently only support Azure Active Directory for SSO. You will need two bits of information from the Azure portal: 1) Client ID (also can be displayed as Application ID) 2) Client Secret First, we need to create a secret to hold the sensitive clientSecret value. kubectl create secret generic \"foobar-sso\" -n cluedin --from-literal=clientSecret=\"1234-5678-9ABC\" apiVersion: v1 kind: Secret metadata: name: foobar-sso data: clientSecret: c3RvcCBsb29raW5nIGF0IG15IHNlY3JldAo= Next, we need to create the custom Feature resource .. # File: foobar-sso.yaml apiVersion: api.cluedin.com v1 kind: Feature metadata: name: foobar-enablesso spec: enableSso: clientId: \"5f9c3386-c9e3-4232-918b-ea9c303bf10e\" organizationName: \"foobar\" clientSecretName: \"foobar-sso\" clientSecretKey: \"clientSecret\" kubectl apply -n cluedin -f foobar-sso.yaml Note: This action will cause Cluedin Server CluedIn Processing CluedIn Crawler to restart their pods. The RollingUpdate strategy should prevent any disruption but please only execute in a known maintenance window. organizationName is the name of an existing organization (it will search the Organization resources for one with a spec.name field that matches.) clientSecretName is the name of the secret we created in the previous step. This process will update two SQL tables with this information and restart the pods of Check SSO status The STATUS field of a Feature displays its current progress. You can list the Feature resources in a namespace by using : kubectl get features -n cluedin If the STATUS is EnableSSO Feature [...] active. then we know SSO has been enabled. Any error messages from the process will be stored in the STATUS field and theController will re-try periodically. Disable SSO You can disable SSO by deleting the custom Feature resource that was created previously. It can be deleted by using the file it was created with .. kubectl delete -n cluedin -f foobar-sso.yaml .. or by using its name (e.g. foobar-enablesso) directly .. kubectl delete feature -n cluedin foobar-enablesso Note: This action will cause Cluedin Server CluedIn Processing CluedIn Crawler to restart their pods. The RollingUpdate strategy should prevent any disruption but please only execute in a known maintenance window."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-81-home-dashboard-html": {
						"id": "docs-00-gettingstarted-81-home-dashboard-html",
						"title": "Home Dashboard",
						"category": "",
						"url": " /docs/00-gettingStarted/81-home-dashboard.html",
						"content": "Navigating the Home Dashboard Your home screen will show you notifications and high-level actions that span your entire CluedIn account. For each major pillar of CluedIn (e.g. Integration, Governance), you will have the most important notifications and actions on your home screen and you can drill into your respective menu options to explore more notifications and actions specific to that part of the application. You can click on the notifications to see more details. The main concept of the home screen and the notifications is to direct you towards actioning a task that needs actioning. Such actions include: Integrations that have lost connectivity due to authentication issues. Number of Entity Types Number of broken data policies Number of Possible Duplicate Records If you have an empty dashboard, it is simply that you don’t have anything that needs actioning. As you use more and more parts of the platform, you will see more and more notifications show in your home screen. The goal of CluedIn is to make sure you have a robust flow of data from data sources through to consuming applications. The goal of your home screen is to alert you to anything that is stopping that happening. The home screen is filtered by what type of authorization you have within CluedIn. You will only see actions and notifications that need to be addressed by you or others in your role. Also available on your home screen is quick links to view notifications, statistics on performance, your help screens and to expore the different parts of the applications that you have access to. You will also have your search bar available at all times as you browse throughout the application. This will allow you to filter and search for the data you would like to view and action."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-90-frequently-asked-questions-html": {
						"id": "docs-00-gettingstarted-90-frequently-asked-questions-html",
						"title": "Frequently asked questions",
						"category": "",
						"url": " /docs/00-gettingStarted/90-frequently-asked-questions.html",
						"content": "What is the different between CluedIn and other data integration platforms? Whether you are using SAP Data Services, Azure Data Factory, SSIS, Talend, Python, SAS Institute, Informatica Powercenter or other ETL based tools, they all hold something in common - they require you as the engineer, architecture or business user to map different systems together. At CluedIn, we have found that the process of finding out what Id’s need to be connected between systems as to unify or connect the records is actually the hardest part of the entire process - in fact, it gets to a point where it is just impossible to do. CluedIn uses a schema-less based integration pattern that doesn’t require a developer, architect or business user to instruct our platform of how different systems connect together - rather, we ask for a much more simpler and streamlined mapping and CluedIn will do all the work to find the relationships between the systems. In fact, we will often find connections that you could never find yourself. What happens with the data ingested that is bad quality, has non-allowed values or is noise or mess? CluedIn is an ELT type of integration platform where we want to take the data in as it is. No transforms. No cleaning. Nothing. That is our job! Bad quality data will happen all the time and hence this is one of the big reasons why enterprise companies are taking so long to get access to ready-to-use data. There are so many places in CluedIn that will catch and distribute the work involved in fixing and addressing the data quality issues that stream through the CluedIn processing engine. Firstly, it starts with CluedIn having many prebuilt processing steps to automatically clean and normalise data into the highest fidelity format. Secondly, you have your ability to build up lists of “black-listed” values and to either remove them or automatically transform them into another value on processing. Finally, you can setup rules that dictate good versus bad values. This means that data keeps flowing for data that meets the standards - everything else will be put into a quarantine section for you to resolve so that less and less data has issues. What do you do about sensitive and personal data? CluedIn will natively detect personal data and will start by simply letting you know where it is. Depedent upon your requirements, we then allow simple actions to either mask, anonymise or remove the data. As for sensitive data, this is often open to interpretation. What is sensitive for you might not be for another business and hence CluedIn allows you to flag records as sensitive. This will build up a data model specifically for your account that will be trained to look for records with similar traits and flag them automatically over time. If CluedIn has 4 different databases, all storing my raw data - isn’t that going to be a lot of storage? The first thing to answer here is, that you have full control over what data you ingest into CluedIn. There are many cases where you will not want to bring in all the data. For the data that we do ingest, it is also worth mentioning that for certain types of data, we are not taking a full copy of the data but rather extracting what we need e.g. files only extract content and metadata. CluedIn also compresses all the data that is stored in the databases, meaning that often the storgage size is much smaller than the original source data itself. To put this in perspective, we have customers that have processed 100’s of TB’s of raw data, but in CluedIn the databases will collectively be approximately 2 to 3 TB’s. Once the data is in CluedIn, how do I get it out? CluedIn is designed to sit in-between source and target systems and hence it is in our interest to make it extremely easy to get the data out of us. There are two main ways that we recommend this, but ask that in production, you don’t use the CSV or File options - after all, this is typically the cause for siloed data in the first place! Firstly, our GraphQL API allows you to search through all the data in your CluedIn account and will return the results in JSON format. You can choose what properties you would like to select or project from the query as to limit the data to only what you need. This is useful for when you have other systems that want to “pull” data from CluedIn. Quite honestly, you won’t find a lot of systems that support making GraphQL queries or REST queries and hence we offer another option that we think is more suitable. Secondly, we allow you to create streams of data directly out of CluedIn (Push) to a target. This target could be a SQL database, an Apache Spark stream, direct to Power BI or to the dimension tables of your Data Warehouse. Considering that CluedIn uses databases that are OTS (Off the shelf), can I access data directly from there? We are not going to stop you doing this, but do recommend that you only do this in a developer environment. If you are wanting to talk directly to the databases, CluedIn has built you GraphQL interfaces to be able to do this in a way that guarantees that you have isolated access to only the data from your own account. CluedIn ships with tools to administrate the different datastores and hence we obvioulsy are influencing you to be able to look in these datastores as a developer. The recommendation is to turn off native access to these datastores when you deploy into your production environment - in fact, by default our Kubernetes setup has it disabled by default. When CluedIn says it has prebuilt integrations, what about when we have customised our system so much? The majority of integrations that CluedIn has are working with the assumption that you can change the underlying model of the source system. Hence the majority of our integration will still require you to map the source objects into CluedIn to get the best out of your data. The integrations handle all the connection, delivery and scheduling, paging and sourcing of data. You as a Data Architect will need to map this into the Clue object. Fortunately, our data integration pattern makes this a simple task and unlike other data integration platforms, we don’t need you to tell us how everything wires together - that’s our job! If CluedIn is connecting the data for us, won’t that end up in a data model that is a mess? Yes! Because guess what? Your data landscape is typically a mess! The goal of CluedIn is not to form the perfect model of your data for a paritcular use case - it is to find how the data is naturally connected and then allow you to project your perfect model for ANY use case you can think of. You will find that the model that we build (we use the Graph for this part) will match the model that your data finds is the way to find links between your different systems. Using the same model, we give intuitive and simple-to-use tools to mutate this natural model to a more rigid model for the use cases that you want in that particular model. The good news is that the Graph was designed for running these types of projects - in fact, they are as fast as an index lookup. This means that projecting a “dirty” model into a clean one is something that scales and performs really well. The Admin screen of CluedIn is great, but I want to automate things! How do I do this? The good news is that the native CluedIn administration screen is built ontop of the same REST API that you have direct access to as well. This means that you can do everything in the REST API that you can do in the user interface (and more!). This also means that you can script anything in CluedIn via our REST API, PowerShell or other scripting languages. I have made a change in CluedIn that requires me to re-process all the data, what do I do? This will happen many times with your CluedIn account! In fact, it is extremely normal and expected. CluedIn has been designed with the idea in mind that we will re-process the data all the time. Once you have made your change, you have quite a lot of control over the level of re-processing. You can re-process at a source level, globally, at a record level or even something a little more custom e.g. entity type level. Does CluedIn index everything? Yes. Yes we do. You can however influence the CluedIn engine to store data in a more optimum way. When you are setting up your Vocabularies, you have the chance to set a Vocabulary Type. This will influence how indexes are built and we would strongly recommend doing this. Even if you do not do this, all your content will be search-able, but you can always make things faster and more efficient if you setup your Vocabularies properly. With 4 databases, how does CluedIn guarantee transactions? CluedIn ingests data into an enterprise service bus. This helps us guarantee delivery of data. CluedIn utilises “Eventual Consistency” for the different data stores. This means that CluedIn takes all records off the bus into a transactional log. The different databases will then read off the transaction log as fast as they can. This gurantees transactional support on your data, but “eventual consistency” in the different data stores. With this setup and the fact that the different datastores will consume off the log in batches, it lessens the likelihood that the database will have issues with having a 100% consistency across the datasores at the same time. What happens if I ingest records that don’t have any Id’s that overlap? This happens all the time! In fact, it is the reason why systems like CluedIn exist. Firstly, although it might seem like there won’t be overlap, often there is. You might find with your traditional approaches that tracking down the columns that match from system to system don’t make it easy to find the links - but the eventual connectivity pattern of CluedIn will do all that work for you. For times when you definitely don’t have Id overlaps, there are many fallbacks that yield good results. Firstly, you can use the out of the box integrations to external data sources (or build new providers) that may have the Id’s that you need to link systems. Secondly, you have the CluedIn Fuzzy matching engine which can use non-identifying properties to find possible and statistically confident matches in duplicates or in linkages. Why is CluedIn written in C#, isn’t that slow? The team behind CluedIn purposefully chose .net core and C# as the language for our processing engine for many reasons. 1: Our core development team has decades of combined experience in building large, scalable and performant .net applications. 2: The .net environment has jumped leaps and bounds and has continued support for Microsoft and the community. The fact that it is open-source helped in the decision as well. 3: .net is designed to write very large applications and scale in maintaining, seperation of concern and ease of deployment. 4: .net and C# is very much, not slow. In fact, compared to the majority of popular languages today it is very fast. Some could argue that it is not as memory-efficient, but we think that the trade-offs to deliver solutions to you faster is well worth it. I am not a C# developer, does that mean I can’t extend CluedIn? The interfaces to communicate with CluedIn are actually HTTP i.e. REST. Our C# layer essentially wraps this and talks REST. This means that you can actually use any language that you want to talk to CluedIn, however there are some restrictions or limitations if you do this: You cannot inject custom logic into the same application domain as CluedIn. You will have to host your custom code in another host that is not CluedIn. You cannot currently use the crawler templates that are available only in C#. Let’s go with some examples for you. What if you are a Python, R, SQL or GO developer? Let’s say you are wanting to build a custom integration that can be added through the user-interface but the code itself is written in Python. What type of data is CluedIn not suited for? There are many use-cases where CluedIn could be considered overkill or not fit for purpose. There are many use-cases that do not need all of the data preperation and cleaning pieces of CluedIn. It could very much be argued that IOT data doesn’t suffer from the same problems or require the same data preperation than other data such as operation business data. Hence here is a list of use-cases or data that we don’t think is necessarily suitable for CluedIn: 1: Processing Signal IOT data that doesn’t have data quality issues or doesn’t need cataloging, governance - but rather needs to be made available in a Dashboard as fast as humanly possible. 2: If you have a limited amount of systems to integrate and there is no problems with determining how the records are conneceted between the different systems. A good example would be if you just wanted to intergate your Dropbox account with your CRM. 3: Where you do not want to move data. CluedIn copies data from sources to target. It won’t necessarily take all the data, but it is an ingestion engine. You have to be very careful with this use-case as many problems cannot be properly solved without taking copies of the data."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-running-20cluedin-20efficiently-html": {
						"id": "docs-00-gettingstarted-running-20cluedin-20efficiently-html",
						"title": "Running CluedIn Efficiently for Cost",
						"category": "",
						"url": " /docs/00-gettingStarted/Running%20CluedIn%20Efficiently.html",
						"content": "Introduction When running big data platforms in the cloud, it is important to remember that things cost money to run. There are mainly two main expenses in running CluedIn and they are processing of data and storage. To put things in perspective, processing is usually 90% of the cost, storage being the other. Hence, CluedIn can be run in a way to optimise for cost, but can also be run in a way that does not. Setting the baseline, CluedIn costs roughly $10 a day to run in Microsoft Azure with no data in it. As soon as you add data to CluedIn, and in turn, require processing of that data, the cost goes up. You can see some approximations of cloud costs in Microsoft Azure here. With processing being the main expense, one can also optimise workloads in order to capitilise on doing many things at the same time. For example, imagine that you add a new processing rule where every record that has a name that includes “Lego” should be tagged with Lego. Naturally, the only records that need to be reprocessed are those that have “Lego”“in the name. This does not require a full re-process of all data, and hence although you can do this, it would not be optiminal for cost as you would need to load up all records back into the processing pipeline to have it only pay attention to the few records that actually meet the requirements of change. CluedIn will still need to evalulate all of its processes to see what it needs to do. CluedIn is an application that ships as a product with many dependencies. These dependencies are packaged up into Docker Containers. For example, CluedIn use ElasticSearch as one of its dependencies. This means that the cost of running this service is embedded into the cost of running CluedIn. There are many times where you would want to pull this service out of the CluedIn stack and move it onto a PAAS or even SAAS version. In our experience, this is typically more expensive, but it does also come with its benefits. Running CluedIn with all dependencies internally hosted within CluedIn itself will typically yield the most cost effective way to host CluedIn. Batching up Changes If we look at the way we run projects, we will typically make changes, add functionality and more. To run CluedIn as “cheaply” as possible, your best option is to have all of the additions and changes have as much overlap in reprocessing as possible. For example, if we have 1000 records in our CluedIn instance and we have added 10 rules, 3 more streams and 2 new quality metrics. Your best and cheapest option is to only need to reprocess the records that would be matched by the 3 rules, 3 streams and new quality metrics. The easiest way to do this today is to use your “actions” in GraphQL. In GraphQL, you can come up with your filter using the search function and for the matched records, you can run the postProcessing action. { search(query:\"entityType: Movie\") { entries { actions { postProcessing } } } } ###Only store what needs attention Often, the question is asked, “What data should CluedIn process?”. The answer depends on the situation, but in general, the answer is “The data that needs attention in cleaning, governing, integrating, enriching and all the other goodness that CluedIn brings”. You might find that some transactional and analytical data meets this need, you might find that other transactional data does not. Naturally, the less data you give to CluedIn the less it costs to operate. This is why you should be careful to analyse the data coming into CluedIn and decide if it actually needs to be processed by CluedIn. ###Downscale the application when it is not being used CluedIn uses Kubernetes for many reasons, but one of those is to make sure that the environment can scale up and down when needed. CluedIn can scale down different parts of the application to run at a lower cost while heavy processing is not occuring. ###Setup retention policies within the platform Although CluedIn is useful for storing the history of data as it changes, you might find that there is certain data that can benefit from CluedIn’s ability to setup retention of data. This will either remove or archive the data on a time schedule. The less data in CluedIn, the cheaper it is to operate. ###Turn features of CluedIn off There are many features of CluedIn that you might find are not necessary to solve your usecases. By default there are a combination of features turned on or off in the release of CluedIn. You can discover these from your settings within the User Interface. Naturally, the more features that are turned on, the more CPU cycles are needed to run per record that enters CluedIn or is reprocessed. You can systematically turn features off in CluedIn to lower the operational cost if these features are not needed. As an example, you might find that calculating quality metrics or running sensitivity detection is not needed for your use cases and hence these can be globally disabled."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-dns-hostnames-html": {
						"id": "docs-00-gettingstarted-dns-hostnames-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/dns-hostnames.html",
						"content": "Hostnames and addresses CluedIn is a web application, so users will access it from their browser. The main URL will take the form: https: &lt;app-segment&gt;.&lt;prefix&gt;.&lt;hostname&gt;. CluedIn is a multi-tenant application, so you can have different organizations units in your company that can use CluedIn with total separation. So in order to use CluedIn you will need to create at least one organization. Each organization gets a different URL: https: &lt;organization&gt;.&lt;prefix&gt;.&lt;hostname&gt;. Since any number of organizations can be created from the application, the recommendation would be to map *.&lt;prefix&gt;.&lt;hostname&gt; to the public IP of the ingress of the cluster. The &lt;prefix&gt; and &lt;hostname&gt; can be adjusted by changing the helm chart values inthe dns section. dns: prefix: # defaults to the release name, set to none to disable prefixes. hostname: # add the hostname suffix - i.e. example.com"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-email-html": {
						"id": "docs-00-gettingstarted-email-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/email.html",
						"content": "In order to send emails, CluedIn must be configured with an SMTP server. This can be a company owned one for your organization or a temporary one using a service such as MailTrap or Sendgrid, for example, which are useful for simple testing. Email Configuration - Docker Local Machine Email configuration is simplified when running localy by using the cluedin.ps1 helper script. You can configure any email settings as environment variables that will be passed into the application at runtime. This is acheive by using the env command : . cluedin.ps1 env -set [NAME]=[VALUE] The possible values for configuring email in CluedIn are: CLUEDIN_EMAIL_HOST (default: &lt;blank&gt;) CLUEDIN_EMAIL_PASS (default: &lt;blank&gt;) CLUEDIN_EMAIL_PORT (default: 587) CLUEDIN_EMAIL_SENDER (default: noreply@cluedin.com) CLUEDIN_EMAIL_USER (default: &lt;blank&gt;) Email Configuration - Kubernetes WHen using Kubernetes the SMTP setting can be configured in the values.yaml. This can be done by setting the following properties: email: host: port: user: password: senderName: senderDisplayName: This will create a secret storing the user and password information. Should you want to pass a secret already containing those details, you can create a secret with the keys: apiVersion: v1 kind: Secret metadata: name: &lt;my-email-secret&gt; type: Opaque data: EmailUserName: EmailPassword: And pass the name of the secret in the property email: secretRef: &lt;my-email-secret&gt; Passing a secret in this way will override the use of explicit user password properties."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-kubernetes-sample-prerequisites-html": {
						"id": "docs-00-gettingstarted-kubernetes-sample-prerequisites-html",
						"title": "Example pre-requisite configuration",
						"category": "",
						"url": " /docs/00-gettingStarted/kubernetes-sample-prerequisites.html",
						"content": "Example pre-requisite configuration Creating the Kubernetes cluster is outside the scope of this guide. Refer to: Microsoft Azure AKS documentation. The ability of creating AKS clusters with Windows support is currently in Preview. If you have multiple accounts in Azure then you will need to use the az account set -s &lt;AccountName&gt; so that you can set the right context for the deployment. You must have: a local install of kubectl configured to talk to the cluster a local install of the CLI for helm. Create a service account. If you are using RBAC in your Kubernetes cluster you will need to grant permissions to Tiller for it to be able to create resources in the cluster. Check Helm’s documentation. In test environments, you may consider just granting Tiller cluster admin permissions: Create a file with the following content apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system In production scenarios you will have to be more restrictive with the permissions. See Helm’s documentation for advice on security for production environments Run kubectl apply -f &lt;path-of-file&gt; to create the role binding If not using RBAC, you will need to run the following kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}} If using RBAC, execute helm init --service-account tiller. This will install Tiller in the cluster, so you can install helm charts directly. If not using RBAC, do not use the service account parameter. Install the ingress controller: helm install stable nginx-ingress \\ --namespace ingress \\ --name ingress \\ --set rbac.create=\"true\" \\ After a while the ingress controller will have a public IP that can be used to access the cluster. If you don’t want a public IP (because you have something else, like an application gateway in front of it), you can modify the installation of the ingress controller in the step above - see Helm chart documentation (controller.service.loadBalancerIP). To retrieve the public IP: kubectl get svc -n ingress -o wide -l 'component=controller' In your own DNS, configure that IP to whatever host you want to use for CluedIn. You could map it to a wildcard record; alternatively you can use more specific entries. Create a secret with your docker hub login credentials: kubectl create secret docker-registry docker-registry-key \\ --docker-server='&lt;repository-url&gt;' \\ --docker-username='&lt;your username&gt;' \\ --docker-password='&lt;your password&gt;' \\ --docker-email='&lt;your email&gt;' For Docker Hub, the repository-url is docker.io. You should request access to the CluedIn Docker Hub repo for those credentials so you can pull the private Docker images with the application. Register the CluedIn helm chart helm repo add cluedin https: cluedin-io.github.io CluedIn.Helm helm repo update NOTE: You can also place secrets into a Vault or Key Vault from your cloud provider of choice."
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-monitoring-html": {
						"id": "docs-00-gettingstarted-monitoring-html",
						"title": "Monitoring the application",
						"category": "",
						"url": " /docs/00-gettingStarted/monitoring.html",
						"content": "Logging CluedIn uses structured logging. You can configure any sink, but only 3 have been tested with the application. Console: this sink is enabled by default. Seq: To enable it, you just need to add the seq image you want to use: seq: image: datalust seq You can access it using port-forwarding or you could enable an ingress route: seq: public_endpoint: seq By default the seq endpoint is protected with an Oauth2 proxy. Azure Application Insights All you need to do is add the key for the Application Insights instance you want to use: logging: appInsightsKey: 'your-app-instance-key-guid' By default it will send full telemetry of the frontend application, and all the logs from the CluedIn server will be sent as trace information. Admin UIs It is sometimes useful, for example for debugging purposes, to be able to log in to some of the tools dependencies that CluedIn uses. The easiest way is to set up a proxy using a machine that has kubectl configured to access the cluster. In the following statements the &lt;name-of-release&gt; is how you named your helm deployment. You can see the list of releases using helm list. You can proxy several ports at the same time if you want to use several tools simultaneously. The port-forward instruction used to set up the proxy, will just remain running. The proxy will be available whilst you don’t terminate the port-forward instruction. Neo4j Graph database used to store the relationships between entities. kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=neo4j') 7474 7687 Then point your browser to localhost:7474 RabbitMq Messaging bus. kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=rabbitmq') 15672 Then point your browser to localhost:15672 Redis Cache and key-value pair storage. kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=redis') 6379 Redis has no default frontend. But you could stand one up in your computer using docker. docker run --rm -p 8081:8081 -e REDIS_HOSTS=local:host.docker.internal:6379 rediscommander redis-commander Then point your browser to localhost:8081 ElasticSearch Search index. kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=elasticsearch') 9200 Then point your browser to localhost:9200 _plugin inquisitor # SqlServer Well-known relational Database. You can retrieve the password by executing this command in a bash shell: kubectl get secret &lt;release-name&gt;-cluedin-sql-password -o jsonpath=\"{.data.SA_PASSWORD}\" | base64 --decode Or using Powershell: [System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String($(kubectl get secret &lt;release-name&gt;-cluedin-sql-password -o jsonpath=\"{.data.SA_PASSWORD}\"))) The port can be exposed locally using regular Kubernetes port-forwarding: kubectl port-forward $(kubectl get pod -o name -l 'release=&lt;name-of-release&gt;,app=sqlserver') 1433 You can then use Visual Studio, or the MSSQL Management Studio to connect to the database on localhost. If there is already a SQLServer instance in your machine, there will be a port clash. See note below to map to a different local port. Note: You can map the port to a different local port if there is a conflict with existing open ports in your machine using the syntax kubectl port-forward &lt;pod&gt; &lt;local-port&gt;:&lt;remote-port&gt;"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-oauth2-html": {
						"id": "docs-00-gettingstarted-oauth2-html",
						"title": "Securing components with OAuth2",
						"category": "",
						"url": " /docs/00-gettingStarted/oauth2.html",
						"content": "With the default values of the chart, OAuth2 authentication will be enabled to access the clean feature (URL https: &lt;clean-segment&gt;.&lt;prefix&gt;.&lt;hostname&gt;) and the Seq log monitoring tool. Internally it uses an Oauth2 proxy, which supports all the most common providers (Azure, Google, etc.). Read their documentation for further information. The chart assumes the usage of Azure, however it is possible to use other providers: Create a new Application registration in Azure AD. Allow the following endpoints for redirection in the Azure AD application https: &lt;clean-segment&gt;.&lt;prefix&gt;.&lt;hostname&gt; oauth2 callback https: &lt;app-segment&gt;.&lt;prefix&gt;.&lt;hostname&gt; oauth2 callback (refer to the DNS section). Create a new client secret (under Certificates and Secrets). Create a secret with the keys OAUTH2_PROXY_CLIENT_ID: this is the Application (client) ID from Azure AD OAUTH2_PROXY_CLIENT_SECRET: A client secret generated in Azure AD OAUTH2_PROXY_COOKIE_SECRET: A random string Set the following in your values.yml file override. oauth2: environment: OAUTH2_PROXY_EMAIL_DOMAIN: &lt;email-domain-for-authentication&gt; secretRefName: &lt;name-of-secret-created above&gt; Disabling OAuth2 You can disable it entirely by setting: oauth2: enabled: false You can also disable it for individual components by only adding to the for key those elements you want to have OAuth2 enabled for: oauth2: for: - seq - openrefine"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-persistence-html": {
						"id": "docs-00-gettingstarted-persistence-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/persistence.html",
						"content": "Persistence By default all deployments that store state (sqlserver, elasticsearch, rabbitmq, redis, openrefine) are defined with some storage in order to persist the state. You can adjust the size of the storage by setting the following property (for each deployment): neo4j: #name of the deployment persistence: storageSize: 1G Alternatively, you can also supply your own persistence volume claims that you may have created manually outside the chart. To do so, for each deployment, you can set the claimName property, e.g.: neo4j: #name of the deployment persistence: claimName: my-claim-name-I-have-already-created Note that using persistence in this manner a volume can only be linked to a single pod; so you won’t be able to scale the number of pods. In addition, the strategy for updating the pods is set to Recreate for exactly the same reason (as setting it to Rollout would require to have two pods accessing the volume simultaneously). If you need to scale the number of pods, further customization would be required to use persistence in a different way. Persistence can also be turned off, for each deployment, through the setting: neo4j: #name of the deployment persistence: enabled: false Azure Disk If you are running on Azure, you can enable persistence to be configured so that it will store data on dedicated azure disks. This allows you to use some of the management and backup features of this product. To configure addd the following section to the persistence section in your values file.. azureDisk: data: diskName: diskURI: diskKind:"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-scaling-html": {
						"id": "docs-00-gettingstarted-scaling-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/scaling.html",
						"content": "Scaling the CluedIn Server The CluedIn Server is performing 3 main functions: responding to web API calls retrieving data from integrations (crawling) processing updated data These three functions can be segregated into specialized containers - it is also possible to scale the number of pods performing each function and the resources allocated to each. If the count property for the processing and or crawling roles is set to 0, the main role will take over those tasks. All this is controlled by the following section of the values.yaml configuration: cluedin: roles: main: count: 1 resources: limits: cpu: \"1\" memory: \"8Gi\" requests: cpu: \"0.5\" memory: \"4Gi\" processing: count: 1 resources: limits: cpu: \"1\" memory: \"8Gi\" requests: cpu: \"0.5\" memory: \"4Gi\" crawling: count: 1 resources: limits: cpu: \"1\" memory: \"8Gi\" requests: cpu: \"0.5\" memory: \"4Gi\""
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-sqlserver-html": {
						"id": "docs-00-gettingstarted-sqlserver-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/sqlserver.html",
						"content": "SQL Server Inside a pod If using SQLServer as a deployment inside the cluster (instead of for example SQLAzure) two secrets will get created. One will contain the password for the SQLServer (which will be a randomly generated password), the other will contain the connection strings that will be consumed by various deployments. Custom SQL server (Sql Azure) If you are using your own SQL installation, like SQL Azure, you will need to: Install the database definitions (DACPACs) to your SQL instance. This can be done from the command line using SqlPackage.exe. Create a secret with the connection strings for each database. The secret should have the following keys: apiVersion: v1 kind: Secret metadata: name: my-connection-string-secret type: Opaque data: AuthenticationStore: &lt;connection-string&gt; BlobStorage: &lt;connection-string&gt; ConfigurationStore: &lt;connection-string&gt; CluedInEntities: &lt;connection-string&gt; TokenStore: &lt;connection-string&gt; Training: &lt;connection-string&gt; ExternalSearch: &lt;connection-string&gt; ML-Logging: &lt;connection-string&gt; Metrics: &lt;connection-string&gt; You should then pass the name of the secret in the values.yaml override file: sqlserver: connectionsSecretName: my-connection-string-secret"
					}

					
				
			
		
			
				
					,
					

					"docs-00-gettingstarted-ssl-html": {
						"id": "docs-00-gettingstarted-ssl-html",
						"title": "Configuring the Helm Chart",
						"category": "",
						"url": " /docs/00-gettingStarted/ssl.html",
						"content": "SSL configuration The website communication would usually terminate the SSL at the ingress level. As such you have the option to either provide a certificate in a secret, or use CertManager. To create create a secret with your own certificate, you can run the following command: kubectl create secret tls &lt;secret-name&gt; \\ --key &lt;private-key-path&gt; \\ --cert &lt;public-certificate-path&gt; then you have to assign the certificate name in the values yaml file tls: ingressCertSecret: &lt;secret-name&gt; Alternatively, if you use CertManager you will need to have either an Issuer or a ClusterIssuer. Also, since you are using a wildcard route in the ingress (to cater for the multi-tenancy), the certificates will require the DNS protocol, i.e. CertManager requires access to your DNS to be able to set the TXT entries for the certificate challenges (to prove you control that DNS and be granted a certificate). CertManager supports various DNS (Azure DNS amongst them). You will need to set the following properties tls: issuer: &lt;name of your issuer or cluster issuer&gt; dns01_provider: # default `azure-dns` This means you will require a secret to be able to access the DNS and create the entries. Further documentation can be found in CertManager. Server SSL configuration As explained above (see Hostnames and addresses) the api, auth, public-api and webhooks components are accessible from the outside of the cluster through SSL only. They are all running in a Windows application which needs to receive the certificate for the SSL connection in PFX format. You should therefore create certificates for all those hostnames (or the same certificate with wildcard entries or multiple SANs). Those certificates need to be added to a secret. Assuming you have the public certificate and private key files you need to: transform them to pfx format, for example using openssl. If you have a terminal open in the location of your cert files: docker run --rm -ti -v ${PWD}: certs --workdir certs alpine sh apk add openssl # you will get asked to create a secret openssl pkcs12 -export -out cert.pfx -in cert.pem -inkey key.pem base64 encode the pfx file $cert=[Convert]::ToBase64String([System.IO.File]::ReadAllBytes('&lt;path-to-pfx&gt;')) create the secret. It expects the following keys: CERT_PASSWORD,CERTVALUE_API,CERTVALUE_AUTH,CERTVALUE_PUBLIC,CERTVALUE_WEBHOOK kubectl create secret generic &lt;secret-name&gt; ` --from-literal=CERT_PASSWORD=&lt;password-for-pfx&gt; ` --from-literal=CERTVALUE_API=$cert ` --from-literal=CERTVALUE_AUTH=$cert ` --from-literal=CERTVALUE_PUBLIC=$cert ` --from-literal=CERTVALUE_WEBHOOK=$cert ` Once of you have created the secret, its name needs to be assigned to the following parameter tls: serverCertSecret: # name of the k8s secret with the TLS certs for the backend server. If Undefined it will use self-signed certificate Root CA If you are using a private Certification Authority to create your certificates, you will need to provide the public key of your CA to the Website component so it can talk through SSL to the CluedIn server. Put the certificate chain for your CA in a file named ca.crt Create a secret with the contents of the ca.crt file: kubectl create secret generic &lt;secret-name&gt; --from-file=ca.crt Add the name of the secret overriding the parameter: tls: rootCASecret: &lt;secret-name&gt;"
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-10-profile-index-html": {
						"id": "docs-05-administration-10-profile-index-html",
						"title": "Profile",
						"category": "",
						"url": " /docs/05-Administration/10-Profile/index.html",
						"content": "Changing your password If you have decided to not use your Single Sign On provider to authenticate with your CluedIn instance, then you have the possibility to change your password by clicking on the user profile in the top right hand corner of the user interface and selecting “Settings”. This will ask you to fill in your existing password, your new password and then confirm your new password again."
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-20-users-adding-20team-20members-html": {
						"id": "docs-05-administration-20-users-adding-20team-20members-html",
						"title": "Users",
						"category": "",
						"url": " /docs/05-Administration/20-Users/Adding%20Team%20Members.html",
						"content": "Adding new users manually New team members can be added to an existing account via their email address. This new member is by default added as a regular user with no extra permissions. It is the responsibility of an Administrator to change their roles after they have been added to the system. Users must have a matching email domain if using the manual approach of User management. Please see the Access Control part of our documentation to see what rights a regular User has within CluedIn. To invite a new user to CluedIn, navigate to the Users menu option and click on the Administration menu option and click on Users. You will see a breakdown of roles and users. In the top right hand corner there is a button Invite a New Team Member. You will be prompted with the ability to fill in an email. This can be internal or external users. They will receive an email that will be able to guide them through the process. If you click on the Suggested Users tab, CluedIn will suggest people to invite based on the data you have already added to CluedIn. This allows you to discover users you might not have thought about adding. Instead of adding users manually to the system, it is generally better to to use an LDAP Single Sign On Provider for managing users and groups within CluedIn. In this way, Users are provisioned through Active Directory and no sensitive user information is ever stored on the CluedIn side. This also allows you to manage access, users and roles from a central place. Allowing new users to sign up You can configure CluedIn to allow users with a particular email domain to sign up automatically. Follow the instructions in the Authentication section."
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-30-authentication-index-html": {
						"id": "docs-05-administration-30-authentication-index-html",
						"title": "Authentication",
						"category": "",
						"url": " /docs/05-Administration/30-Authentication/index.html",
						"content": "Default Authentication Provider By default CluedIn uses Microsoft Identity Server as its authentication provider. These means that users and roles are managed within the CluedIn application itself. It is recommended to enable SSO in staging and production environments, but for locally development, it is fine to use this authentication mechanism. CluedIn will also expire the authentication after 15 minutes of inactivity within the CluedIn user interface. Single Sign On with Azure AD App Registration Redirect URIs: https: app.hostname https: accountSubdomain.hostname https: app.hostname auth signin-oidc Logout Uri logout API Permissions: Microsoft Graph Email Offline_access Openid Profile User.Read Implicit Grant: id_token (This is a checkbox) Expose an API: Scopes Scope: https: www.cluedin.net sso user_impersonation Who can consent: Admins and Users Authorized client applications Authorized Scopes https: www.cluedin.net sso user_impersonation (scope created in the step above) Details required to enable SSO: Application (client) ID Client Secret Enabling SSO The following SQL query will have to be executed against Authentication database: This may require you to port-forward to the SQL instance running inside the kubernetes cluster using kubectl port-forward. Navigate to the Database called Authentication, and either run this command from a command line or using your SQL Management Client such as SQL Server Management studio. ( Fill out all values in the diamond operators in the command below) INSERT [SingleSignOn] ([Id],[OrganizationId],[LoginUrl],[LogoutUrl],[Active],[ChangePasswordUrl],[SingleSignOnProviderId],[ExternalId],[IssuerUrl],[SamlVersion],[Certificate],[CustomErrorUrl],[ExternalSecret],[AuthenticationScheme],[AuthorityUrl]) SELECT '{a421253e-9086-4202-bfcc-c42eed712987}','&lt;organization id&gt;','&lt;org account url&gt; ssocallback','&lt;org account url&gt; logout',1,' ','{54118954-951f-41a9-b0a7-6de7d47e6c17}','&lt;client id&gt;',' ',0,' ',' ','&lt;client secret&gt;','aad','https: login.microsoftonline.com common'; You will now need to update the OrganizationAccount tables ExternalAuthenticationId column for this organization you have created to be 54118954-951f-41a9-b0a7-6de7d47e6c17. This will signal to CluedIn that this particular account uses Azure Active Directory as its identity provider. Restart CluedIn API Pod kubectl delete pod -l role=main *Dependent upon the version of kubectl you are using you may need to also append the namespace in the command above. kubectl delete pod -l role=main -n cluedin After doing this, visit the login page for your CluedIn account again and you will notice that it will redirect you to the office 365 login page to authenticate. By default, your user will be created as a “User” and not an Administrator. You can now manage this all within your Active Directory now. Cleaning up after you have switched to a SSO provider After you have switched over to a Single Sign On provider you will still have your original user in the database. This user will not be able to login anymore so it is best to deactivate this user. To do this, you will also need to port-forward to the SQL database like above and switch the “Active column” on the AspNetUser table to false for this user. Mapping Active Directory Groups to Roles within CluedIn If you have switched to an SSO provider then you will need to map Groups in Active Directory to Roles within CluedIn. This is done using configuration files within CluedIn. User Role mapping from configuration Roles returned from the IDP in tokens are mapped to built-in CluedIn Role types using configuration. The default configuration for this Role mapping is shown below: The configuration key starts with the prefix Security.Roles.Mapping. followed by the CluedIn Role name. The supported CluedIn roles are User, OrganizationAdmin and ReportManager, these roles are defined in the CluedIn.Core.Accounts.UserRoleType enumeration. The value specified by the Role mapping setting is a regular expression to match incoming Role names against. Delegating access to CluedIn from Active Directory Now that you have moved to using a SSO provider, then all access to users and groups is now done through Active Directory. Here is an example of a guide in Azure Active Directory that guides you through doing this against the Enterprise Application that you setup above. NGINX Bad Request If you are running NGINX as your Ingress Controller and upon redirect you have a 502 Bad Request, you are missing the following annotations to your Ingress definition: nginx.ingress.kubernetes.io proxy-buffering: \"on\" nginx.ingress.kubernetes.io proxy-buffer-size: \"128k\" nginx.ingress.kubernetes.io proxy-buffers-number: \"4\""
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-40-theme-index-html": {
						"id": "docs-05-administration-40-theme-index-html",
						"title": "Theme",
						"category": "",
						"url": " /docs/05-Administration/40-Theme/index.html",
						"content": "Changing the style of the UI You can change your theme of the user interface in CluedIn. You may do this to align the UI more closely with the branding of your company. You can change the colours of the user interface by navigating to the Administration and then the Theme menu option. Remember to click the Save Colours button after you have made your changes. This change affects every user in the account, not just yourself."
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-60-accounts-multitenancy-0-index-html": {
						"id": "docs-05-administration-60-accounts-multitenancy-0-index-html",
						"title": "Multitenancy",
						"category": "",
						"url": " /docs/05-Administration/60-Accounts_Multitenancy/0-index.html",
						"content": "Multiple accounts CluedIn has multi-tenant support natively. This means that you can run multiple different CluedIn accounts, all from the same CluedIn instance. The data is isolated from each other, there is no way to bridge data from different CluedIn accounts, even within the same CluedIn instance. Multi-tenant solutions are appropriate when you need to isolate data into respective groups. It is required that each account has its own unique name and mapped to a unique subdomain. It is also required that each user is unique, even across accounts i.e. a user can only exist in one account. Data isolation is implemented differently at each store level. In the Search Store a separate index is created per account, but remains within the same cluster. Data in all the other stores, Graph, Relational, Blob and Redis, exists within the same database but is isolated at an application level. There are some restricted names on what you can call your CluedIn account. These are controlled via the configuration setting ReservedOrganizationIds in your container.config (or via the values.yaml file if using Kubernetes for deployment). Check the installation instructions to create new accounts. FAQ Can you bridge data from two accounts into one? There are special administrator end points that allow you to do this. You might find that you want to start out ingesting data from different sources into different accounts and only merge them when you are happy with the results. Why would I want to use multi-tenancy? If you have one installation of CluedIn, but you have very different use cases and data sources that must be isolated with no blending of data across data sources, then this would be a good reason to support multitenancy. Your CluedIn license has no restrictions on how many accounts you can have - so it is more about organization of data more than anything. Can I host all my customers on one CluedIn instance, but have different accounts for each? If you are a partner of CluedIn, you will know that you cannot use the same license across customers unless agreed with your CluedIn account manager. Can I have multi-tenant support, but each customer gets their own database instances? Yes, you technically can but you have to be aware of the cost overheads of running it like this. It will most likely be more advantageous to simply run different instances. Can I run CluedIn without an organisation account? No. CluedIn is intended to be an application that runs on a server, not on your local developer machine and hence everything is setup in a way that is secure by default. This shows itself in many ways including the fact that you cannot run CluedIn without HTTPS and SSL."
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-70-removing-data-index-html": {
						"id": "docs-05-administration-70-removing-data-index-html",
						"title": "Removing data",
						"category": "",
						"url": " /docs/05-Administration/70-Removing_data/index.html",
						"content": "Removing Data for an existing account You might need to reset your account for many reasons. You could either reset your entire CluedIn instance, create a new account, or reset a particular account. A reset of an account will remove all data except the initial 2 records of a User and Account that are created when creating a new CluedIn account. You will need to use our REST API, and be an Administrator to be able to run this particular REST command. You will need to supply the OrganizationId of the account that you would like to reset. You can run a HTTP DELETE against the api Account client?clientId=&lt;Name of Account&gt; which will achieve this for you. Note: On developer machines, it may be easiest to use the Docker option to simply reset your volumes to a default installation of CluedIn. Removing specific data from CluedIn On occasion you might find that you need to delete data from an account that came from a particular data source. You have many options to take in this situation. You can retain the data, but set the read access to false for all users in the system for this particular integration point. Beware though that if this data merged with any records from other sources then those records will remain with read access. You can run a REST API command to remove this data. You will need to provide the ProviderDefinitionId. This command may take some time to finish because it needs to split entities that involve Clues from that source, remove those Clues and then reprocess. You can run a HTTP DELETE against api v1 organization providers clear?id=&lt;Provider Definition Id&gt;"
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-80-upgrading-20cluedin-index-html": {
						"id": "docs-05-administration-80-upgrading-20cluedin-index-html",
						"title": "Changing versions",
						"category": "",
						"url": " /docs/05-Administration/80-Upgrading%20CluedIn/index.html",
						"content": "Upgrading CluedIn To upgrade CluedIn, you will need to take many things into consideration. CluedIn deploys new versions of its software, crawlers and other products to the CluedIn Private Docker Repository. We deploy Stable, Beta and Alpha versions so that our customers can try out different parts of the platform at their discretion. It is always recommended to first check the release notes of the version you are upgrading to. You might find that if you are jumping a large number of versions, that the upgrade path is harder than smaller increments. If you have adhered to the best practices of CluedIn, then upgrading should be very straight forward. In essence, we offer ways for Developers to inject changes to CluedIn but very much do not recommend changing default CluedIn behaviour. If you have adhered to this then it will make your upgrade paths easier. It is also a recommended practice that you also don’t directly connect to the CluedIn data stores, but rather use our APIs to interact with the underlying data stores. If you have stuck to this principal, then changes to the underlying data stores can be automated during the upgrade process. You may find that certain updates will require you to reprocess the data within your account. The release notes will detail if you will need to run this process or other processes to make data updates if necessary. If you have chosen the Docker path of deploying CluedIn then you will need to use your Docker-Compose.yml file to change to the version you would like to upgrade to. We do not suggest changing the dependency versions of other CluedIn dependencies unless it has been specifically sanctioned and supported by CluedIn. For example, changing the version of one of the data stores could result in issues and will not be covered in the support models we provide. Each version of CluedIn will specifically detail which versions of dependencies that it supports. If you are using Kubernetes to deploy CluedIn, you have built-in support for rolling back a deployment if it fails. This means that you can upgrade or attempt to upgrade your CluedIn instance with full confidence that if it fails, you should not break anything. Downgrading CluedIn Although downgrading is not typically something that is done, there may be times where you need to rollback a deployment. If you have installed CluedIn in Kubernetes you can easily downgrade rollback. If you have used the Helm chart you can then use the native Helm commands to rollback. Alternatively, if you have just used the Helm chart to create the definition files, those will be committed in a repository with source control, so you can just checkout a different branch or commit and apply the changes to Kubernetes."
					}

					
				
			
		
			
				
					,
					

					"docs-05-administration-90-backup-restore-index-html": {
						"id": "docs-05-administration-90-backup-restore-index-html",
						"title": "Backup and Restore",
						"category": "",
						"url": " /docs/05-Administration/90-Backup_Restore/index.html",
						"content": "Backup and Restore of the Managed disks If managed disks are being used to store CluedIn’s databases data, the preferred way of backup and restore is conducted by taking a snapshot of disks with in-built Azure functions from the outside of the cluster. In order to backup a disk and ensure the integrity of data, CluedIn workloads needs to be spun down. First workloads to be shutodwn are the pods that are accessing databases either by writing, or reading. You can use the following kubectl commands: kubectl scale deployment -l role=processing --replicas=0 kubectl scale deployment -l role=main --replicas=0 kubectl scale deployment -l role=crawling --replicas=0 Once containers has finished terminating, we can spin down the databases itself for disks to detach from the nodes: kubectl scale deployment -l app=sqlserver --replicas=0 kubectl scale deployment -l app=neo4j --replicas=0 kubectl scale statefulset -l chart=elasticsearch --replicas=0 kubectl scale deployment -l app=rabbitmq --replicas=0 kubectl scale deployment -l app=redis --replicas=0 kubectl scale deployment -l app=openrefine --replicas=0 Lastly, we can backup the disks using in-built Azure functions as instructed in Azure Disk’s documentation: https: docs.microsoft.com en-us azure backup backup-managed-disks Same documentation can be used to restore from managed disks: https: docs.microsoft.com en-us azure backup restore-managed-disks Upon a successful backup or restore operation, the workloads can be spun back up: kubectl scale deployment -l app=neo4j --replicas=1 kubectl scale statefulset -l chart=elasticsearch --replicas=1 kubectl scale deployment -l app=rabbitmq --replicas=1 kubectl scale deployment -l app=redis --replicas=1 kubectl scale deployment -l app=openrefine --replicas=1 kubectl scale deployment -l app=sqlserver --replicas=1 kubectl scale deployment -l role=processing --replicas=1 kubectl scale deployment -l role=main --replicas=1 kubectl scale deployment -l role=crawling --replicas=1 Backup and Restore using Velero If not using managed disks or looking into snapshotting disks from inside the cluster, you can use Velero. Refer to the official Velero documentation to find guidance on installation: https: velero.io docs v1.5 basic-install Pre-requisites Storage account Blob Container Velero Service Principal Velero server installed Configuration: Subscription Id Resource Group Name Storage Account Name Blob Container Name Backup location Velero SP Account Name AKS Resource Group Name RBAC assignments required: For apiGroup ‘velero.io’ access needs to be granted for all (*) resources in the cluster. Velero SP requires the following roles: o “Microsoft.Compute disks read” for managed disks Contributor role for Storage Account Contributor role for Storage Blob data The following needs to be added to your values.yaml to configure Velero installation: velero: image: repository: vmware-tanzu velero tag: v2.12.17 pullPolicy: IfNotPresent installCRDs: false initContainers: - name: velero-plugin-for-microsoft-azure image: velero velero-plugin-for-microsoft-azure:v1.0.0 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: target name: plugins configuration: provider: azure backupStorageLocation: name: azure-bucket provider: azure bucket: &lt;VELERO_BLOB_CONTAINER_NAME&gt; config: region: &lt;VELERO_REGION&gt; resourceGroup: &lt;VELERO_RESOURCE_GROUP_NAME&gt; subscriptionId: &lt;SUBSCRIPTION_ID&gt; storageAccount: &lt;VELERO_STORAGE_ACCOUNT_NAME&gt; serviceAccount: &lt;VELERO_SP_NAME&gt; volumeSnapshotLocation: name: velero config: apitimeout: \"5m\" resourceGroup: &lt;VELERO_RESOURCE_GROUP_NAME&gt; subscriptionId: &lt;SUBSCRIPTION_ID&gt; credentials: useSecret: true existingSecret: 'cloud-credentials' schedules: daily-backup: schedule: \"0 0 * * *\" template: includeClusterResources: true includedNamespaces: - '*' includedResources: - '*' storageLocation: azure-bucket snapshotVolumes: true ttl: 720h0m0s Backup Backup procedure is scheduled to run every 24 hours at 01:00AM. Backup job time to live is set to 48 hours. Time to live for a backup is set to 72 hours. Schedule is configured via Helm chart at installation, or via Velero CLI. It snapshots &amp; moves the backup automatically to the blob storage without any manual work required. Whole cluster will be backed up as .json or .tar.gz files respectively, and moved to Blob Storage with a timestamp. Read more about how Velero compresses output files. To take a manual backup, execute: velero create backup &lt;backup name&gt; Restore When restoring from the backup, there are a couple of options. Restoring from schedule: This is a preferred way to restore. It means it will restore from the last successfully ran scheduled backup. Execute the following command to restore: velero restore create --from-schedule daily Restoring from manual backup Identify the backup you want to restore: velero backup get Restoring from the backup: velero restore create --from-backup &lt;backup name&gt; Examples A simple PowerShell script to install kubectl on Linux, and spin down the workloads and databases: Param ([Parameter(Mandatory)]$namespace) Write-Host $namespace function Install-Kubectl { # This code is for Linux. # Please, check the official documentation for other options to install kubectl: https: kubernetes.io docs tasks tools #kubectl Write-Host \"Installing kubectl.`n\" -ForegroundColor Yellow curl -LO \"https: dl.k8s.io release $(curl -L -s https: dl.k8s.io release stable.txt) bin linux amd64 kubectl\" mkdir -p ~ .local bin kubectl mv . kubectl ~ .local bin kubectl export PATH=\"$HOME local bin:$PATH\" } function Stop-Deployment { param ( [Parameter(Mandatory=$true)] [string]$namespace, [Parameter(Mandatory=$true)] [string]$name ) Write-Host \" Stopping deployment '$name' in namespace '$namespace'.\" kubectl scale deployment --namespace $namespace -l role=$name --replicas=0 } function Stop-StatefulSet { param ( [Parameter(Mandatory=$true)] [string]$namespace, [Parameter(Mandatory=$true)] [string]$name ) Write-Host \" Stopping stateful set '$name' in namespace '$namespace'.\" kubectl scale statefulset --namespace $namespace -l chart=$name --replicas=0 } Install-Kubectl Write-Host \"Shutting down the workloads.\" -ForegroundColor Cyan Stop-Deployment $namespace \"processing\" Stop-Deployment $namespace \"main\" Stop-Deployment $namespace \"crawling\" # Wait for the pods to shut down $secondsToWait = 60 Write-Host \"`nWaiting $secondsToWait seconds for the workloads to stop.`n\" -ForegroundColor Yellow Start-Sleep $secondsToWait Write-Host \"Stopping the databases:\" -ForegroundColor Cyan Stop-Deployment $namespace \"sqlserver\" Stop-Deployment $namespace \"neo4j\" Stop-StatefulSet $namespace \"elasticsearch\" Stop-Deployment $namespace \"rabbitmq\" Stop-Deployment $namespace \"redis\" Stop-Deployment $namespace \"openrefine\" After the backup is completed, spin up the pods with a script like this: Param ([Parameter(Mandatory)]$namespace) Write-Host $namespace function Start-Deployment { param ( [Parameter(Mandatory=$true)] [string]$namespace, [Parameter(Mandatory=$true)] [string]$name ) Write-Host \" Starting deployment '$name' in namespace '$namespace'.\" kubectl scale deployment --namespace $namespace -l app=neo4j --replicas=1 } function Start-StatefulSet { param ( [Parameter(Mandatory=$true)] [string]$namespace, [Parameter(Mandatory=$true)] [string]$name ) Write-Host \" Starting stateful set '$name' in namespace '$namespace'.\" kubectl scale statefulset --namespace $namespace -l chart=$name --replicas=1 } Write-Host \"Starting the databases:\" -ForegroundColor Cyan Start-Deployment $namespace \"sqlserver\" Start-Deployment $namespace \"neo4j\" Start-Deployment $namespace \"elasticsearch\" Start-Deployment $namespace \"rabbitmq\" Start-Deployment $namespace \"redis\" Start-Deployment $namespace \"openrefine\" # Wait for the pods to start up $secondsToWait = 60 Write-Host \"`nWaiting $secondsToWait seconds for the workloads to start.`n\" -ForegroundColor Yellow Start-Sleep $secondsToWait Write-Host \"Starting the workloads.\" -ForegroundColor Cyan Start-Deployment $namespace \"processing\" Start-Deployment $namespace \"main\" Start-Deployment $namespace \"crawling\""
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-0-index-html": {
						"id": "docs-10-integration-0-index-html",
						"title": "What are integrations",
						"category": "",
						"url": " /docs/10-Integration/0-index.html",
						"content": "Introduction The first thing you need to do when CluedIn is running, is to feed it with Data. You need to choose which data you want to add to CluedIn. Data is pushed into CluedIn via integrations. You have two options Installing an existing integration Building a custom integration There are two main types of integrations: Providers These integrations allow you to add data into CluedIn. They can connect to cloud tools, databases, file systems, etc. extract the data you want to send to CluedIn and assemble it in a format CluedIn will understand. There are many providers available in our GitHub, but alternatively you can also build your own to cater for your specific requirements. In order to do this though, you will require some C# coding experience. Enrichers Their mission is to add extra information to improve data that is already in CluedIn. Data in CluedIn is structured in entities; these are similar to records. They can contain information about a person, a company, a task, etc. An enricher will use the existing information CluedIn to then query other external systems to try to find out more information about that entity, i.e. enrich it. We have a list of available enrichers in our GitHub, but you can also build your own, as long as you have some C# coding experience."
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-crawling-html": {
						"id": "docs-10-integration-crawling-html",
						"title": "Crawling",
						"category": "",
						"url": " /docs/10-Integration/Crawling.html",
						"content": "Crawling servers are stateful but are also transactional in nature i.e. if a complete crawl does not finish, then it will never have a state of “complete”. This also means that if you were to cancel a crawler half way through a job, that you will need to start the crawl from the start again or potentially from the data that had changed state in the source system since the last successful crawl time. Crawlers are responsible for fetching data from a source system. Crawlers are self hosted and when they are run, the CluedIn server is responsible for giving it everything that is necessary to successfully connect and pull the right data from a source system. If you were to cancel a crawl half way through, CluedIn will ingest the data that has been ingested. It is not recommended to run a Crawling server and a Processing Server in this same CluedIn host, due to the change in nature of stateless and stateful environments. There are many crawlers that are provided for you in CluedIn, but if you are building your own, it is important to remember that the crawlers should not contain business logic and should not attempt to fix the data from the source. Crawling data can be complex and error-prone and many things can go wrong such as: Time-outs when calling sources Network errors when calling sources The source changes structure mid crawl Authentication can change mid crawl Source systems don’t have an obvious way to filter the data Your crawlers will need to cater for all of the complexities mentioned above and more. Fortunatly, the CluedIn crawler framework helps with all of the above mentioned complexities. You have full control over the granularity of error handling that you would like in your crawlers e.g. managing how often to retry or how many times. Here is a list of things you should be covering with any custom crawlers that you extend CluedIn with: Supports Webhooks - Does this provider support Webhooks? Are Webhooks Manual or Automatic - Does the provider support programmatic webhook creation or does the user need to manually set this up? Test Webhook processing, creation, deletion. - Does the webhook creation process and deletion process work? Management Endpoint for Webhooks - Is there a link in the User Interface where the user can manage any webhooks that CluedIn creates? Test that you are handling for configuration filters e.g. Folders, Labels. - What configuration can the user set to filter what the crawler goes out and gets? Configuration of Providers -&gt; List, Tree, Setting, Checkbox? Type of Provider - Is it Cloud? Is it On-Premise? Test API Token or Credentials - Does the provider support Developer API tokens. API Rate Limiting - What rules are in place to limit what the crawler does? Test 429 Status Code Handling - Does the crawler handle limiting requests? OAuth2, OAuth, Basic, ApiToken, Custom - What type of authentication can this API support? Remember that you can support many different types of Authentication. OAuth 2 flow endpoints. - What are the Oauth 2 flow endpoints? Refresh Token - How does a provider refresh an access token? Expired Token - When does an access token expire? ReAuth Endpoint - Endpoint in CluedIn to run the ReAuth Process. Requirements and APP to be installed - Does CluedIn need to install an APP before it can talk to it? App Install Url - What is the Install Url of an App if it requires an App to be installed first? Get Permissions - Are there any persmissions set in the provider for read write access? Requires a Paid Account Premium Account - Will CluedIn only work if the provider is a paid account? Configuration for AutoWatching new things in providers - Will CluedIn automatically sync NEW folders and channels that are added into the provider. Paging - How to page results in a reasonable amount of data at a time? Schedule of how often it runs - How often should a crawl run? Filtering on only getting the latest data (delta auto crawls) - By default, CluedIn runs a job every 4 hours for a new crawl. This should only pull the latest data since the previous crawl. Please refer to Delta Auto crawls - Change Data Capture Has a Dynamic Template - Does the provider support custom and dynamic objects e.g. SharePoint Podio Hubspot etc. Getting the Entity Statistics - Does the provider support a way to figure out how much data of different types they have? Does the source use a cursor to get latest or a date filter (what format) - If the provider uses a cursor instead of a date filter. Logo - Does the provider have a good high quality 400x400 image icon of the provider? Description of Provider - Small description of the provider. How to Get Account Id and Account Display - What is the unique ID of the provider per user and what is the friendly name to display in the UI. Id - What is the GUID of the provider? Schema for Connections - Does the crawler connect properly with the right edge type, edge direction. Data formats - Is the data in the format you mapped it to be? Vocabulary mappings - Is all the data mapped to relevant core vocabularies? Test that Dates are in the right format. - If we are using dates in the Vocabulary, do they format to ToString(“o”)?"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-help-20with-20building-20robust-20integrations-html": {
						"id": "docs-10-integration-help-20with-20building-20robust-20integrations-html",
						"title": "Help with building robust integrations",
						"category": "",
						"url": " /docs/10-Integration/Help%20with%20building%20robust%20integrations.html",
						"content": "With the CluedIn team building just over 220 integrations to date, we have learnt quite a lot of tips and tricks in guiding developers to build good and robust integrations. Here is an exhaustive list of advice from the team to help you also build robust integrations. Make sure you attend the 3 day developer training given by the CluedIn team. You will be given first hand advice and labs on building solid integrations. Your integrations are responsible for fetching data, not cleaning it, not access control, not joining or blending data. In an ETL world, CluedIn wants your integrations to be more like ELT i.e. just extract and load. The CluedIn server will do the “T” or the Transform. Build your crawlers with a mindset that the structure can and will change on a regular basis (even if it does not). Many source systems will contain “Discovery Endpoints” in which you can “sniff” what types of data is possible for you to fetch. These are great to use, because they make your integration more dynamic than static. In this way you can build integrations that are very flexible and can use the source system to discover what objects or data is available and then “iterate” over all objects to fetch all the rows or instances of those objects. It is quite typical that these types of sources will also describe the data in forms of “type”, “constraints” and more. Use the inbuilt Crawler Validation Framework within the Crawler Templates to help validate if you are building your crawler with best practices. Your crawlers may be responsbile for fetching billions of rows of data and hence this should be designed into your crawler as well. This means that practices like paging, filtering and more is recommended. You might find that many CluedIn integrations will use “Service Accounts” to access data. This means that CluedIn will need to “iterate” all the individual accounts that this “Service Account” has access to. There is a default timeout on crawlers in that if they have not fetched data within a 10 minute window, the CluedIn server assumes that something has gone wrong and will terminate the job. You can sometimes cause this accidentally or it might be that sometimes you will need to increase this timeout. For example, if your integration was built in a way to sort a table of data before ingesting it (because of a performance reason) then you might find that your table takes longer than 10 minutes to sort before it can start iterating and ingesting data from the platform. Increasing the timeout is fine and supported. You can change this in your container.config file. The best way to learn how to build a new integration is typically to view the over 220 prebuilt integrations as to see what the final solution looks like. You will notice that CluedIn is using the advice above in its integrations as to also abide by best practices. You can probably imagine that an integration that we build and deploy today, may change requirements tomorrow. Due to the nature of the integrations being very simple in nature and just tasked with fetching data, you will often find that the common failures are due to the underlying system changing in some way. The typical issues are: The location of the source has changed e.g. a connection string or physical location. The data itself has changed structure and can no longer be properly deserialised into the JSON or XML format that CluedIn is asking for. There are some of these changes that are more drastic than others and CluedIn integrations can be setup in a way to “survive” some changes without the need for new deployments and changes to integration code. For example, if your underlying data has simply added a new column or property, CluedIn can “survive” this change, but eventually you should map that new property into its rightful Entity Code, Edge, Vocabulary or other. At least data will continue to flow from source to CluedIn. If the object structure changes dramatically, you will notice that CluedIn will throw notifications and alerts that we detect that a change is dramatic enough to stop the flow of data suffering from this issue until it is addressed. Making the changes in your code and redeploying will address this issue and data will continue to flow. You might also find that endpoints (REST) change, but we do find that most providers are quite good at versioning their endpoints so that data will continue to flow. However, in this case you might find that changing to the new endpoints will require to to run a complete re-crawl as you might be getting more rich data from the new endpoints. Therefor some sources might be sophisticated enough that we don’t require to fetch all data, but only the new fields that we are interested in. This complicates the crawler too much and it should potentially be avoided. It really depends on the source itself and if you find that you are charged a lot of money for pulling data. There are many integration sources that can be like this and it means that you may have to control this in your integration code. The CluedIn Crawler framework ships with a validation framework in which this can be extended as well. To add new Rules, all you need to do is implement the IClueValidationRule interface and then compile and drop this into your CluedIn directory to reboot. The Framework currently watches for: DATA_001_File_MustBeIndexed ENTITYTYPE_001_Person_MustNotBeUsedDirectly ENTITYTYPE_002_Document_MustNotBeUsedDirectly METADATA_001_Name_MustBeSet METADATA_002_Uri_MustBeSet METADATA_003_Author_Name_MustBeSet METADATA_004_Invalid_EntityType METADATA_005_PreviewImage_RawData_MustBeSet METADATA_006_Created_Modified_Date_InFuture METADATA_007_Created_Modified_Date_InPast METADATA_008_Created_Modified_Date_UnixEpoch METADATA_009_Created_Modified_Date_MinDate METADATA_010_Created_Modified_Date_MaxDate EDGES_001_Outgoing_Edge_MustExist EDGES_002_Incoming_Edge_ShouldNotExist PROPERTIES_001_MustExist PROPERTIES_002_Unknown_VocabularyKey_Used PROPERTIES_003_Value_ShouldNotBeQuoted"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-home-20screen-html": {
						"id": "docs-10-integration-home-20screen-html",
						"title": "Home Screen",
						"category": "",
						"url": " /docs/10-Integration/Home%20Screen.html",
						"content": "Your integration Home Screen will give you details of how many data sources you have connected and actions to be able to add new integrations or configure existing sources. You will also be able to see information on the amount of data that is stored in CluedIn based off the data you have ingested. Your integration home screen will notify and alert you if there is a data source that has stopped working. This could be for many reasons, but it typically falls within: The authentication has stopped working due to expiry of authentication tokens or change in authentication. The configuration needs attention e.g. a data source has moved location and will need to be updated to support a new source location. An Administrator has paused or stopped the integration. CluedIn will maintain the state of when the integration has stopped working and so on resolution, we know exactly the offset or point in time in which we need to sync data to get it back to a 100% sync state."
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-push-20data-20to-20cluedin-html": {
						"id": "docs-10-integration-push-20data-20to-20cluedin-html",
						"title": "Push Data to CluedIn",
						"category": "",
						"url": " /docs/10-Integration/Push%20data%20to%20CluedIn.html",
						"content": "There are many ways to get data to CluedIn, including the prebuilt connectors, Change Data Capture (via our support for Debezium) and webhooks. CluedIn also supports data to be pushed to it via HTTP POSTS. To do this, you will need to create an “Ingestion Endpoint” in the user interface in which CluedIn will create a public route, sitting behind Bearer Authentication, that any third part system can push a JSON array of any object to. CluedIn will store all JSON pushed to this point in a sandbox until a user has mapped the data within the UI, in which it will then maintain a live stream of incoming data. For example, you can see push data to CluedIn for popular Data Integration services like Azure Data Factory and Apache NIFI. Apache NIFI: View Video Azure Data Factory: View Video"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-add-integration-html": {
						"id": "docs-10-integration-add-integration-html",
						"title": "How to add an Integration",
						"category": "",
						"url": " /docs/10-Integration/add-integration.html",
						"content": "Introduction An integration should not be specific to a user. When an integration is installed, it generally takes a number of parameters to be able to add it multiple times for different user. Eg: Slack account of multiple organizations, multiple shared email inbox from the same Exchange server, multiple Office 365 accounts… Please refer to the documentation on How to build an integration When installing an integration, if you want CluedIn to only read, add the integration with a user that has read-only on ALL information if you want CluedIn to push-back to the integration, add it with an administrator account Types of integration Cloud integration Cloud integration are generally integration for existing SaaS product such as Hubspot, Slack, Dropbox. To authenticate to this integration, we have 3 authentication methods which variates based on the product. Notice, if you are running CluedIn, on-prem, you will need to setup the Oauth process with the product you want to add. Refer to the provider’s documentation and follow the steps. Oauth authentication In this type of integration, you will be redirect to the Integration’s website where he will ask your permission for CluedIn to access the data. API Token In this type of integration, you will need to provide a valid API token so CluedIn could access the data. Form Fields Sometimes, the integration required multiple fields such as a ‘URL’, a username… Be sure to have the correct information before adding them. On-premise integration Another type of integration are ‘on-prem’, they are integration that you need to install on your servers. A good example is a File system provider which will scan all the files located into a physical hard-drive. Adding an Integration Login to CluedIn Go to the integration section Click on Available integrations. Click on ‘Add configuration’ Follow the authentication process Configure your integration and add it Congratz, your configuration is now added Data coming in Once the integration added, CluedIn will ingest the date, once that is done, you will receive a notification. Product Owner You can set the Product Owner when you add an integration. You can have multiple Product Owners and it can change throughout the lifetime of the integration. Setting the Product Owner will dictate certain actions and responsibilities. These responsibilites include: They are responsible for accepting or rejecting Mesh Commands. They will be notified when a Subject Request Access contains data for this integration point. They will be responsible for the CluedIn Clean projects that contain data from this integration point. They will be notified when their system is involved in a Data Breach. They will be respomsible for accepting or rejecting the Data involved in a retention setup. They are responsible for setting the Consent for the properties in their integration. They are responsible for resolving duplicates in their integration point. They are responsible for the provider specific Vocabularies and the mappings to core Vocabularies. Integration Access"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-build-enricher-html": {
						"id": "docs-10-integration-build-enricher-html",
						"title": "Build Enricher",
						"category": "",
						"url": " /docs/10-Integration/build-enricher.html",
						"content": "Introduction In CluedIn, an Enricher allows you to take input from data flowing through the processing pipeline and then lookup services (typically external API’s) as to enrich just a single particular entity. In our HelloWorld example (see [CluedIn.Enricher.HelloWorld(]https: github.com CluedIn-io CluedIn.Enricher.HelloWorld)) we will obtain Person data from an external source called JSON Placeholder Pre-requesite CluedIn is a .NET platform. So you will need: .NET installed Visual Studio installed Docker Creating initial template To avoid cumbersome boilerplating, CluedIn provides you a script to generate a working Visual studio solution. Install the generation tool You will need to install node, npm, yeoman and the generator itself. First, install Yeoman and generator-cluedin-externalsearch using npm (we assume you have pre-installed node.js). Create a new folder to store the new project, and from the command prompt, run the following to install the generator npm install -g yo npm install -g generator-cluedin-externalsearch then run the generator, providing a Name and Entity Type (ie Person, Company etc), with the following: yo cluedin-externalsearch See the Naming Integrations in the Build Integration documentation for more information on how the CluedIn Server finds and loads type from Integration assemblies. Adding models First we will setup a User class to receive the data. See User.cs: public class User { public int id { get; set; } public string name { get; set; } public string username { get; set; } public string email { get; set; } } Adding client Next we will add a client class, and associated interface, to fetch data from the external source. See HelloWorldClient.cs and IHelloWorldClient.cs public class HelloWorldClient : IHelloWorldClient { private const string BaseUri = \"https: jsonplaceholder.typicode.com\"; private readonly IRestClient _client; public HelloWorldClient(IRestClient client) { _client = client ?? throw new ArgumentNullException(nameof(client)); client.BaseUrl = new Uri(BaseUri); } public async Task&lt;User&gt; GetUser(string id) =&gt; await GetAsync&lt;User&gt;($\"users {id}\"); private async Task&lt;T&gt; GetAsync&lt;T&gt;(string url) { var request = new RestRequest(url, Method.GET); var response = await _client.ExecuteTaskAsync(request); if (response.StatusCode != HttpStatusCode.OK) { var diagnosticMessage = $\"Request to {_client.BaseUrl}{url} failed, response {response.ErrorMessage} ({response.StatusCode})\"; throw new InvalidOperationException($\"Communication to jsonplaceholder unavailable. {diagnosticMessage}\"); } var data = JsonConvert.DeserializeObject&lt;T&gt;(response.Content); return data; } } public interface IHelloWorldClient { Task&lt;User&gt; GetUser(string id); } Adding Vocabulary Then we will define our vocabulary classes. See HelloWorldVocabulary.cs and HelloWorldVocabularies.cs public class HelloWorldVocabulary : SimpleVocabulary { public HelloWorldVocabulary() { VocabularyName = \"HelloWorld User\"; KeyPrefix = \"helloworld.user\"; KeySeparator = \".\"; Grouping = EntityType.Unknown; AddGroup(\"HelloWorld Details\", group =&gt; { Id = group.Add(new VocabularyKey(\"Id\", VocabularyKeyDataType.Integer, VocabularyKeyVisibility.Visible)); Name = group.Add(new VocabularyKey(\"Name\", VocabularyKeyDataType.PersonName, VocabularyKeyVisibility.Visible)); Username = group.Add(new VocabularyKey(\"Username\", VocabularyKeyDataType.PersonName, VocabularyKeyVisibility.Visible)); Email = group.Add(new VocabularyKey(\"Email\", VocabularyKeyDataType.Email, VocabularyKeyVisibility.Hidden)); }); } public VocabularyKey Id { get; private set; } public VocabularyKey Name { get; private set; } public VocabularyKey Username { get; private set; } public VocabularyKey Email { get; private set; } } public static class HelloWorldVocabularies { public static HelloWorldVocabulary User { get; } = new HelloWorldVocabulary(); } Adding the Provider Lastly we will add the provider class (see HelloWorldExternalSearchProvider.cs) public class HelloWorldExternalSearchProvider : ExternalSearchProviderBase { private static readonly Guid ProviderId = Guid.Parse(\"2261b8f8-00b7-45bb-8112-5cc897fb16d8\"); TODO replace with new guid private readonly IHelloWorldClient _client; public HelloWorldExternalSearchProvider(IHelloWorldClient client) : base(ProviderId, EntityType.Person) { _client = client; } public override IEnumerable&lt;IExternalSearchQuery&gt; BuildQueries(ExecutionContext context, IExternalSearchRequest request) { if (!Accepts(request.EntityMetaData.EntityType)) yield break; var entityType = request.EntityMetaData.EntityType; var id = request.QueryParameters.GetValue(HelloWorldVocabularies.User.Id, new HashSet&lt;string&gt;()); var person = new Dictionary&lt;string, string&gt; { { \"id\", id.FirstOrDefault() } }; if (person.Any()) yield return new ExternalSearchQuery(this, entityType, person); } public override IEnumerable&lt;IExternalSearchQueryResult&gt; ExecuteSearch(ExecutionContext context, IExternalSearchQuery query) { var id = query.QueryParameters[\"id\"].FirstOrDefault(); if (string.IsNullOrEmpty(id)) yield break; var user = _client.GetUser(id).Result; if (user != null) yield return new ExternalSearchQueryResult&lt;User&gt;(query, user); } public override IEnumerable&lt;Clue&gt; BuildClues(ExecutionContext context, IExternalSearchQuery query, IExternalSearchQueryResult result, IExternalSearchRequest request) { var resultItem = result.As&lt;User&gt;(); var code = GetOriginEntityCode(resultItem); var clue = new Clue(code, context.Organization); PopulateMetadata(clue.Data.EntityData, resultItem); return new[] {clue}; } public override IEntityMetadata GetPrimaryEntityMetadata(ExecutionContext context, IExternalSearchQueryResult result, IExternalSearchRequest request) { var resultItem = result.As&lt;User&gt;(); return CreateMetadata(resultItem); } public override IPreviewImage GetPrimaryEntityPreviewImage(ExecutionContext context, IExternalSearchQueryResult result, IExternalSearchRequest request) { return null; } private IEntityMetadata CreateMetadata(IExternalSearchQueryResult&lt;User&gt; resultItem) { var metadata = new EntityMetadataPart(); PopulateMetadata(metadata, resultItem); return metadata; } private EntityCode GetOriginEntityCode(IExternalSearchQueryResult&lt;User&gt; resultItem) { return new EntityCode(EntityType.Person, CodeOrigin.CluedIn.CreateSpecific(\"helloworld\"), resultItem.Data.id); } private void PopulateMetadata(IEntityMetadata metadata, IExternalSearchQueryResult&lt;User&gt; resultItem) { var code = GetOriginEntityCode(resultItem); metadata.EntityType = EntityType.Person; metadata.Name = resultItem.Data.name; metadata.OriginEntityCode = code; metadata.Codes.Add(code); metadata.Properties[HelloWorldVocabularies.User.Email] = resultItem.Data.email; } }"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-build-integration-html": {
						"id": "docs-10-integration-build-integration-html",
						"title": "Build Integration",
						"category": "",
						"url": " /docs/10-Integration/build-integration.html",
						"content": "Introduction CluedIn official integrations are one size fits all integrations. They will generally try to ingest as much data as they can. If you want to ingest data in a precise fashion or want to ingest data from an in-house tool, an old tool, from some custom APIs, you will need to create your own integration. Pre-requesite CluedIn is a .NET platform. So you will need: .NET installed Visual Studio installed Docker Creating initial template To avoid cumbersome boilerplating, CluedIn provides you a script to generate a working Visual studio solution. Create a folder for your provider mkdir my-first-integration cd my-first-integration Run the generator docker run --rm -ti -v ${PWD}: generated cluedin generator-crawler-template The generator will ask some questions and then generate all your solution files: _-----_ ╭──────────────────────────╮ | | │ Welcome to the awesome │ |--(o)--| │ CluedIn integration │ `---------´ │ generator! │ ( _´U`_ ) ╰──────────────────────────╯ ___A___\\ | ~ | __'.___.'__ ´ ` |° ´ Y ` ? Name of this crawler? MyFirstIntegration ? Will it support webhooks? No ? Does it require OAuth? No Initialize a git repo git init git add . git commit -m \"Initial commit\" Open the solution in Visual Studio and build it or alternatively you should also build it from the command line using the dotnet cli: dotnet build Adding a Model There are several steps needed to create a crawler that fetches data, creates Clues and passes them back to CluedIn for processing. Please refer to our Hello World sample repository for a working example. This is based on a simple external JSON service The following is the minimal steps required to replicate the Hello World example: Create model classes. You can use a subgenerator for this: docker run --rm -ti -v ${PWD}: generated cluedin generator-crawler-template crawler-template:model Answer the questions as follows, to create a User model and vocabulary, similar to the one in the example User.cs _-----_ ╭──────────────────────────╮ | | │ This sub-generator │ |--(o)--| │ allows to create new │ `---------´ │ vocabularies │ ( _´U`_ ) ╰──────────────────────────╯ ___A___\\ | ~ | __'.___.'__ ´ ` |° ´ Y ` ? What is the model name? User ? What is the entity type? Person ? Enter a comma separated list of properties to add to the model id,name,username,email ? Choose the visibility for key: id(undefined) Visible ? Choose the type for key id Integer ? Should key id map to a common vocab? None ? Choose the visibility for key: name(undefined) Visible ? Choose the type for key name Text ? Should key name map to a common vocab? None ? Choose the visibility for key: username(undefined) Visible ? Choose the type for key username Text ? Should key username map to a common vocab? None ? Choose the visibility for key: email(undefined) Hidden ? Choose the type for key email Email ? Should key email map to a common vocab? ContactEmail create src MyFirstIntegration.Core Models User.cs create src MyFirstIntegration.Crawling ClueProducers UserClueProducer.cs create src MyFirstIntegration.Crawling Vocabularies UserVocabulary.cs create test MyFirstIntegration.Crawling.Unit.Test ClueProducers UserClueProducerTests.cs This will generate 4 files as shown above. If you try to run the tests you will notice there is a failing one, as we need to complete some work in the ClueProducer. Go to the src MyFirstIntegration.Crawling ClueProducers UserClueProducer.cs file, in line 29 uncomment the following code: if(input.Name != null) data.Name = input.Name; Delete all other comments in the UserClueProducer.cs file. Open the src MyFirstIntegration.Infrastructure MyFirstIntegrationClient.cs and modify line 16 with the URL for the endpoint: private const string BaseUri = \"https: jsonplaceholder.typicode.com\"; Since this is a public endpoint we don’t need to pass any tokens. Remove or comment out line 42 client.AddDefaultParameter(\"api_key\", myfirstintegrationCrawlJobData.ApiKey, ParameterType.QueryString);` Add a method to retrieve users (you will need to import some namespaces too): public async Task&lt;IList&lt;User&gt;&gt; GetUsers() =&gt; await GetAsync&lt;IList&lt;User&gt;&gt;(\"users\"); In the src MyFirstIntegration.Crawling MyFirstIntegrationCrawler.cs you retrieve the data you want to insert in CluedIn. Add the following inside the GetData method: retrieve data from provider and yield objects foreach( var user in client.GetUsers().Result) { yield return user; } In order to test the provider, you can use the Integration test provided. Open the test integration Crawling.MyFirstIntegration.Integration.Test MyFirstIntegrationDataIngestion.cs file, and in the CorrectNumberOfEntityTypes method add a new annotation to indicate the expectation of receiving 10 Persons (that’s what the sample endpoint returns by default): [Theory] [InlineData(\" Provider Root\", 1)] [InlineData(\" Person\", 10)] public void CorrectNumberOfEntityTypes(string entityType, int expectedCount) Execute the tests - they should all pass. Before adding the integration to CluedIn, open the file src\\MyFirstIntegration.Core\\MyFirstIntegrationConstants.cs and modify the values for the constants before the TODO comment. This information will be used in the GUI of CluedIn to show information about the integration. In particular you should set the CrawlerDescription, Integration, Uri (if this integration corresponds to an online tool), and IconResourceName. This last property corresponds to the path of an embedded resource in the Provider project. Architecture As you can see in the example - these are the main components: A client that knows how to retrieve data from your source (e.g. MyFirstIntegrationClient.cs). It has methods to produce plain objects with the information. The method GetData in the main Crawling class MyFirstIntegrationCrawler.cs - you can consider this as the entrypoint for the provider. This method will invoke the correct methods of the client, in order to yield plain objects. A Vocabulary class (e.g. UserVocabulary.cs) which is for the most part generated automatically. This class defines the different keys of the data you are processing and how they map to generic terms (email, address, company) also in use in other sources. In addition it can define the relationship with other Vocabularies (also known as edges). For example the relationship between a user and a company. A ClueProducer (e.g. UserClueProducer.cs) which essentially translates the plain object (retrieved by the client) into a clue, which is the object understood by CluedIn. It uses the keys from the Vocabulary to map the data from the object to the clue. In this case the sample API was very open and generic, however in other cases you may need extra information (credentials, datasources, etc.) on how to connect to the source, or what data to retrieve. This can be captured in the CrawlJobData (e.g. MyFirstIntegrationCrawlJobData.cs). You can enrich it with whatever properties you need. However, you will also need to expand two methods in the Provider (e.g. MyFirstIntegrationProvider.cs): GetCrawlJobData which translates the keys from a generic dictionary into the CrawlJobData object and GetHelperConfiguration which performs the opposite translation (from the CrawlJobData to a dictionary) Deploying the provider locally If you are running CluedIn locally for testing purposes using Docker, you can follow these instructions to add the integration. You most likely used the Home github repo to pull your CluedIn environment down and boot it up. You can now use this to inject extra components into CluedIn. Under the env folder you can use the default folder or you can create new environments (See Home Github Readme). Within this folder there is a components folder. Create a new folder in here called ServerComponent. This is essentially a folder in which you can inject your own DLL files and CluedIn will look in this folder on boot of the CluedIn Server Docker Container and load these aseemblies as well. In the example of a Crawler, you will need to copy the Dll files produced by your different projects (not including the test dll’s), the .json dependency file, any third party libraries you used in your crawler (e.g. a custom Nuget package for talking to a service) and optionally you will want the pdb files if you would like to debug. Copy all of these into your newly created ServerComponent folder and restart the CluedIn Server Docker container. Make sure that the version of your CluedIn dependencies are exactly the same as the version you are running of CluedIn. You can check this in your packages.props file. &lt;PropertyGroup Label=\"Dependency Versions\"&gt; &lt;_ComponentHost&gt;2.0.0-alpha-14&lt; _ComponentHost&gt; &lt;_AutoFixture&gt;4.11.0&lt; _AutoFixture&gt; &lt;_CluedIn&gt;3.2.2&lt; _CluedIn&gt; &lt; PropertyGroup&gt; Testing the provider in your environment Please refer to install an integration Generating Models, Vocabularies and ClueProducers Please refer to the FileGenerator GitHub Repository. This can be used to generate basic models, vocabularies and clue producers using one of three options: Metadata file; CSV files with data; Microsoft SQL Server. The generators need to be updated depending on each data source - more details can be found in the README section of the repository."
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-delta-crawls-html": {
						"id": "docs-10-integration-delta-crawls-html",
						"title": "Delta Crawls",
						"category": "",
						"url": " /docs/10-Integration/delta-crawls.html",
						"content": "What are delta crawls By default, when adding a new provider, CluedIn sets up two recurring jobs - one that runs every 4 hours and one that runs every 7 days. These jobs can be customised for different intervals or simply disabled. The weekly job is supposed to run a full crawl again. The one that runs every 4 hours is called delta crawl - a crawl job designed to bring only the data that is new and hasn’t been ingested into CluedIn yet. To setup the crawler to support delta crawls as well as full crawls, you can use the LastCrawlFinishTime property of the JobData object that is available in the crawlers. Once a crawl job has finished, this property is updated with the corresponding date and time and it is available at runtime in the crawler. An example of how delta crawls can be implemented while still accepting full crawls can be found below: In the crawler’s JobData, we can add a new configurable property as such: public bool FullCrawl { get; set; } = false; And in the JobData’s constructor, we can give the property the value from configuration: if (configuration.ContainsKey(\"FullCrawl\") &amp;&amp; !string.IsNullOrEmpty(configuration[\"FullCrawl\"].ToString())) FullCrawl = GetValue&lt;bool&gt;(configuration, \"FullCrawl\"); In the Client class we check for the FullCrawl’s value - if it’s false, we continue with the delta crawl; if it’s true, we continue with the full crawl. Below we can find two examples of how this can be implemented for an API and for SQL. Using Eloqua’s Bulk API publicly available filters documentation: https: docs.oracle.com en cloud saas marketing eloqua-develop Developers BulkAPI Tutorials Filtering.htm. var exportDefinitionRequest = new ExportDefinitionRequest { Name = \"CluedIn Contact Export\", Fields = new Dictionary&lt;string, string&gt; { { \"EmailAddress\", \"Contact.Field(C_EmailAddress)\"}, { \"FirstName\", \"Contact.Field(C_FirstName)\"}, { \"LastName\", \"Contact.Field(C_LastName)\"} } }; if (!_jobData.FullCrawl &amp;&amp; _jobData.LastCrawlFinishTime &gt; DateTime.MinValue) { exportDefinitionRequest.Filter = \"'Contact.Field(C_DateModified)' &gt; '\" + $\"{_jobData.LastCrawlFinishTime.ToString()}\" + \"'\"; } var exportDefinitionResponse = await PostAsync&lt;ExportDefinitionResponse&gt;(\"contacts exports\", exportDefinitionRequest); return exportDefinitionResponse; Using an imaginary Oracle SQL database example: public IEnumerable&lt;SqlEntity&gt; GetObject() { var offsetInitValue = GetInitialOffset(); var maxNumberOfRows = GetMaxNumberOfRows(tableName); var whereStatement = string.Empty; if (!_jobData.FullCrawl &amp;&amp; _jobData.LastCrawlFinishTime &gt; DateTime.MinValue) whereStatement = $\"WHERE ModifiedDateColumn &gt; {_jobData.LastCrawlFinishTime}\"; for (var offset = offsetInitValue; offset &lt; maxNumberOfRows; offset += _jobData.PageSize) { using (var connection = new OracleConnection(_jobData.ConnectionString)) using (var command = connection.CreateCommand()) { OracleDataReader reader = null; try { connection.Open(); reader = ActionExtensions.ExecuteWithRetry(() =&gt; { command.CommandText = $@\"SELECT f.* FROM ( SELECT t.*, rownum r FROM ( SELECT * FROM SqlTable {whereStatement} ORDER BY ModifiedDateColumn) t WHERE rownum &lt;= {offset + _jobData.PageSize}) f WHERE r &gt; {offset}\"; command.CommandTimeout = 180; return command.ExecuteReader(); }, isTransient: ex =&gt; ex is OracleException || ex.IsTransient()); } catch (Exception exception) { _log.LogError(exception.Message, exception); yield break; } while (reader.Read()) { SqlEntity sqlEntity = null; try { sqlEntity = new SqlEntity(reader); } catch (Exception exception) { _log.LogError(exception.Message, exception); continue; } if(sqlEntity != null) yield return sqlEntity; } } } } Please note that the syntax Contact.Field(&lt;field name&gt;) should be put in double brackets (as per API’s documentation), which have not been included in the code snippet for formatting reasons. Stream Delta Crawls Delta crawls can also be setup as consumers for stream platforms such as Kafka. If a Kafka stream has not been set up yet, make sure Change Data Capture is enabled for the database and CluedIn can implement Debezium as a Kafka stream (if the database of choice is supported). After we implemented Debezium, we then integrate the crawlers with the stream. One simple example of how a Client can be implemented with Kafka can be found in our public Kafka example crawler."
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-install-integration-html": {
						"id": "docs-10-integration-install-integration-html",
						"title": "Install Integration",
						"category": "",
						"url": " /docs/10-Integration/install-integration.html",
						"content": "Installing integrations in Kubernetes Via the helm chart In a production environment, using Kubernetes, you can add the components you want to install through the values.yml file. You can specify your own packages and the versions to be installed. You can also provide your own package feeds, authentication, and even an alternative installer image. cluedin: components: image: '' # name of the container to use as an installer - will default to 'cluedin # nuget-installer' packages: [] # list of extra Nuget Packages to install in server in name, or name version pairs # version should be a supported nuget version format. sources: {} # Nuget sources to use At pod startup time, the packages will be passed from an init container to the CluedIn container. Configuring Packages Packages should be listed using their full package id and an optional version. When supplying the version, you may use floating versions to allow the version to be resolved at startup time. cluedin: components: packages: - name: CluedIn.Crawling.HubSpot - name: CluedIn.Provider.HubSpot version: 3.0.0-* In this example the latest version of CluedIn.Crawling.HubSpot will be installed, while the latest 3.0.0 pre-release, or full-release version of CluedIn.Provider.HubSpot will be installed. Configuring Sources The packages to install may be resolved from one or more nuget feeds. Each feed can be configured under sources with optional authentication details cluedin: components: sources: nuget: url: https: api.nuget.org v3 index.json custom: url: https: myorg.myget.org F customfeed api v3 index.json user: OrgUser pass: OrgPass Via Custom Image You can build your own docker image to contain the integration packages. This has the advantage that the packages and assets are already contained in the init container and do not need to be downloaded. Create a packages.txt file with the integrations to be installed. Versions can be specified after the package name CluedIn.Crawling.HubSpot CluedIn.Provider.HubSpot 3.0.0-* Create a nuget.config with the feeds to be used. This is a standard nuget.config. &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;configuration&gt; &lt;packageSources&gt; &lt;add key=\"custom\" value=\"https: myorg.myget.org F customfeed api v3 index.json\" &gt; &lt; packageSources&gt; &lt; configuration&gt; The public nuget.org feed is already included by default Use the following Dockerfile to build your image FROM cluedin nuget-installer as base # If your feed requires passing credentials ARG CUSTOMCRED ENV NUGETCRED_CUSTOM=$CUSTOMCRED COPY . . packages RUN . Install-Packages.ps1 FROM alpine as final COPY --from=base . components . tmp components ENTRYPOINT [\"sh\", \"-c\", \"false | cp $(ls -d tmp components * ) -ir . components 2&gt; dev null\"] If one or more feeds requires credentials, you’ll need to expose an ARG and an ENV named NUGETCRED_&lt;feedname&gt; to allow passing to the install script. Validate your custom image # Build the image &gt; docker build -t my cluedin-installer . # OR build the image with creds - pass aregument as pipe delimited user|password &gt; docker build -t my cluedin-installer --build-arg CUSTOMCRED='user|pass' . # Mount components folder which is where the integration components would be placed &gt; docker run --rm -it -v '.: components' my cluedin-installer In the values.yal configure your custome image for the components install cluedin: components: image: 'my cluedin-installer'"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-integration-compatible-with-ui-html": {
						"id": "docs-10-integration-integration-compatible-with-ui-html",
						"title": "Integration and the CluedIn UI",
						"category": "",
						"url": " /docs/10-Integration/integration-compatible-with-UI.html",
						"content": "Introduction To be able to let someone adding and managing an Integration from the user interface, you need to add some information in your provider. If you don’t, the provider will be valid but you will deteriorate the User experience of the product. IExtendedProviderMetadata interface To provide the information to the CluedIn.UI, you will need to implement the IExtendedProviderMetadata interface. Please read the sections below to understand what each property of that interface is used for. A concrete example: HelloWorldProvider.cs The code: public interface IExtendedProviderMetadata { string Icon { get; } see notes below string Domain { get; } The Url to your application string About { get; } A sentence describing the purpose of your application string AuthMethods { get; } A serialised JSON array of Authentication Methods supports (TODO Obtain list of ) string Properties { get; } TODO find out how this is used by UI string ServiceType { get; } IE CRMType etc (TODO get full list) string Aliases { get; } TODO find out how used in the UI Guide Guide { get; set; } Instructions on how to configure your Provider in the UI string Details { get; set; } Details of how this provider will interact with your application string Category { get; set; } The category your provider fall under to allow filtering in the UI string Type { get; set; } Available options [cloud, on-premise] } Icon The Icon is used by the CluedIn.UI to quickly show to all users what the provider is. The ‘ideal’ size of the image is 128x128px. For your icon to be found, you must add it as an Embedded Resource via the Build Action property in your Provider project. See Build actions. The convention we are using is to place the icon image file under a Resources folder. The Icon property above must point to this file using ‘.’ notation rather than ‘'. For example: - Provider.HellowWorld.csproj \\Resources \\cluedin.png would be represented as Resources.cluedin.png Here is an example of how the Provider Icon is used in the application: Domain This is the URL of your application. Leave it empty if the integration does not have a website. The value is used by the CluedIn.UI to redirect to the integration’s website if he needs more information. Example: if you build a Slack integration, you would have: https: slack.com assigned to the Domain property. About About is the description of Integration. Example: For a Zendesk integration, you would write: Zendesk makes better experiences for agents, admins, and customers. As employees, we encourage each other to grow and innovate. AuthMethods The authentication methods is a JSON object used to explain to the CluedIn.UI how the user needs to authenticate towards the integration. Oauth CluedIn provides you a mechanism to get permission on the integration that requires an ‘Oauth’ dance, never the less, you still need to add ‘some’ URL for the UI to know where he should redirect correctly. Example: \"authMethods\": { \"oauth\": { \"oauthCallbackKey\": \"office365azureactivedirectory\", \"initial\": \"api office365azureactivedirectory?authError=none\", \"callback\": \"api office365azureactivedirectory?authError\", }, }, NOTE: For future version, CluedIn will work to remove the needs of those value by creating a generic controllers, but due to some ‘exceptions’ we have encounter with some Oauth mechanism, it is still required to mention those values. Credentials Credentials is generally use for the system that requires a BASIC authentication. This will be used by the integration to pull the data. Example: \"credentials\": [{ \"displayName\": \"User Name\", \"type\": \"input\", \"isRequired\": true, \"name\": \"username\", }, { \"displayName\": \"Password\", \"name\": \"password\", \"type\": \"password\", \"isRequired\": true, }], API Token Some integrations require sometimes an API token to be passed along the request. Example: \"token\": [{ \"displayName\": \"Api Token\", \"type\": \"input\", \"isRequired\": true, \"name\": \"apiToken\", }], Custom If you need a ‘custom’ field to be sent to your Integration, you can use the Credentials object with extra fields. \"credentials\": [{ \"type\": \"subdomain\", \"name\": \"websiteName\", \"displayName\": \"Website Name\", \"protocol\": \"https: \", \"domain\": \"zendesk.com\", \"isRequired\": true, }, { \"type\": \"input\", \"name\": \"username\", \"displayName\": \"Username\", \"isRequired\": true, }, { \"type\": \"password\", \"name\": \"password\", \"displayName\": \"Password\", \"isRequired\": true, }], Properties The properties used to setup more precisely what you want to crawl from that integration. If you want the integration to get ALL data, leave it empty but from time to time, you want the User adding the integration to pick a specific ‘project’ or ‘folder’ or any other kind of segmentation that your integration might have. Example: List of Projects [{ \"displayName\": \"Projects to include\", \"type\": \"list\", \"isRequired\": true, \"name\": \"projects\", \"options\": { \"key\": \"Id\", \"displayName\": \"Name\", \"stateField\": \"State\", \"states\": [\"ACTIVE\", \"INACTIVE\"], } }] displayName: the label that would be displayed in the UI once the integration is rendered. type: The type of data that would be returned list or tree. isRequired: mentioned if it is needed for the User to setup this information. name: The name of the field to setup (taken from the HelperConfiguration). options: The ‘value’ that should be set for each value selected by the user. Example: Tree of folders [{ \"displayName\": \"Folders to include\", \"name\": \"folders\", \"type\": \"tree\", \"isRequired\": true, }] NOTE: In the case you more options, please contact us. Type A list of type for the integration. Useful when you have hundreds of integration installed. Values can be: “Cloudfile” “Support” “CRM” “Social” “Code” “Task” “Communication” Example: type: [\"Task\", \"Support\"] In the UI:"
					}

					
				
			
		
			
				
					,
					

					"docs-10-integration-modify-integration-html": {
						"id": "docs-10-integration-modify-integration-html",
						"title": "Modify an Integration",
						"category": "",
						"url": " /docs/10-Integration/modify-integration.html",
						"content": "There will be many times where you will need to change an integration. This may be due to changes in the source, changes in versions or fixing mistakes. There are some situations where you may need to cleanup changes. These include: You change the name of a Vocabulary Key You need to remove edges You need to change existing edges You need to remove Vocabularies Due to CluedIn being a append-only system (with support for deleting if necessary) it means that certain changes require cleanup. For removing or changing edges, you can use the “ObsoleteSince” extension method to instruct to CluedIn that since a particular Version if your Crawlers, you had an edge, and after the case you need to remove or change the data in the edge. If you do this, this CluedIn will do the cleanup for you. You can also perform this using Post Processors. If you decide to do it in your CluedIn Crawlers then it means that your Crawlers might become a bit harder to manage. If you solve it in Post Processing, then you make sure that your crawlers always stay business-logic-agnostic. For changing or removing Vocabulary Key Names, the same method applies. There is the “ObsoleteSince” extension methods available on the VocabulayKey class which will allow you to instruct CluedIn to cleanup the mistakes."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-aggregate-aggregation-html": {
						"id": "docs-20-preparation-aggregate-aggregation-html",
						"title": "Aggregation",
						"category": "",
						"url": " /docs/20-Preparation/Aggregate/Aggregation.html",
						"content": "CluedIn has a processing pipeline that takes Clue objects and runs processing over them to turn into entities. It is through this process that data is matured, enriched and much more. One process that typically is required is to aggregate or calculate functions at processing time. This could be as simple as calculating two values and persisting the calculated value into a new property. This could be as complex as running Graph Queries at processing time that will calculate what clusters or graph patterns are in the data so that at query time, the results are already prepared. CluedIn has a specific processing pipeline to handle aggregations."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-cluedin-connect-cluedin-20connect-html": {
						"id": "docs-20-preparation-cluedin-connect-cluedin-20connect-html",
						"title": "CluedIn Connect",
						"category": "",
						"url": " /docs/20-Preparation/CluedIn-Connect/CluedIn%20Connect.html",
						"content": "Whereas your crawlers are responsible for fetching data from datasources and creating edges. It is not the responsiblity of the crawlers to determine what type of relationship that linkage is. This is due to the nature that determining this is usually not static in nature and can sometimes take complex calculations to determine. Hence it is alwasy best practice to specify simple edge types in crawlers and then rules can be set in CluedIn Connect to run the smarts. This is also due to the fact that you might need to query other data from other systems to properly be able to determine what type of relationship exists between two records. The simple edge types include: Parent PartOf CluedIn connect will help you build rules that determine what strongly typed relationship should be constructed. A simple example would be an edge type that has a temporal factor to it e.g. past, present, future. If we statically were to create a relationship of type WorksFor from a Person to an Organization then chances are this data might not be current and hence the relationship could actually be WorkedFor or SoonWorkFor. The good thing is that other metadata available might be able to help us dynamically determine these edge types. For example, you might connect your HR system to CluedIn in which it will have the start date and end date of employment. Using the rules engine in CluedIn Connect you could determine that if the employment date is within this time frame then you could also determin the relationship type beteen these two records. CluedIn Connect also specifies the ability to have complex processors that can use more than just simple rules to determing edge types. You might find that you will want to use Natural Language Processing to determine the edge type between two records."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-data-metrics-10-data-quality-metrics-html": {
						"id": "docs-20-preparation-data-metrics-10-data-quality-metrics-html",
						"title": "Data Quality Metrics",
						"category": "",
						"url": " /docs/20-Preparation/Data-Metrics/10-Data-Quality-Metrics.html",
						"content": "Data Metrics will allow you to score and monitor the history of data quality over time. Data Metrics are aggregated on many different levels and you might find that you need to control how that aggregation is done at each level. At the core of the Metrics are a way to calculate different types of scores given certain data values. This could be in the form of a percentage between 0 and 100, a simple integer or something more advanced. Most of the out of the box data metrics to do with Data Quality are in the form of a percentage between 0 and 100. To date, we support: Accuracy Completeness Relevance Connectivity To add new metrics, you will simply need to create a new C# class and inherit from Metric. You will then need to implement the calculation piece and the aggregation piece. It is always recommended that you will start calculation at the highest precision point possible i.e. Property History. It might not always be appropriate for you to calculate at this granularity level e.g. Connectivity is only calculated at the Entity level due the fact that you will most likely only be interested in knowing that after the Entity has been constructed, is it connected or not. There are other metrics, like Accuracy where we calculate down to the property history as we are wanting to measure how all permutations of a value correlate with each other. In some cases you might find that the parent levels of metrics are simply an average, mean or sum of the higher precision scores. In other cases you might find that you calculate a score differently at each level. Here is an example of how the default Accuracy implementation is structured: Accuracy For all properties, how much are the same or similar in the property history. Use simple normalization to cater for more values. If you only have one value then accuracy is 50%. How to fix : Mesh to change the values in the source. CluedIn Clean: Reconciliation. Manual Edit. CluedIn Train: Questions in the form of “Is Martin’s best email msh@cluedin.com?”, Are these photos of the same person? • Example of calculation ○ Property level ○ Property, integration type level ○ Property integration level ○ Entity integration type level ○ Entity integration level ○ Entity level ○ Integration type level ○ Integration level ○ Global level Property Level • Entity ○ Branch A (SharePoint, Integration 1) § Version 1 □ Property 1 Old Value □ Property 3 Value § Version 2 (HEAD) □ Property 2 My Value ○ Branch B (SharePoint, Integration 2) § Version 1 (HEAD) □ Property 1 Latest Value □ Property 2 My Value ○ Branch C (LDAP, Integration 3) § Version 1 (HEAD) □ Property 2 My Old Value □ Property 3 Value ○ Branch D (LDAP, Integration 4) § Version 1 (HEAD) □ Property 3 Value • Branch A Merged Record ○ Property 1 Old Value ○ Property 2 My Value ○ Property 3 Value • Branch B Merged Record ○ Property 1 Latest Value ○ Property 2 My Value • Branch C Merged Record ○ Property 2 My Old Value ○ Property 3 Value • Branch D Merged Record ○ Property 3 Value Property 1 Values from each branch merged records: “Old Value”, “Latest Value”, [Missing], [Missing] Unique Values: Value Count Count at Head Old Value 1 0 Latest Value 1 1 [Missing] 2 - Number of branches: 4 Property 2 Values “My Value”, “My Value”, “My Old Value”, [Missing] Unique Values: Value Count Count at Head My Value 2 2 My Old Value 1 1 [Missing] 1 - Property 3 Values “Value”, [Missing], “Value”, “Value” Unique Values: Value Count Count at Head Value 3 2 Formula: [Max Unique Value Count] ([Sum of value counts] + ([Missing values] [Branch Count])) max Max Unique Value Count populated Sum of value counts Count populated records missing Number of records with missing value branches Branch Count Alternative Min(1, ([Max Unique Value Count] + ([Count at head] [Branch Count])) ([Sum of value counts] + ([Missing values] [Branch Count])) ) Property 1 1 (2 + (2 4)) = 0,4 Min(1, (1 + (1 2)) (2 + (2 4))) = 0,6 Property 2 2 (3 + (1 4)) = 0,6154 Min(1, (2 + (2 4)) (3 + (1 4))) = 0,7692 Property 3 3 (3 + (1 4)) = 0,9231 Min(1, (3 + (2 4)) (3 + (1 4))) = 1,00 Other Examples: Branches: 10 Max Populated Missing Result 1 10 0 0,1000 1 9 1 0,1099 1 8 2 0,1220 1 7 3 0,1370 1 6 4 0,1563 1 5 5 0,1818 1 4 6 0,2174 1 3 7 0,2703 1 2 8 0,3571 1 1 9 0,5263 2 10 0 0,2000 2 9 1 0,2198 2 8 2 0,2439 2 7 3 0,2740 2 6 4 0,3125 2 5 5 0,3636 2 4 6 0,4348 2 3 7 0,5405 2 2 8 0,7143 3 10 0 0,3000 3 9 1 0,3297 3 8 2 0,3659 3 7 3 0,4110 3 6 4 0,4688 3 5 5 0,5455 3 4 6 0,6522 3 3 7 0,8108 4 10 0 0,4000 4 9 1 0,4396 4 8 2 0,4878 4 7 3 0,5479 4 6 4 0,6250 4 5 5 0,7273 4 4 6 0,8696 5 10 0 0,5000 5 9 1 0,5495 5 8 2 0,6098 5 7 3 0,6849 5 6 4 0,7813 5 5 5 0,9091 6 10 0 0,6000 6 9 1 0,6593 6 8 2 0,7317 6 7 3 0,8219 6 6 4 0,9375 7 10 0 0,7000 7 9 1 0,7692 7 8 2 0,8537 7 7 3 0,9589 8 10 0 0,8000 8 9 1 0,8791 8 8 2 0,9756 9 10 0 0,9000 9 9 1 0,9890 10 10 0 1,0000 Property integration type level Filter the version branches down to integration type • Branch A (SharePoint) Merged Record ○ Property 1 Old Value ○ Property 2 My Value ○ Property 3 Value • Branch B (SharePoint) Merged Record ○ Property 1 Latest Value ○ Property 2 My Value • Branch C (LDAP) Merged Record ○ Property 2 My Old Value ○ Property 3 Value • Branch D (LDAP) Merged Record ○ Property 3 Value SharePoint Property 1 Values “Old Value”, “Latest Value” Unique Values: Value Count Count at Head Old Value 1 0 Latest Value 1 1 1 (2 + (0 2)) = 0,5 Min(1, (1 + (1 2)) (2 + (0 2)) ) = 0,75 SharePoint Property 2 Values “My Value”, “My Value” Unique Values: Value Count Count at Head My Value 2 2 2 (2 + (0 2)) = 1 Min(1, (2 + (2 2)) (2 + (0 2)) ) = 1 SharePoint Property 3 Values “Value”, [Missing] Unique Values: Value Count Count at Head Value 1 0 [Missing] 0 - 1 (1 + (1 2)) = 0,6667 Min(1, (1 + (0 1)) (1 + (1 2)) ) = 0,6667 Ldap Property 2 Values “My Old Value”, [Missing] Unique Values: Value Count Count at Head My Old Value 1 1 [Missing] 1 - 1 (1 + (1 2)) = 0,6667 Min(1, (1 + (0 1)) (1 + (1 2)) ) = 0,6667 Ldap Property 3 Values “Value”, “Value” Unique Values: Value Count Count at Head Value 2 2 2 (2 + (0 2)) = 1 Min(1, (2 + (2 2)) (2 + (0 2)) ) = 1 Property integration level Evaluate all branch versions match the integration • Branch A (SharePoint, Integration 1) ○ Version 1 § Property 1 Old Value § Property 3 Value ○ Version 2 (HEAD) § Property 2 My Value Integration 1 Property 1 Values “Old Value” Unique Values: Value Count Count at Head Old Value 1 0 [Missing] 1 - 1 (1 + (1 2)) = 0,6667 Min(1, (1 + (0 2)) (1 + (1 2)) ) = 0,6667 Property 2 Values [Missing], \"My Value\" Unique Values: Value Count Count at Head My Value 1 1 [Missing] 1 - 1 (1 + (1 2)) = 0,6667 (1 + (1 2)) (1 + (1 2)) = 1 Property 3 Values \"Value\", [Missing] Unique Values: Value Count Count at Head Value 1 0 [Missing] 1 - 1 (1 + (1 2)) = 0,6667 (1 + (0 2)) (1 + (1 2)) = 0,6667 Integration 2 Property 1 Property 2 Integration 3 Property 2 Property 3 Integration 4 Property 3 Why do my records show “Not enough data to show trend”? This is due to the fact that since the first metric for this record and for this specific metric (e.g. Completeness), the value of this score has not changed and hence CluedIn will not record this as a new value. This is partially due to the fact that these metrics can take up quite a lot of storage when run over huge amounts of data."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-data-metrics-20-most-used-data-quality-metrics-html": {
						"id": "docs-20-preparation-data-metrics-20-most-used-data-quality-metrics-html",
						"title": "Most Used Data Quality Metrics",
						"category": "",
						"url": " /docs/20-Preparation/Data-Metrics/20-Most-Used-Data-Quality-Metrics.html",
						"content": "In this article, we describe the most used data quality metrics. You can pick the ones that you want to see on your data or implement your metrics. Accountability Accuracy Completeness Complexity Connectivity Dark Data Flexibility Interpretability Noise Orderliness Relevance Reliability Sparsity Staleness Stewardship Timeliness Uniformity Usability Data Accountability Description: The Data Accountability metric helps to keep track of ownership of the data. The more people are responsible for a record, the higher the accountability. How to improve this metric: To improve Data Accountability, assign a product owner to every integration configuration. The more integration configurations have set product owners, the higher Data Accountability. Data Accuracy Description: The Accuracy is scored by the number of different data sources that are eluding to the same value of data for the same property. This does not mean that the values have to be the same but are eluding to the same value. For example, +45 53 53 53 53 might be an accurate phone number for a person, but so is 53 53 53 53. They are physically different values, but essentially, both are accurate. How to improve this metric: Use CluedIn Mesh to change the values in the source. Use CluedIn Clean to reconcile the data in CluedIn databases. Data Completeness Description: The Completeness is scored on whether a value is present or not. A value in our world is one that is not an empty string or Null. We treat values like Unknown, N A, 0 as values. How to improve this metric: Add data that matches more or all properties of your Vocabulary. Data Complexity Description: The Complexity signals how many properties and edges does an entity have. How to improve this metric: The more properties and edges an entity has, the higher its Complexity. For some data, it’s expected to be complex. If you start to ingest data with fewer properties or edges, the Complexity will eventually go down. Data Connectivity Description: It is unique to CluedIn because, as part of its data layer, one of the constructs of your data that you get is a Graph. This Graph is a native graph, not just a triplet store. This means that we have a fully connected network of your data. Very similar to Google, Twitter, Facebook, and LinkedIn, the more “dense” your record is, the more “important” it is. This is not always the case, and in some cases, we are more interested in incoming connections than outgoing connections. More data that is directly or indirectly connected to a record will gain a higher connectivity score. How to improve this metric: Add more connections between your data records. Dark Data Description: This measures how many Shadow Entities you have. Shadow Entities are pointers to records that don’t exist yet, e.g., You are referring to a Movie, and you have the ID, but you don’t have the actual movie in your system. A high Dark Data indicates that you have to find more datasets to ingest because you are missing obvious data that exists in your business. How to improve this metric: For this metric, the less is better. Add data that necessary to reduce the number of shadow entities. For example, if you have orders with customer IDs, but you didn’t ingest the customers with such IDs, add them to CluedIn, and the Dark Data eventually will go down. Data Flexibility Description: Given a record, how versatile is it in being used many times for many different reasons, i.e., how many streams involve this record where the TYPE of stream Export Target is different, e.g., Power BI is BI, Azure ML is ML. Raise flexibility by having streams go to different Export Targets. How to improve this metric: Stream your data to more platforms like PowerBI, Azure ML, SQL Server, etc. Data Interpretability Description: How perfect is your data at aligning to a core or non-provider-specific Vocabulary.? As soon as you see a record in CluedIn that has provider-specific Vocab, this gives the impression that records are harder to understand and indicates that you are bringing in data that you probably will not use because it is not part of a common semantic. Raise your Interpretability by adding more CORE Vocabularies to map your input data The metric shows how many ingested properties match the core vocabulary versus the properties that don’t match. Interpretability is similar to the Complexity. How to improve this metric: Map more properties to the core vocabularies. Data Noise Description: This is calculated by External Data (from Enrichment Services) that are also Shadow Entities. This means that you are doing a lot of fuzzy lookups on records that have no Entity Codes to map back to your records, or your fuzzy merging is not doing a good enough job, or the data merging is not obvious. How to improve this metric: To fix this, open the Shadow Entity Studio and start to map records together manually. Data Orderliness Description: This is about how pedantic you are with every data part having a matching CluedIn Clean part. This would indicate that your team essentially does a manual check and clean on every record that comes through the platform, not just the Golden Record. How to improve this metric: To fix this, you need to basically clean every data part that has ever come through. Data Relevance Description: Calculating Data Relevance can be tricky. Not to you and me, but for technology. First, we need to pin Relevance on something. Mostly, it will be pinned on YOU, but in this case, we are pinning it on the company, i.e., what is relevant for the company. Why? Because this is not only your data in the data hub. Sure, when you search through the data in CluedIn, it will show more relevant results to you, but that is a different use case. We score Relevance on how many hops a record is away from the business, e.g., if you are an employee of a company, you are directly connected. If you are a contact of an employee of a company, then you are two hops away. However, this is not enough. We have also coupled this with the Relevance of the actual metadata you have on those records. For example, how relevant today is it that you have the Fax number of a business? I would argue, it is not. Having their annual revenue, employee count, and website is much more relevant. We have also decided that anything that is five or more hops away from the business has a very low relevance. The Relevance is like the Interpretability, but the Relevance does not decrease if you ingest properties that are not mapped to the core vocabularies. How to improve this metric: Increase the number of properties matched to core vocabularies. Data Reliability Description: It comes down to trust. Trust comes in many ways. First, it can come from a product owner being able to influence reliability. So, the first influence to reliability is a static score of how reliable the source is. For example, it is typically mandated that HR systems keep very high-quality data. Because of this, when adding integration to CluedIn, you can set the expected reliability of the source. This is a “gut feeling,” but this also helps us give an impression of how far off your gut feeling is. How to improve this metric: Increase the source quality for your integrations. Data Sparsity Description: Do your records come together from a lot of places? Don’t get us wrong, high sparsity is good! It means that CluedIn is doing its job and bring records together. A Low sparsity means that your records might not be joining well. How to improve this metric: Ingest data from more sources. Data Staleness Description: Essentially scored based on the rate of updates in records in respect to the accuracy. A good example would be the phone number or an email. An email on a person today may be stale tomorrow. We are scoring on how long your data hub has the wrong value for a property. How to improve this metric: There’s no good or bad value for the Staleness. The more times you have updated your data in the last 30 days, the lower is Staleness. Data Stewardship Description: The Data Stewardship metric is scored based on how much manual cleaning, labeling, and curation has been done on a record. The Stewardship is very similar to the Orderliness. While the Orderliness ensures that for each non CluedIn Clean historical record, there is one from CluedIn Clean, the Stewardship calculates the ratio between the records that are coming from CluedIn Clean versus the ones that don’t come from CluedIn Clean. How to improve this metric: The more times you cleaning and editing your data, the higher the metric. Data Timeliness Description: Timeliness is scored on time to value and delivery. The more real-time that data is synced to the CluedIn hub and the consumers of the data, the better the Timeliness. Sometimes, you might think that you are not in control of this, and yes, there are many cases when you are not. However, one way is to increase the Timeliness is to run syncs of data more often. Yes, this will require more infrastructure, but there is a reality in real-time data that the more mature you need the data, the less real time you can get your insights. It shows the time between ingesting a record and outputting it to a stream. How to improve this metric: The more data is used in streams, the higher the metric. The metric will eventually go down and become zero if no data was streamed in the last 30 days. Data Uniformity Description: Measured very similar to Accuracy but is much stricter. Uniformity is scored on how many different sources are eluding to the same value in the same format. The closer the format, the higher the Uniformity. For example, if one record was telling me that the industry of a company was “Software” and another record was telling me “software”, then this is very close Uniformity, but not 100%. The more divergence in values, the lower in Uniformity. It is similar to Accuracy, but the Uniformity is harsher and takes data types and string casing into account. This is why the Uniformity will always be lower than Accuracy. How to improve this metric: Use CluedIn Mesh to change the values in the source. Use CluedIn Clean to reconcile the data in CluedIn databases. Data Usability Description: Usability is scored based on the number of consumers. If you have data that many consumers constantly use, it helps speak to the data’s usability. Of all the streams that were created, how many involve this record. How to improve this metric: Stream you data to more streams."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-data-metrics-30-how-to-add-a-data-quality-metric-html": {
						"id": "docs-20-preparation-data-metrics-30-how-to-add-a-data-quality-metric-html",
						"title": "How to add a new Data Quality Metric",
						"category": "",
						"url": " /docs/20-Preparation/Data-Metrics/30-How-To-Add-a-Data-Quality-Metric.html",
						"content": "Here is an example of how to calculate Accuracy of data. You can use this as a guide to implement your own custom data metrics. using System; using System.Collections.Generic; using System.Globalization; using System.Linq; using System.Text; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Metrics; using CluedIn.Processing.Processors; namespace Custom.Metrics.Implementations { public class AccuracyMetric : PercentageMetric { private readonly IMetricProviderResolver providerResolver; public AccuracyMetric(IMetricProviderResolver providerResolver) { this.providerResolver = providerResolver; } public override short ValueSize =&gt; sizeof(ushort); public override Guid Id { get; } = new Guid(\"{987D2A08-E9A0-4661-BC8A-468E7BB34873}\"); public override string[] Categories { get; } = { MetricCategories.DataQuality }; protected override PercentageMetricValue CalculatePct( MetricsProcessingContext context, IMetricDimension dimension, IMetricValues&lt;short&gt; existingMetricValues, Entity entity) { if (context == null) throw new ArgumentNullException(nameof(context)); if (dimension == null) throw new ArgumentNullException(nameof(dimension)); if (existingMetricValues == null) throw new ArgumentNullException(nameof(existingMetricValues)); * * Dimension Table: * * | DimensionType | DetailType | ProviderDefinitionId | ProviderId | Detail | Persistence | * |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * Global | Global | | | | | | * │ Global Property | Global | Property | | | Property Name | | * │ │ Global Provider | GlobalIntegrationType | | | Id | | | * │ │ │ Global Provider Definition | GlobalIntegration | | Id | Id | | | * │ │ │ │ |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * └───│───│───│── Entity | Entity | | | | | Blob, Graph, Search, EntityMetric | * └───│───│────└─ Entity Property | Entity | Property | | | Property Name | | * └───│── Entity Provider | EntityIntegrationType | | | Id | | EntityMetric | * │ └─ Entity Provider Property | EntityIntegrationType | Property | | Id | Property Name | | * └── Entity Provider Definition | EntityIntegration | | Id | Id | | EntityMetric | * └─ Entity Provider Definition Property | EntityIntegration | Property | Id | Id | Property Name | | * * if (dimension.DimensionType.HasFlag(MetricDimensionType.Entity)) { if (entity == null) throw new ArgumentNullException(); var isProperty = dimension.DimensionDetailType == MetricDimensionDetailType.Property; var hasProviderDefinition = dimension.ProviderDefinitionId.HasValue; var hasProvider = dimension.ProviderId.HasValue; EntityIntegration if (hasProviderDefinition &amp;&amp; hasProvider) { if (isProperty) { var uniqueValue = this.GetUniqueValuesMap(entity, dimension.DimensionDetail, v =&gt; v.DataPart.OriginProviderDefinitionId == dimension.ProviderDefinitionId.Value); var average = CalculateMetricValue(uniqueValue); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetExplanation(context, uniqueValue)); } else { var average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.EntityIntegration &amp;&amp; v.Dimension.DimensionDetailType == MetricDimensionDetailType.Property); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetAggregatedValueExplanation(context, MetricDimensionType.EntityIntegration, MetricDimensionDetailType.Property)); } } EntityIntegrationType if (hasProvider) { if (isProperty) { var uniqueValue = this.GetUniqueValuesMap(entity, dimension.DimensionDetail, v =&gt; this.providerResolver.ResolveProvider(context, v.DataPart.OriginProviderDefinitionId)?.Id == dimension.ProviderId.Value); var average = CalculateMetricValue(uniqueValue); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetExplanation(context, uniqueValue)); } else { var average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.EntityIntegrationType &amp;&amp; v.Dimension.DimensionDetailType == MetricDimensionDetailType.Property); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetAggregatedValueExplanation(context, MetricDimensionType.EntityIntegrationType, MetricDimensionDetailType.Property)); } } Entity if (isProperty) { var uniqueValue = this.GetUniqueValuesMap(entity, dimension.DimensionDetail); var average = CalculateMetricValue(uniqueValue); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetExplanation(context, uniqueValue)); } else { var average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.Entity &amp;&amp; v.Dimension.DimensionDetailType == MetricDimensionDetailType.Property); return new PercentageMetricValue(dimension, entity.Id, average).WithExplanation(this.GetAggregatedValueExplanation(context, MetricDimensionType.Entity, MetricDimensionDetailType.Property)); } } else if (dimension.DimensionType.HasFlag(MetricDimensionType.Global)) { var dateDimension = MetricDateDimension.Today; short average; string explanation; switch (dimension.DimensionType) { case MetricDimensionType.GlobalIntegration: average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.EntityIntegration); explanation = this.GetAggregatedValueExplanation(context, MetricDimensionType.EntityIntegration, null); break; case MetricDimensionType.GlobalIntegrationType: average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.EntityIntegrationType); explanation = this.GetAggregatedValueExplanation(context, MetricDimensionType.EntityIntegrationType, null); break; case MetricDimensionType.Global: average = (short)existingMetricValues.Average(this, v =&gt; v.Dimension.DimensionType == MetricDimensionType.Entity); explanation = this.GetAggregatedValueExplanation(context, MetricDimensionType.Entity, null); break; default: throw new Exception(); } return new PercentageMetricValue(dimension, dateDimension, average).WithExplanation(explanation); } throw new Exception(); } private string GetExplanation(MetricsProcessingContext context, UniqueValuesMap uniqueValue) { if (!context.MetricsExecutionOptions.HasFlag(MetricsExecutionOption.Explanation)) return null; var sb = new StringBuilder(); var table = AsciiTableGenerator.GenerateTable( new[] { \"Value\", \"Count\", \"CountAtHead\"}, uniqueValue.Values.OrderByDescending(v =&gt; v.Value != null).ThenByDescending(v =&gt; v.Count).ThenByDescending(v =&gt; v.CountAtHead), v =&gt; v.Value ?? \"[Missing]\", v =&gt; v.Count.ToString(), v =&gt; v.CountAtHead.ToString() ); sb.AppendLine(table); sb.AppendLine(); sb.AppendLine($\"MaxUniqueValueCount: {uniqueValue.MaxUniqueValueCount}\"); sb.AppendLine($\"ValuesCount: {uniqueValue.ValuesCount}\"); sb.AppendLine($\"MissingValuesCount: {uniqueValue.MissingValuesCount}\"); sb.AppendLine($\"BranchCount: {uniqueValue.BranchCount}\"); sb.AppendLine(); sb.AppendLine(\"Calculation:\"); sb.AppendLine($\"{uniqueValue.MaxUniqueValueCount} ({uniqueValue.ValuesCount} + ({uniqueValue.MissingValuesCount} {uniqueValue.BranchCount})) = {CalculateMetricValue(uniqueValue).ToString(CultureInfo.InvariantCulture)}\"); return sb.ToString(); } private static double CalculateMetricValue(UniqueValuesMap v) { * Formula: [Max Unique Value Count] ([Sum of value counts] + ([Missing values] [Branch Count])) max Max Unique Value Count populated Sum of value counts Count populated records missing Number of records with missing value branches Branch Count Alternative Min(1, ([Max Unique Value Count] + ([Count at head] [Branch Count])) ([Sum of value counts] + ([Missing values] [Branch Count])) * var average = (double)v.MaxUniqueValueCount ((double)v.ValuesCount + ((double)v.MissingValuesCount (double)v.BranchCount)); Alternative var average = Math.Min(1d, ((double)v.MaxUniqueValueCount + ((double)v.HeadValuesCount (double)v.BranchCount)) ((double)v.ValuesCount + ((double)v.MissingValuesCount (double)v.BranchCount))); return average; } public override bool ShouldPersist(IMetricDimension dimension) { throw new NotImplementedException(); } public override IEnumerable&lt;IMetricDimension&gt; GetDimensions(MetricsProcessingContext context, IMetricsModel model) { var existingDimensions = model.MetricDimensions.Where(d =&gt; d.MetricId == this.Id); * * Dimension Table: * * | DimensionType | DetailType | ProviderDefinitionId | ProviderId | Detail | Persistence | * |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * Global | Global | | | | | | * │ Global Property | Global | Property | | | Property Name | | * │ │ Global Provider | GlobalIntegrationType | | | Id | | | * │ │ │ Global Provider Definition | GlobalIntegration | | Id | Id | | | * │ │ │ │ |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * └───│───│───│── Entity | Entity | | | | | Blob, Graph, Search, EntityMetric | * └───│───│────└─ Entity Property | Entity | Property | | | Property Name | | * └───│── Entity Provider | EntityIntegrationType | | | Id | | EntityMetric | * │ └─ Entity Provider Property | EntityIntegrationType | Property | | Id | Property Name | | * └── Entity Provider Definition | EntityIntegration | | Id | Id | | EntityMetric | * └─ Entity Provider Definition Property | EntityIntegration | Property | Id | Id | Property Name | | * * var entityIntegrationDimensions = existingDimensions.Where(d =&gt; d.DimensionType == MetricDimensionType.EntityIntegration &amp;&amp; d.DimensionDetailType == MetricDimensionDetailType.None &amp;&amp; d.ProviderDefinitionId.HasValue &amp;&amp; d.ProviderId.HasValue); if (entityIntegrationDimensions.Any()) { foreach (var entityDimension in entityIntegrationDimensions) yield return this.GetDefaultGlobalDimension(context, entityDimension.ProviderDefinitionId, entityDimension.ProviderId); } var entityIntegrationTypeDimensions = existingDimensions.Where(d =&gt; d.DimensionType == MetricDimensionType.EntityIntegrationType &amp;&amp; d.DimensionDetailType == MetricDimensionDetailType.None &amp;&amp; d.ProviderId.HasValue); if (entityIntegrationTypeDimensions.Any()) { foreach (var entityDimension in entityIntegrationTypeDimensions) yield return this.GetDefaultGlobalDimension(context, entityDimension.ProviderId); } if (existingDimensions.Any(d =&gt; d.DimensionType == MetricDimensionType.Entity)) yield return this.GetDefaultGlobalDimension(context); } public override IEnumerable&lt;IMetricDimension&gt; GetDimensionsToCalculate(MetricsProcessingContext context, Entity entity) { * * Dimension Table: * * | DimensionType | DetailType | ProviderDefinitionId | ProviderId | Detail | Persistence | * |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * Global | Global | | | | | | * │ Global Property | Global | Property | | | Property Name | | * │ │ Global Provider | GlobalIntegrationType | | | Id | | | * │ │ │ Global Provider Definition | GlobalIntegration | | Id | Id | | | * │ │ │ │ |-----------------------|------------|----------------------|------------|---------------|-----------------------------------| * └───│───│───│── Entity | Entity | | | | | Blob, Graph, Search, EntityMetric | * └───│───│────└─ Entity Property | Entity | Property | | | Property Name | | * └───│── Entity Provider | EntityIntegrationType | | | Id | | EntityMetric | * │ └─ Entity Provider Property | EntityIntegrationType | Property | | Id | Property Name | | * └── Entity Provider Definition | EntityIntegration | | Id | Id | | EntityMetric | * └─ Entity Provider Definition Property | EntityIntegration | Property | Id | Id | Property Name | | * * Provider Definition foreach (var group in entity.Details.DataEntries.GroupBy(d =&gt; d.OriginProviderDefinitionId)) { if (group.Key == null) continue; var providerDefinition = context.Organization.Providers.GetProviderDefinition(context, group.Key.Value); if (providerDefinition == null) break; var keys = group.SelectMany(d =&gt; d.ProcessedEntityData.Properties.Keys).Distinct().ToList(); var globalDimension = this.GetDefaultGlobalDimension(context, group.Key, providerDefinition.ProviderId); var entityLevelDimension = new MetricDimension(context, this, MetricDimensionType.EntityIntegration, MetricDimensionDetailType.None, providerDefinitionId: group.Key, providerId: providerDefinition.ProviderId, persistence: MetricDimensionPersistence.EntityMetric) { ParentDimension = globalDimension }; foreach (var key in keys) yield return new MetricDimension(context, this, entityLevelDimension, MetricDimensionType.EntityIntegration, MetricDimensionDetailType.Property, providerDefinitionId: group.Key, providerId: providerDefinition.ProviderId, dimensionDetail: key); if (keys.Any()) yield return entityLevelDimension; } Provider foreach (var group in entity.Details.DataEntries.Where(d =&gt; d.OriginProviderDefinitionId.HasValue) .GroupBy(d =&gt; this.providerResolver.ResolveProvider(context, d.OriginProviderDefinitionId.Value))) { if (group.Key == null) continue; var keys = group.SelectMany(d =&gt; d.ProcessedEntityData.Properties.Keys).Distinct().ToList(); var globalDimension = this.GetDefaultGlobalDimension(context, group.Key.Id); var entityLevelDimension = new MetricDimension(context, this, MetricDimensionType.EntityIntegrationType, MetricDimensionDetailType.None, providerId: group.Key.Id, persistence: MetricDimensionPersistence.EntityMetric) { ParentDimension = globalDimension }; foreach (var key in keys) yield return new MetricDimension(context, this, entityLevelDimension, MetricDimensionType.EntityIntegrationType, MetricDimensionDetailType.Property, providerId: group.Key.Id, dimensionDetail: key); if (keys.Any()) yield return entityLevelDimension; } Entity Property { var globalDimension = this.GetDefaultGlobalDimension(context); var entityLevelDimension = new MetricDimension(context, this, MetricDimensionType.Entity, MetricDimensionDetailType.None, persistence: MetricDimensionPersistence.Blob | MetricDimensionPersistence.Graph | MetricDimensionPersistence.Search | MetricDimensionPersistence.EntityMetric) { ParentDimension = globalDimension }; foreach (var key in entity.Properties.Keys) { yield return new MetricDimension(context, this, entityLevelDimension, MetricDimensionType.Entity, MetricDimensionDetailType.Property, dimensionDetail: key, persistence: MetricDimensionPersistence.None); } Entity if (entity.Properties.Any()) yield return entityLevelDimension; } } private UniqueValuesMap GetUniqueValuesMap(Entity entity, string propertyName, Func&lt;IVersionPart, bool&gt; versionFilter = null) { if (entity.Details.VersionHistory.Versions.Count != entity.Details.DataEntries.Count || entity.Details.VersionHistory.Versions.Any(v =&gt; v.DataPart == null)) VersionHistoryProcessing.CreateChangeHistory(entity); var branches = entity.Details.VersionHistory.Branches; var mergedBranches = new List&lt;MergedBranch&gt;(); foreach (var branch in branches) { var versionParts = entity.Details.VersionHistory.GetBranch(branch); var allVersionParts = versionParts; if (versionFilter != null) versionParts = versionParts.Where(versionFilter); if (!versionParts.Any()) continue; var mergedBranch = this.MergeDataParts(versionParts, allVersionParts); mergedBranches.Add(mergedBranch); } var uniqueValue = this.GetUniqueValues(mergedBranches, p =&gt; p.Properties.GetValue(propertyName)); return uniqueValue; } public UniqueValuesMap GetUniqueValues(IEnumerable&lt;MergedBranch&gt; mergedBranches, Func&lt;IProcessedEntityMetadata, string&gt; func) { var groups = mergedBranches.Select(m =&gt; func(m.MergedData)).GroupBy(v =&gt; v); var headGroups = mergedBranches.Select(m =&gt; func(m.HeadData)).GroupBy(v =&gt; v); var branchCount = mergedBranches.Count(); return new UniqueValuesMap(branchCount, groups, headGroups); } ********************************************************************************************************** * INNER TYPES ********************************************************************************************************** public struct UniqueValuesMap { public UniqueValuesMap( int branchCount, IEnumerable&lt;IGrouping&lt;string, string&gt;&gt; groups, IEnumerable&lt;IGrouping&lt;string, string&gt;&gt; headGroups) { var heads = headGroups.ToLookup(g =&gt; g.Key); this.Values = groups.Select(g =&gt; new UniqueValueEntry(g.Key, g.Count(), heads.Contains(g.Key) ? heads[g.Key].Sum(l =&gt; l.Count()) : 0)).ToList(); this.ValuesCount = this.Values.Where(v =&gt; v.Value != null).Sum(v =&gt; v.Count); this.MissingValuesCount = this.Values.Where(v =&gt; v.Value == null).Sum(v =&gt; v.Count); this.MaxUniqueValueCount = this.Values.Where(v =&gt; v.Value != null).Max(v =&gt; v.Count, 0); this.HeadValuesCount = this.Values.Where(v =&gt; v.Value != null).Max(v =&gt; v.CountAtHead, 0); this.BranchCount = branchCount; } public ICollection&lt;UniqueValueEntry&gt; Values { get; } public int MaxUniqueValueCount { get; } public int ValuesCount { get; } public int MissingValuesCount { get; } public int HeadValuesCount { get; } public int BranchCount { get; } } public struct UniqueValueEntry { public UniqueValueEntry(string value, int count, int countAtHead) { this.Value = value; this.Count = count; this.CountAtHead = countAtHead; } public string Value { get; } public int Count { get; } public int CountAtHead { get; } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-data-metrics-40-data-quality-metrics-explanations-html": {
						"id": "docs-20-preparation-data-metrics-40-data-quality-metrics-explanations-html",
						"title": "Data Quality Metric Explanations",
						"category": "",
						"url": " /docs/20-Preparation/Data-Metrics/40-Data-Quality-Metrics-Explanations.html",
						"content": "CluedIn calculates data quality metrics based off all data that is ingested into the platform. There are many times where you will want a verbose explanation on why a certain data quality score was given. For example, imagine if your had what you think is golden records, but CluedIn gave it very low quality scores - you would want to know exactly why. The first option is to visualise this explanation within the user inteface by clicking on the “Explain” button when you see a data quality score widget. The second example would be to get this data from our GraphQL endpoint. Here is an example that will fetch a detailed explanation of the “Accuracy” metric for Global and Entity Level dimensions. metrics { name dimensions { id type ... on GlobalMetricDimension { children { id } } ... on MetricDimension { valueHistogram } } globalLevel: dimension(id : \"25118d8c-696e-591f-9eb5-083bff9488d4\") { id ... on GlobalMetricDimension { children { id } } } entityLevel: dimension(id: \"b545b625-5a9b-5614-ae57-f2c61e02bdb9\") { id ... on MetricDimension { valueHistogram } } globalDimension { id latestValue parentId children { id parentId type detailType detail providerDefinitionId providerId providerName # valueHistogram entityValues( sort: VALUE, sortDirection: ASCENDING, cursor: \"ewAiAFAAYQBnAGUAIgA6ADUALAAiAFAAYQBnAGUAUwBpAHoAZQAiADoAMgAwAH0A\" ) { entries { value dimension { detailType latestValue children { detailType detail latestValue } } entity { name entityType } } cursor } children { id parentId type detailType detail providerDefinitionId providerId providerName valueHistogram entityValues(sortDirection: ASCENDING) { entries { value } cursor } } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-engine-room-engine-20room-html": {
						"id": "docs-20-preparation-engine-room-engine-20room-html",
						"title": "Engine Room",
						"category": "",
						"url": " /docs/20-Preparation/Engine-Room/Engine%20Room.html",
						"content": "The processing pipeline in CluedIn can be described as a tree of different processing steps. Each processing step has dependencies on previous steps being run and hence you can conceptualise it as a dependency tree of processing steps. The Engine Room is a visualisation of this processing workflow. It will give you statistics on amount of data processing in a particular state, error rates and will even allow you to introduce new processing sub-workflows. If you have added an integration to your account, the Engine Room will be able to report on what state that data is in the overall process. It will also help you gauge how much infrastructure you will need to scale to the point where you can keep up with the number of processing servers required. You can click on any of the processing steps and you will see details as to the state and processing speed of that particular step. You can use this to gauge what is happening in CluedIn at any moment of processing. It is often required to understand what is happening under the hood of CluedIn. A lot of this can be sourced from the many Adminstrator screens that come with CluedIn for the underlying systems. Due to complex security an infrastructure setups, many times you might find that you don’t have access to these systems, but would still like to see some metrics and progress statistics. For this, we have our Statistics API which aggregates the statistics from across the different underlying stores. There are 4 types of statistics that we offer through this API: Processing Crawling Configuration Footprint CluedIn uses a queuing system that operates the many different operations that CluedIn does on your data. This can be thought of as a Tree of processes. You can see that process tree below or by calling our api queue map endpoint. { \"CluedIn\": { \"Incoming\": { \"CluedIn.Core.Messages.Processing.ProcessBigClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessLowPriorityClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessPrioritizedClueCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.IProcessingCommand:CluedIn.Core_CluedIn_Clues\": { \"CluedIn.ExternalSearch.ExternalSearchCommand:CluedIn.ExternalSearch_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.SplitEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeduplicateCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ParentsProcessingCommand:CluedIn.Core_CluedIn_ParentIds\": {}, \"CluedIn.Core.Messages.Processing.ProcessEdgesCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessVersionHistoryCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.SaveEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.PercolateEntityUpdateCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.MergeEntitiesCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeduplicateEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeleteEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.PostProcessingEntityCommand:CluedIn.Core_CluedIn\": {} }, \"Webhooks\": { \"CluedIn.Core.Messages.Processing.WebhookDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ProcessWebHookClueCommand:CluedIn.Core_CluedIn\": {} } }, \"Events\": { \"CluedIn.Core.Messages.Processing.AnonymiseDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.DeAnonymiseDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Export.IExportCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.MeshDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.SendMailCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.AgentController.EnqueueAgentJobCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RemoveDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RemoveFromProcessingDataCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.ResyncEntityCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.WebApi.IWebApiCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.RefreshEntityBlobCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn\": {} }, \"Errors\": { \"CluedIn.Logging.Errors.ErrorPacket:CluedIn.Logging.Errors_CluedIn\": {} }, \"Outgoing\": { \"CluedIn.WebHooks.Commands.OutgoingCustomWebHookCommand:CluedIn.WebHooks_CluedIn\": {} }, \"Metrics\": { \"CluedIn.Core.Messages.Processing.Metrics.ProcessEntityMetricsCommand:CluedIn.Core_CluedIn\": {}, \"CluedIn.Core.Messages.Processing.Metrics.ProcessGlobalMetricsCommand:CluedIn.Core_CluedIn\": {} } } } Each Queue will have its own statistics and you can either call api queue statistics to get all statistics of all queues or you can get an individual queue by calling api queue statistics?queueName=CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn You can also add an “expand=true” which will give you the aggregate values of all child items or a parent (and child’s children etc.) It will respond with the following details. Comments are for guidance in the user interface { \"Queues\": { \"memory\": 17500, \"message_stats\": { \"disk_reads\": 0, \"disk_reads_details\": null, \"disk_writes\": 0, \"disk_writes_details\": null, \"publish\": 5, \"publish_details\": { \"rate\": 0.0 }, \"publish_in\": 0, \"publish_in_details\": null, \"publish_out\": 0, \"publish_out_details\": null, \"ack\": 5, \"ack_details\": { \"rate\": 0.0 }, \"deliver_get\": 6, \"deliver_get_details\": { \"rate\": 0.0 }, \"confirm\": 0, \"confirm_details\": null, \"return_unroutable\": 0, \"return_unroutable_details\": null, \"redeliver\": 1, \"redeliver_details\": { \"rate\": 0.0 }, \"deliver\": 6, \"deliver_details\": { \"rate\": 0.0 }, \"deliver_no_ack\": 0, \"deliver_no_ack_details\": { \"rate\": 0.0 }, \"get\": 0, \"get_details\": { \"rate\": 0.0 }, \"get_no_ack\": 0, \"get_no_ack_details\": { \"rate\": 0.0 } }, \"reductions\": 255588, \"reductions_details\": { \"rate\": 0.0 }, \"messages\": 0, \"messages_details\": { \"rate\": 0.0 }, \"messages_ready\": 0, \"messages_ready_details\": { \"rate\": 0.0 }, \"messages_unacknowledged\": 0, \"messages_unacknowledged_details\": { \"rate\": 0.0 }, \"idle_since\": \"2019-12-21 7:32:15\", \"consumer_utilisation\": null, \"policy\": null, \"exclusive_consumer_tag\": null, \"consumers\": 1, \"recoverable_slaves\": null, \"state\": \"running\", \"garbage_collection\": { \"max_heap_size\": 0, \"min_bin_vheap_size\": 46422, \"min_heap_size\": 233, \"fullsweep_after\": 65535, \"minor_gcs\": 34 }, \"messages_ram\": 0, \"messages_ready_ram\": 0, \"messages_unacknowledged_ram\": 0, \"messages_persistent\": 0, \"message_bytes\": 0, \"message_bytes_ready\": 0, \"message_bytes_unacknowledged\": 0, \"message_bytes_ram\": 0, \"message_bytes_persistent\": 0, \"head_message_timestamp\": null, \"disk_reads\": 0, \"disk_writes\": 0, \"backing_queue_status\": { \"mode\": \"default\", \"q1\": 0, \"q2\": 0, \"delta\": [ \"delta\", 0, 0, 0, 0 ], \"q3\": 0, \"q4\": 0, \"len\": 0, \"target_ram_count\": \"infinity\", \"next_seq_id\": 5, \"avg_ingress_rate\": 5.5897681059900364E-190, \"avg_egress_rate\": 5.5897681059900364E-190, \"avg_ack_ingress_rate\": 5.5897681059900364E-190, \"avg_ack_egress_rate\": 1.2720273332920905E-189 }, \"node\": \"rabbit@cluedin-dev\", \"exclusive\": false, \"auto_delete\": false, \"durable\": true, \"vhost\": \" \", \"name\": \"CluedIn.Core.Messages.Processing.Metrics.ArchiveMetricsValuesCommand:CluedIn.Core_CluedIn\" } } The Crawling statistics will report on the metrics for the fetching and mapping part of the process. It is often that Crawlers are run using Agents and these Agents may run on-premise and separate to the processing boxes. The Crawling statstics will report on many things including: Number of Tasks generated by Crawl (number of records to crawl) Number of Completed Tasks (number of records that successfully crawled) Number of Failed Tasks (number of records that failed to crawl) Status Estimated Number of Records (optional) Configuration = [HttpGet] api configuration Configuration is managed using Yaml and .Config files, however these are closed down from the User Interface. It is however very useful to explore the possible configuration options and also to be aware of the current state of the application. For example, we may want to know if a certain feature is enabled or not. The Configuration Endpoint exposes parts of the Configuration that are for read only access. We will not expose Secrets, Passwords, API Tokens or anything that exposes credentials. This is also useful for debugging and exploring potential issues. For example, you can configure CluedIn with many different parameters and features, but if there are settings that are against our recommended settings, we can expose this in the configuration user interface to alert the first places to potentially look. This is also important to validate if the configuration that has been set is actually in action in the running state of the application. All settings a read-only. Footprint Graph = api graph configuration Search = api search health It is valuable to know the running state of the CluedIn application. It is not as important for Data Stewards and Business Users to know this, but very much more adminstrators or systems owners that are not necessarily aware of how to operate the sub systems. For this, we expose some underlying metrics and statistics around memory, desk, cpu and utilisation. This will help you to understand if it might be necessary to increase the infrastructure of your CluedIn installation or potentially to dedicate more resources to a particular process. All values are read-only. For more advanced exploration, please use the underlying system adminstrator interfaces."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-entity-types-entity-20type-20translation-html": {
						"id": "docs-20-preparation-entity-types-entity-20type-20translation-html",
						"title": "Entity Type Translation",
						"category": "",
						"url": " /docs/20-Preparation/Entity-Types/Entity%20Type%20Translation.html",
						"content": "Similar to Edge Types, there are many times where you will find that determining the type of data you are ingesting cannot be known upfront or it is not static in nature. Because of this, Entity Type Translation allows you to change the type of data at processing time. For example, CluedIn uses this engine by default to transform Infrastructure User clues into Person or Contact or maybe even a Service Account. This is due to the nature of data in source systems often being mixed and a more specific typing needs more analysis to determine the fact. For example, if you were bringing in an Infrastructure User and you found that the FirstName and LastName were both valid person names, that an Age was set and that they also had a record in the HR system, then this would allow you to transform this into a Person object."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-shadow-entities-shadow-20entities-html": {
						"id": "docs-20-preparation-shadow-entities-shadow-20entities-html",
						"title": "Shadow Entities",
						"category": "",
						"url": " /docs/20-Preparation/Shadow-Entities/Shadow%20Entities.html",
						"content": "In an effort to remove “noise” from your CluedIn data, we have the idea of “Shadow Entitites”. A Shadow entitiy is a record that exists in the datastores but has been flagged as a record that will be hidden from the results in your data access layers of CluedIn. This is based off the “Eventual Connectivity” pattern in that this Shadow Entity may turn into a real Entity when it is exposed to new and more data. If you find that you are ingesting data and you expect a result to show up in the search catalog (and it does not), chances are it is a Shadow Entity. Shadow Entities are usually constructed from “Unstrcutured Text” i.e. we have a reference to a Person or Company that is only a “soft” reference. Shadow Entities are also constructed when the “Eventual Connectivity” patterns constructs an edge to an Entity via an Entity Code and that Entity Code doesn’t not exist in any other data source yet. The Shadow Entity Center is the User Interface that CluedIn provides in which a Data Steward can manually link Shadow Entities to Entities. The main use-case of this is when we find a “soft” reference to a Person or Company in unstructured text and the Data Steward knows the exact Entity that this can be linked to or merged with. The idea behind a Shadow Entity is key to our Data Integration Pattern (Eventual Connectivity). Think of it like a placeholder to a reference that doesn’t exist yet - but we think it will in the future. A Shadow Entity is “essentially” an Entity Code reference and hence like an Entity Code it will have a Type, Origin and Id. There are many Shadow Entities that will have an Id such as 1234567 Some Guid 82901848093485 These types of Shadow Entities are not the main focus of this Shadow Entity UI tool. The other types of Shadow Entities that are common are references to People and Companies such as: Sitecore Tim Ward Timothy Ward sig@cluedin.net This could be generated from Crawlers, but a majority of the time it is generated from the NER (Named Entity Recognition) process of processing text to try and attempt to find references to “Objects”. CluedIn currently does this for companies, locations, people and a few other object types. These will also cause Entity Codes but a special type of Entity Code (name Entity Codes) that doesn’t act as a unique reference to an Object. Hence you will have many of these in an account (especially if you crawl files) and they will rarely be able to find a link or merge with another Entity based off these codes. Because of this, a Data Steward will need to manually link these up. Imagine the siutation where there is a reference to a “Tim Ward” or even a “Tim” in a Word document. It is hard, and in some cases close to impossible to know which Tim or Tim Ward you are referring too. If a human was to read the text and see the context then they might be able to tell or have a good guess - but more importantly if someone that is “close” to the data in the Graph might be better to add context around this “Soft reference”. Even better, if we chose 2 or 3 people that are “close” to the data - that between them they could probably agree on who this “Tim” or “Tim Ward” is. It might be that these 2 or 3 people are not users of the CluedIn platform and hence they will never have the option of adding context because they don’t have access to CluedIn. Because of this CluedIn will choose the next most relevant people in line that DO have a user account in CluedIn, they will be the ones that are sent the data to label. In the picture above I can explain what needs to be shown on the screen for this to be useful. 1: General Meta Data on the Shadow Entity reference that is “soft”. 2: The name of the Shadow Entity. 3: The text or content where the Shadow Entity reference was found. 4: A list of REAL Entities that are possible matches to the soft reference (you can only choose one) 5: The ability to go to the next, back or skip. The UI picture is only a quick “mockup” but as you can imagine it would also be nice to be able to: 1: Click on a real entity and see more details. 2: Click on the “Content” (3) and see more details on that Entity (3) 3: See a Graph Network of the records that are connected to the Entity (3) and even a Shortest Path between the Shadow Entity (1) and the list of REAL entities (4) To build the UI you will need to use the following backend endpoints GET [Route(“api v1 shadow all”)] public async Task Get(string entityType, int page, int take) This will allow you to page through the Shadow Entities, probably 20 at a time. As you can see you can also filter by entityType e.g. “ Organization” but if you don’t specify one then “ Organization” will be chosen by default. When you select a record (4) and you want to merge it with the Shadow Entity you will want to use the Entity (4) as the Target and the Shadow Entity as the Source and call the Merge endpoint that already exists in the platform and that is in use on the Search page when you click the “Merge” button. There is no need for delete endpoints as once the merge is finished, that Shadow Entity will no longer be a shadow Entity. You will also notice that in the GET call that I return the TOTAL amount of Shadow Entites which allows you to serve the data for a Notification on the homescreen but also know how many records you need to page through to finish. The final question you might have is “How do I get the content for the Content (2) part of the UI?” For this, we are using the Highlights that come from Elastic Search to allow you to see where we found a reference to this person. This comes back in the GET response."
					}

					
				
			
		
			
				
					,
					

					"docs-20-preparation-entity-setup-html": {
						"id": "docs-20-preparation-entity-setup-html",
						"title": "Entity Type and UI",
						"category": "",
						"url": " /docs/20-Preparation/entity-setup.html",
						"content": "Introduction When CluedIn starts ingesting data, it will create ‘entities’ which has an Entity Type. The entity type is a string which represent a type of Entity. Ex: Organization, Person. CluedIn ships with some pre-defined values for common Entity Type (document, file, organization…), however, you might want to have some very specific Entity Type that may not be configured by CluedIn. It means the UI won’t really understand what this Entity Type is and will threat it as a Generic Entity. Impacts on the UI When you have an Entity Type that is not configured for the UI, you will see a ‘Circle’ as an Icon and the name displayed will the entity type value. Here are some examples of how you might recognize an Entity Ex: Not recognized entity into search Ex: Not recognized entity into the Entity detail page Ex: Default Entity URL for the not recognized entity. How to configure? Please go to the Settings, Entity Type Sections ([cluedin-url] admin settings entitytype). Type in the search the Entity Type you are after and click on the one. If the entity type is not showing, it means there are actually not Entities in CluedIn with that Entity Type. You will not be abel to setup the values correctly. Display Name is the name that will be used to mention the entity type. Icon is the icon used in several places in the UI so User can quickly see what kind of Entity it is. Path is the URL to access that specific type (please notice that it must be unique) Layout, the type of layout you can use (organization person document discussion default) Ex: Organization Setup Currently CluedIn does not support custom layout, the rendering engine is not yet exposed to external people to be used. If you are interested by such a feature, please contact CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-10-data-training-cluedin-20train-html": {
						"id": "docs-30-manipulation-10-data-training-cluedin-20train-html",
						"title": "CluedIn Train",
						"category": "",
						"url": " /docs/30-Manipulation/10-Data_Training/CluedIn%20Train.html",
						"content": "In data there are rules and then there is data that does not abide to any rules. These fuzzy pieces of data cannot be catalogued or solved with classic business rules. CluedIn Train is a reinforcement learning platform that allows you to train CluedIn to curate and steward data that does not abide by rules. Amongst other use cases, common use cases for CluedIn Train include: Detecing objects in unstructured text (Named Entity Recognition) Detect Personal Identifiers that are not covered by business rules Detect Duplicates Categorise and Classify Data How long do I need to train for before this system becomes useful? This is a very valid question, but it also is not easy to answer. However, what is clear is that CluedIn Train will prompt you with a confidence level as to indicate if the system is getting more and more confident. You can use this confidence mechanism to help you understand when the system is ready to take over in making decisions on your behalf based off historical and present data. The training studio is where you can train the CluedIn platform using a Supervised Reinforcement Learning Technique. You don’t need to do any training in the system if when you open the studio, it says “No Tasks Available”. This essentially means that the system is well trained and is merging data to a very high precision. If you see tasks in your Training Studio, essentially you will be giving a yes, no or skip answer to labelling your data. Annotation is usually the part where projects stall. Instead of having an idea and trying it out, you start scheduling meetings, writing specifications and dealing with quality control. The Training Studio generates you a data model, so that it can actively participate in the training process and learn as you go. The model uses what it already knows to figure out what to ask you next and is updated with the answers you provide."
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-20-data-cleaning-cluedin-20clean-html": {
						"id": "docs-30-manipulation-20-data-cleaning-cluedin-20clean-html",
						"title": "CluedIn Clean",
						"category": "",
						"url": " /docs/30-Manipulation/20-Data_Cleaning/CluedIn%20Clean.html",
						"content": "CluedIn Clean is an application targeted towards cleaning data so that downstream consumers of CluedIn can have a much more ready to use set of data. This includes: Normalising values to a common standard for downstream consumers. Manually enriching entities from online and external data sets. Having a bulk way to clean data and generate Mesh commands back to the source systems. CluedIn Clean is targeted at Data Engineers and Business Analysts. The closer to the context of the data the resource is, the better. CluedIn Clean currently only supports cleaning data in Core Vocabularies. This is on purpose, as the Core Vocabularies are typically what downstream consumers will want to use. You can create brand new Cleaning projects and choose the type of data you would like to clean and the number of records as well. This will generate a cleaning project for you and make it available in the list of projects when you first enter the cleaning menu option. CluedIn Clean should be used for the following reasons: To normalise representations of values that are the same, but sometimes in different formats e.g. Addresses that are the same location, but the order of the different pieces of the address are different. To standardise representations of labels, geographical locations. Although not mandated, it is quite common to standardise on a language of data to downstream consumers e.g English, Danish, Italian. Hence CluedIn Clean can help with standardising this. To fix uniformity in casing of values e.g. First and Last Names of people. To manually enrich data using online external services and to manually choose which records are matching the ones that you intend to enrich. To move values from one property to another. Sometimes you will find that you receive that where the intention was to have the city in the City column, but instead the Country was entered. CluedIn Clean can help with these types of issues. To detect outliers in values for a particular property. CluedIn Clean should not be used for the following reasons: To correct data that requires business domain knowledge to fix e.g. First and Last Names spelt differently to how they are regularly spelt. Add new fields. Correct Entity Codes. CluedIn Clean can handle transliteration, which is the ability to handle normalisation of text with accented and diacritic characters. One of the main use cases of CluedIn Clean is to clean erroneous Entity Code values before persisting them into the Datastores e.g. often you will find that we will mark certain properties as Entity Codes but then realise in the data that the values can have values like -1, N A, Empty String etc. When you are adding an integration there is a simple flag that can be set to ingest and process or ingest and place into a sandbox where only after the data has been cleaned will be then persist and create the proper entity codes. It is suggested that for the initial ingestion of data that it will go into CluedIn Clean in this sandbox environment and then for subsequent runs, we will accecpt that new types of anomaly data will have to be dealt with pro-rata. This will be handled by our Quarantine application that flags data that doesn’t conform to what is expected. CluedIn Clean projects are only committed and processed when the Commit button is pressed. Until that point, the projects are kept in their own sandbox environment. Once committed, CluedIn will take every row of data in that project and will create a respective Clue out of it, marking the source as “CluedIn Clean”. This will show itself in the history just like any other Clue that would have come from a source of data. This will provide you with the data lineage necessary to trace what clean operations have been made on the data as well. It is recommended to remove a cleaning dataset after you have committed it. You can always create another cleaning dataset at a later point. Try to keep your cleaning projects relatively small i.e. less than 50,000 records at a time. This means that your cleaning will go a lot smoother as it doesn’t have to analyse (potentially) millions of records every time that you want to clean the data. Once you are finished with cleaning the data, you will have a commit button available on the main list of cleaning projects. This will then write the data back to CluedIn and will create any mesh commands that need to be created to write this clean data back to the source systems as well."
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-30-merging-records-fuzzy-20merging-20explained-html": {
						"id": "docs-30-manipulation-30-merging-records-fuzzy-20merging-20explained-html",
						"title": "Fuzzy Merging Explained",
						"category": "",
						"url": " /docs/30-Manipulation/30-Merging_Records/Fuzzy%20Merging%20Explained.html",
						"content": "Fuzzy Merging is the process of accepting that certain records will not have a unique way to find each other. This documents the process of how CluedIn makes a decision whether two or more records should merge with each other. If an incoming record has not been able to find an existing record in CluedIn with a perfect Entity Code match, then the next thing it will try is to run Fuzzy Merging. FuzzyEntityMatching can be disabled at a Global level using the configuration switch of ClueProcessing.FuzzyEntityMatching.Enabled set to either false or true. We then evaluate if the particular clue that we are going to lookup has processing intructions on it that either ignores or subscribes to the fuzzy matching process. Now is your change to introduce custom logic in code to ignore certain records from continuing using the IProcessingFilter “IsFuzzyMatchEntityPrefiltered()” method. After this CluedIn will then look at all implementations of the IFuzzyEntityMatcher interface including many that ship with CluedIn out of the box. These IFuzzyEntityMatchers are the pieces of logic that try to determine a group of possible matches of entities. After this, CluedIn will choose the entity that has the best possible match. Below is an example of how you could extend the Fuzzy merging with your own logic. using System; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; namespace CluedIn.Processing.EntityResolution { &lt;summary&gt;A custom fuzzy matcher&lt; summary&gt; &lt;seealso cref=\"CluedIn.Processing.EntityResolution.FuzzyEntityMatcherBase\" &gt; public class CustomFuzzyMatchingMatcher : FuzzyEntityMatcherBase { ********************************************************************************************************** * PROPERTIES ********************************************************************************************************** &lt;summary&gt; Gets a value indicating whether the matcher should be used in clue to entity matching. &lt; summary&gt; &lt;value&gt; &lt;c&gt;true&lt; c&gt; if the matcher should be used in clue to entity matching; otherwise, &lt;c&gt;false&lt; c&gt;. &lt; value&gt; public override bool UseInClueToEntityMatching { get { return false; } } ********************************************************************************************************** * METHODS ********************************************************************************************************** &lt;summary&gt; Checks if the specified entity type can be matched be this instance. &lt; summary&gt; &lt;param name=\"type\"&gt;The type.&lt; param&gt; &lt;returns&gt; &lt;c&gt;true&lt; c&gt; if the entity matcher can match the specified entity type; otherwise &lt;c&gt;false&lt; c&gt;. &lt; returns&gt; &lt;exception cref=\"System.ArgumentNullException\"&gt;type&lt; exception&gt; public override bool Accepts(EntityType type) { return true; } &lt;summary&gt;Finds the matches.&lt; summary&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; &lt;param name=\"metadata\"&gt;The metadata.&lt; param&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;returns&gt;The matches.&lt; returns&gt; public override MatchGrouping FindMatches(Guid id, IEntityMetadata metadata, ExecutionContext context) { IEnumerable&lt;string&gt; emailTexts; if (!metadata.Properties.TryGetValues(out emailTexts, Vocabularies.CluedInUser.Email, Vocabularies.CluedInUser.EmailAddresses, Vocabularies.CluedInPerson.Email)) return null; emailTexts = emailTexts.SelectMany(e =&gt; e.Split(new [] {';', ',', '|'}, StringSplitOptions.RemoveEmptyEntries)).ToList(); var emails = emailTexts.Where(e =&gt; MailAddressUtility.IsValid(e) &amp;&amp; !MailAddressUtility.IsNoReplyEmailAddress(context, e)).Select(v =&gt; new MailAddress(v)); var names = emails.SelectMany(m =&gt; { DomainName domainName; if (DomainName.TryParse(m.Host, out domainName)) return new[] { m.User, domainName.Domain, m.ToString() }; return new[] { m.User, m.ToString() }; }); if (!names.Any()) return null; var nicknameModel = context.ApplicationContext.Container.TryResolve&lt;NicknameModel&lt;string&gt;&gt;(\"PersonNicknameModel\") ?? new NicknameModel&lt;string&gt;(); var emailProviders = context.ApplicationContext.Container.TryResolve&lt;BagOfWordsModel&lt;string&gt;&gt;(\"EmailProviders\") ?? new BagOfWordsModel&lt;string&gt;(); var recordFactory = new RecordFactory(nicknameModel, emailProviders); var hits = FuzzyEntityMatchQueries.FindEmailMatches(context, metadata, names); if (hits == null || !hits.Any()) return null; return this.BuildMatchGroupings(context, id, metadata, recordFactory, hits); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-30-merging-records-merging-20records-html": {
						"id": "docs-30-manipulation-30-merging-records-merging-20records-html",
						"title": "Merging Records",
						"category": "",
						"url": " /docs/30-Manipulation/30-Merging_Records/Merging%20Records.html",
						"content": "Duplicate records are big typical problem with data sources. It will show itself in many ways, but here are some simple examples: Whereas Entity Code matches are the way to 100% merge two records, it is not always the case that you will have this possibility. With that in mind there are other ways that you can help merge records. The current merging techniques in CluedIn include: Manual Merge CluedIn Fuzzy Merging Engine (utilising Core Vocabularies and Aliases) CluedIn Train You can manually merge records in the user interface or using the REST API. CluedIn will ask for a Target record and one or many Source records. The intention is that you will merge the many source records into the Target record. The easiest way to manually merge records is to first search for the records you would like to merge and then select the “Merge Records” button from the search interface. You will be prompted to select the many duplicates you will want to merge and then you will go into a merge resolution screen if necessary."
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-40-mesh-center-mesh-20center-html": {
						"id": "docs-30-manipulation-40-mesh-center-mesh-20center-html",
						"title": "Mesh Center",
						"category": "",
						"url": " /docs/30-Manipulation/40-Mesh_Center/Mesh%20Center.html",
						"content": "The Mesh Center will list all Mesh commands. You can filter your Mesh Center by the source at which it will need to run. Mesh commands can be generated for many reasons, but essentially, this is a list of commands that CluedIn would like to run against the source systems where it originally had retrieved the data. Mesh Commands do not run automatically, they need to be manually approved or rejected by an Administrator. You will notice that CluedIn also shows an example of the query that will run against the source system and make it easily available to copy this to your clipboard so that you can “Reject” the command in CluedIn and run this command manually in the way you choose to. It is also important to note that Mesh Commands don’t necessary need to run directly against the source systems i.e. they can be setup in a way to write to a ticketing or support desk application. Approving or Rejecting a Mesh Command will remove it from the Mesh Center but we will maintain the history of these commands so that you have an audit trail of who ran what commands against source systems."
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-50-splitting-records-splitting-20records-html": {
						"id": "docs-30-manipulation-50-splitting-records-splitting-20records-html",
						"title": "Splitting Records",
						"category": "",
						"url": " /docs/30-Manipulation/50-Splitting_Records/Splitting%20Records.html",
						"content": "Over time you will realise that there are certain situations where the fuzzy merging engine will merge incorrectly or you have an EntityCode that you thought was unique, but isn’t. In this case you will need to split entities. One can use the CluedIn API to be able to split an Entity out into a tree of Clues and then choose the particular Clue objects that caused the bad merge to occur. After this split, CluedIn will then process the split Clues (minus the removed Clues) back into the Entity. The bad entity code will then be kept in a curated datastore so that future recrawls will not cause this issue to occur again. You can split an entity by making a call to the REST API via a POST to api admin commands split entity?organizationId=02712a2b-b36a-4f11-95ba-d1850a311419&amp;id=1c436fa6-f2a9-5564-aa1b-3fc61e4fd576&amp;removeClues=2c436fa6-f2a9-5564-aa1b-3fc61e4fd576,3c436fa6-f2a9-5564-aa1b-3fc61e4fd576&amp;executeImediately=true This command will split the Entity record with the Id of 1c436fa6-f2a9-5564-aa1b-3fc61e4fd576 into as many clues that it was made from and then remove the two Clues with an Id of 2c436fa6-f2a9-5564-aa1b-3fc61e4fd576 and 3c436fa6-f2a9-5564-aa1b-3fc61e4fd576 and then will immediately reprocess all clues except for the 2 in the removeClues parameter. You will then (in most situations) end up with 3 records and dependant upon the data, it might be 3 entities, or a combination of 1 entity and shadow entities. When executeImediately is set to false only the analysis is done and the split operation is not performed. This will return a log of groupings of data parts that merges together. Example output: {\"m_MaxCapacity\":2147483647,\"Capacity\":222080,\"m_StringValue\":\"Id: 3b107fbc-d64a-56e8-8970-fe7fcd0e0f87 OriginEntityCode: Infrastructure User#CustomerOrigin:175000304003611 RSwoosh Groups Group: 156 DataPart: 5716; Name: [EntityName] DataPart: 2692311; Name: [EntityName] DataPart: 1399417; Name: [EntityName] ... Group: 7438 DataPart: 413478; Name: [EntityName] DataPart: 413215; Name: [EntityName] DataPart: 413114; Name: [EntityName] DataPart: 7614; Name: [EntityName] DataPart: 2879604; Name: [EntityName] DataPart: 61680; Name: [EntityName] DataPart: 728938; Name: [EntityName] DataPart: 1234206; Name: [EntityName] DataPart: 1889361; Name: [EntityName] DataPart: 32481; Name: [EntityName] ... TargetGroup: 7438 DataPart: 413478; Name: [EntityName] DataPart: 413215; Name: [EntityName] DataPart: 413114; Name: [EntityName] DataPart: 7614; Name: [EntityName] DataPart: 2879604; Name: [EntityName] DataPart: 61680; Name: [EntityName] DataPart: 728938; Name: [EntityName] DataPart: 1234206; Name: [EntityName] .... Record Attribute: Codes; [EntityCodes in group] Person#CluedIn(email):[email@address.com]; Customer#CustomerOrigin:170000351928273; Infrastructure User#CustomerOrigin:170000351928273; Person#CluedIn(email):[email@address.com]; Customer#CustomerOrigin:196000222419740; Infrastructure User#CustomerOrigin:196000222419740; Customer#CustomerOrigin:196000222321166; Infrastructure User#CustomerOrigin:196000222321166; Customer#CustomerOrigin:196000221939939; Infrastructure User#CustomerOrigin:196000221939939; ... Record Attribute: Name; [Names in group] ... Record Attribute: EntityType; Infrastructure User; Record Attribute: FirstName; [First names in group] ... Record Attribute: LastName; [Last names in group] ... Record Attribute: Email; [Emails in group] [email@address.com]; ... Record Attribute: EmailMask; [email@address.dummy]; Record Attribute: Birthday; 00000000; 19901231; 19710517; 19411124; 19340922; Record Attribute: Location; Denmark; Record Attribute: Address; [Addresses in group] ... Record Attribute: AddressCountry; DK; ... Record Attribute: Organization; ... Record Attribute: Phone; ... Record Attribute: UserName; [email@address.com]; Record Attribute: MiddleName; ... Entity: 3b107fbc-d64a-56e8-8970-fe7fcd0e0f87 - Infrastructure User#CustomerOrigin:175000304003611 Removed Codes: 4; Customer#CustomerOrigin:14700061000005; Customer#CustomerOrigin:1674001800100; Infrastructure User#CustomerOrigin:14700061000005; Infrastructure User#CustomerOrigin:1674001800100; Added Codes: 0; All Codes: 1036; Infrastructure User#Globase:7985; Infrastructure User#CustomerOrigin:146000586143288; Customer#CustomerOrigin:146000586143288; Infrastructure User#CustomerOrigin:157000129430958; Customer#CustomerOrigin:157000129430958; Infrastructure User#CustomerOrigin:146000586144209; ... Operations: 27 Save Operations : 1 Post Commands : 26 \",\"m_currentThread\":0}}"
					}

					
				
			
		
			
				
					,
					

					"docs-30-manipulation-index-html": {
						"id": "docs-30-manipulation-index-html",
						"title": "Transforming Data",
						"category": "",
						"url": " /docs/30-Manipulation/index.html",
						"content": "Introduction When CluedIn starts ingesting data, it will create ‘entities’ which has an Entity Type. The entity type is a string which represent a type of Entity. Ex: Organization, Person. CluedIn ships with some pre-defined values for common Entity Type (document, file, organization…), however, you might want to have some very specific Entity Type that may not be configured by CluedIn. It means the UI won’t really understand what this Entity Type is and will threat it as a Generic Entity. Impacts on the UI When you have an Entity Type that is not configured for the UI, you will see a ‘Circle’ as an Icon and the name displayed will the entity type value. Here are some examples of how you might recognize an Entity Ex: Not recognized entity into search Ex: Not recognized entity into the Entity detail page Ex: Default Entity URL for the not recognized entity. How to configure? Please go to the Settings, Entity Type Sections ([cluedin-url] admin settings entitytype). Type in the search the Entity Type you are after and click on the one. If the entity type is not showing, it means there are actually not Entities in CluedIn with that Entity Type. You will not be abel to setup the values correctly. Display Name is the name that will be used to mention the entity type. Icon is the icon used in several places in the UI so User can quickly see what kind of Entity it is. Path is the URL to access that specific type (please notice that it must be unique) Layout, the type of layout you can use (organization person document discussion default) Ex: Organization Setup Currently CluedIn does not support custom layout, the rendering engine is not yet exposed to external people to be used. If you are interested by such a feature, please contact CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-access-control-access-20control-html": {
						"id": "docs-40-governance-access-control-access-20control-html",
						"title": "Access Control",
						"category": "",
						"url": " /docs/40-Governance/Access-Control/Access%20Control.html",
						"content": "CluedIn can enable or disable Read access at a datasource level. If you require more granular level security control, please talk to your CluedIn Account Manager to discuss the options. Access Control in CluedIn dictates if certain data is visible to a particular account user group in the CluedIn Data Hub. By default, Read access is denied to all users except the user that was the original user to integrate a particular source. CluedIn requires an opt-in policy on read access. You have two ways to apply access control within CluedIn. The first is by visitings a particular user and setting which data sources that user has access to. The other is by selecting a particular integration source and setting which users have read access to the integration point. Step 1: Navigate to your Integrations page. Step 2: Click on the integration you are wanting to change permissions for and you will see an “Access” button. Step 3: Click on this access button and choose the users or roles that you would like to grant read access too."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-consent-management-consent-20management-html": {
						"id": "docs-40-governance-consent-management-consent-20management-html",
						"title": "Consent Management",
						"category": "",
						"url": " /docs/40-Governance/Consent-Management/Consent%20Management.html",
						"content": "The CluedIn consent management support allows you to map consent against Vocabulary Keys. This allows a Governance Compliance team to map which Vocabulary keys they will need to prove consent for and then map that to the actual records that are in the CluedIn Data Hub. Consent Entries are typically mapped towards the use of the data itself. For example, if you had a use case to determine if a person was a potential customer, then you are best to map the specific Vocabulary Keys that you will need for that use case, even if you are capturing the consent for those Keys already from another use case. By mapping the consent entries, you will be able to determine where those Vocabulary Keys are being used both incoming and out going sources and targets and you will be able to determine if you have the right consent from an individual to use their data for that specific reason. By creating a new consent entry, you will specify a Description, a flag specifying if you require all entries to have a value and finally, the Vocabulary Keys that you are binding for consent. You can then manually, or using our API, tell CluedIn which individuals you have actually captured consent from. This will allow you to filter your consumption endpoints to only use the data that you have consent for."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-breaches-data-20breaches-html": {
						"id": "docs-40-governance-data-breaches-data-20breaches-html",
						"title": "Data Breaches",
						"category": "",
						"url": " /docs/40-Governance/Data-Breaches/Data%20Breaches.html",
						"content": "Data Breaches allow you to generate reports that will let you know which Person objects have been sourced from a particular data source. This will help you register data breaches and have CluedIn inform you of what people were affected by that breach. This is due to the requirement of many businesses to be able to quickly report data breaches to local authorites. The Data Breaches feature of CluedIn does not detect breaches, but rather allows you to quickly report on the affected persons after the breach has been detected. If you find that you have a Data Subject that requests for a right of portability to data then you will also find that all registered breaches will show up in their respective Subject Access Request report."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-catalog-data-20catalog-html": {
						"id": "docs-40-governance-data-catalog-data-20catalog-html",
						"title": "Data Catalog",
						"category": "",
						"url": " /docs/40-Governance/Data-Catalog/Data%20Catalog.html",
						"content": "Using the Vocabularies in your integrations, CluedIn will automatically build you up a Data Catalog of all the properties that are available across all integrations that have been enabled in the platform. This means that the Catalog will give you a list of all property names, both Core and Provider Specific Vocabulary Keys that exist. This means that that these will be the exact property names that you will use to query data out of CluedIn. You can set the details of each Vocabulary key in your Crawler Templates such as Data Type, Visibility, Description and more. You can also use this Data Catalog to be able to view the distribution of values you have for you Vocabulary Keys. They are available in the “Values” tab of an individual Vocabulary Key when you click on it in the user interface. Clicking on an individual value will take you to all records that have that value for that Vocabulary Key. You can filter your Data Catalog by the Entity Type, the source (e.g. Hubspot) or the type of Vocabulary it is e.g. Core, Provider."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-glossary-data-20glossary-html": {
						"id": "docs-40-governance-data-glossary-data-20glossary-html",
						"title": "Data Glossary",
						"category": "",
						"url": " /docs/40-Governance/Data-Glossary/Data%20Glossary.html",
						"content": "The Data Glossary allows Data Governance teams to map a proper semantic model over data that typically will come from many different sources. The role of the Data Glossary is to map specific business terminolgy to one or many Vocabularies via a rules engine. For example, you might find that your definition of a “Customer” is an Entity of type Organization where the organization.type = ‘Enterprise’ and the salesforce.accounting.isPaid = ‘true’ and the dinero.invoices.paymentDate is less than today. Making these mappings will make it easier to query CluedIn, not having to know all the terminolgy that might not relate to your part of the business. To create new glossary entries, you simply will need to enter a Name, Description and map this using the rules builder. When querying based off this Glossary, CluedIn will do the underlying mapping to resolve what you actually mean when you ask CluedIn to give you a list of “Customers”. The Glossary also allows you to set a list of allowed or preferred values per Vocabulary. For example, it might be that you would like to give your downstream consumers the ability to query by a company founding date, but the value of that needs to be a 4 digit representative of a year and it can’t be less that 1900 and greater than the current year. This will not only allow CluedIn to report to you on records that do not adhere to this, but it will help your Data Engineers using CluedIn Clean to understand how they should clean certain data when they see it. The glossary will also help different parts of the business understand what data they are working with. If you find certain terminology hard to understand, the glossary will help describe what the intention behind certain data is. For example, if we wanted to know what the salesforce.accounting.isPaid value describes, we may set a description of “this details that a customer has paid on their end, but the money has not necessarily made it into our accounts yet. To see if the money is in our account, please use dinero.invoice.isPaid and check if it is true or false”. The Business Glossary is a way to document and map common business terms into underlying queries against Vocabularies or potentially to more complex queries. The main value is to have a common set of business terms, but predominantly it is to fast-track requests for data when they are recieved. As an example, without a business glossary, if a request came in to give all customer data to the sales team - it is most likely that a lot of research and meetings would be needed to get to the bottom of what is deemed a “Customer”. If this is mapped into CluedIn already then it is simply a matter of choosing a Glossary term which will evaluate the underlying filter for you. The Glossary will also be able to handle more complex requirements such as “All Customers, what products they have purchased, and the invoices.” You can imagine that this might be a bit harder to serve as we would need to know where this data is, how it connects (if it can at all) and then determin the model in which that request would like the data served. It might be that they want it in seperate files, it might be that they want it in a SQL database with all tables connected in a certain way or it might want it all as documents. From a governance perspective CluedIn also tracks where this Glossary term is being utilised. Currently this will only be tracked if you use the Streams and if you use the Glossary term to consume the data. As soon as a Term is used in a Stream, Data Mart or Cleaning Project - it will be linked as a Linked Asset. There are 3 main objects in the Glossary: Term Definition Rule set to Vocab Creation Date Modified Date Expiry Date Is Obsolete Key to Use (If obsolete) Icon Relevant Departments CreatedBy ModifiedBy References Usage (example used by 12 streams) Tags Example Use Cases Lineage (from rule that is built) Version Number of Matches (number of records that bind to this concept) Endorsed By Description Acronym Lexicon ParentId Term Lexicon Tag Endorsement A Term is the business terminology that you will be mapping. Good examples include Customer, Product, Transaction, Sale, Deal, Issue but can also extend to something more specific such as Churned Customer, Longterm Customer, Loyal Customer or more. You will need to set some metadata for this Term such as Description, Definition, Expiry Date an more - but the main concept is to map this to an underlying filter of Vocabularies that will derive the underlying data that matches this business terminology. A key entry in the business glossary is a business term. It is a business concept or entity identified by a unique name and defined by a meaningful description specific to the organization, in a language understood by everyone, IT and business people. A Term could also be very specific such as EBITDA, however many of these Glossary Entries come out of the box with CluedIn as they are industry terms that are not specific to your business. A Term Lexicon is an alias for a Term. A Term can have 0 or many Term Lexicon entries. You will find that different parts of a business will refer to the same underlying data in different business terms. For example, if you talk to the Sales department they might refer to the term as “Customers” but if you ask the Marketing department, they might call them “Champions” and move onto the Accounting department and they might call them “Billable Entities”. You may find that underneath the Term, they all map to the same filter. Common uses of Lexicon include: Synonyms, e.g., Consumer reporting agency Credit bureau Lexical variants, e.g., Organization Organisation, Fiber Fibre Quasi-synonyms, e.g., Borrower Mortgagor Abbreviations, initialisms, and acronyms, e.g., ARM Adjustable-rate mortgage Nonproprietary name to trade name, e.g., Adhesive bandage Band-Aid®, Acetaminophen Tylenol® In some cases you might find that they don’t exactly match, but can at least be determined as related or as a “child” of one of the other terms. Common “dimensions” of definitions include: Industry Geography Business unit Business function Subject area Department Glossary entries should be created after the Vocabularies have been finished. You can have multiple layers of Vocabularies and hence there is the need for a mapping process that may span multiple layers. For example, at CluedIn we have a Glossary Term of “Customer” as a Legal Entity (Entity Type) where the hubspot.deal.dealStage=’Closed Won’ and where the dinero.invoice.isPaid=’true’ and where the danskeBank.income.amount = hubspot.deal.dealAmount and where the cluedin.organization.isBankrupt = false. This ruleset will expland and collapse in the future, but it is highly unlikely that the business terminology will change. A Tag is simply a category or classification that you can apply to a Term. This is useful for finding and filtering on Glossary Term’s when there are many. A good example would be the “Commercial” Tag which allows me to tag certain Terms as being commercial in nature. For more complex Glossary Terms, CluedIn users can pre-setup complex mapping and modelling rules. For example, if there was a term for SVC or KYC that mapped to a single view of the customer and all data related to it. This not only means that we need to filter by “Customer” but we need to return a “Graph” model. Also notice the complexity that “Customer” in this context can sometimes mean all Legal Entities - even if they are not a “Customer”. It is a good idea to pre-map models to Export Targets such as BI platforms, ML platforms, Knowledge Graphs or Relational Databases. For example, common BI platforms would either like the data in a relational model or in JSON packets of a particular format. To streamline data proliferation, Glossary Terms can be prebuilt to support different models. When creating defintions for Term’s, here are some rules to abide by: Definition must be stated in the present tense Definition must be stated in a descriptive phrase or sentence Definition should avoid acronyms and abbreviations Definition must not contain the words used in the term (tautology) Is stated in the singular States what the concept is, not (only) what it isn’t. if a negative is included, it should come after the positive in order to reinforce it. Is stated in a descriptive phrase or sentence that can on its own Contains only commonly-understood abbreviations Does not contain other definitions Is unambiguous Is not circular (“A: see B”; “B: see A”) Is not tautological (“An A is an A”) Can be substituted for its term without a loss or change of meaning Is concise and precise (minimize op-ed, repetition, redundancy, rationale, and elaboration) Is not a list of values Is coextensive with its concept Covers the concept completely; if an exception can be found then the definition is incomplete Does not cover more than--or exceed--the concept Does not begin with an infinitive Minimizes conjunctions Does not begin with “any,” or “some” Starts with a noun (after the article) Doesn’t begin with the term being defined (“An A is…”) Begins with a word in the same class as the term An important element of the business glossary are relationships between the terms, policies, and rules. Examples of such relationships: Is a synonym to Is calculated from Replaced by Related to Is modifier of It is important to note that the Glossary is a Business Glossary, not a Data Dictionary and not a Vocabulary. In the flow of terms, the most raw entries are the Data Dictionary (keys from source systems) -&gt; then the Vocabulary which is raw keys mapped into Vocabularies -&gt; then the Business Glossary which is abstracting away underlying Vocabulary and raw keys. Data Dictionaries are managed by Data Engineers, Vocabularies are managed by Vocabulary Owners and Business Glossaries are managed by Glossary Owners. You can generate Glossary Analytics in CluedIn by calling the analytics endpoint. This will show you information on usage, but more importantly it will show the lineage of the data that matches the Glossary filter and a set of histograms that aggregate the values within those terms. For example, this will allow you to easily see: Number of matched records for a Glossary Term Distribution of values for records that match a Glossary Term e.g. You Glossary Term Geographical Distribution. % of incomplete Glossary entries. For example, each Glossary term should have a definition, description and more. This can help you understand how “mapped” your business is. The Glossary Terms can also be used in the search user interface for CluedIn. For example, if you wanted to search for “Customers” then you would be given an option to execute ther underlying rules set to run that search instead of just searching for the word “Customers”. To form your Glossary, we often recommend to interview the business users and to listen carefully to the words and terms that the business is using to request data. For example, if a business user asked for “I need all of our customers in Denmark that we have not signed and NDA with.” You could interpret this many ways, but it is always best to start out granular and expand if necessary e.g. you could interpret “our customers” as a special type of customers. Start simple. From this we can extract a few Glossary Terms with some being mapped automatically out of the box from CluedIn: Customer Denmark Non Disclosure Agreement (NDA) Signed Denmark has already been mapped out of the box i.e. where organization.address.CountryCode = “DK”. The reason we do not use the Country Name is because it is common that systems have multi-lingual content and instead of mapping every permutation of the name, we can use standards instead. Non Disclosure Agreement (NDA) could come in multiple forms. It could be that we don’t have a Word Document that contains the word NDA in the title attached to the company record. It could be that there is a property in the CRM for setting a true or false if the NDA is signed or not. Signed could mean multiple things. For example, many digital signature platforms are avialable today that have API’s that allows us to check if a certain agreement has been digitally signed and by whom. Many documents will be scanned PDF documents where smarted and more sophisticated techniques will need to be utilised to determined if a document is signed. At the end of the day, they will all map to Vocabularies, even if that Vocabulary came from a third party of external process. For example, if you utilised a third party machine learning platform to upload a document and it told us if it was signed or not then we will have a Vocabulary to has the value of the result in it. There are 2 quality metrics that are calculated off Glossary Terms and usage. Trust, Usability and SOMETHING. The way to increase these quality metrics is to have more people create streams off Glossary Terms, to create Clean projects based off Glossary Terms and to have your other Quality MEtrics for the data associated with a Glossary Term be very high. Usablity is also calculated off: Are all modelling formats supported by the Glossary Term e.g. Supports Excel, Power BI, Data Robot, Azure ML. Often we will need to support writing to an online file so that systmes like Excel can use its “from file feature”. How many streams are running off this Term versus others. Trust is calculated off: The average ratings of the Glossary Terms and the data associated with it. Is all lineage available for all data sources for a Glossary Term. If a Glossary Term changes its RuleSet, all streams that use this term will prompt you to reprocess the stream. This typically means removing all data and streaming it all again. This howvere is down to the implementation of that reprocessing logic. A Rulesset is a Query Filter that determines the records assocaited with the Glossary Term. This takes the shape of a string query that uses Lucene Syntax such as +entityType: Organization +hubspot.deal.dealStage:123 or something more complex like: +(entityType:” Organization” entityType:” Business”) +(+hubspot.deal.dealStage:”123” -hubspot.deal.dealAmount:100000) which equates to “Results must have an Entity Type of Organization or Business and must have Deal Stage of 123 and not have a deal amount of 100000”. Due to the Lexicon, you will be able to use the LexiconResolver.Resolve() class which can take e.g. “ Company” and resolve it to the underlying Entity Type. If you run the resolver over a query like above, it will resolve different terms to the correct underlying term such as “hubspot.sale.dealStage” can resolve to “hubspot.deal.dealStage” if there is a lexicon entry for “deal” of “sale”. Glossary Terms must be unique and you cannot have a Lexicon Entry that is the same as a Glossary Term. The Data Glossary will be evaluated in the following cases: Using GraphQL Creating Outgoing Streams"
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-policies-data-20policies-html": {
						"id": "docs-40-governance-data-policies-data-20policies-html",
						"title": "Data Policies",
						"category": "",
						"url": " /docs/40-Governance/Data-Policies/Data%20Policies.html",
						"content": "Data Policies will allow you to generate Rules and Restrictions. Using the CluedIn Rules engine, you will be able to listen to certain data flowing through CluedIn or already persisted into the CluedIn Datastores and then react based off that. Rules are in place to flag when there are matches and will provide an optional operation on the data associated with the match. This includes actions such as: Masking Data Replacing the value with another value Deleting Data Restrictions are in place to stop the processing of data if it matches the respective business rules. For example, if you had a business rule was that you did not want to send a persons Gender to any system or make it available for the business to query, then a restriction will not only not persist this data to CluedIn, but will also not allow this data to continue processing and hence, will not be made available in data streams or data available via the CluedIn Graph API. You can think of Rules as being notifications with optional actions, and restrictions being that we simply discard the data when it enters CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-privacy-data-20privacy-html": {
						"id": "docs-40-governance-data-privacy-data-20privacy-html",
						"title": "Data Privacy",
						"category": "",
						"url": " /docs/40-Governance/Data-Privacy/Data%20Privacy.html",
						"content": "CluedIn detects PII data by default. Here is a description of how it is implemented. Data Extractors Implemented data extractors: BitcoinAddressDataExtractor CreditCardDataExtractor CurrencyDataExtractor EmailDataExtractor IBANDataExtractor IpAddressDataExtractor NamedEntityDataExtractor NationalIdDataExtractor PhoneNumberDataExtractor UncDataExtractor UriDataExtractor Data Classification Types You can implement your own new PII detection rules, and can use the above implementations as guides on how to achieve that. Type list: None DocumentBody Email Uri Unc PhoneNumber PassportNo OrganizationName Identifier SocialMediaIdentifier IPAddress TimeZone Tag JobTitle Date Currency EmployeeId MessagingIdentifier WebProfile HRData CreditCard Number NationalId Person Name Person FirstName Person MiddleName Person LastName Person MaidenName Person Gender Person Birthday Person Age Person Identifier SSN SSN DK SSN US Location Location Geocode Location Address Location PostalCode Geography Geography City Geography Country Geography State Employment Employment Status Finance Finance BankAccount Finance BankAccount IBAN Finance BankAccount AccountNumber CryptoCurrency CryptoCurrency BitcoinAddress Extension to the entity level processed data &lt;dataDescription&gt; &lt;dataClasses&gt; &lt;class type=\" documentBody\" &gt; &lt;class type=\" emailAddress\" &gt; &lt;class type=\" uri\" &gt; &lt;class type=\" personName\" &gt; &lt;class type=\" phoneNumber\" &gt; &lt;class type=\" ssn\" &gt; &lt;class type=\" ssn dk\" &gt; &lt; dataClasses&gt; &lt; dataDescription&gt; ``` DataClassificationTypeMetric Measures instance counts of found data classification types. For example an entity containing the content tiw@cluedin.com, pid@cluedin.com, msh@cluedin.com will result in a measurement of 3 email addresses. Individual unique instances of data is measured, only the data class of the found is measured. Dimension Table: | DimensionType | DetailType | ProviderDefinitionId | ProviderId | Detail | Persistence | |-----------------------|------------------------|----------------------|------------|---------------------|-----------------------------------| Global Data Classification | Global | DataClassificationType | | | Classification Type | EntityMetric | │ Global Provider Data Classification | GlobalIntegrationType | DataClassificationType | | Id | Classification Type | EntityMetric | │ │ Global Provider Definition Data Classification | GlobalIntegration | DataClassificationType | Id | Id | Classification Type | EntityMetric | │ │ │ |-----------------------|------------------------|----------------------|------------|---------------------|-----------------------------------| └───│───│────── Entity Data Classification | Entity | DataClassificationType | | | Classification Type | EntityMetric | │ └────── Entity Provider Data Classification | EntityIntegrationType | DataClassificationType | | Id | Classification Type | EntityMetric | └────────── Entity Provider Definition Data Classification | EntityIntegration | DataClassificationType | Id | Id | Classification Type | EntityMetric | GraphQL example Get the id for the CreditCard Number dimension SELECT * FROM [dbo].[Dimension] WHERE DetailType = 4 AND Type = 1 AND Detail = ' CreditCard Number' or from graph ql dataclassificationtype: metrics(names: [\"dataclassificationtype\"]) { dimensions { id type detailType detail } } Get values for CreditCard Number dimension ```json dataclassificationtype: metrics(names: [\"dataclassificationtype\"]) { globalLevel: dimension(id : \"11431454-A2BA-5F3C-94A2-17067A18EF23\") { id type detailType detail ... on GlobalMetricDimension { children { id type detailType detail entityValues { cursor entries { value entity { id name entityType } } } } } } } Output \"dataclassificationtype\": [ { \"globalLevel\": { \"id\": \"11431454-a2ba-5f3c-94a2-17067a18ef23\", \"type\": \"GLOBAL\", \"detailType\": \"DATA_CLASSIFICATION_TYPE\", \"detail\": \" CreditCard Number\", \"children\": [ { \"id\": \"bc9d7576-2a8a-50d6-a479-e4b885d1b9e4\", \"type\": \"ENTITY\", \"detailType\": \"DATA_CLASSIFICATION_TYPE\", \"detail\": \" CreditCard Number\", \"entityValues\": { \"cursor\": \"ewAiAFAAYQBnAGUAIgA6ADEALAAiAFAAYQBnAGUAUwBpAHoAZQAiADoAMgAwAH0A\", \"entries\": [ { \"value\": { \"entityId\": \"ec4950e5-413d-54ef-92d4-014e969ecab0\", \"value\": 1 }, \"entity\": { \"id\": \"ec4950e5-413d-54ef-92d4-014e969ecab0\", \"name\": null, \"entityType\": \" Mail\" } } ] } } ] } } ]"
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-data-retention-data-20retention-html": {
						"id": "docs-40-governance-data-retention-data-20retention-html",
						"title": "Data Retention",
						"category": "",
						"url": " /docs/40-Governance/Data-Retention/Data%20Retention.html",
						"content": "Data Retention allows CluedIn to set temporal based limits on how long we can retain certain data. Rules on retention will change per industry, country and use-case. Becasue of this, CluedIn supports a generic retention framework that allows a user to specify some rules, which will then, inturn, either archive, purge (or other) data from CluedIn. If you have chosen to implement the Mesh API queries for these mutation commands, then these will also be triggered. This means that CluedIn can support Retention policies in integrated systems as well. There are two ways to setup retention periods in CluedIn. The first is to set a Rule or Graph QL query that matches the data you would like to periodically action. You will also set a retention period. When those periods are set to run, it will instruct CluedIn to queue all operations needed to complete the process and will ask the approriate owners to do the work in the Mesh Command Center. If you have not implemented the Mesh API for the source systems, the retention will simply notify you that you have a record in that system that needs to be manually actioned. The other option is for setting retention on an individual record."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-governance-html": {
						"id": "docs-40-governance-governance-html",
						"title": "Data Governance",
						"category": "",
						"url": " /docs/40-Governance/Governance.html",
						"content": "Introduction The first thing you need to do when CluedIn is running, is to feed it with Data. You need to choose which data you want to add to CluedIn. Data is pushed into CluedIn via integrations. You have two options Installing an existing integration Building a custom integration There are two main types of integrations: Providers These integrations allow you to add data into CluedIn. They can connect to cloud tools, databases, file systems, etc. extract the data you want to send to CluedIn and assemble it in a format CluedIn will understand. There are many providers available in our GitHub, but alternatively you can also build your own to cater for your specific requirements. In order to do this though, you will require some C# coding experience. Enrichers Their mission is to add extra information to improve data that is already in CluedIn. Data in CluedIn is structured in entities; these are similar to records. They can contain information about a person, a company, a task, etc. An enricher will use the existing information CluedIn to then query other external systems to try to find out more information about that entity, i.e. enrich it. We have a list of available enrichers in our GitHub, but you can also build your own, as long as you have some C# coding experience."
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-mesh-api-mesh-20api-html": {
						"id": "docs-40-governance-mesh-api-mesh-20api-html",
						"title": "Mesh API",
						"category": "",
						"url": " /docs/40-Governance/Mesh-API/Mesh%20API.html",
						"content": "The Mesh API allows a developer to implement the ability for CluedIn to write back values from CluedIn to source systems. This comes in the form of the following operation types: Update Delete Archive Associate Disassociate Anonymise De-anonymise Remove from Processing Due to the nature of this feature, there are no mesh API’s that are implemented in a blank installation of CluedIn. This is due to the nature that most mutation operations will change requirements per customer. This does not mean however that a lot of the framework around the operations is not turned on by default. The following actions will cause mesh commands to be generated: Manually edit a property in the Property view within CluedIn Any operation within CluedIn Clean that changes values When a value is determined through the Data Quality scores to be a higher statistical confidence than what is in the source system. When a core vocabulary is empty from one source and available in another. When a core vocabulary is enriched from Enrichers. When the data governance pieces around Subject Access Requests are initiated i.e. Anonymisation, Magnification, Deletion. When a record merges within CluedIn or is Split within CluedIn. All Mesh Commands are queued by default i.e. they don’t automatically run. It requires the product owner of that integration point to click the “Approve” operation on that mesh command to physically run the command. You can see these commands in your Mesh Center from the main menu. It also requires that the authorisation and authentication that has been provided for that source is given the right permissions to fulfil the operation. It may have been that for ingestion of data you are using an account that has read access, but for Mesh commands it will typically require write, delete and other types of permissions. Notes from the field: Especially when talking to databases directly, it is often hard to gauge what operations are needed to run to properly run an expected operation. For example, cascading support is often needed on databases if you are trying to run an operation that has referential integrity in another table and hence when you try to run an operation it may respond with exceptions. It is often the case that with proper base implementations, that Mesh API implementations can become cleaner and less work, with the only changes per endpoint being the Url to call and the package of data to send. By implementing the base classes for Mesh Processors you will receive a flow as follows: Step 1: Given input from CluedIn, you can construct a textual preview of what command you will be running against the source system. Step 2: If the user decides to run the command, it will make the appropriate call to the endpoint to make the mutation. Step 3: CluedIn will run the validation function so that you can check if your operation worked successfully e.g. if you ran a delete operation, you would expect that if you tried to lookup the value again that you would not get a record back. If your validation fails, then CluedIn will retry for a number of times and then mark the mesh command in a failed state. This means that there is a strong chance that the operation failed. When mesh operations are run, it will update the records in the source systems and in most cases will update their modification stamp as well. This means the next time that CluedIn runs a scheduled crawl against that system, if the source sends us the updated modification stamp then CluedIn will process it; if it does not send us the updated modification date then CluedIn will assume it is the same and throw it away in processing. This is due to the hash that CluedIn generates on entities as to do quick comparisons of records. Mesh API commands can be generated through the CluedIn REST API, or it can be all managed through the CluedIn User Inteface. There are many operations that will generate a Mesh Command, but in summary - any mutation operation that runs in CluedIn will cause Mesh API commands by default. These include: Any of these operations will generate Mesh API commands and will place these under the affected Entities in the “Pending Changes” tab of the entity page and in the Global Mesh Center where all Mesh Commands are visible. All Mesh Commands that are generated through the CluedIn User Interface are by default “Queued” and are not run until the Product Owner of the record that is being mutated accepts the change. You might find that an Entity has been merged from many sources and hence every single individual Product Owner will need to be involved in the Approval Process. Mesh Commands are Vocabulary enabled, meaning that they will utilise the hierarchy of Vocabularies that you have built so that you can change a particular value and CluedIn will “unravel” the Vocabulary of that value as to create Mesh Commands for all child and sub-child nodes of the records edited. Due to the nature of certain data, for the MESH API, you might find that while developing your crawlers, you will want to add an extra Entity Code to uniquely identify an individual row of data. Although Entity Codes should uniquely identify an object, this is an example where it is fine to add an Entity Code that is Mesh specific. An Entity Code example of this would be “ Person#CluedIn(Mesh):\". Let’s use the following Mesh implementation as an example on how to implement others. https: github.com CluedIn-io CluedIn.Crawling.HubSpot tree develop src HubSpot.Provider Mesh HubSpot ﻿using System; using System.Collections.Generic; using System.Linq; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Mesh; using CluedIn.Core.Messages.Processing; using CluedIn.Core.Messages.WebApp; using CluedIn.Crawling.HubSpot.Core; using RestSharp; namespace CluedIn.Provider.HubSpot.Mesh.HubSpot { public abstract class HubSpotUpdateBaseMeshProcessor : BaseMeshProcessor { public EntityType[] EntityType { get; } public string EditUrl { get; } protected HubSpotUpdateBaseMeshProcessor(ApplicationContext appContext, string editUrl, params EntityType[] entityType) : base(appContext) { EntityType = entityType; EditUrl = editUrl; } public override bool Accept(MeshDataCommand command, MeshQuery query, IEntity entity) { return command.ProviderId == this.GetProviderId() &amp;&amp; query.Action == ActionType.UPDATE &amp;&amp; EntityType.Contains(entity.EntityType); } public override void DoProcess(CluedIn.Core.ExecutionContext context, MeshDataCommand command, IDictionary&lt;string, object&gt; jobData, MeshQuery query) { return; } public override List&lt;RawQuery&gt; GetRawQueries(IDictionary&lt;string, object&gt; config, IEntity entity, Core.Mesh.Properties properties) { var hubSpotCrawlJobData = new HubSpotCrawlJobData(config); return new List&lt;Core.Messages.WebApp.RawQuery&gt;() { new Core.Messages.WebApp.RawQuery() { Query = string.Format(\"curl -X PUT https: api.hubapi.com \" + EditUrl + \"{1}?hapikey={0} \" + \"--header \\\"Content-Type: application json\\\"\" + \" --data '{2}'\", hubSpotCrawlJobData.ApiToken, this.GetLookupId(entity), JsonUtility.Serialize(properties)), Source = \"cUrl\" } }; } public override Guid GetProviderId() { return Constants.Providers.HubSpotId; } public override string GetVocabularyProviderKey() { return \"hubspot\"; } public override string GetLookupId(IEntity entity) { var code = entity.Codes.ToList().FirstOrDefault(d =&gt; d.Origin.Code == \"HubSpot\"); long id; if (!long.TryParse(code.Value, out id)) { It does not match the id I need. } return code.Value; } public override List&lt;QueryResponse&gt; RunQueries(IDictionary&lt;string, object&gt; config, string id, Core.Mesh.Properties properties) { var hubSpotCrawlJobData = new HubSpotCrawlJobData(config); var client = new RestClient(\"https: api.hubapi.com\"); var request = new RestRequest(string.Format(EditUrl + \"{0}\", id), Method.PUT); request.AddQueryParameter(\"hapikey\", hubSpotCrawlJobData.ApiToken); adds to POST or URL querystring based on Method request.AddJsonBody(properties); var result = client.ExecuteTaskAsync(request).Result; return new List&lt;QueryResponse&gt;() { new QueryResponse() { Content = result.Content, StatusCode = result.StatusCode } }; } public override List&lt;QueryResponse&gt; Validate(ExecutionContext context, MeshDataCommand command, IDictionary&lt;string, object&gt; config, string id, MeshQuery query) { var hubSpotCrawlJobData = new HubSpotCrawlJobData(config); var client = new RestClient(\"https: api.hubapi.com\"); var request = new RestRequest(string.Format(EditUrl + \"{0}\", id), Method.GET); request.AddQueryParameter(\"hapikey\", hubSpotCrawlJobData.ApiToken); adds to POST or URL querystring based on Method var result = client.ExecuteTaskAsync(request).Result; return new List&lt;QueryResponse&gt;() { new QueryResponse() { Content = result.Content, StatusCode = result.StatusCode } }; } } } In the example above, we are actually building an abstraction layer to handle all UPDATE types of operations due to the fact that Hubspot (this may differ from source to source) has very similar patterns for updating records, even if they are a different type of record. The only thing that changes are the Url and the Body of the HTTP PUT. To implement the Mesh API, you will want to inherit from the BaseMeshProcessor class as this is your way to tell CluedIn that your new implementation should be included at boot time. Coming with this inheritence will be a set of methods that you can optionally override. Be aware that this class itself will require you to implement the abstract class in other implementations due to the fact that CluedIn’s container will not boot abstract implementations of Mesh API’s. For example here is an implementation of the abstract class above for handling the editting updating of Contacts in Hubspot: https: github.com CluedIn-io CluedIn.Crawling.HubSpot blob develop src HubSpot.Provider Mesh HubSpot HubSpotContactMeshProcessor.cs ﻿using CluedIn.Core; namespace CluedIn.Provider.HubSpot.Mesh.HubSpot { public class HubSpotContactMeshProcessor : HubSpotUpdateBaseMeshProcessor { public HubSpotContactMeshProcessor(ApplicationContext appContext) : base(appContext, \"contacts v1 contact vid :vid profile\", CluedIn.Core.Data.EntityType.Infrastructure.Contact, CluedIn.Core.Data.EntityType.Person) { } } } Notice how the implementation is literally only a constructor and changes the Url and the supported Entity Types in CluedIn that will use this Url to modify the values in Hubspot. Once you have implmemented the Mesh API, you can deploy it exactly like you deploy any other code to CluedIn - you will need to copy the dll’s into the running application and reboot those parts. ###Making multiple source changes The Mesh API framework is setup to be very flexible on what you run against the source system to implement the changes. Imagine a situation where given a change in CluedIn that triggers this Mesh API to be evaluated, you would like to remove all records from a system, not just the Golden Record. For this, you could essentially create a foreach loop that loops through all the Entity Codes with an Origin of your source (e.g. Hubspot) and then create a Mesh Command per instance of this. This is also why the Mesh Base Processor allows a List of queries to be returned. ###Common things to think about Making changes in source systems is much more disruptive and invasive than simply reading data from a system. For this reason, we have compiled a list of things to think about when you are implementing mesh commands. The CluedIn Mesh API is simply a framework that listens to events within CluedIn and then allows you to subscribe to these events and handle them how you would like to. CluedIn is responsible for passing the Event Type, the Entity in question, the proposed change as well as the Old Value and the New Value and some metadata around how to authenticate with the source system in question. Mesh commands are handled at a ProviderDefinition level which means that one source system is not aware of how other systems are handling the change and can also be run independently of each other in no particular order. If order of change is needed then it is important to make these all within the same Mesh implementation. The Mesh API can be setup to completely bypass the approval process that is within the MESH Center within the UI. This can be done within CluedIn but we recommend this is only done if you have some other way of approving changes or you are very confident with the implementation of your Mesh API. ###Mesh from CluedIn Clean changes By default, changes that come from CluedIn Clean will not cause Mesh Commands. If you would like this to happen then you will need to change your configuration to enable this. You will need to set the Feature.Clean.CreateMeshCommands to true or if you are operating in a Kubernetes cluster it would be setting CLUEDIN_appSettings__Feature_Clean_CreateMeshCommands : true"
					}

					
				
			
		
			
				
					,
					

					"docs-40-governance-subject-access-requests-subject-20access-20requests-html": {
						"id": "docs-40-governance-subject-access-requests-subject-20access-20requests-html",
						"title": "Subject Access Requests",
						"category": "",
						"url": " /docs/40-Governance/Subject-Access-Requests/Subject%20Access%20Requests.html",
						"content": "CluedIn allows a user to generate a subject access request to fulfil data privacy acts around the world. The process of generating a request assumes that you have integrated the sources that contain personal data. We also assume that you have deduplicated the records to a high precision using the CluedIn list of duplicate suspects. The first stage of registering a Subject Access Request is to specify that the identity of the request has been confirmed. This is important for not generating reports for an individual that has not confirmed their identity. After this step you will be asked to look for the individual you will need to generate the subject access report for. By default CluedIn will ship with the ability to lookup via the Name, Email or Phone Number of an individual, however you can modify this in the settings menu of Data Governance to include Vocabulary lookup values. Using the search facilities, CluedIn will only return matches for records that are of type Person, Infrastructure Contact or Infrastructure User. All other records will be hidden by default. You cannot generate a Subject Access Request for any other Entity Type in CluedIn. It is at this point that you will notice two filters. The first is to include results from External Data Sources (where they have not merged with internal records) and Shadow Entities (where we only have unidentified references to people e.g. Name). You might find that during this process you will need to merge records manually. After you have chosen your subject, you will click Next, which will start to generate the report. You may find that this report can take multiple minutes to generate. The more data you have assocaited with a person, the more time it will typically take. You do not need to stay on this report generation page for it to continue its work, you can continue on generating other reports or using CluedIn for other reasons. After the report generation has finished, you will have the opportunity to review and edit the report before moving onto the next step. This gives you time to exclude parts of the report that you deem not necessary to share with the subject. If you find that you exclude parts of the report, then you can click the “Regenerate Report” button to asak for a new report with the modifications made. Once you are happy with the report, you can either see this in a PDF document, download the compressed version of the report with Json files pertaining to each individual record associated with the subject or finally you can move to the next stage. Your next stage will ask you if you would like to send the report and give you the option of entering an email address of the subject, in which the report will be sent to the subject. It will be registered in CluedIn that the report has been sent. You can also select “No” and manually send the report. The next stage allows you to run the mutation pieces of the subject access request, including: Rectification (Edit) Minification (Modify records to only include what you have consent mappings for) Pseudonimisation (Generate a masked version of data that you do not have consent for) Delete (Delete all records associated with this subject) For these operations to fulfil their role, the respective Mesh API’s must be implemented. Each operation above may generate many mesh api commands. These mesh commands are only generated and queued. By default, CluedIn will not run these individual commands until a product owner of that data has manually instructed CluedIn to do so in the Pending Changes tab on the entity pages of the modified records in CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-50-consume-generating-20an-20api-20token-html": {
						"id": "docs-50-consume-generating-20an-20api-20token-html",
						"title": "Generating an API Token",
						"category": "",
						"url": " /docs/50-Consume/Generating%20an%20API%20Token.html",
						"content": "To consume that from CluedIn, you may need to generate an API token so that external sources can talk to CluedIn through its GraphQL endpoint. To generate a new API token, you can head to the Settings part of CluedIn where it will ask you to give the API Token a name. You can generate as many API Tokens as necessary and you can revoke these tokens, which will render them depreciated and unable to communicate with CluedIn via the Rest API anymore."
					}

					
				
			
		
			
				
					,
					

					"docs-60-management-duplicate-20list-html": {
						"id": "docs-60-management-duplicate-20list-html",
						"title": "Duplicate List",
						"category": "",
						"url": " /docs/60-Management/Duplicate%20List.html",
						"content": "Your duplicates menu will give you a list of different types of possible duplicate records. Clicking on the duplicate entries will allow you to see the list of possible duplicates and then place them into a merge operation as to choose which properties are more likely to be the golden value. As a developer, you can also add your own types of duplicate detection as well. To do this, create a C# class and inherit from the IDuplicateQuery interface and CluedIn will then run the SearchDescriptors to generate the list. Compile your class and drop the DLL into the ServerComponent folder and reboot CluedIn. Here as an example of a Duplicate Query detection that you could add yourself: using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using CluedIn.Core; using CluedIn.Core.Configuration; using CluedIn.Core.Data; using CluedIn.Core.DataStore; using Core.Data.Repositories; using Core.Data.Repositories.Search; using Core.Data.Repositories.Search.Aggregations; using Core.Data.Repositories.Search.Filtering; namespace CluedIn.Processing.EntityResolution.Queries { public class DuplicateEntityNameQuery : IOnlineDuplicateEntityQuery { public string Name { get; } = \"Organization Name\"; public string DisplayName { get; } = \"Duplicate Vocabulary Key for Organization Names\"; public async Task&lt;IEnumerable&lt;IDuplicateEntityQueryResultSet&gt;&gt; GetPotentialDuplicatesAsync(ExecutionContext context, EntityType entityType = null) { var repos = new CluedInRepositories(); IEnumerable&lt;EntityType&gt; entityTypes; if (entityType == null) { var query = new ParsedQuery(); query.Query = \"*\"; query.Fields = new List&lt;string&gt;() { \"entityType\" }; query.Cursor = PagingCursor.Default; query.Aggregations = new List&lt;AggregationQuery&gt;() { new TermAggregationQuery(\"entityType\", 150)}; query.RankingSettings = ParsedQuery.DefaultRanking; query.IncludeUnstructuredData = !ConfigurationManagerEx.AppSettings.GetFlag(\"Feature.Filters.ShadowEntities\", true); query.OptionalFields = new List&lt;string&gt;(); query.SearchSpecificEntityTypesByName = new List&lt;string&gt;(); var results = await repos.Search.ExecuteQuery(context, query); var entityTypeAggregation = (TermAggregationBucket)results.Aggregations.First().Value; entityTypes = entityTypeAggregation.Items.Select(t =&gt; (EntityType)t.Name).ToList(); } else entityTypes = new[] { entityType }; var resultSets = new List&lt;IDuplicateEntityQueryResultSet&gt;(entityTypes.Count()); foreach (var type in entityTypes) { var query = new ParsedQuery(); query.Query = \"*\"; query.Fields = new List&lt;string&gt;() { \"properties.organization.name\" }; query.Cursor = PagingCursor.Default; query.Aggregations = new List&lt;AggregationQuery&gt;() { new TermAggregationQuery(\"properties.organization.name\", 150)}; query.RankingSettings = ParsedQuery.DefaultRanking; query.IncludeUnstructuredData = !ConfigurationManagerEx.AppSettings.GetFlag(\"Feature.Filters.ShadowEntities\", true); query.OptionalFields = new List&lt;string&gt;(); query.SearchSpecificEntityTypesByName = new List&lt;string&gt;(); query.Filters = ParsedFilteringQuery.Parse(context, query, null, new[] { new FilterQuery() { FieldName = \"entityType\", AggregationName = \"entityType\", Operator = DefaultSearchOperator.And, Value = type.ToString() } }); var results = await repos.Search.ExecuteQuery(context, query); var nameAggregation = (TermAggregationBucket)results.Aggregations.First().Value; if (nameAggregation.Items.Any(f =&gt; f.Count &gt; 1)) { resultSets.Add( new DuplicateEntityQueryResultSet( this, type, $\"Possible {type} Duplicates\", nameAggregation.Items.Where(f =&gt; f.Count &gt; 1).Select(f =&gt; new DuplicateEntityQueryGrouping(f.Name, f.Name, f.Count))) ); } } return resultSets; } public async Task&lt;PagedDataResultWithCount&lt;IEntity&gt;&gt; GetPotentialDuplicateEntityInstancesAsync(ExecutionContext context, string resultSetKey, string itemGroupingKey, PagingCursor cursor = null) { var repos = new CluedInRepositories(); cursor = cursor ?? PagingCursor.Default; var query = new ParsedQuery(); query.Query = \"*\"; query.Fields = new List&lt;string&gt;() { \"properties.organization.name\" }; query.Cursor = cursor; query.RankingSettings = ParsedQuery.DefaultRanking; query.IncludeUnstructuredData = !ConfigurationManagerEx.AppSettings.GetFlag(\"Feature.Filters.ShadowEntities\", true); query.OptionalFields = new List&lt;string&gt;(); query.SearchSpecificEntityTypesByName = new List&lt;string&gt;(); query.Filters = ParsedFilteringQuery.Parse(context, query, null, new[] { new FilterQuery() { FieldName = \"entityType\", AggregationName = \"entityType\", Operator = DefaultSearchOperator.And, Value = resultSetKey }, new FilterQuery() { FieldName = \"properties.organization.name\", AggregationName = \"properties.organization.name\", Operator = DefaultSearchOperator.And, Value = itemGroupingKey } }); var results = await repos.Search.ExecuteQuery(context, query); return new PagedDataResultWithCount&lt;IEntity&gt;(results.Entries.Select(e =&gt; e.Entity), results.TotalResults, ((cursor.Page + 1) * cursor.PageSize) &lt; results.TotalResults ? results.NextCursor : null); } } } ##Multi-Field Duplicates It is often that you will want to check for parity across multiple fields to help validate if you have a possible duplicate. The recommended way to handle this is to concatenate your values using a pre-processor within CluedIn so that you are not running this calculation at query time, but rather at indexing time. To do this, create a pre-processor and have your logic to take entities and combine properties together into a new value. For example if you were wanting to see if the organization.name, organization.website and organization.employeeSize was the same then you would create a new Vocabulary called combined.organization.keys and you would concatenate the values together of the 3 keys into one. You then can use the code above and instead of using organization.name as your field, you can use your new combined.organization.keys. You will most likely also want to support the case where this data changes and that this field also gets changed in that process. For this, implement a post-processor with the same logic. This will take care of the situations where you Clean data in CluedIn Clean and if one of those 3 values is update, you would also want to update the combined key as well."
					}

					
				
			
		
			
				
					,
					

					"docs-60-management-modelling-modelling-html": {
						"id": "docs-60-management-modelling-modelling-html",
						"title": "Modelling",
						"category": "",
						"url": " /docs/60-Management/Modelling/Modelling.html",
						"content": "As you will have learnt from our Eventual Connectivity pattern, you do not need to do the classic modelling that you will typically do with data integration projects. Saying that, you can have complete control over how your systems are modelled. The model that is generated in the Graph database of CluedIn is dictated by the Entity Codes and Edges that you specify in your crawler or mappings. Because of this, you don’t have direct access to the databases themselves and you are not going to be creating database models like you are used to - rather the models will be inferred from the database. Although this might seem like an odd approach, effectively what we are achieving here is a data model that is for connecting records. This model will not be normalised in any shape or form - quite the opposite. Once the model is formed in the graph, you will be able to use CluedIn’s other tooling to project or form this model into all different types of models for downstream consumption. This is made possible as the graph that CluedIn generates is a high-fidelity data model that can downcast itself into all different types of sub-models e.g. relational model. When you think of building Master Data Management models, you often think in a modelling-first mindset. The challenge with this approach is that you are possibly setting an end-goal that is too complex to achieve. By having the model inferred you are essentially letting technology do a lot of the mind-bending modelling work on your behalf. Your perfect model will not be formed in CluedIn itself, but something even better - you will have your data in a model that can project itself into a perfect MDM model as well as other ad-hoc models that might come in the future. This flexibility allows you to have an MDM platform that can change with your business and the demands of the MDM itself. If you are reading this and you are responsible for the modelling of data within CluedIn, it is important to remember that you are not trying to enforce a model onto the data coming into CluedIn. This is quite different to the way that other MDM providers do this, but in the end it yields a much better result. It is quite challenging to accept that you will bring data into your MDM system with a denormalised model, but the thing to remember is that you can put all different types of “views” over your data to make it represented in a way that fits into your perfect MDM model. The goal of the process is to get the data in a shape, quality and standard that can be adapted to different modelling needs without the need to redesign schemas and models as new requirements come up. For this it is also important to remember that when you are ingesting data into CluedIn, you need to start thinking in the terms of business objects, not rows or records. If you think in rows or records you will not get the true benefit of CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-60-management-rules-rules-html": {
						"id": "docs-60-management-rules-rules-html",
						"title": "Rules",
						"category": "",
						"url": " /docs/60-Management/Rules/Rules.html",
						"content": "Rules are your way to setup business logic within the CluedIn user interface. Rules are made up of a tree of conditions and a single action. Conditions can be used to build a predicate based of the values of Entity properties, including using the Vocabularies from CluedIn. ##Automate Rules with CluedIn Clean Rules can manually be created, but they can also be automated by cleaning data through CluedIn Clean. You will need to make sure this feature is enabled with the Feature.Clean.AutoCreateRules. All mass edit operations within CluedIn Clean will now automatically create rules however it will mark them as deactived by default. It will require you to enabled the rules in the user interface. ##Build new Actions New rule actions can be added in the platform by implementing the IRuleAction interface. Here is an example of how you could add an action of adding an Alias to an entity. using System.Collections.Generic; using CluedIn.Core.Data.Parts; using CluedIn.Core.Processing; using CluedIn.Core.Rules; using CluedIn.Core.Rules.Models; namespace CluedIn.CustomRules.Actions { public class AddAlias : IRuleAction { public string Name =&gt; \"Add Alias\"; public bool SupportsPreview =&gt; false; [RuleProperty] public string Value { get; set; } public RuleActionResult Run(ProcessingContext context, IEntityMetadataPart entityMetadataPart, bool isPreview) { entityMetadataPart.Aliases.Add(Value); return new RuleActionResult { IsSuccess = true, Messages = new string[] { $\"Added alias {Value}\" } }; } } } ##Clean Data with History Because CluedIn stores all the history of a record and how that record was composed, by default Cleaning projects will only clean the Golden Records. Although this is a valid use case, there are times when we would want to clean historical data that was not in the Golden Record. Cleaning the history will also play a major role in raising the data quality scores as well as being able to create all types of automated rules based off what data has been exposed to the CluedIn platform. To clean data with history, make sure you select the “Clean with History” checkbox as you create your cleaning projects."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-10-administration-operations-administration-20operations-html": {
						"id": "docs-91-developer-portal-10-administration-operations-administration-20operations-html",
						"title": "Administration Operations",
						"category": "",
						"url": " /docs/91-Developer-Portal/10-Administration_Operations/Administration%20Operations.html",
						"content": "You will need to have a role of “Admin” to be able to run these operations. There is only one way to become an Admin and that is by manually adding a mapping in the Relational Store of CluedIn in the AspnetUserRoles table. Post Processing There are many times within CluedIn where you need to apply an operation on data that already exists within your datastores. This will typically happen when you need to change the values of data in bulk. The Post Processing Admin Endpoint will allow you to run a Post Processing operation which will load all matching data into a queue called PostProcessing and will run the full post processing operation including all custom and included Post Processors of CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-100-logging-building-20a-20new-20logging-20provider-html": {
						"id": "docs-91-developer-portal-100-logging-building-20a-20new-20logging-20provider-html",
						"title": "Building a new Logging Provider",
						"category": "",
						"url": " /docs/91-Developer-Portal/100-Logging/Building%20a%20new%20Logging%20Provider.html",
						"content": "There is a strong chance that you will have a Logging Provider of choice. CluedIn ships with a few providers, but they might not suit. You will need to create a new C# class and inherit from the BaseLoggingTarget. Here is an example below: namespace CluedIn.Logging.Serilog { public class SerilogLoggingTarget : BaseLoggingTarget { private readonly ILogger _logger; public SerilogLoggingTarget(ILogger logger) { _logger = logger ?? throw new ArgumentNullException(nameof(logger)); } public override void Write(LogLevel logLevel, DateTimeOffset date, KeyValuePair&lt;string, object&gt;[] contexts, string message, Exception exception = null) { if (message == null) { throw new ArgumentNullException(nameof(message)); } _logger.Write(logLevel.ConvertToLogEventLevel(), exception, message); } public override void Write(LogLine logLine) { if (logLine == null) { throw new ArgumentNullException(nameof(logLine)); } var parameters = logLine.ConvertToLogParameters(); Write(parameters.LogLevel, parameters.Date, parameters.Contexts, parameters.Message, parameters.Exception); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-change-20verbs-html": {
						"id": "docs-91-developer-portal-110-object-model-change-20verbs-html",
						"title": "Change Verbs",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Change%20Verbs.html",
						"content": "On each Clue, the Change Verb can be used to flag to CluedIn what type of “event” this Clue is referring to. This is typically only necessary when data is being pushed to CluedIn and can be used as a way to indicate to CluedIn on how to process this data. Imagine a situation where you have a record deleted in a source system and you would like CluedIn to also reflect this deletion in it and any downstream consumer. This Change Verb can be used to manage this. It might be that you would like to mark this as a deletion in CluedIn but maintain the history, or it might be that you want to actually delete the related record in CluedIn. When you are using the prebuilt connectors of CluedIn, it is not expected to set this value, in fact in a lot of cases you don’t know that they data you are receiving is an addition or a change (unless that source system gives you that metadata)."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-clue-html": {
						"id": "docs-91-developer-portal-110-object-model-clue-html",
						"title": "Clue",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Clue.html",
						"content": "A Clue is the object model that CluedIn expects from third party systems in the form of data. It is the language that CluedIn speaks. It is the role of the developer that is implementing the CluedIn solution to map custom data formats into this universal data model. The Clue can take two main formats, JSON or XML. Despite what programming language is used to send this data to CluedIn, CluedIn only requires that you send it in either of these two formats. A Clue is an object model that CluedIn provide because it has generically mapped every object type that we commonly work with. It is an all purpose data structure and object model that can generically allow you to map Companies, People, Tasks, Documents or other custom types. This means that each Clue has some generic, catch-all properties, but also the ability to map properties that we have never seen before. It is the goal of a Clue to find other Clue or Entity objects that already exist within your CluedIn account and either merge, connect or enrich those records. A Clue is made up of generic properties and then a property bag to place all data that doesn’t not fit into a common well known object type. The Clue also has a way to add Entity Codes, which are a unique reference to an object. It also allows you to map Edges, which are references to other objects that may exist now, or in the future or never. For more advanced uses, the Clue will also allow you to map: A Preview Image Authors Aliases Change Verbs Extracted Content e.g. Content of a file. Uri Tags External Uri’s A Clue can have many Entity Codes associated with it. Imagine that you were integrating a record on a company and you had data on their local business identifier, website, LinkedIn Url, Facebook Url. These would all be considered ways to uniquely identify a company. Some are questionable as being unique identifiers, but most of the time we can accept that in more cases than not, they will be unique. For example, you could argue that a website is not a unique reference as chances are that you have purchased the website domain off a business that went bankrupt. This is completely fair, but we need to realise if this is the greater “evil” or not. More than often, a website will be unique to a company, but potentially not for a particular legal entity of that company, and for those times that it is not, we know that we might need to manually intervene in those records at a later point in time. An Entity Code is made up from 5 different pieces: Entity Type Provider Definition Id Id Origin Organization Id This combination will allow us to achieve absolute uniqueness across any datasource that we interact with. If two Clues have exactly the same EntityCode, they have a 100% chance of merging. More often than not, Clues need to be processed to turn what is usually unique into something that would overlap with another system. For example, if we integrated two records on the same person from two different systems with exactly the same email address then this would never merge. EntityType: Person Provider Definition Id: Id:john.smith@fakecompany.com Origin:Salesforce Organization Id: But many would argue that an email is unique, and they are true in the case where we say that an email will uniquely identify “something”. It might not be a person, it may be a group or in some cases it may be a randomly generated email address that is used in automation or to track if an email is opened or not. This is why Vocabularies are very important. Vocabularies are your way to map certain properties to Core Vocabularies and in-turn you will be telling CluedIn to do some special processing on those values. One of the goals of the core vocabulary mappings is for CluedIn to make the bridge of Entity Codes between different integration sources. This also means that you don’t need to place any special business logic into your custom integrations. Your custom integrations should remain as simple as possible and only fetch and map data into Clues. If you find that there is not a suitable Core Vocabulary to map your data into then you will need to implement a new processor on the CluedIn server - your integrations are not a place to place this complex logic. Some good examples of Entity Code Id’s are: Passport Number Employee Id Twitter handle Tax File Number Social Security Number Company Id IBAN Edges are a part of the Clue object that is responsible for pointing to reference objects that may or may not exist now or in the future. Edges are one of the key pieces of CluedIn that allow you to properly map data across many different data sources. To create an Edge, you will only need the Entity Type and the Id of the target record. Most importantly, you don’t need to specify what source that CluedIn will actually find this record in. The only important part is that the Entity Type and Id need to 100% match. If you have this in place, then CluedIn will do all the work to join your data where a join is identified. Aliases are your way to set values that are a hint to a unique identity, but in no way are unique. Some good examples include references to a record that could uniquely identify different companies, people etc. The following are good examples of Aliases: Phone Numbers Nick Names User Names Initials Addresses The Created and Modified Dates for a Clue are important. If these are available in your source system, it is important to map these into your Clues. Do not manually set these Dates e.g. DateTimeOffset.UtcNow as this means that CluedIn will think that this record has changed, and then on evaluation we will realise that potentially, no properties were changed. CluedIn will generate a DiscoveryDate for you, so you do not need to manually set this. When searching for records in CluedIn and sorting by “New”, CluedIn will always set the highest of the 3 Date Properties (Created, Modified, Discovered) and will set the SortDate for you automatically. Setting the Authors in your Clue is your way to add references to users that have modified, created or worked on your records. Each Clue can have many Authors. For setting Authors, you will set a PersonReference, which is very similar to an Entity Code, just without an Origin. Clues can be submitted to CluedIn in many ways. The preferred method is via the SDK’s and Crawler Templates that we make available, however it can also be done using JSON. For example, you could post a Clue like so: { \"clue\": { \"attribute-organization\": \"62d0a0d7-8871-4d0c-957e-71a24961d28e\", \"attribute-origin\": \" Infrastructure User#PeopleSoft:300036761\", \"attribute-appVersion\": \"1.8.0.0\", \"clueDetails\": { \"data\": { \"attribute-originProviderDefinitionId\": \"6b82efc3-3a50-42e5-b401-d217b7d83bae\", \"attribute-origin\": \" Infrastructure User#PeopleSoft:300036761\", \"attribute-appVersion\": \"1.8.0.0\", \"attribute-inputSource\": \"cluedin\", \"entityData\": { \"attribute-origin\": \" Infrastructure User#PeopleSoft:300036761\", \"entityType\": \" Infrastructure User\", \"name\": \"Blue,Matthew\", \"aliases\": [ \"300036761\", \"36927\" ], \"codes\": [\" Infrastructure User#PeopleSoft:300036761\"], \"createdDate\": \"0001-01-01T00:00:00Z\", \"modifiedDate\": \"0001-01-01T00:00:00Z\", \"edges\": { \"outgoing\": { \"edge\": { \"attribute-type\": \" root\", \"attribute-creationOptions\": \"Default\", \"attribute-from\": \"C: Infrastructure User#PeopleSoft:300036761\", \"attribute-to\": \"C: Provider Root#PeopleSoft:PeopleSoft\" } } }, \"properties\": { \"attribute-type\": \" Metadata KeyValue\", \"property-peoplesoft.person.address\": \"860 West Levoy Drive\", \"property-peoplesoft.person.city\": \"Taylorsville\", \"property-peoplesoft.person.country\": \"USA\", \"property-peoplesoft.person.firstName\": \"Matthew\", \"property-peoplesoft.person.lastName\": \"Blue\", \"property-peoplesoft.person.postal\": \"84123\", \"property-peoplesoft.person.sex\": \"U\", \"property-peoplesoft.person.state\": \"UT\" } } } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-core-20vocabularies-html": {
						"id": "docs-91-developer-portal-110-object-model-core-20vocabularies-html",
						"title": "Core Vocabularies",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Core%20Vocabularies.html",
						"content": "Core Vocabularies are where a lot of the “smarts” in CluedIn exist. If you map your custom integration sources into Core Vocabularies, there is a high likelihood that something will be done on the data to your advantage. For example, if you map a phone number into the Core Vocabulary Phone Number then CluedIn will, amongst other things: Create new normalised representations of the phone number in industry standards e.g. E164. Will see if we can confidentially identify or merge records based off this phone number. Important Core Vocabularies EditUrl PreviewUrl Body The role of Core Vocabularies is to merge records from different systems based off knowing that these Clues will have a different Origin. As you have already learnt about Entity Codes, they need to be exact for a merge to occur and simply the change of the Origin could cause records not to merge. More often than not, you might find that you will need to introduce new Vocabularies that help you map data between different systems. It is however that you will not introduce your own Core Vocabularies. Core Vocabularies are what CluedIn produce and if you need to introduce your own Vocabulary mappings then you will build your own keys. Each key that you will introduce will need you to introduce a Processor in the processing pipeline that instructs CluedIn how to process data in those keys. It could be something as simple as it “Lowercases” the values or it could be as complex as “It runs fuzzy string manipulation over the values”. As you learnt from the Vocabularies section, you can have a hierarchy of Vocabularies. The Core Vocabularies from CluedIn are the highest possible root in this tree. You cannot map Core Vocabularies down to lower levels, but you should map lower level Vocabularies up to CluedIn Core Vocabularies. Core Vocabularies do not include the namespace for a source system. It will typically have the Entity Type, the name of the key - or if it is a nested key like and address, it will have the nesting shown in the key name e.g. organization.industry organization.address.city organization.address.countryName person.age person.gender These are all valid Core Vocabularies. Can I add my own Core Vocabularies? Although you can, we suggest that you do not. The reason is mainly due to upgrade support and making sure your upgrades are as seemless and automated as possible. How do I nest my Vocabularies? Instead of creating a VocabularyKey in your Vocabularies, you can create a CompositeVocabularyKey. This will ask you to set a Key and a Base Key. This is how you will build up your tree of Vocabularies."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-data-20part-html": {
						"id": "docs-91-developer-portal-110-object-model-data-20part-html",
						"title": "Data Part",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Data%20Part.html",
						"content": "When a Clue becomes or is merged with an Entity, it becomes a “Data Part” of that Entity. This is a more smaller representation of a Clue that holds only the necessary parts for the Entity. This is important to know as sometimes you may want to transform data at a “Data Part” level instead of at an Entity Level. A good example would be cleaning of data. Imagine a situation where you have an Entity for a person that was composed of many Data Parts where one part said this persons age was 44 and another part said forty four. Even though the Golden Record Evaluator might have chosen 44 as the “best” value - it still means that if you went to clean this data at the Entity Level, you would only have the opportunity to clean the 44 value and not the “forty four”. This is an example where instead of cleaning records at the entity level, you will want to do it at the data part level instead. You can view all the data parts of a record by viewing the “History” tab on the Entity Page."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-dynamic-20vocabularies-html": {
						"id": "docs-91-developer-portal-110-object-model-dynamic-20vocabularies-html",
						"title": "Dynamic Vocabularies",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Dynamic%20Vocabularies.html",
						"content": "There are many times where you will get data that cannot be statically mapped in code or via a mapping in a user interface. In CluedIn these will map to dynamic vocabularies. Dynamic vocabularies are a way to tell CluedIn that a particular value has no Vocabulary mapping. Essentially, CluedIn will start to inspect the values in these properties to infer the types and details of the static Vocabularies. This is not necessarily a bad thing and even in the static mapping cases, CluedIn will analyse the values anyway as to determine if a Vocabulary mapping was done incorrectly. This is important for anomaly detection and outlier detection of values. You can set a Dynamic Vocabulary by adding the “-custom” flag at the end of your Vocabulary Key. This will instruct the CluedIn Processing Server that it will need to do extra analysis on the values to help determine things that can statically be set with normal Vocabularies such as Data Types. Be aware that every time you use the “-custom” flag, that you will incur more than normal processing on the server. If you use the DynamicVocabulary class for your Vocabulary then it will handle all of the “-custom” flag for you."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-edge-20types-html": {
						"id": "docs-91-developer-portal-110-object-model-edge-20types-html",
						"title": "Edge Types",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Edge%20Types.html",
						"content": "Edges Types are a way to determine the relationships between data. This is typically in the structure of a Object - Verb - Object. For example, John - works at - Lego. In CluedIn, edges can store properties such as weights and general metadata, but the main idea behind these edges is to describe the relationship of two nodes for processing and querying purposes. In CluedIn, there are static Edge Types and Dynamic Edge Types. Static Edge Types are your way to set an Edge Type based off known rules that will not change. All other Edge Types should be Dynamic. It is always recommended to leave Edges in crawlers as generic as possible and introduce new processors in the processing server to dynamically resolve generic edge types into specific ones. Imagine you have an edge type of “Works At” that you set statically in your crawlers - you can see that it has a temporal factor to it, in that you have no guarantee that this will always be “Works At”. Due to this, you can introduce new processors that would check other values e.g. A Job start and end date, and use this to dynamically change the edge type to “Worked At” if this person was ever to leave. Here is a list of the Edge Types that are in the base installation of CluedIn: namespace CluedIn.Core.Data { &lt;summary&gt; The entity edge type. &lt; summary&gt; public partial class EntityEdgeType { ********************************************************************************************************** * FIELDS ********************************************************************************************************** &lt;summary&gt;At edge type&lt; summary&gt; public static readonly EntityEdgeType At = \" At\"; &lt;summary&gt;The created at&lt; summary&gt; public static readonly EntityEdgeType CreatedAt = \" CreatedAt\"; &lt;summary&gt;The modified at edge type&lt; summary&gt; public static readonly EntityEdgeType ModifiedAt = \" ModifiedAt\"; &lt;summary&gt;The discovered at edge type&lt; summary&gt; public static readonly EntityEdgeType DiscoveredAt = \" DiscoveredAt\"; &lt;summary&gt;The author edge type&lt; summary&gt; public static readonly EntityEdgeType Author = \" Author\"; &lt;summary&gt;The birthday edge type&lt; summary&gt; public static readonly EntityEdgeType Birthday = \" Birthday\"; &lt;summary&gt;The part of edge type&lt; summary&gt; public static readonly EntityEdgeType PartOf = \" PartOf\"; &lt;summary&gt;The parent edge type&lt; summary&gt; public static readonly EntityEdgeType Parent = \" Parent\"; &lt;summary&gt;The located in&lt; summary&gt; public static readonly EntityEdgeType LocatedIn = \" LocatedIn\"; &lt;summary&gt;The follows&lt; summary&gt; public static readonly EntityEdgeType Follows = \" Follows\"; &lt;summary&gt;The used by&lt; summary&gt; public static readonly EntityEdgeType UsedBy = \" UsedBy\"; &lt;summary&gt;The requested by&lt; summary&gt; public static readonly EntityEdgeType RequestedBy = \" RequestedBy\"; &lt;summary&gt;The mentioned&lt; summary&gt; public static readonly EntityEdgeType Mentioned = \" Mentioned\"; &lt;summary&gt;The represents.&lt; summary&gt; public static readonly EntityEdgeType Represents = \" Represents\"; &lt;summary&gt;The action&lt; summary&gt; public static readonly ActionEntityEdgeType Action = new ActionEntityEdgeType(); ********************************************************************************************************** public static readonly EntityEdgeType SimilarTo = \" SimilarTo\"; public static readonly EntityEdgeType WorksFor = \" WorksFor\"; public static readonly EntityEdgeType Read = \" Read\"; public static readonly EntityEdgeType ReadWrite = \" ReadWrite\"; public static readonly EntityEdgeType InvitedTo = \" InvitedTo\"; public static readonly EntityEdgeType StartedOn = \" StartedOn\"; public static readonly EntityEdgeType EndedOn = \" EndedOn\"; public static readonly EntityEdgeType Received = \" Received\"; public static readonly EntityEdgeType DueOn = \" DueOn\"; public static readonly EntityEdgeType WitheldIn = \" WitheldIn\"; public static readonly EntityEdgeType DeletedBy = \" DeletedBy\"; public static readonly EntityEdgeType CompletedOn = \" CompletedOn\"; public static readonly EntityEdgeType ManagedIn = \" ManagedIn\"; public static readonly EntityEdgeType AttachedTo = \" AttachedTo\"; public static readonly EntityEdgeType Deployed = \" Deployed\"; public static readonly EntityEdgeType Has = \" Has\"; public static readonly EntityEdgeType Involves = \" Involves\"; public static readonly EntityEdgeType MemberOf = \" MemberOf\"; public static readonly EntityEdgeType WorkedOn = \" WorkedOn\"; public static readonly EntityEdgeType WorkedOnBy = \" WorkedOnBy\"; public static readonly EntityEdgeType Recipient = \" Recipient\"; public static readonly EntityEdgeType CreatedBy = \" CreatedBy\"; public static readonly EntityEdgeType Created = \" Created\"; public static readonly EntityEdgeType For = \" For\"; public static readonly EntityEdgeType Attended = \" Attended\"; public static readonly EntityEdgeType Presented = \" Presented\"; public static readonly EntityEdgeType Owns = \" Owns\"; public static readonly EntityEdgeType OwnedBy = \" OwnedBy\"; public static readonly EntityEdgeType Registered = \" Registered\"; public static readonly EntityEdgeType ManagedBy = \" ManagedBy\"; public static readonly EntityEdgeType Modified = \" Modified\"; public static readonly EntityEdgeType ModifiedBy = \" ModifiedBy\"; public static readonly EntityEdgeType Competitor = \" Competitor\"; public static readonly EntityEdgeType IsType = \" Is\"; public static readonly EntityEdgeType Investor = \" Investor\"; public static readonly EntityEdgeType ApprovedBy = \" ApprovedBy\"; ********************************************************************************************************** * INNER TYPES ********************************************************************************************************** public class ActionEntityEdgeType : HierarichalEntityEdgeType { public readonly EntityEdgeType ModifiedBy = \" Action ModifiedBy\"; public readonly EntityEdgeType CreatedBy = \" Action CreatedBy\"; public readonly EntityEdgeType RemovedBy = \" Action RemovedBy\"; public readonly EntityEdgeType RevertedBy = \" Action RevertedBy\"; public readonly EntityEdgeType SynchedBy = \" Action SynchedBy\"; protected override EntityEdgeType BaseType { get { return \" Action\"; } } } public abstract class HierarichalEntityEdgeType { &lt;summary&gt;Gets the entity type base.&lt; summary&gt; &lt;value&gt;The entity type base.&lt; value&gt; protected abstract EntityEdgeType BaseType { get; } &lt;summary&gt;Implements the operator ==.&lt; summary&gt; &lt;param name=\"type1\"&gt;The type1.&lt; param&gt; &lt;param name=\"type2\"&gt;The type2.&lt; param&gt; &lt;returns&gt;The result of the operator.&lt; returns&gt; public static bool operator ==(EntityEdgeType type1, HierarichalEntityEdgeType type2) { return type1 == (EntityEdgeType)type2; } &lt;summary&gt;Implements the operator !=.&lt; summary&gt; &lt;param name=\"type1\"&gt;The type1.&lt; param&gt; &lt;param name=\"type2\"&gt;The type2.&lt; param&gt; &lt;returns&gt;The result of the operator.&lt; returns&gt; public static bool operator !=(EntityEdgeType type1, HierarichalEntityEdgeType type2) { return type1 != (EntityEdgeType)type2; } &lt;summary&gt;Implements the operator ==.&lt; summary&gt; &lt;param name=\"type1\"&gt;The type1.&lt; param&gt; &lt;param name=\"type2\"&gt;The type2.&lt; param&gt; &lt;returns&gt;The result of the operator.&lt; returns&gt; public static bool operator ==(HierarichalEntityEdgeType type1, EntityEdgeType type2) { return ((EntityEdgeType)type1) == type2; } &lt;summary&gt;Implements the operator !=.&lt; summary&gt; &lt;param name=\"type1\"&gt;The type1.&lt; param&gt; &lt;param name=\"type2\"&gt;The type2.&lt; param&gt; &lt;returns&gt;The result of the operator.&lt; returns&gt; public static bool operator !=(HierarichalEntityEdgeType type1, EntityEdgeType type2) { return ((EntityEdgeType)type1) != type2; } &lt;summary&gt; Performs an implicit conversion from &lt;see cref=\"HierarichalEntityType\" &gt; to &lt;see cref=\"EntityType\" &gt;. &lt; summary&gt; &lt;param name=\"type\"&gt;The type.&lt; param&gt; &lt;returns&gt;The result of the conversion.&lt; returns&gt; public static implicit operator EntityEdgeType(HierarichalEntityEdgeType type) { return type.BaseType; } &lt;summary&gt; Performs an implicit conversion from &lt;see cref=\"HierarichalEntityEdgeType\" &gt; to &lt;see cref=\"System.String\" &gt;. &lt; summary&gt; &lt;param name=\"type\"&gt;The type.&lt; param&gt; &lt;returns&gt;The result of the conversion.&lt; returns&gt; public static implicit operator string(HierarichalEntityEdgeType type) { return type.BaseType; } &lt;summary&gt; Returns a &lt;see cref=\"System.String\" &gt; that represents this instance. &lt; summary&gt; &lt;returns&gt;A &lt;see cref=\"System.String\" &gt; that represents this instance.&lt; returns&gt; public override string ToString() { return ((EntityEdgeType)this).ToString(); } &lt;summary&gt;Returns a hash code for this instance.&lt; summary&gt; &lt;returns&gt; A hash code for this instance, suitable for use in hashing algorithms and data structures like a hash table. &lt; returns&gt; public override int GetHashCode() { return ((EntityEdgeType)this).GetHashCode(); } &lt;summary&gt; Determines whether the specified &lt;see cref=\"System.Object\" &gt;, is equal to this instance. &lt; summary&gt; &lt;param name=\"obj\"&gt;The &lt;see cref=\"System.Object\" &gt; to compare with this instance.&lt; param&gt; &lt;returns&gt; &lt;c&gt;true&lt; c&gt; if the specified &lt;see cref=\"System.Object\" &gt; is equal to this instance; otherwise, &lt;c&gt;false&lt; c&gt;. &lt; returns&gt; public override bool Equals(object obj) { return this.BaseType.Equals(obj); } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-edges-html": {
						"id": "docs-91-developer-portal-110-object-model-edges-html",
						"title": "Edges",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Edges.html",
						"content": "An Edge is your way to be able to create relationships between records. This relationship will consist of a source and a target, or a “to” and a “from”. As well as this, you will have an Edge Type. This Edge Type will explain the relationship e.g. WorksFor, LivesWith. Each edge can also contain properties including a Weight and a general property bag. When you are creating edges in your solution, you will also notice that you can set a “Weight”. This value is to indicate the strength of the relationship between two entities. This weight is used in the processing pipeline to help evaluate decisions. How do I pass data to another record through and edge There are many cases where you will denormalise references to other entities, but you do want to have some friendly values to reference these records. When you are creating your edges, you will notice that you can pass in extra properties on either the “from” reference or the “to” reference. Placing properties on these references will propagate these values onto the appropriate entity. For example, if you were creating a clue for a company and it had a reference to a country via an Id. In this case you would want to reference that country, but that the country bring back friendly values to the company clue so that it is easy to look at in the user interface. In this way, if you update the values in the country clue then CluedIn will automatically update all references to this country. The properties is only for passing Vocabularies. By default, it will only copy these properties to the target reference when that target reference exists."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-20codes-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-20codes-html",
						"title": "Entity Codes",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity%20Codes.html",
						"content": "An Entity Code is a way to instruct CluedIn to know what a completey unique reference to a Clue is. It is important to establish that an Entity Code will cause two Clue objects to merge with 100% confidence. An Entity Code is made up from 5 different pieces: Entity Type Provider Definition Id Id Origin Organization Id This combination will allow us to achieve absolute uniqueness across any datasource that we interact with. Entity Codes will often be unique to the source system only, this is fine. Every now and then you will ingest a property that allows you to bridge different sources. It is not recommended to build Entity Codes where you force the Origin to be that of a common Origin e.g. “CluedIn”. The role of the Vocabulary mappings is to handle this type of operation on the processing server. There are many times where Identifiers for Entity Codes are dirty, or not ready to be made into a unique Entity Code. Examples include data where you have default or fallback values where a real value is not present. Imagine that you had an EmployeeId property or column in your data and when a value was not present, someone would place “NONE”, “”, “N A” or other values. These are obviously not valid Entity Codes pertaining to the EmployeeId. This is completely fine in CluedIn, in fact we expect this to happen all the time. The important aspect is that you will not be able to upfront handle all permutations of this “placeholder” and hence you should still create Entity Codes with the intention that these values are unique. We will fix and clean this up later. There are many aspects of an Entity Code that will lead to slower processing time. If you find that you have a large amount of duplicate data that would cause you to have Entities with 1000’s of EntityCodes, then processing this record will take significant time. Be careful of using Entity Codes that would cause this type of behaviour. It doesn’t mean that you should not, it just means that you will pay a processing penalty. There are also CluedIn specific Entity Codes. These are codes that use the Origin of “CluedIn” and then have a paramter in brackets that specifies a type of Origin e.g. CluedIn(cvr). This Origin shows that CluedIn has a generic, non-source specific Id from a record. How do I make sure Entity Codes will blend across different data sources? Now that we know that an Entity Code will only merge with another Entity Code if they are 100% the same, we have to talk about how you would be able to merge records across different systems if the Origin will be different. There are many ways to achieve this, but one of the interesting ways is the “GUID trick”. If a record has an Identifier that is a Guid UUID, you can set the Origin as CluedIn as no matter the system, this should be unique. (Unless you are using determenistic GUIDS). We would recommend to do this and run tests to find out if this is the case. If you are wondering if you do use determinstic GUID’s, doing some pre analysis on the data will help you but figuring our if many GUIDS overlap in a certain sequence of the GUID e.g. you might mind the first chunk of the GUID is replicated many times. This is a strong indicator you are using determistic Id’s. GUID’s (random ones) are so unique that the chance of them being the same is close to impossible. Here is the other interesting part of this “trick”. You could even determine that the EntityType can be generic as well due to its ability to be unique. You will have to craft these special entity codes in you Clues to e.g. look somemthing like Generic#CluedIn:. You will need to make sure your edges support the same mechanism. In doing this, your are instructing CluedIn that no matter the Entity Type, no matter the origin of the data, this record can be uniquely idetified by JUST the GUID. What if your record doesn’t have a unique reference to construct an Entity Code? This happens all the time. Often you will find that you need to merge or link records across systems that don’t have Id’s but rather require a more rules based or fuzzy merging to be able to link records. In this case, we will often suggest to create a composite Entity Code i.e. an Entity Code that you have constructed from a combination of column or property values that guarantee uniqueness. For example, if you have a Transaction record, you might find that a combination of the Transaction Date, Product, Location and Store will guarantee uniqueness. It is best to calculate a “Hash” of these values combined which means that we can calculate an Entity Code from this."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-20type-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-20type-html",
						"title": "Entity Type",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity%20Type.html",
						"content": "An Entity Type is your way to tell a Clue its base object type. This could be any of the inbuilt Entity Types from CluedIn, or it could be custom types as well. A Clue can only have one type, but an entity may have decided its type based off different entity types from many Clues. For example, if one Clue was of type “ Infrastructure Contact” and one was “Person”, then the final Entity will choose one of these types - you cannot have more than one Entity Type. Why do Entity Types have a “ ” in them? CluedIn has a nested hierarchy of Entity Types that will separate the types by a forward slash. This hierarchy could be something such as “ Sales Deal”, indicating that the “Deal” type is a child of the “Sale” entity type. We sometimes refer to these as “Namespaces”. By choosing certain Entity Types, CluedIn will do some automatic processing on that data. For example, if you add a custom Entity Type of “ Car”, then by default, CluedIn will most likely do absolutely no “smarts” on this entity. This is simply due to the fact that the CluedIn processing server will only listen to certain Entity Types and run extra processing on it. Your custom data will be persisted and made available - there is no issues with adding new types. If you would like to add “smarts” into the CluedIn processing server to handle your new Entity Type and either clean, validate, enrich or other - then you will need to add new Processors, based off the IProcessing interface. You can access all the default Entity Types using the Static EntityType class. Think of Entity Types like Domain Objects. Here is the list of supported Entity Types in a base install of CluedIn: namespace CluedIn.Core.Data { public partial class EntityType { private static readonly IDictionary&lt;string, EntityTypeSettings&gt; SettingsCache = new ConcurrentDictionary&lt;string, EntityTypeSettings&gt;(); public static readonly TemporalEntityType Temporal = new TemporalEntityType(); public static readonly GeographyEntityType Geography = new GeographyEntityType(); public static readonly LocationEntityType Location = new LocationEntityType(); public static readonly ProviderEntityType Provider = new ProviderEntityType(); public static readonly InfrastructureEntityType Infrastructure = new InfrastructureEntityType(); public static readonly EntityType Person = \" Person\"; public static readonly OrganizationEntityType Organization = new OrganizationEntityType(); public static readonly FilesEntityType Files = new FilesEntityType(); public static readonly DocumentsEntityType Documents = new DocumentsEntityType(); public static readonly EntityType Document = \" Document\"; public static readonly ImagesEntityType Images = new ImagesEntityType(); public static readonly MailEntityType Mail = new MailEntityType(); public static readonly MarketingEntityType Marketing = new MarketingEntityType(); public static readonly EntityType Project = \" Project\"; public static readonly EntityType Topic = \" Topic\"; public static readonly EntityType Tag = \" Tag\"; public static readonly CommentEntityType Comment = new CommentEntityType(); public static readonly EntityType Announcement = \" Announcement\"; public static readonly EntityType Partner = \" Partner\"; public static readonly EntityType Discussion = \" Discussion\"; public static readonly EntityType Note = \" Note\"; public static readonly EntityType PressRelease = \" PressRelease\"; public static readonly EntityType Account = \" Account\"; public static readonly EntityType Activity = \" Activity\"; public static readonly EntityType Form = \" Form\"; public static readonly EntityType Process = \" Process\"; public static readonly EntityType ProcessStage = \" ProcessStage\"; public static readonly EntityType Question = \" Question\"; public static readonly ListEntityType List = new ListEntityType(); public static readonly EntityType News = \" News\"; public static readonly EntityType FAQ = \" FAQ\"; public static readonly EntityType Task = \" Task\"; public static readonly EntityType Skill = \" Skill\"; public static readonly IssueEntityType Issue = new IssueEntityType(); public static readonly CalendarEntityType Calendar = new CalendarEntityType(); public static readonly SourceCodeEntityType SourceCode = new SourceCodeEntityType(); public static readonly PlanningEntityType Planning = new PlanningEntityType(); public static readonly SalesEntityType Sales = new SalesEntityType(); public static readonly EntityType Component = \" Component\"; TODO: Change to Software Component public static readonly EntityType Channel = \" Channel\"; public static readonly EntityType Product = \" Product\"; public static readonly EntityType Enquiry = \" Enquiry\"; public static readonly EntityType Idea = \" Idea\"; public static readonly EntityType Link = \" Links\"; public static readonly EntityType Certification = \" Certification\"; public static readonly EntityType Unknown = \" Unknown\"; public static readonly EntityType Card = \" Card\"; public static readonly EntityType Template = \" Template\"; public static readonly EntityType Industry = \" Industry\"; public static readonly EntityType PhoneNumber = \" PhoneNumber\"; public static readonly EntityType PhoneCall = \" PhoneCall\"; public static readonly EntityType Sms = \" Sms\"; public static readonly EntityType VoiceMail = \" VoiceMail\"; public static readonly WebEntityType Web = new WebEntityType(); public static readonly AccountingEntityType Accounting = new AccountingEntityType(); public static readonly SupportEntityType Support = new SupportEntityType(); public static readonly PaymentEntityType Payment = new PaymentEntityType(); public static readonly FinanceEntityType Finance = new FinanceEntityType(); public static readonly ConfigurationEntityType Configuration = new ConfigurationEntityType(); public static readonly HumanResourcesEntityType HR = new HumanResourcesEntityType(); public static EntityType FromFileCategory(FileCategory category) { switch (category) { case FileCategory.Audio: return EntityType.Documents.Audio; case FileCategory.Video: return EntityType.Documents.Video; case FileCategory.PlainText: return EntityType.Documents.PlainText; case FileCategory.TextDocument: return EntityType.Documents.Document; case FileCategory.Presentation: return EntityType.Documents.Presentation; case FileCategory.Spreadsheet: return EntityType.Documents.Spreadsheet; case FileCategory.ImageBitmap: return EntityType.Images.Image; case FileCategory.Diagram: return EntityType.Documents.Diagram; case FileCategory.CompressedFileArchive: return EntityType.Files.CompressedFileArchive; case FileCategory.Win32Executable: break; } return null; } public class TemporalEntityType : HierarichalEntityType { public readonly EntityType Date = \" Temporal Date\"; public readonly EntityType Week = \" Temporal Week\"; public readonly EntityType Quarter = \" Temporal Quarter\"; public readonly EntityType Month = \" Temporal Month\"; public readonly EntityType Year = \" Temporal Year\"; public readonly EntityType Decade = \" Temporal Decade\"; public readonly EntityType Century = \" Temporal Century\"; public readonly EntityType Millennium = \" Temporal Millennium\"; protected override EntityType BaseType { get { return \" Temporal\"; } } } public class InfrastructureEntityType : HierarichalEntityType { public readonly EntityType User = \" Infrastructure User\"; public readonly EntityType Location = \" Infrastructure Location\"; public readonly EntityType Contact = \" Infrastructure Contact\"; public readonly EntityType License = \" Infrastructure License\"; public readonly EntityType Policy = \" Infrastructure Policy\"; public readonly EntityType Application = \" Infrastructure Application\"; public readonly EntityType Domain = \" Infrastructure Domain\"; public readonly EntityType Printer = \" Infrastructure Printer\"; public readonly EntityType Folder = EntityType.Create(\" Infrastructure Folder\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Cloud = \" Infrastructure Cloud\"; public readonly EntityType Tenant = \" Infrastructure Tenant\"; public readonly EntityType Site = EntityType.Create(\" Infrastructure Site\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Position = \" Infrastructure Position\"; TODO public readonly InfrastructureGroupEntityType Group = new InfrastructureGroupEntityType(); public readonly InfrastructureHostEntityType Host = new InfrastructureHostEntityType(); public readonly EntityType DirectoryItem = \" Infrastructure DirectoryItem\"; public readonly EntityType NetworkAddress = \" Infrastructure NetworkAddress\"; protected override EntityType BaseType { get { return \" Infrastructure\"; } } } public class InfrastructureGroupEntityType : HierarichalEntityType { public readonly EntityType Security = \" Infrastructure Group Security\"; public readonly EntityType Distribution = \" Infrastructure Group Distribution\"; protected override EntityType BaseType { get { return \" Infrastructure Group\"; } } } public class InfrastructureHostEntityType : HierarichalEntityType { public readonly EntityType Server = \" Infrastructure Host Server\"; public readonly EntityType Computer = \" Infrastructure Host Computer\"; public readonly EntityType Laptop = \" Infrastructure Host Laptop\"; protected override EntityType BaseType { get { return \" Infrastructure Host\"; } } } public class CommentEntityType : HierarichalEntityType { public readonly EntityType Social = \" Comment Social\"; protected override EntityType BaseType { get { return \" Comment\"; } } } public class OrganizationEntityType : HierarichalEntityType { public readonly EntityType Competitor = \" Organization Competitor\"; public readonly EntityType Department = \" Organization Department\"; public readonly EntityType Unit = \" Organization Unit\"; protected override EntityType BaseType { get { return \" Organization\"; } } } public class IssueEntityType : HierarichalEntityType { public readonly EntityType Type = \" Issue Type\"; public readonly EntityType Resolution = \" Issue Resolution\"; protected override EntityType BaseType { get { return \" Issue\"; } } } public class CalendarEntityType : HierarichalEntityType { public readonly EntityType Event = \" Calendar Event\"; public readonly EntityType Meeting = \" Calendar Meeting\"; protected override EntityType BaseType { get { return \" Calendar\"; } } } public class SourceCodeEntityType : HierarichalEntityType { public readonly EntityType Repository = EntityType.Create(\" SourceCode Repository\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Solution = EntityType.Create(\" SourceCode Solution\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Project = EntityType.Create(\" SourceCode Project\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Library = \" SourceCode Library\"; public readonly EntityType Branch = \" SourceCode Branch\"; public readonly EntityType PullRequest = \" SourceCode PullRequest\"; public readonly EntityType File = \" SourceCode File\"; public readonly EntityType ChangeSet = \" SourceCode ChangeSet\"; public readonly EntityType Build = \" SourceCode Build\"; public readonly EntityType Deployment = \" SourceCode Deployment\"; public readonly EntityType DeploymentStatus = \" SourceCode DeploymentStatus\"; protected override EntityType BaseType { get { return \" SourceCode\"; } } } public class PlanningEntityType : HierarichalEntityType { public readonly EntityType ProjectPlan = EntityType.Create(\" Planning ProjectPlan\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Iteration = EntityType.Create(\" Planning Iteration\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType KanbanBoard = EntityType.Create(\" Planning KanbanBoard\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Workspace = EntityType.Create(\" Planning Workspace\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Proposal = \" Planning Proposal\"; public readonly PlanningScrumEntityType Scrum = new PlanningScrumEntityType(); protected override EntityType BaseType { get { return \" Planning\"; } } } public class PlanningScrumEntityType : HierarichalEntityType { public readonly EntityType Sprint = \" Planning Scrum Sprint\"; public readonly EntityType BacklogItem = \" Planning Scrum BacklogItem\"; public readonly EntityType Epic = \" Planning Scrum Epic\"; public readonly EntityType SprintReview = \" Planning Scrum SprintReview\"; public readonly EntityType SprintRetrospective = \" Planning Scrum SprintRetrospective\"; public readonly EntityType DefinitionOfDone = \" Planning Scrum DefinitionOfDone\"; protected override EntityType BaseType { get { return \" Planning Scrum\"; } } } public class SalesEntityType : HierarichalEntityType { public readonly EntityType Sale = \" Sales Sale\"; public readonly EntityType Contract = \" Sales Contract\"; public readonly EntityType Deal = \" Sales Deal\"; public readonly EntityType Quote = \" Sales Quote\"; public readonly EntityType Lead = \" Sales Lead\"; public readonly EntityType Opportunity = \" Sales Opportunity\"; public readonly EntityType Order = \" Sales Order\"; protected override EntityType BaseType { get { return \" Sales\"; } } } public class SupportEntityType : HierarichalEntityType { public readonly EntityType Entitlement = \" Support Entitlement\"; public readonly EntityType Ticket = \" Support Ticket\"; protected override EntityType BaseType { get { return \" Support\"; } } } public class LocationEntityType : HierarichalEntityType { public readonly EntityType Address = \" Location Address\"; protected override EntityType BaseType { get { return \" Location\"; } } } public class GeographyEntityType : HierarichalEntityType { public readonly EntityType Country = \" Geography Country\"; public readonly EntityType City = \" Geography City\"; public readonly EntityType State = \" Geography State\"; public readonly EntityType TimeZone = \" Geography TimeZone\"; public readonly EntityType Territory = \" Geography Territory\"; protected override EntityType BaseType { get { return \" Geography\"; } } } public class FilesEntityType : HierarichalEntityType { public readonly EntityType File = \" Files File\"; public readonly EntityType Directory = EntityType.Create(\" Files Directory\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType CompressedFileArchive = EntityType.Create(\" Files CompressedFileArchive\", settings =&gt; settings.IsEntityContainer = true); protected override EntityType BaseType { get { return \" Files\"; } } } public class DocumentsEntityType : HierarichalEntityType { public readonly EntityType Database = \" Document Database\"; public readonly EntityType PlainText = \" Document PlainText\"; public readonly EntityType Document = \" Document Document\"; public readonly EntityType Presentation = \" Document Presentation\"; public readonly EntityType Spreadsheet = \" Document Spreadsheet\"; public readonly EntityType Audio = \" Document Audio\"; public readonly EntityType Video = \" Document Video\"; public readonly EntityType Diagram = \" Document Diagram\"; protected override EntityType BaseType { get { return \" Document\"; } } } public class ImagesEntityType : HierarichalEntityType { public readonly EntityType Image = \" Image\"; public readonly EntityType Album = EntityType.Create(\" Album\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Diagram = \" Image Diagram\"; public readonly EntityType Photograph = \" Image Photograph\"; protected override EntityType BaseType { get { return \" Image\"; } } } public class ProviderEntityType : HierarichalEntityType { public readonly EntityType Root = \" Provider Root\"; protected override EntityType BaseType { get { return \" Provider\"; } } } public class MailEntityType : HierarichalEntityType { public readonly EntityType Folder = EntityType.Create(\" Mail Folder\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Thread = EntityType.Create(\" Mail Thread\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Attachment = \" Mail Attachment\"; public readonly EntityType Task = \" Mail Task\"; [Obsolete] public readonly EntityType Event = \" Mail Event\"; TODO: Remove this protected override EntityType BaseType { get { return \" Mail\"; } } } public class MarketingEntityType : HierarichalEntityType { public readonly EntityType Campaign = \" Marketing Campaign\"; public readonly EntityType Goal = \" Marketing Goal\"; public readonly EntityType Ad = \" Marketing Ad\"; public readonly EntityType Persona = \" Marketing Persona\"; TODO: Remove this protected override EntityType BaseType { get { return \" Marketing\"; } } } public class WebEntityType : HierarichalEntityType { public readonly EntityType Site = EntityType.Create(\" Web Site\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Page = \" Web Page\"; protected override EntityType BaseType { get { return \" Web\"; } } } public class AccountingEntityType : HierarichalEntityType { public readonly EntityType Year = \" Accounting Year\"; public readonly EntityType Period = \" Accounting Period\"; public readonly EntityType CostCenter = EntityType.Create(\" Accounting CostCenter\", settings =&gt; settings.IsEntityContainer = true); public readonly EntityType Group = EntityType.Create(\" Accounting Group\", settings =&gt; settings.IsEntityContainer = true); protected override EntityType BaseType { get { return \" Accounting\"; } } } public class ListEntityType : HierarichalEntityType { public readonly EntityType Item = \" List ListItem\"; private readonly EntityType baseType = EntityType.Create(\" List\", settings =&gt; settings.IsEntityContainer = true); protected override EntityType BaseType =&gt; baseType; } public class PaymentEntityType : HierarichalEntityType { public readonly PaymentCardEntityType Card = new PaymentCardEntityType(); protected override EntityType BaseType =&gt; \" Payment\"; public class PaymentCardEntityType : HierarichalEntityType { public readonly EntityType CreditCard = \" Payment Card CreditCard\"; public readonly EntityType DebitCard = \" Payment Card DebitCard\"; public readonly EntityType ChargeCard = \" Payment Card ChargeCard\"; public readonly EntityType ATMCard = \" Payment Card ATMCard\"; public readonly EntityType StoreValueCard = \" Payment Card StoreValueCard\"; public readonly EntityType GiftCard = \" Payment Card GiftCard\"; public readonly EntityType StoreCard = \" Payment Card StoreCard\"; protected override EntityType BaseType =&gt; \" Payment Card\"; } } public class FinanceEntityType : HierarichalEntityType { public readonly EntityType BankAccount = \" Finance BankAccount\"; protected override EntityType BaseType =&gt; \" Finance\"; } public class ConfigurationEntityType : HierarichalEntityType { public readonly EntityType Setting = \" Configuration Setting\"; protected override EntityType BaseType =&gt; \" Configuration\"; } public class HumanResourcesEntityType : HierarichalEntityType { public readonly EntityType WorkSchedule = \" HR WorkSchedule\"; protected override EntityType BaseType =&gt; \" HR\"; } public abstract class HierarichalEntityType { protected abstract EntityType BaseType { get; } public static bool operator ==(EntityType type1, HierarichalEntityType type2) { return type1 == (EntityType)type2; } public static bool operator !=(EntityType type1, HierarichalEntityType type2) { return type1 != (EntityType)type2; } public static bool operator ==(HierarichalEntityType type1, EntityType type2) { return ((EntityType)type1) == type2; } public static bool operator !=(HierarichalEntityType type1, EntityType type2) { return ((EntityType)type1) != type2; } public static implicit operator EntityType(HierarichalEntityType type) { return type.BaseType; } public static implicit operator string(HierarichalEntityType type) { return type.BaseType; } public override string ToString() { return ((EntityType)this).ToString(); } public override int GetHashCode() { return ((EntityType)this).GetHashCode(); } public override bool Equals(object obj) { return this.BaseType.Equals(obj); } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-20types-infrastructure-20user-20entity-20type-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-20types-infrastructure-20user-20entity-20type-html",
						"title": "Infrastructure User Entity Type",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Infrastructure%20User%20Entity%20Type.html",
						"content": "The Infrastructure User Entity Type is for a Clue where it is ambiguous what the Entity Type of a record is. It is mainly reserved for the following types of Entity Types where you cannot statically set a type: Contact Person Group You should use this Entity Type when you are not guaranteed that the data your are pulling, represents a Person and you would like CluedIn to do some extra processing to determine it for you. Examples could include: If you have mapped a Birthday of a record into the Core Person Vocabulary and it has a valid Birthday, then CluedIn will change this into a Person from an Infrastructure User. If you have a valid First Name and Last Name then this will be changed into a Person. If a Gender is set then this will change into a Person. If an age is set then this will change into a Person."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-20types-organization-20entity-20type-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-20types-organization-20entity-20type-html",
						"title": "Organization Entity Type",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Organization%20Entity%20Type.html",
						"content": "The Organization Entity Type is a very special Entity type, where by marking one of your Clues as an Organization, then many “smarts” of the CluedIn processing engine will kick in. Here is a list of the types of processing that will happen on a Clue if you make the Entity Type of that clue an “ Organization”. Fuzzy Merging If you have decided to make your Clue objects of type “ Organization”, then you will find that by default, CluedIn will run Fuzzy Merging on your records. By default, we will use the Vocabularies that are mapped into the “Organization” Vocabulary as “features” that play a role in the Fuzzy Merging of records. This means that if the properties of your Clue are similar enough to be over the built in statistical thresholds, then CluedIn will merge many records for you automatically. Web Crawling If you have enabled the CluedIn Web Crawler (External Search) and that you have mapped a value for the Organization Website Vocabulary, then the inbuilt Web Crawler for CluedIn will lookup that website and will crawl the website for extra information."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-20types-person-20entity-20type-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-20types-person-20entity-20type-html",
						"title": "Person Entity Type",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity%20Types/Person%20Entity%20Type.html",
						"content": "The Person Entity Type is a special Entity Type that if you are to set your Clue objects with this Entity Type, then many “smarts” will kick in on the CluedIn processing server. Examples of this “smarts” include: Map First, Middle and Last Names given statistical models e.g. If you have set the Name of your Clue as “John Henry Smith”, then CluedIn will attempt to set the First Name, Middle Name and Last Name if it passes our inbuilt models and tests. Automatically normalise Phone Numbers to E164, RFC3966 and International standards. Attempts will be made to validate if an email address is for a person or a group contact."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-entity-html": {
						"id": "docs-91-developer-portal-110-object-model-entity-html",
						"title": "Entity",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Entity.html",
						"content": "An Entity is the object model that CluedIn builds based off one or many Clue objects. An Entity will contain a history of every single Clue that was merged into, connected to or enriched a particular Entity. An Entity will store an object, referred to as the ProcessedMetaData, which is a collection of data that CluedIn has decided is the “instance” or “version” of this record that it deemed to be the highest fidelity, most accurate and most statically confident permutation of the data for this record. Hence in the many different datastores for CluedIn, some of the database providers will only store a single permutation of an Entity, where others will store the full history. Graph Store: Will only store the Processed Golden Version of a record. Search Store: Will store only the Processed Golden Version of the record. Blob Store: Will store every single piece of data on the record and all history. When an Entity is made from multiple Clue objects, CluedIn will store the history of this record. You can see this history in the “History” tab of our Entity record. An Entity can be seen as the end goal of data within CluedIn. When accessing the data within your CluedIn account, you will always be accessing Entities, never Clues. The reason for this is that an Entity is one many Clues that have passed all the checks that tell CluedIn that a Clue is valuable, valid and has enough data in it to be useful. An Entity has a special property called the ProcessedEntityMetaData. This contains an amalgamation of the properties that have come from the different respective Clues or “Data Parts”. CluedIn will decide which properties from which Data Parts are elavated to the ProcessedEntityMetaData based off many factors including, Modification or Creation Dates, Data Metrics, Source System. What you will receive in the end is a ProcessedEntityMetaData object that contains what CluedIn thinks is the most valid representation of an Entity."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-origin-html": {
						"id": "docs-91-developer-portal-110-object-model-origin-html",
						"title": "Origin",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Origin.html",
						"content": "The Origin of a Clue determines the lineage of source. Dependant upon the use case, you might find that sometimes your Origin will differ based off different fields. For example, imagine you are pulling data in from Microsoft Dynamics, but you are aware that one of the fields on a Lead object in Microsoft Dynamics actually comes from a particular Oracle database. It is fine to be able to set the Origin of this Clue to a hybrid of Oracle and Microsoft Dynamics. This will also help CluedIn reconstruct the full data lineage path of where data comes from and where it flows. You can solve this in two ways, but there is a preferred way. We would rather receieve two clues - one that contains all the Dynamic sourced data and the Oracle data, the other that ONLY contains the Oracle data. In this way, we can show that the data has come from both systems and exists in both systems. An Origin can be any value, but typically it will be the name of the source system where you are pulling data from e.g. Hubspot, Microsoft Dynamics, Oracle, Workday."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-vocabularies-html": {
						"id": "docs-91-developer-portal-110-object-model-vocabularies-html",
						"title": "Vocabularies",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Vocabularies.html",
						"content": "Vocabularies are your way to be able to describe properties that are coming in from datasources. Each Vocabulary has the following properties to set: Key Name Display Name Data Type (of value) Description Visibilty Editable Removable Personally Identifying To maintain proper data lineage, it is recommended that the Key Name is set to the exact same value of the property name coming in from the source system. For example, if you are integrating data from a CRM system and one of the properties is named “Column_12”, then even though it is tempting to change the Key Name, we would recommend that you maintain that as the Key Name and that you can set the Display Name as something different for aesthetic reasons. The goal of a Vocabulary is to map into a Core Vocabulary (eventually) if possible. If you find that you have multiple properties from the source systems that map into the same Vocabulary Key, then it is recommend to only map one and keep the others as non-mapped Vocabulary Keys to Core Vocabularies. An example would be if you have 3 properties called Address1, Address2, Address3 then only one of these should map into the Core Vocabulary for a Company Address. You can concatenate all Address Columns into one and set it as the Core Vocabulary Key, but you might need to make a decision on what is the best way to implement this piece. There are many times where certain Vocabularies map complex objects. Let’s assume for a moment that a particular source system gives you the address of a company in a complex object instead of a single string. This would mean that you receive data that contains a Street Number, Street Name, City, Post Code and more. It is typical that you will create Vocabularies that flatten this object and then map it into the Core CluedIn Vocabularies. To support the Mesh API piece of CluedIn, you might need to reconstruct this address object when you need to mutate the source system. For this reason, you will find that you have an Extension Method on your Vocabularies called “DataAnnotations”. Data Annotations are a way for you to instruct to the Mesh API that when you need to update part of this Address, that in fact, you will need to send the entire Address object back to the source system, instead of simply sending the changed part e.g. Street Name. Vocabularies are allowed (and designed) to be hierarchical in nature. This means that you can have many (max 10) levels of hierarchy in your Vocabularies. This is to allow you to be able to map Source systems into a Company, Department, Team or other levels. Here is a simple example of how a Vocabulary Key could map from source to Core. hubspot.contact.contactName - Provider Level lego.sales.contact.fullName - Department Level lego.marketing.contact.leadName - Department Level lego.contact.Name - Company Level person.fullName - CluedIn Core Vocabulary NOTE: When you map your Vocabularies, by default, CluedIn will only store the data for the final mapped key. For example, if you mapped from sap.person.jobTitle to user.jobTitle then CluedIn would not store the values of that key in both sap.person.jobTitle and user.jobTitle. If you would like to change the behaviour, make sure that you have one key that maps to user.jobTitle and one key that does not for the same input key."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-vocabularies-organization-20vocaulary-html": {
						"id": "docs-91-developer-portal-110-object-model-vocabularies-organization-20vocaulary-html",
						"title": "Organization Vocaulary",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Vocabularies/Organization%20Vocaulary.html",
						"content": "The CluedIn Organization Vocabulary is a very important Vocabulary. By mapping your custom Clue Vocabularies into this core Organization Vocabulary, a lot of “smarts” will kick in on the processing server to help automate cleaning, normalization, insights and more. This Vocabulary also includes many inbuilt global company identifiers, including: Duns LEI SIC CVR Company House Local Business Id Perm Id This Vocabulary also includes Vocabulary mappings for all the popular Social Accounts and it is worth mentioning that if you have two Clues that have the same Social Accounts or Global Company Identifiers then by default, these Clues will merge with 100% confidence if they are valid representations of those values."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-vocabularies-person-20vocabulary-html": {
						"id": "docs-91-developer-portal-110-object-model-vocabularies-person-20vocabulary-html",
						"title": "Person Vocabulary",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Vocabularies/Person%20Vocabulary.html",
						"content": "The CluedIn Person Vocabulary is a very important Vocabulary. By mapping your custom Clue Vocabularies into this core Person Vocabulary, a lot of “smarts” will kick in on the processing server to help automate cleaning, normalization, insights and more. This Vocabulary also includes Vocabulary mappings for all the popular Social Accounts and it is worth mentioning that if you have two Clues that have the same Social Accounts then by default, these Clues will merge with 100% confidence if they are valid representations of those values."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-vocabulary-20groupings-html": {
						"id": "docs-91-developer-portal-110-object-model-vocabulary-20groupings-html",
						"title": "Vocabulary Groupings",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Vocabulary%20Groupings.html",
						"content": "For aesthetic and organisation reasons, Vocabulary Groupings allow developers to group Vocabulary keys by logical groupings. There is no performance or processing benefit for this, it is only for aesthetics. Imagine that you are wanting to give your CluedIn users the ability to easily filter and find a property on an object. By adding intuitive Vocabulary Groupings, you will be helping the user find this data. For example, placing all Account Numbers, IBAN, SWIFT and Bank Type properties into a Vocabulary Grouping called “Bank Details” will make it easier for users to consume this data."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-110-object-model-vocabulary-20values-html": {
						"id": "docs-91-developer-portal-110-object-model-vocabulary-20values-html",
						"title": "Vocabulary Values",
						"category": "",
						"url": " /docs/91-Developer-Portal/110-Object_Model/Vocabulary%20Values.html",
						"content": "A user can set an allowed and not allowed list of terms for Vocabulary Keys. Anything that is not in the allowed list will by default be processed as normal. Any record that has a term that is in the not allowed list will be placed into the quarantine section as to resolve manually, using rules or in CluedIn Clean. A cleaning rule could be as simple as “If I see the value GB. then change it to GB”. The allowed list is useful in times where you know the exhaustive list of values that you are allowed to have, such as country codes, dial codes and more. Most Vocabulary Keys will only need to map a not allowed list, due to the nature of not being able to guess what all the possible permutations of values are. These lists can be managed in the Data Catalog where you can see the distribution of all values currently in the system. Using the + or - buttons, you can add these to the allowed or not allowed list. If you leave catalog values without being on a list, by default CluedIn will process them as normal. If source systems are sending data through with values that exist on the allowed list, this will add to the overall integrity quality metric of that source system. If it is the opposite and the values continue to come through on the not-allowed list, this will not affect the integrity quality metric as it is quite normal that source systems handle data differenetly. In the profile section of the user interface, you will be able to see your data in a tabular view and the heuristics and histograms of values per “field” that don’t comply to an allowed list or are on the not allowed list. This can help give an indication of the quality of data. Allowed and not allowed values can be statically entered. You can specify these as ranges, individual values or simple rules e.g. “cannot be larger than 15” or “must be hubspot.dealAmount x 12”. The Profile section is run in realtime and hence as soon as a value is detected as not allowed, it will populate this in the quarantine and that particular record will be queued separately for resolution. It can also be configured that only the property that has the non-allowed value will be queued, the rest of the record will be processed. When the resolution has been made in the quarantine section, it will show the lineage that the record came from the source AND that it was then resolved in the quarantine section. For more complex rules such as relationship requirements or exceptions e.g. “Person cannot be connected to Contact Directly” then please see the developer section on how to implement IQuarantineRule. Main features: 1: Allows you to set auto tranform correct rules on values seen in the past. 2: Allows CluedIn to pre-load our core Vocabularies with allowed values for anomaloy detection. 3: Allows profiling and analytics to be run on the distribtuion of quality on a Vocabulary level. 4: Allows our MDM component to select a better display value. 5: Allows CluedIn Clean to indicate what you should normalise too. Also, it can use the Regex to determine patterns for known things like Post Codes etc. Maybe it could say “This looks like a British Post Code” 6: Allows you to set rules like “This must be uppercase” 7: If I am projecting out MONEY, I need to know denomination . 8: We can list Edges per Entity Type i.e. the relationships that exist. 9: Marking allowed terms will train CluedIn to detect bad values in the future."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-120-other-common-20design-20decisions-html": {
						"id": "docs-91-developer-portal-120-other-common-20design-20decisions-html",
						"title": "Common Design Decisions",
						"category": "",
						"url": " /docs/91-Developer-Portal/120-Other/Common%20Design%20Decisions.html",
						"content": "Multiple Email Addresses There is a special Core Vocaulary for mapping multiple Email Addresses instead of having to map just one. It is the CluedIn.Core.Data.Vocabularies.CluedInOrganization.EmailAddresses Integrating Databases with Seed Identity It is quite normal for Databases to generate Identifiers for records starting from the Id of 0 and then incrementing this Id for every new record by 1. This might lead you to think that this will not be a unique reference to a record, but rest assured that it can be. If you need to create an Entity Code out of this record, you have to remember that this Entity Code will also contain a Entity Type, Code Origin and an Identifier. Flattening Records Without a doubt, if two clues or entities have the same Entity Code then they will be flattened into the same record and will have the history of both clues contained within it. It can sometimes be hard to determine if records should flatten or not, but this is more a business based decision than a technical one. Build Hierarchies or not? There are many times where you will have data that has a hierachy to it. For example, imagine a company that has a parent or mother company. If we had the same Entity Codes for these 2 records then CluedIn will treat these as the same company. This is not always the intention of your data project. The other option is to create hierarchies where if these two company objects need to be linked, instead of merged - you will often need to look at creating edges between these objects instead of entity codes. The most common use-cases with hierarchies is typically for companies and people. When it comes to companies, you can have a varied detail of hierarchy, but the following is what CluedIn usually includes: Ultimate Parent—The very top company listed in a company hierarchy and the ultimate controlling company within a corporate structure. Parent—The top tier within an organization but may not be the “ultimate parent.” It should have other companies reporting to it, and would itself report to another legal entity. In many cases the terms, “parent” and “ultimate parent” are used synonymously. Subsidiary—Separate corporate legal entity owned by the company at 50.1% or more. Joint Venture—A business in which two or more companies share responsibility and ownership. Affiliate—A separate legal entity in which there is an ownership interest by the parent company of less than 50%. Division, Unit, Factory or Plant—An internal unit of a company, not incorporated or a separate legal entity. Usually tends to have many employees. Branch—An internal unit of a company, not incorporated or a separate legal entity. Usually tends to have a small number of employees. Group—Corporate classification grouping “like” industries or businesses, Holding—A business whose voting stock is owned to influence its board, policies, and management. Non–Operating Entities (Shells)—Legal non-operating entities (displayed at the bottom of its immediate parent’s hierarchy). How to you build these hierachies in CluedIn? There are some easy, easier and harder approaches that you can take. If you enable our Duns and Bradstreet, Lexis Nexus or Open Corporates external search providers then CluedIn will use this to be able to build hierarchies. The next approach is to handle this all manually which mappings of edges between companies if you already have this data and hierachy in your own data. The other is to use CluedIn’s fuzzy-linking engine to build the hierarchies automatically for you."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-120-other-common-20problems-html": {
						"id": "docs-91-developer-portal-120-other-common-20problems-html",
						"title": "Common Problems",
						"category": "",
						"url": " /docs/91-Developer-Portal/120-Other/Common%20Problems.html",
						"content": "Floating Edges You may find that you are browsing your data in CluedIn and you see edges to records that only have an Id. This could be because that is simply the name of that record, but more likely is that you have a floating edge. A floating edge is an edge to a record that doesn’t exist yet. Imagine that you are importing your contact list and on every contact there is a property that points to a companyId. You would naturally create an edge out of this in your crawlers and hence if we were only to ingest the contacts then we would have floating edges to records that is waiting for us to ingest the list of companies for the link to be made by CluedIn. Mail Previews Mail is a bit sensitive and hence you might want to not show the previews within the UI. This is simply a matter of changing the Visibility type of the Core and Provider Specific Vocabularies to Hidden. This means that it will not only be hidden in the User Interface, but also it will not appear in the results from the Rest API of CluedIn. There are some properties that I don’t want to expose to downstream consumers It is common when ingesting data from multiple different sources that there are some properties are there as metadata or supplementary fields and hence are not intended to be projected out to downstream consumers. This is why, as developers, you can control the Visibility of Vocabularies. This is an enumeration which also has Flags, meaning that you can combine different Visibility properties together. Order of Processing There are many times where the order of the initial processing of data can yield better results if ordered differently. For example, for performance reasons, it is always a good idea to pre-clean data in CluedIn Clean before ingesting it and creating Entity Codes. Often this will lead to a faster processing, higher data quality and better results. Although possible, it is often best to be able to clean all the different permutations of bad entity codes before ingesting data into CluedIn. For lineage purposes, we would recommend that you store the original and bad entity codes in the property bag and fix the data for the Entity Codes. Heavy Records There are times when you have records that are very “dense”. It happens, it is natural. It is however that you should expect that these records take a lot longer to process than others. If possible, try to keep the number of versions clues per records entity to a minimum. There are many strategies that you can use to do this. In your crawlers you can break up records into sub-records. Heavy Out-going Relationships It is just a side-effect of using a Graph Database, that there are times where certain operations are more expensive than others. If you find that you have records that have a lot of outgoing references, then every time that we try to add a new outgoing reference, CluedIn will load all existing reference in order to check if the relationship exists already or not. If you have designed your integration correctly then this should never happen, but keep a look out for it if you find that your processing servers are slow. The concept in a graph is to not have “dense” nodes and hence if you can break up your nodes into sub-nodes then this will alleviate this type of problem. Crawling Data The nature of integrating and crawling data sources is that things go wrong all the time - more than you would probably like to admit. Imagine that you have 1 million records to fetch from a system and you find that you page through the data 100 records at a time. When you get to page 500, you find that it fails in fetching that page. You might have retry mechanisms in place that you will retry 10 times and if it still fails then you will continue. You might find that the rest of the crawl goes completely fine. So you have 999,900 records and are missing 1,000,000 records. How do you handle this in CluedIn? The first question would be, how would you even know that this happened? Your logs will show this, but they could be very large. You could view your log provider of choice where these logs are being processed and you will most likely easily see that this happened. You will also have this logged with the Agent Job Run tables which gives you a report of all the completed and failed operations within the job. Once you have this you can do many things. You can issue a new crawl at a later point and add in configuration to your crawler to be able to take a range of pages that you would like to crawl. This does mean that you would need to write your integrations to support skipping to a particular page before ingesting data. You might find that some integrations do not support this - and there is sometimes not a lot you can do about it."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-130-parent-aggregation-parent-20aggregation-html": {
						"id": "docs-91-developer-portal-130-parent-aggregation-parent-20aggregation-html",
						"title": "Parent Aggregation",
						"category": "",
						"url": " /docs/91-Developer-Portal/130-Parent_Aggregation/Parent%20Aggregation.html",
						"content": "Although CluedIn will persist data into many datastores, CluedIn will construct a parent hierarchy of data based off certain patterns in the graph. This becomes useful for when we would like to aggregate subtrees of parent child relationships. Imagine you wanted to know an aggregate breakdown of all records that are linked to a particular company whether it is a direct child reference, or even a sub-child - parent aggregation will give you this but there are only certain edge types that will react to this functionality. Those include: Parent PartOf All other types of edges will not play a role in parent processing. You can extend the inbuilt implementation to add your own Edge types to watch, but we do recommend against creating hard coded new edge types in your crawlers. For a detailed list of supported Parent Aggregation types: var ignoredParentEdgeTypes = new EntityEdgeType[] { \" Code\", EntityEdgeType.Follows, EntityEdgeType.For, EntityEdgeType.WorkedOn, EntityEdgeType.Read, EntityEdgeType.ReadWrite, }; var nonTemporalLocalParentEdgeTypes = new[] { EntityEdgeType.At, EntityEdgeType.CreatedAt, EntityEdgeType.Modified, EntityEdgeType.ModifiedAt, EntityEdgeType.DiscoveredAt, EntityEdgeType.IsType, EntityEdgeType.LocatedIn, EntityEdgeType.Recipient, EntityEdgeType.RequestedBy, EntityEdgeType.WorkedOn, EntityEdgeType.WorkedOnBy, EntityEdgeType.Action, EntityEdgeType.Mentioned, EntityEdgeType.Author, EntityEdgeType.Birthday, EntityEdgeType.Created, EntityEdgeType.CreatedBy, EntityEdgeType.DeletedBy, EntityEdgeType.ManagedBy, EntityEdgeType.ModifiedBy, EntityEdgeType.OwnedBy, EntityEdgeType.Owns, EntityEdgeType.RequestedBy, EntityEdgeType.WorkedOnBy, EntityEdgeType.Received, EntityEdgeType.Attended, EntityEdgeType.Deployed, EntityEdgeType.DueOn, EntityEdgeType.InvitedTo, EntityEdgeType.MemberOf, EntityEdgeType.Presented, EntityEdgeType.Received, EntityEdgeType.Recipient, EntityEdgeType.Registered, EntityEdgeType.RequestedBy, EntityEdgeType.StartedOn, EntityEdgeType.EndedOn, EntityEdgeType.WitheldIn, EntityEdgeType.Competitor, EntityEdgeType.Investor, EntityEdgeType.ApprovedBy }; var nonTemporalParentEdgeTypes = new[] { EntityEdgeType.Parent, EntityEdgeType.PartOf, EntityEdgeType.AttachedTo, EntityEdgeType.ManagedIn, EntityEdgeType.WorksFor, EntityEdgeType.UsedBy, }; var parentOnlyEdgeTypes = new[] { EntityEdgeType.Represents }; return new RuleSet { IgnoredParentEdgeTypes = ignoredParentEdgeTypes, NonTemporalLocalParentEdgeTypes = nonTemporalLocalParentEdgeTypes, NonTemporalParentEdgeTypes = nonTemporalParentEdgeTypes, ParentOnlyEdgeTypes = parentOnlyEdgeTypes, SelectLocalParentEdges = SelectLocalParentEdgesV2, SelectParentEdges = SelectParentEdgesV2, SelectParentOnlyEdges = SelectParentOnlyEdgesV2, FilterIgnoredEdges = FilterIgnoredEdgesV2 };"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-140-preview-image-setting-20a-20preview-20image-20for-20a-20clue-html": {
						"id": "docs-91-developer-portal-140-preview-image-setting-20a-20preview-20image-20for-20a-20clue-html",
						"title": "Setting a Preview Image for a Clue",
						"category": "",
						"url": " /docs/91-Developer-Portal/140-Preview_Image/Setting%20a%20Preview%20Image%20for%20a%20Clue.html",
						"content": "Your data will often contain much more than metadata in the form of textual values. You will find that binary data is often associated as metadata such as pictures, files, videos, audio and more. When it comes to data discovery, you will find that certain visual hints will make it easier to discover the data you are looking for. As a simple example, the Logo of an organization. The Clue object has the ability to store a single Preview Image. If you have many Preview Images, you can create multiple Clues for the same record. To set the Preview Image on a Clue you will need to download the binary and prepare it into a format that CluedIn would like. You can use the inbuilt functions that CluedIn provides to download and store the Preview Image like so: var previewImagePart = _fileFetcher.FetchAsRawDataPart(\"Your Picture Url\", \" RawData PreviewImage\", \"preview_{0}\".FormatWith(data.Name)); if (previewImagePart != null) { clue.Details.RawData.Add(previewImagePart); clue.Data.EntityData.PreviewImage = new ImageReferencePart(previewImagePart, 255, 255); }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-adding-20a-20new-20processing-20pipeline-html": {
						"id": "docs-91-developer-portal-150-processing-adding-20a-20new-20processing-20pipeline-html",
						"title": "Adding a new Processing Pipeline",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Adding%20a%20new%20Processing%20Pipeline.html",
						"content": "There are many reasons to add a new processing pipeline. The concept behind a processing pipeline is that it allows you to process input data and produce an output. The CluedIn processing pipeline can be conceptualised as a never ending stream of incoming data in which this data will flow through many processing pipelines. Some of the pipelines will take input, will fix or clean certain types of data and then output it for it to be input to another processing pipeline step. CluedIn already has many processing pipeline steps that it ships with, including (but not limited to): Detecting Duplicate Records. Automatically normalising dates, phone numbers, abbreviations. Running entity detection on text. Imagine if you are ingesting data from a new system that you have built in-house. This system stores data on Dogs and their Vetenary visits. These are, by default, not well known Domain objects to CluedIn. Although it is completely fine to ingest these types of Domain objects, you can probably imagine that there will not be much out of the log logic that knows how to clean, enrich, measure and normalise data on Dogs. This is the reason why you would introduce new processing pipeline steps. In very high level pseduo code, you would visualise that you will have a process that will look something like this: (if EntityType == Dog) { Do some magic } Here as an example of a Processor that you could implement that will run some custom entity merging logic that you may want: using System; using System.Collections.Generic; using System.Linq; using System.Text; using CluedIn.Core; using CluedIn.Core.Agent; using CluedIn.Core.Data; using CluedIn.Core.Diagnostics; using CluedIn.Core.Messages.Processing; using CluedIn.Core.Processing; using CluedIn.Core.Workflows; namespace Your.Custom.Namespace { public class CustomMergeProcessing : ProcessingBase&lt;MergeEntityCommand&gt; { public CustomMergeProcessing([NotNull] ApplicationContext appContext) : base(appContext) { } protected override ProcessingContext CreateProcessingContext(MergeEntityCommand processCommand) { var context = this.appContext.CreateProcessingContext(processCommand).WithExecutionOption(ExecutionOptions.PreferMasterDataStore).WithExecutionOption(ExecutionOptions.Overwrite).WithExecutionOption(ExecutionOptions.Force); return context; } protected override IWorkflowStepResult ProcessWorkflowStep(ProcessingContext context, MergeEntityCommand processCommand) { if (processCommand == null) throw new ArgumentNullException(nameof(processCommand)); var result = SaveResult.Unknown; using (processCommand.JobId != null ? context.CreateLoggingScope(new { processCommand.JobId }) : (IDisposable)new DummyDisposable()) { var entity = context.Organization.DataStores.BlobDataStore.GetById(context, processCommand.SourceId); if (entity == null) return WorkflowStepResult.Ignored; var entity2 = context.Organization.DataStores.BlobDataStore.GetById(context, processCommand.TargetId); if (entity2 == null) return WorkflowStepResult.Ignored; if (entity1.Name == entity2.Name) { Save into the databasess } return new WorkflowStepResult(operations, SaveResult.Complete); } } } } There is, however, the restriction that you must write this code in .net. There are many developers and users of CluedIn that will not know this and hence what we often suggest and recommend that, in fact, at the end of the day - CluedIn is really just talking in JSON and REST. Hence, if you need to create a custom processing pipeline, you can also build it completely outside of CluedIn completely and take the data from CluedIn using either our GraphQL api or using our streams mechanism to push data to another platform to process data there. Once you have processed your data, all we ask is that you send that back to CluedIn with our REST endpoint for accepting clues in JSON or XML format."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-cluedin-20performance-20metrics-html": {
						"id": "docs-91-developer-portal-150-processing-cluedin-20performance-20metrics-html",
						"title": "CluedIn Performance Metrics",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/CluedIn%20Performance%20Metrics.html",
						"content": "Introduction Conceptually, you can think of CluedIn as a Streaming platform with multiple persistence stores. It just so happens that CluedIn also ships with a lot of functionality to integrate, manipulate, govern and steward this data on its way to downstream consumers. With this in mind, the question of how long does it take to process data from an input to its output arises. Naturally, it strongly depends on many factors, including: The number of CPU cores assigned to processing. The amount of RAM assigned to processing. The complexity and size of the data being processed. What CluedIn features you are running on the processing pipeline (we will assume you are using all features) To provide some baselines and benchmarks, we will introduce some datasets coming from different types of platforms that speak to the complexity and size of data. CluedIn utilises an Enterprise Service Bus to queue the incoming data. This data is then monitored by many different “workers”. These “workers” are in-memory .net core processes that take a record at a time and run it through 100’s of validations, automated cleaners and more. The reason to explain this, is that, in its raw form, the Enterprise Service Bus can process incoming messages very fast, it is the 100’s of validations and automated processors that will (for example) take this rate from 30,000 messagess to 1000 messages per second i.e. the more the system automates, the slower it can process the data. CluedIn scales both vertically and horizontally, meaning that you can increase the speed of processing by either using bigger machines or more machines. Due to the stateless nature of the processing components of CluedIn, it means that you can have 0 processing services or 100 running. Although CluedIn can technically run using 1 CPU core, it is not optimal for any real workloads within CluedIn. The amount of CPU Cores and RAM that you assign to processing services is all set in your Helm charts. Within Kuberenets you will still need to allocate enough in your Node Pools to be able to scale your processing servers. Let’s start to talk about the data itself. We will typically talk about simple data vs complex data in CluedIn. Simple data is data that uses as little of the 100’s of inbuilt services as possible i.e. less work, means quicker processing. Good examples of simple data usually comes in the form of well structured and small records such as rows in a table or JSON XML from a REST service. Complex data usually will come in the form of Physical files or large records in tables or JSON XML packages. We refer to it as complex data due to the fact that it will be enabling a lot of the inbuilt processing functions and require their attention. NOTE: It has to be mentioned that you should still think about whether you need to bring in all columns in all tables from all sources. It is a hard question to answer, as it may be that the data you do not bring in, was the exact data you needed in a particular situation. What should be remembered is that you can always bring this data in later without the need for complex remodelling. There are many ways to monitor the performance of your processing pipeline, one of which is exposed to Administrators with the CluedIn User Interface itself. The Engine Room will give you a breakdown of the overall speed of the processing pipeline and then a zoom in on each processing pipeline step to show the rate of each step. This also gives you the idea that you can turn different processing pipelines if you want to. For example, if you are not wanting some of the more “expensive” processing pipeline steps then these can be disabled through configuration."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-extending-20an-20existing-20pipeline-20process-html": {
						"id": "docs-91-developer-portal-150-processing-extending-20an-20existing-20pipeline-20process-html",
						"title": "Extending an Existing Pipeline Process",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Extending%20an%20Existing%20Pipeline%20Process.html",
						"content": "Knowing that there are many processing pipelines that ship with the CluedIn product, you might find that you will want to only override small parts of CluedIn, rather than introducing your own custom pipeline processing steps. You will notice in the ProcessingBase class that we offer many methods that you can override and hence introduce your own logic. using System; using System.Collections.Generic; using System.Linq; using System.Text; using CluedIn.Core; using CluedIn.Core.Agent; using CluedIn.Core.Data; using CluedIn.Core.Diagnostics; using CluedIn.Core.Messages.Processing; using CluedIn.Core.Processing; using CluedIn.Core.Workflows; namespace Your.Custom.Namespace { Extend an existing implementation in CluedIn. public class MyCustomMergeProcessing : AggregateEntityProcessing { public CustomMergeProcessing([NotNull] ApplicationContext appContext) : base(appContext) { } protected override ProcessingContext CreateProcessingContext(MergeEntityCommand processCommand) { var context = this.appContext.CreateProcessingContext(processCommand).WithExecutionOption(ExecutionOptions.PreferMasterDataStore).WithExecutionOption(ExecutionOptions.Overwrite).WithExecutionOption(ExecutionOptions.Force); return context; } protected override IWorkflowStepResult ProcessWorkflowStep(ProcessingContext context, MergeEntityCommand processCommand) { Run something custom Then run the base implementation this.base(context); } } } Compile the code and then drop the DLL into the ServerComponent directory of CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-ignoring-20insignificant-20changes-html": {
						"id": "docs-91-developer-portal-150-processing-ignoring-20insignificant-20changes-html",
						"title": "Ignoring Insignificant Changes",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Ignoring%20Insignificant%20Changes.html",
						"content": "When CluedIn processes Clues, it will generate a Hash value that will take certain values on the Clue and create a unique value that represents a hash of those values. By default, all changes and properties in a Clue are treated as something that will play a role in the hashing process. Developers can instruct Vocabularies to “Ignore Hashing”, which means that even if these values change on subsequent crawls, it won’t play a role in telling CluedIn that the data has changed. Only properties that have changed or been added that are not marked with “Ignore Hashing” will instruct CluedIn that things have changed. For example, many tools will change a timestamp value when the record has been viewed - not modified, but simply opened or viewed. You might find that this is important to change the clue, but in many occasions you will find that this change is insignificant and you will want to use your Vocabulary mappings to instruct CluedIn to ignore this change and throw away the Clue from processing. This is typically to help increase the performance and lack of load placed onto the CluedIn processing servers."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-jobs-agent-20jobs-html": {
						"id": "docs-91-developer-portal-150-processing-jobs-agent-20jobs-html",
						"title": "Agent Jobs",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Jobs/Agent%20Jobs.html",
						"content": "Your user interface will show you a list of the jobs that are running, have run and are scheduled to run within the platform. You can drill into the information on an individual job to see more details. A design is missing for the Job Details. [HttpGet] [Route(“api v1 jobs”)] This call will return 5 counts for the dashboard including completed, executing, failed, queued, cancelled. [HttpGet] [Route(“api v1 jobs”)] int page, int take, string state = null, Guid? configurationId = null You can page through all jobs with all states or you can filter by State and Congifuration Id (AKA ProviderDefinitionId) e.g. State can be one of the following: Unknown , &lt;summary&gt;The queued job state&lt; summary&gt; Queued , &lt;summary&gt;The executing job state&lt; summary&gt; Executing , &lt;summary&gt;The execution completed&lt; summary&gt; ExecutionCompleted , &lt;summary&gt;The cancelling job state&lt; summary&gt; Cancelling , &lt;summary&gt;The submitting job state&lt; summary&gt; Submitting , &lt;summary&gt;The completed job state&lt; summary&gt; Completed , &lt;summary&gt;The cancelled job state&lt; summary&gt; Cancelled , &lt;summary&gt;The failed job state.&lt; summary&gt; Failed [HttpGet] [Route(“api v1 jobrun”)] id To call this you pass in the Job Id (that you have from the previous calls) and it will show you the details of the Job. There is LOTS of details here that will look nice in a UI. [Id] ,[JobId] ,[AgentId] ,[AgentKey] ,[UpdateDate] ,[AgentResultIsSuccess] ,[AgentResultIsCancellationRequested] ,[AgentResultMessage] ,[AgentState] ,[AgentQueuedDate] ,[AgentStartTime] ,[AgentEndTime] ,[AgentElapsedQueuedTime] ,[AgentElapsedExecutionTime] ,[AgentPercentCompleted] ,[AgentLastSubmittedDate] ,[AgentExpectedTasksCount] ,[AgentTasksCount] ,[AgentTasksQueuedCount] ,[AgentTasksCompletedCount] ,[AgentTasksFailedCount] ,[AgentCluesCount] ,[AgentCluesQueuedCount] ,[AgentCluesCompletedCount] ,[AgentCluesFailedCount] ,[ProcessingState] ,[ProcessingQueuedDate] ,[ProcessingStartTime] ,[ProcessingEndTime] ,[ProcessingTasksCount] ,[ProcessingTasksQueuedCount] ,[ProcessingTasksCompletedCount] ,[ProcessingTasksFailedCount] ,[ProcessingTasksSkippedCount] ,[ProcessingTaskRetriesCount] ,[ProcessingTasksAlreadyExistsCount] ,[ProcessingTasksAlreadyUpToDateCount] ,[ProcessingTasksIgnoredCount] ,[ProcessingTasksReceivedPayloadsCount] ,[ProcessingTasksReceivedCluesCount] ,[ProcessingTasksReceivedLogLinesCount] ,[ProcessingUpdatedDate] [HttpGet] [Route(“api v1 jobs executing”)] This will get you JUST the executing jobs. The idea being that if you are wanting to show the executing jobs in a place like this:"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20pipeline-20overview-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20pipeline-20overview-html",
						"title": "Processing Pipeline Overview",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Pipeline%20Overview.html",
						"content": "The processing pipeline of CluedIn is the engine that takes Clues and turns them into Entities. The processing pipeline exposes some extensibility endpoints that allow you to inject your own custom logic at certain stages of the pipeline. Below is a diagram that shows the full journey of the processing pipeline. This digram shows you the different stages of processing, but also exposes the different entry points that you have as a developer to inject your own custom logic. It is important to mention that there are many ways to extend CluedIn and the way we are talking about here is for running your code in the same process as CluedIn. In the majority of cases, this is the most efficient way to inject custom processing logic. You can of course let CluedIn process the data and only interact with it once this data processing has finished. You could use the GraphQL API or Streams to move the appropriate data to another system so you can apply your own custom logic after CluedIn."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20stages-adding-20a-20preprocessor-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20stages-adding-20a-20preprocessor-html",
						"title": "Adding a new PreProcessor",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20a%20Preprocessor.html",
						"content": "If you look at the processing pipeline flow diagram, you can see that you have the ability very early on to inject custom logic to change a Clue before the processing starts. There are many good use cases this e.g. removing bad Entity Codes before data is processed. The main use of the PreProcessor stage is that it is your first chance to manipulate the Clue in the context of the pipeline to be changed in any way that you want before CluedIn takes over and applies some automated processing. Below is an example of removing all Codes from all Clues coming through the processing pipeline if the Value of the EntityCodes contains the word “SomeBadValue” with a perfect case match. using System.Collections.Generic; using System.Linq; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Processing.Models.Utilities; namespace CluedIn.Processing.Processors.PreProcessing { &lt;summary&gt;The hubspot pre processor.&lt; summary&gt; &lt;seealso cref=\"PropertyUtility{TMetadata}.Core.Data.Parts.IEntityMetadataPart}\" &gt; &lt;seealso cref=\"CluedIn.Processing.Processors.PreProcessing.IPreProcessor\" &gt; public class RemoveBadEntityCodesPreProcessor : PropertyUtility&lt;IEntityMetadataPart&gt;, IPreProcessor { &lt;inheritdoc &gt; public bool Accepts(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes) { return codes.Any(c =&gt; c.Origin.Value.Contains(\"SomeBadValue\")); } &lt;summary&gt;Processes the specified data.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"metadata\"&gt;The metadata.&lt; param&gt; &lt;param name=\"data\"&gt;The data part.&lt; param&gt; public void Process(ExecutionContext context, IEntityMetadataPart metadata, IDataPart data) { metadata.Codes.Where(c =&gt; c.Origin.Value.Contains(\"SomeBadValue\")).Remove(); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20stages-adding-20a-20new-20iprocessingfilter-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20stages-adding-20a-20new-20iprocessingfilter-html",
						"title": "Adding a new IProcessingFilter",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20a%20new%20IProcessingFilter.html",
						"content": "The main use of the IProcessingFiltering interface is to introduce your ability to define logic to Ignore a Clue completely. Imagine that you only want to accept Clues where they are of a particular Entity Type. This is your ability to inject this logic very early in the processing of data. using System; using System.Collections.Generic; using System.Linq; using CluedIn.Core; using CluedIn.Core.Configuration; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Serialization; namespace CluedIn.Processing { &lt;summary&gt;The clue filtering.&lt; summary&gt; public class ClueProcessingFiltering : IProcessingFiltering { ********************************************************************************************************** * FIELDS ********************************************************************************************************** &lt;summary&gt;The context&lt; summary&gt; private readonly ApplicationContext context; ********************************************************************************************************** * CONSTRUCTORS ********************************************************************************************************** &lt;summary&gt; Initializes a new instance of the &lt;see cref=\"ProcessingFiltering\" &gt; class. &lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public ProcessingFiltering(ApplicationContext context) { this.context = context; } ********************************************************************************************************** * METHODS ********************************************************************************************************** public bool IsPreFiltered(CompressedClue clue) { if (clue.EntityType != \" Organization\") return true; return false; } public bool IsPreFiltered(Clue clue) { Additional checks not possible by IsPreFiltered(CompressedClue) return false; } public bool IsPreFiltered(IEntityMetadata entity) { return false; } public bool IsMergePrefiltered(Clue clue) { return false; } public bool IsMergePrefiltered(IEntityMetadata entity) { return false; } public bool IsEdgeProcessingPrefiltered(IEntityMetadata entity) { return false; } public bool IsFuzzyMatchEntityPrefiltered(Clue clue) { return false; } public bool IsCreateNewEntityPreFiltered(Clue clue) { return false; } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20idatapartprocessorexecutor-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20idatapartprocessorexecutor-html",
						"title": "Adding an IDataPartProcessor",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IDataPartProcessorExecutor.html",
						"content": "This is the first part in the processing stage where you get to inject custom logic into the middle of the CluedIn Processing Pipeline. It runs after some of the critical parts of the inbuilt data processing has run from CluedIn. using System; using System.Linq; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Data.Vocabularies; using CluedIn.Core.Processing; namespace CluedIn.Processing.ContentProcessing { public class AutoTagProcessor : IDataPartProcessor { public void Process(ProcessingContext context, IDataPart dataPart, IProcessedEntityMetadataPart processedMetadata) { processedMetadata.Tags.Add(\"I made it here\"); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20ientitytypeprocessor-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20ientitytypeprocessor-html",
						"title": "Adding an IEntityTypeProcessor",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IEntityTypeProcessor.html",
						"content": "This is your ability to interact with the data just before it is about to finish the processing pipeline and before it will write the data to the databases. using System; using System.Collections.Generic; using System.Globalization; using System.Linq; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Data.Vocabularies; using CluedIn.Processing.Models.Utilities; namespace CluedIn.Processing.Processors.Specific { &lt;summary&gt; The task processor. &lt; summary&gt; public class DogProcessor : PropertyUtility, IEntityTypeProcessor { &lt;summary&gt; Checks if the specified entity type can be matched be this instance. &lt; summary&gt; &lt;param name=\"type\"&gt;The type.&lt; param&gt; &lt;returns&gt; &lt;c&gt;true&lt; c&gt; if the entity matcher can match the specified entity type; otherwise &lt;c&gt;false&lt; c&gt;. &lt; returns&gt; public bool Accepts(EntityType type) { return type.Is(\" Dog\"); } &lt;summary&gt;Processes the specified data.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"processedMetadata\"&gt;The processed metadata.&lt; param&gt; &lt;param name=\"data\"&gt;The data part.&lt; param&gt; public void Process(ExecutionContext context, IProcessedEntityMetadataPart processedMetadata, IDataPart data) { if (context == null) throw new ArgumentNullException(nameof(context)); if (processedMetadata == null) throw new ArgumentNullException(nameof(processedMetadata)); processedMetadata.Tags.Add(\"Woof!\"); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20imergepreprocessor-html": {
						"id": "docs-91-developer-portal-150-processing-processing-20stages-adding-20an-20imergepreprocessor-html",
						"title": "Adding an IMergePreProcessor",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Processing%20Stages/Adding%20an%20IMergePreProcessor.html",
						"content": "The IMergePreProcessor is your ability to provide custom input before records will Merge. This is useful if you want to inject your own custom logic to either cancel a merge or to modify the record or add logging before a Merge happens. using System; using System.Collections.Generic; using System.Linq; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Processing; using CluedIn.Processing.Models.Utilities; namespace CluedIn.Processing.Processors.PreProcessing { &lt;summary&gt;A merge pre processor.&lt; summary&gt; &lt;seealso cref=\"IEntityMetadataPart\" &gt; &lt;seealso cref=\"IMergePreProcessor\" &gt; public class TimeStampMergePreProcessor : PropertyUtility&lt;IEntityMetadataPart&gt;, IMergePreProcessor { &lt;inheritdoc &gt; public bool Accepts(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes) { return true; } public SaveResult Process(ExecutionContext context, IEntityMetadataPart metadataToMerge, IDataPart dataToMerge, Entity targetEntity) { if (dataToMerge == null) return SaveResult.Ignored; metadataToMerge.Properties[\"mergeTimeStamp\"] = DateTimeOffset.UtcNow.ToString(); return SaveResult.Success; } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-turning-20clues-20into-20processedentitymetadata-html": {
						"id": "docs-91-developer-portal-150-processing-turning-20clues-20into-20processedentitymetadata-html",
						"title": "Turning Clues into ProcessedEntityMetadata",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Turning%20Clues%20into%20ProcessedEntityMetadata.html",
						"content": "An Entity contains a property called a ProcessedEntityMetadata. This stores the preferred values for properties from all the clues into a unified set of properties. What dictates these values will vary, but it essentially comes down to: The values that are the latest instance of a property. The values that have the highest data metric scores. A combination of the highest data metrics and the source “trust” score that is set on the integration."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-150-processing-workflows-workflow-20event-20handlers-html": {
						"id": "docs-91-developer-portal-150-processing-workflows-workflow-20event-20handlers-html",
						"title": "Workflow Event Handlers",
						"category": "",
						"url": " /docs/91-Developer-Portal/150-Processing/Workflows/Workflow%20Event%20Handlers.html",
						"content": "CluedIn processes your data with a Workflow System that runs in a series of stages. You can use Workflow Event Handlers to hook into data at different stages and apply your own actions. Think of this as a way of subcribing to certain events within the processing of data and getting a trigger to apply your own logic when this event is triggered. using System; using System.Collections.Generic; using System.Configuration; using System.Linq; using CluedIn.Core; using CluedIn.Core.Configuration; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.Events.Types; using CluedIn.Core.Mesh; using CluedIn.Core.Messages.Processing; using CluedIn.Core.Processing; using CluedIn.Core.Processing.Statistics; using CluedIn.Core.Serialization; using CluedIn.DataStore.Security; using Microsoft.Extensions.Logging; namespace CluedIn.Processing.Events { &lt;summary&gt;The workflow event handlers&lt; summary&gt; &lt;seealso cref=\"System.IDisposable\" &gt; public class MergeWorkflowEventHandlers : IDisposable { ********************************************************************************************************** * PROPERTIES ********************************************************************************************************** &lt;summary&gt;The context&lt; summary&gt; private readonly ApplicationContext context; &lt;summary&gt;The subscription&lt; summary&gt; private IDisposable subscription; ********************************************************************************************************** * CONSTRUCTORS ********************************************************************************************************** &lt;summary&gt; Initializes a new instance of the &lt;see cref=\"WorkflowEventHandlers\" &gt; class. &lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public MergeWorkflowEventHandlers(ApplicationContext context) { this.context = context; this.subscription = this.context.System.Events.Local.Subscribe&lt;WorkflowFinishedEvent&gt;(this.ProcessEvent); } ********************************************************************************************************** * METHODS ********************************************************************************************************** &lt;summary&gt; Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources. &lt; summary&gt; public void Dispose() { if (this.subscription != null) this.subscription.Dispose(); } &lt;summary&gt;Processes the event.&lt; summary&gt; &lt;param name=\"eventData\"&gt;The event data.&lt; param&gt; private void ProcessEvent(WorkflowFinishedEvent eventData) { if (eventData.WorkflowCommandType == null) return; if ((typeof(MergeEntitiesCommand).IsAssignableFrom(eventData.WorkflowCommandType) || eventData.WorkflowCommandType == typeof(MergeEntitiesCommand)) &amp;&amp; eventData.SubjectId.HasValue &amp;&amp; eventData.WorkflowContextData != null &amp;&amp; !eventData.NoChangesMade &amp;&amp; eventData.WorkflowContextData.ContainsKey(\"mergeTargetEntityId\") &amp;&amp; eventData.WorkflowContextData.ContainsKey(\"mergeEntitiesIds\") &amp;&amp; eventData.WorkflowContextData.ContainsKey(\"mergeChanges\")) { if (eventData.WorkflowContextData.ContainsKey(\"mergeChangesWithUserInput\")) TODO: @LJU: Dirty solution to get user input into merged records. { Guid.TryParse(eventData.WorkflowContextData.GetValue(\"organizationId\", null).ToString(), out var organizationId); Guid.TryParse(eventData.WorkflowContextData.GetValue(\"mergeTargetEntityId\", null).ToString(), out var targetEntityId); if (targetEntityId == null) throw new ArgumentNullException(\"targetEntityId\", \"TargetEntityId not existent\"); if (organizationId == null) throw new ArgumentNullException(\"organizationId\", \"OrganizationId not existent\"); var mergePropertiesJson = eventData.WorkflowContextData.GetValue(\"mergeChangesWithUserInput\"); var mergeProperties = JsonUtility.Deserialize&lt;List&lt;MergeWithUserInput&gt;&gt;(mergePropertiesJson.ToString()); if (mergeProperties == null) throw new ArgumentNullException(\"mergeProperties\", \"Merge Properties collection could not be fetched\"); if (!mergeProperties.Any()) { throw new ArgumentNullException(\"No properties were given to merge\"); } if (mergeProperties.Any()) { var executionContext = this.context.CreateExecutionContext(organizationId); var entity = executionContext.Organization.DataStores.PrimaryDataStore.GetById(executionContext, targetEntityId); if (entity == null) throw new ArgumentNullException($\"Entity is not found at merge workflow step. {targetEntityId}\"); var clue = new Clue(entity.ProcessedData.OriginEntityCode, executionContext.Organization.Id); clue.Data.EntityData.Name = entity.Name; clue.Data.EntityData.CreatedDate = entity.CreatedDate; clue.Data.EntityData.ModifiedDate = DateTimeOffset.UtcNow; clue.AllowedEntityOperations = AllowedEntityOperations.MergeIntoExisting; clue.Data.Attributes[\"inputSource\"] = \"user\"; foreach (var mergeProperty in mergeProperties) { clue.Data.EntityData.Properties.Add(mergeProperty.PropertyName, mergeProperty.PropertyValue); } var compressed = CompressedClue.Compress(clue, executionContext.ApplicationContext); var command = new ProcessPrioritizedClueCommand(null, compressed); executionContext.Log.LogDebug($\"Appended user input to merged record: {clue.OriginEntityCode}\"); this.context.System.ServiceBus.Publish(command); } } } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-170-thumbnail-extractors-thumbnail-20extractors-html": {
						"id": "docs-91-developer-portal-170-thumbnail-extractors-thumbnail-20extractors-html",
						"title": "Thumbnail Extractors",
						"category": "",
						"url": " /docs/91-Developer-Portal/170-Thumbnail_Extractors/Thumbnail%20Extractors.html",
						"content": "Thumbnail Extractors allow you to extract images from Data. The most common use case would be to set a Preview Image of a Clue. This is common for files such as Presentations, Spreadsheets and Videos to have a sample picture to preview what the file looks like. There may be times where you are integrating datasources that are not supported by CluedIn. For this, you can create a new thumbnail extractor by creating a new C# class and inheriting from the IThumbnailExtractor interface. As an example, you could implement a new Thumbnail Extractor like so: using System; using System.IO; using System.Drawing; using System.Threading; using CluedIn.Core; using CluedIn.Core.FileTypes; using CluedIn.Core.Logging; using CluedIn.Crawling.ContentExtraction; namespace Custom.Images.Extractors { public class ImageThumbnailGenerator : ContentExtractor, IThumbnailGenerator { private readonly ApplicationContext context; public ImageThumbnailGenerator(ApplicationContext context) { this.context = context; } public int Priority { get { return 10; } } public string Name { get { return this.GetType().Name; } } public bool Accept(MimeType type) { switch (type.Code) { case \"image jpeg\": case \"image png\": case \"image x-png\": case \"image gif\": case \"image pjpeg\": case \"image svg+xml\": case \"image tiff\": return true; default: return false; } } public IThumbnail GetThumbnail(Stream stream, FileInfo fileInfo, ILogger logger, CancellationToken cancellationToken) { using (var image = stream != null ? System.Drawing.Image.FromStream(stream) : System.Drawing.Image.FromFile(fileInfo.FullName)) { var newWidth = 1600; var maxHeight = 1600; if (image.Width &lt;= newWidth) { newWidth = image.Width; } int newHeight = image.Height * newWidth image.Width; if (newHeight &gt; maxHeight) { Resize with height instead newWidth = image.Width * maxHeight image.Height; newHeight = maxHeight; } Image img; if (image.Width != newWidth || image.Height != newHeight) img = image.GetThumbnailImage(newWidth, newHeight, () =&gt; false, IntPtr.Zero); else img = image; return new Thumbnail { Data = ImageToByte(img), Height = img.Height, Width = img.Width, MimeType = MimeType.Jpg }; } } } } You will then compile and drop the DLL into the ServerComponent directory and restart CluedIn. Now, when processing certain files, CluedIn will attemp to extract richer information from it and help you generate a Preview Image automatically."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-180-user-interface-customisations-adding-20new-20search-20filters-html": {
						"id": "docs-91-developer-portal-180-user-interface-customisations-adding-20new-20search-20filters-html",
						"title": "Adding new Search Filters",
						"category": "",
						"url": " /docs/91-Developer-Portal/180-User_Interface/Customisations/Adding%20new%20Search%20Filters.html",
						"content": "You might find that you will want to filter your search queries by very particular Vocabularies in the User Interface. For this, you will want to add custom search filters to CluedIn. You can do this from your search screen in which you can filter by a Vocabulary. As you type the value, you will see that CluedIn will start to aut-suggest what values you would like to find based of the values that are already in the platform. You can also search for values that don’t exist, but this will typically result in 0 results."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-180-user-interface-customisations-searching-20for-20data-html": {
						"id": "docs-91-developer-portal-180-user-interface-customisations-searching-20for-20data-html",
						"title": "Searching for data",
						"category": "",
						"url": " /docs/91-Developer-Portal/180-User_Interface/Customisations/Searching%20for%20data.html",
						"content": "CluedIn provides a simple to use search over your data. This allows you to search through your data like you would run a Google Search. We also offer different ways to search through your data in a more “query” based way. The search for business users will allow them to search for “Marketing” and it will bring back all results that contain the word “Marketing” in any single attribute of the data and will order the results by relevance. Relevance for CluedIn is mesasure on many different properties of data including: The number of term matches (the TFID from Elastic Search) The recency of the data from a created date, discovered date or modified date with a logrithmic decay The number of incoming edges to this record in the graph (density) The number of properties that a record has The Entity Type itself You can use the filters on the left hand side to filter your results by the automatic categorisation that CluedIn has given to the records. By default, the search does not bring back results for Shadow Entities. If you need this, you will need to enable this in your container.config file. We also do not show records that are only from external data sources. Once again, you can enable this in your container.config if you need to be able to search through this data. We have disabled these by default as they often cause clutter and noise in the data. You have different views of your data that you can choose within the CluedIn User Interface."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-180-user-interface-customisations-suggested-20searches-html": {
						"id": "docs-91-developer-portal-180-user-interface-customisations-suggested-20searches-html",
						"title": "Suggested Searches",
						"category": "",
						"url": " /docs/91-Developer-Portal/180-User_Interface/Customisations/Suggested%20Searches.html",
						"content": "Suggested searches are a way for developers to bring in related data into the single unified view of a record in the user interface of CluedIn. For example, for a company page, you might want to show the employees or you might want to show a list of all documents that are connected to this company directly or indirectly. For this, you can add your own custom suggested searches. Here is an example for implementing custom suggested searches. using System.Collections.Generic; using CluedIn.Core; using CluedIn.Core.Data; using CluedIn.DataStore.Document.Models; namespace Custom.SuggestedSearches.Providers { public class DogRelatedEntitiesProvider : IRelatedEntitiesProvider { public IEnumerable&lt;SuggestedSearch&gt; GetRelatedEntitiesSearches(ExecutionContext context, Entity entity) { if (entity.Type != \" Dogs\") return new SuggestedSearch[0]; var searches = new List&lt;SuggestedSearch&gt;(); if (RelatedEntitiesUtility.CypherFluentQueriesCount(\"Dogs connected to \", entity.Id.ToString(), context) &gt; 0) { searches.Add(new SuggestedSearch() { Id = string.Join(\"{0}{1}\", this.ToString(), \"Connected\"), DisplayName = \"Dogs from this site\", SearchQuery = \"Dogs connected to \", Tokens = entity.Id.ToString(), Type = \"List\" }); } if (RelatedEntitiesUtility.CypherFluentQueriesCount(\"Dogs connected to \", entity.Id.ToString(), context) &gt; 0) { searches.Add(new SuggestedSearch() { Id = string.Join(\"{0}{1}\", this.ToString(), \"Owns\"), DisplayName = \"Dog Owners\", SearchQuery = \"Dogs connected to \", Tokens = entity.Id.ToString(), Type = \"List\" }); } return searches; } } } You can then compile and drop this DLL into the ServerComponent folder and reboot CluedIn to see your changes reflected in the User Interface."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-190-webhooks-webhooks-html": {
						"id": "docs-91-developer-portal-190-webhooks-webhooks-html",
						"title": "Webhooks",
						"category": "",
						"url": " /docs/91-Developer-Portal/190-Webhooks/Webhooks.html",
						"content": "CluedIn can listen to web hook endpoints and process the data from those objects. This allows CluedIn to be sent provider specific object types and CluedIn will use the appropriate Clue Producers to turn it into a Clue before processing. Webhooks are implemented in the Crawler Template that CluedIn releases. You will find that you have many places for which you will need to implement to get the full support of the Webhooks. The first “handshake” you will need to make is to register your Incoming webhook with the source system. You will need to implement 3 methods as to be able to handle the Create, Get and Delete. Here is some example code on how you could set this up with Hubspot. public override async Task&lt;IEnumerable&lt;WebHookSignature&gt;&gt; CreateWebHook(ExecutionContext context, [NotNull] CrawlJobData jobData, [NotNull] IWebhookDefinition webhookDefinition, [NotNull] IDictionary&lt;string, object&gt; config) { if (jobData == null) throw new ArgumentNullException(nameof(jobData)); if (webhookDefinition == null) throw new ArgumentNullException(nameof(webhookDefinition)); if (config == null) throw new ArgumentNullException(nameof(config)); var hubSpotCrawlJobData = (HubSpotCrawlJobData)jobData; var webhookSignatures = new List&lt;WebHookSignature&gt;(); try { var client = _hubspotClientFactory.CreateNew(hubSpotCrawlJobData); var data = await client.GetWebHooks(); if (data == null) return webhookSignatures; var hookTypes = new[] { \"contact.creation\", \"contact.deletion\", \"contact.propertyChange\", \"company.creation\", \"company.deletion\", \"company.propertyChange\", \"deal.creation\", \"deal.deletion\", \"deal.propertyChange\" }; foreach (var subscription in hookTypes) { if (config.ContainsKey(\"webhooks\")) { var enabledHooks = (List&lt;Webhook&gt;)config[\"webhooks\"]; var enabled = enabledHooks.Where(s =&gt; s.Status == \"ACTIVE\").Select(s =&gt; s.Name); if (!enabled.Contains(subscription)) { continue; } } try { await client.CreateWebHook(subscription); webhookSignatures.Add(new WebHookSignature { Signature = webhookDefinition.ProviderDefinitionId.ToString(), ExternalVersion = \"v1\", ExternalId = null, EventTypes = \"contact.creation,contact.deletion,contact.propertyChange,company.creation,company.deletion,company.propertyChange,deal.creation,deal.deletion,deal.propertyChange\" }); } catch (Exception e) { _log.Warn(() =&gt; $\"Could not create HubSpot Webhook for subscription: {subscription}\", e); } } webhookDefinition.Uri = new Uri(this.appContext.System.Configuration.WebhookReturnUrl.Trim(' ') + ConfigurationManager.AppSettings[\"Providers.HubSpot.WebhookEndpoint\"]); webhookDefinition.Verified = true; } catch (Exception exception) { _log.Warn(() =&gt; \"Could not create HubSpot Webhook\", exception); } var organizationProviderDataStore = context.Organization.DataStores.GetDataStore&lt;ProviderDefinition&gt;(); if (organizationProviderDataStore != null) { if (webhookDefinition.ProviderDefinitionId != null) { var webhookEnabled = organizationProviderDataStore.GetById(context, webhookDefinition.ProviderDefinitionId.Value); if (webhookEnabled != null) { webhookEnabled.WebHooks = true; organizationProviderDataStore.Update(context, webhookEnabled); } } } return webhookSignatures; } public override async Task&lt;IEnumerable&lt;WebhookDefinition&gt;&gt; GetWebHooks(ExecutionContext context) { var webhookDefinitionDataStore = context.Organization.DataStores.GetDataStore&lt;WebhookDefinition&gt;(); return await webhookDefinitionDataStore.SelectAsync(context, s =&gt; s.Verified != null &amp;&amp; s.Verified.Value); } public override async Task DeleteWebHook(ExecutionContext context, [NotNull] CrawlJobData jobData, [NotNull] IWebhookDefinition webhookDefinition) { if (jobData == null) throw new ArgumentNullException(nameof(jobData)); if (webhookDefinition == null) throw new ArgumentNullException(nameof(webhookDefinition)); await Task.Run(() =&gt; { var webhookDefinitionProviderDataStore = context.Organization.DataStores.GetDataStore&lt;WebhookDefinition&gt;(); if (webhookDefinitionProviderDataStore != null) { var webhook = webhookDefinitionProviderDataStore.GetById(context, webhookDefinition.Id); if (webhook != null) { webhookDefinitionProviderDataStore.Delete(context, webhook); } } var organizationProviderDataStore = context.Organization.DataStores.GetDataStore&lt;ProviderDefinition&gt;(); if (organizationProviderDataStore != null) { if (webhookDefinition.ProviderDefinitionId != null) { var webhookEnabled = organizationProviderDataStore.GetById(context, webhookDefinition.ProviderDefinitionId.Value); if (webhookEnabled != null) { webhookEnabled.WebHooks = false; organizationProviderDataStore.Update(context, webhookEnabled); } } } }); } This will setup all the scaffolding you will need to be able to automatically create webhooks with your source systems when an integration is added to CluedIn. The next step is that you need to be able to process the data that these webhooks will post to CluedIn. If you have used the Crawler Templates that CluedIn provides then you will see a Folder in your Provider project called “Webhooks”. This will have two files within it to start with in the template. The first is the Webhook PreValidator. This is responsible for validating “pre-flight” checks that come as a part of the standard Webhook flow. using CluedIn.Core.Webhooks; using CluedIn.Crawling.Custom.Core; namespace Custom.Provider.YourTool.WebHooks { public class YourTool_WebhookPreValidator : BaseWebhookPrevalidator { public YourTool_WebhookPreValidator() : base(YourToolConstants.ProviderId, YourToolConstants.ProviderName) { } } } You will then also need to implement the WebhookProcessor such as: using System; using System.Collections.Generic; using System.Configuration; using System.Linq; using System.Threading.Tasks; using CluedIn.Core; using CluedIn.Core.Agent.Jobs; using CluedIn.Core.Configuration; using CluedIn.Core.Data; using CluedIn.Core.DataStore; using CluedIn.Core.Messages.Processing; using CluedIn.Core.Providers; using CluedIn.Core.Webhooks; using CluedIn.Crawling; using CluedIn.Crawling.YourTool.Core; namespace Custom.Provider.YourTool.WebHooks { public class YourToolWebhookProcessor : BaseWebhookProcessor { public YourToolWebhookProcessor(ApplicationContext appContext) : base(appContext) { } public override bool Accept(IWebhookDefinition webhookDefinition) { return webhookDefinition.ProviderId == HubSpotConstants.ProviderId || base.Accept(webhookDefinition); } public override IEnumerable&lt;Clue&gt; DoProcess(ExecutionContext context, WebhookDataCommand command) { try { if (ConfigurationManager.AppSettings.GetFlag(\"Feature.Webhooks.Log.Posts\", false)) context.Log.Debug(() =&gt; command.HttpPostData); var configurationDataStore = context.ApplicationContext.Container.Resolve&lt;IConfigurationRepository&gt;(); if (command.WebhookDefinition.ProviderDefinitionId != null) { var providerDefinition = context.Organization.Providers.GetProviderDefinition(context, command.WebhookDefinition.ProviderDefinitionId.Value); var jobDataCheck = context.ApplicationContext.Container.ResolveAll&lt;IProvider&gt;().FirstOrDefault(providerInstance =&gt; providerDefinition != null &amp;&amp; providerInstance.Id == providerDefinition.ProviderId); var configStoreData = configurationDataStore.GetConfigurationById(context, command.WebhookDefinition.ProviderDefinitionId.Value); If you have stopped the provider then don't process the webhooks if (providerDefinition?.WebHooks != null) if (providerDefinition.WebHooks == false || providerDefinition.IsEnabled == false) return new List&lt;Clue&gt;(); if (jobDataCheck != null) { var crawlJobData = new YourToolCrawlJobData(configStoreData); var clues = new List&lt;Clue&gt;(); Create your Clues out of the object coming from the command.HttpPostData } } } catch (Exception exception) { context.Log.Error(new { command.HttpHeaders, command.HttpQueryString, command.HttpPostData, command.WebhookDefinitionId }, () =&gt; \"Could not process web hook message\", exception); } return new List&lt;Clue&gt;(); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-20-configuration-changing-20configuration-20settings-html": {
						"id": "docs-91-developer-portal-20-configuration-changing-20configuration-20settings-html",
						"title": "Changing Configuration Settings",
						"category": "",
						"url": " /docs/91-Developer-Portal/20-Configuration/Changing%20Configuration%20Settings.html",
						"content": "CluedIn provides configuration files that allow developers to modify the behaviour of CluedIn. The default configuration file that is used is called container.config and you can override the configuration using the computer name syntax of container..config. This allows you to have different configuration files that you can have on your local developer machine so everyone is not always using the same configuration files. You can also use this to manage configuration on different hardware when you are not utilising the Kubernetes deployment model of CluedIn. To retrieve the name of your computer you can open a terminal and type “hostname”. This will work across all operating systems. The Configuration file contains: Datastore Connection Strings Settings Feature Toggles Processing Pipeline Steps Global API Tokens Mail Settings If you are deploying using the Kubernetes Docker approach or are using this approach on your local developer machine, you will find that you can use a different mechanism to inject settings into your CluedIn configuration. For this we will use the Docker YAML files to be able to set the settings within the underlying container.config files. Changing the settings in this file, will require you to reset the CluedIn component that you are running. If you are wanting to add your own configuration when you are making extensions to the platform then we also allow you to do this. The recommended approach is that you will need to add your lines in the configuration like: &lt;add key=\"Group.Key\" value=\"true\" xdt:Locator=\"Condition(@key='Group.Key')\" xdt:Transform=\"Replace\" &gt; If you are adding your own configuration, it may be that you will also need to add this to the Docker YAML files. For documentation on describing the individual configuration settings, please install CluedIn and view the container.config file directly for inline comments."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-30-content-extraction-content-20extractors-html": {
						"id": "docs-91-developer-portal-30-content-extraction-content-20extractors-html",
						"title": "Content Extractors",
						"category": "",
						"url": " /docs/91-Developer-Portal/30-Content_Extraction/Content%20Extractors.html",
						"content": "Content Extractors will allow you to extract raw content from files. This is typically used for extracting content from Presentations, Spreadsheets, Images, Video, Audio and other file types. Although CluedIn supports over 350 diffferent file types out of the box, there is often the need to support new file types. To implement a new Content Extractor, you will need to create a new C# class and inherit from the IContentExtractor interface. Once you have implemented the Content Extractor, simply compiling the project and adding this *.dll file to the ServerComponent folder will enable you to now extract content from those files types. Here is a simple example of how to implement a custom content extractor that extracts MarkDown from *.md files. You will need to add references to the: CluedIn.Core nuget package using System.Collections.Generic; using System.IO; using System.Text; using System.Threading; using CluedIn.Core; using CluedIn.Core.Data.Parts; using CluedIn.Core.FileTypes; namespace Custom.Crawling.ContentExtraction { &lt;summary&gt; The mark down content extractor. &lt; summary&gt; public class MarkDownContentExtractor : IContentExtractor { private readonly ApplicationContext context; private HtmlContentExtractor _htmlExtractor { get; set; } public MarkDownContentExtractor(ApplicationContext context) { this.context = context; _htmlExtractor = new HtmlContentExtractor(context); } Friendly Name public string Name { get { return \"Markdown\"; } } A score from 1 to 10 on whether this is a more important content extractor that another provider that could also extract this content. public int Priority { get { return 10; } } public StreamContent Extract(Stream stream, FileInfo fileInfo, CancellationToken cancellationToken) { var m = new MarkdownSharp.Markdown(); var streamContent = new StreamContent(); var documentParts = new List&lt;IDocumentPart&gt; { }; using (var file = new System.IO.StreamReader(stream, Encoding.UTF8, true, 1024, true)) { var raw = m.Transform(file.ReadToEnd()); documentParts.Add(_htmlExtractor.Extract(raw)); streamContent.DocumentParts = documentParts; streamContent.MetadataParts = new List&lt;IMetadataPart&gt;(); return streamContent; } } public bool Accept(MimeType mimeType) { return mimeType.Equals(MimeType.Md); } [CanBeNull] public IEnumerable&lt;EntityEdgeMap&gt; ExtractEdges(string origin, FileInfo fileInfo) { return null; } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-30-content-extraction-downloading-20a-20file-20and-20indexing-20the-20content-html": {
						"id": "docs-91-developer-portal-30-content-extraction-downloading-20a-20file-20and-20indexing-20the-20content-html",
						"title": "Downloading a File and Indexing the Content",
						"category": "",
						"url": " /docs/91-Developer-Portal/30-Content_Extraction/Downloading%20a%20File%20and%20Indexing%20the%20Content.html",
						"content": "As you may be talking to datasources where the data requires you to download files, CluedIn provides a framework to download and index the content of the files. The good news is that CluedIn provides Content Extractors for the most popular of file types such as PPTX, PDF and more. This will allow you to search within the content of files. var download = new RestClient().DownloadData(new RestRequest(input.Url)); this.Index(download, input.Filename, clue); public void Index(byte[] data, string filename, Clue clue) { if (data == null) throw new ArgumentNullException(nameof(data)); if (clue == null) throw new ArgumentNullException(nameof(clue)); if (!ConfigurationManagerEx.AppSettings.GetFlag(\"Crawl.InitialCrawl.FileIndexing\", true)) return; if (data.Length &gt; Constants.MaxFileIndexingFileSize) return; using (var tempFile = new TemporaryFile(filename)) { CreatePhysicalFile(data, tempFile); FileCrawlingUtility.IndexFile(tempFile, clue.Data, clue, null, _applicationContext); } } protected void CreatePhysicalFile(byte[] data, FileInfo info) { using (var stream = new MemoryStream(data)) { using (var fs = new FileStream(info.FullName, FileMode.OpenOrCreate, FileAccess.Write)) { stream.CopyTo(fs); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-33-commands-commands-html": {
						"id": "docs-91-developer-portal-33-commands-commands-html",
						"title": "Commands",
						"category": "",
						"url": " /docs/91-Developer-Portal/33-Commands/Commands.html",
						"content": "Commands are the messages in CluedIn that move through the processing pipeline. You can think of Commands like “events”, in that they represent an action or a desire for an action to do a certain type of process in CluedIn. DeduplicateCommand This command will allow you to run a deduplicate process in bulk for an Organization, with the addtion of being able to filter by a particular Entity Type. You can invoke this command in the following way: var command = new DeduplicateCommand(null, new Guid(\"&lt;Id of the Organization&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); DeduplicateEntityCommand This command will allow you to run a deduplicate process for any Entity, given the Id of the Entity. You can invoke this command in the following way: var command = new DeduplicateEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); DeleteEntityCommand This command allows you to delete an Entity in full, including all history. Beware that this deletes all data parts and all references to this Entity. var command = new DeleteEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\"), null); executionContext.ApplicationContext.System.Processing.SendCommand(command); MergeEntitiesCommand This command will allow you to force a merge on two or more entities. var command = new MergeEntitiesCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid[] { \"&lt;Guid1&gt;\", \"&lt;Guid1&gt;\" } ); executionContext.ApplicationContext.System.Processing.SendCommand(command); ArchiveMetricsValuesCommand This command is scheduled to run every 24 hours for you, but if you would like to force it, you can run the following code. This command will take all records that are stored in the detail EntityMetrics table and calculate them into an aggregation and move the metrics to the Archive table. var command = new ArchiveMetricsValuesCommand(new Guid(\"&lt;Id of the Organization&gt;\")) { MetricId = !string.IsNullOrEmpty(metricId) ? (Guid?)new Guid(metricId) : null, MetricDimensionType = !string.IsNullOrEmpty(metricDimensionType) ? (MetricDimensionType?)Enum.Parse(typeof(MetricDimensionType), metricDimensionType) : null }; executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessEntityMetricsCommand This command allows you to reprocess an Entities metrics. This would be useful for when you are adding your own new data quality metrics and would like to test what score an entity would get. var command = new ProcessEntityMetricsCommand(new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessGlobalMetricsCommand This command is scheduled to run every 24 hours for you, but if you would like to force it, you can run the following code. This command will reprocess the Global Metrics that fuel the quality dashboards in the CluedIn user interface. var command = new ProcessGlobalMetricsCommand(new Guid(\"&lt;Id of the Organization&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ParentsProcessingCommand This command will take an Entity and reprocess its parent to child relationships that are used in calculting the hierarchies of records in the graph. var command = new ParentsProcessingCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\"), ParentProcessingDirection.Up); executionContext.ApplicationContext.System.Processing.SendCommand(command); PostProcessingEntityCommand This command allows you to run the inbuilt post processors of CluedIn. Post processing refers to the logic that runs after CluedIn has finished its default processing of a clue. var command = new PostProcessingEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessBigClueCommand This command is useful for when you know a Clue is big in size. This is typically useful for processing large binary files where you will know that there will be a lot of properties, content, entity codes and edges. This allows Clues to run on a different queue and not block smaller clues from being processed. var command = new ProcessBigClueCommand(JobRunId.Empty, CompressedClue.Compress(clue, executionContext.ApplicationContext)); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessEdgesCommand This command allows you to take an entity and process its edges. This is useful for when you have added, changed or removed logic around what edges should be on a Clue. var command = new ProcessEdgesCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessLowPriorityClueCommand This command allows you to process a Clue but assign a low priority to it over other processes that are happening in CluedIn. This is typically useful for running delta loads of data. var command = new ProcessLowPriorityClueCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessClueCommand This is the generic command for taking a Clue and processing it in CluedIn. var command = new ProcessClueCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessPrioritizedClueCommand This command allows you to process a Clue but assign a high priority to it over other processes that are happening in CluedIn. This is typically useful for data that needs to be as real-time as possible. var command = new ProcessPrioritizedClueCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessVersionHistoryCommand This command allows you to take an Entity and reprocess all the history of that record. var command = new ProcessVersionHistoryCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ProcessWebhookDataCommand This command cannot be invoked from code, but can be used to listen to when you are processing a record where the origin came from a WebhookDataCommand. WebhookDataCommand This command allows you to invoke that a record has come through a webhook. This is not publicly exposed because CluedIn is the engine that invokes this command and it cannot be done outside of the CluedIn engine. RefreshEntityBlobCommand This command allows you to take a record and reprocess it and save back into all databases. var command = new RefreshEntityBlobCommand(null, \"&lt;Insert an Entity Code&gt;\", new Guid(\"&lt;Id of the Organization&gt;\")); executionContext.ApplicationContext.System.Processing.SendCommand(command); ResyncEntityCommand This command will take a record from the CluedIn Primary store and make sure that all other stores have the same values. This can happen in certain situations where databases may get out of sync. var command = new ResyncEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\"), null); executionContext.ApplicationContext.System.Processing.SendCommand(command); SaveEntityCommand This command allows you to take a Clue and save it directly into the database without any processing. var command = new SaveEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), CompressedClue.Compress(clue, executionContext.ApplicationContext), \"&lt;Insert an Entity Code&gt;\"); executionContext.ApplicationContext.System.Processing.SendCommand(command); SplitEntityCommand This command allows you to take a record by its Entity Id and then split it by all of its history and reprocess it again. This is useful for times when you have introduced logic to ignore certain entity codes or you have changed the way that data needs to be processed. This is also useful for deleting individual parts of a record. var command = new SplitEntityCommand(null, new Guid(\"&lt;Id of the Organization&gt;\"), new Guid(\"&lt;Id of the Entity&gt;\"), null); executionContext.ApplicationContext.System.Processing.SendCommand(command); ExternalSearchCommand This command allows you to trigger and external search given an entity that is already in CluedIn. var command = new ExternalSearchCommand(executionContext, new Guid(\"&lt;Id of the Entity&gt;\"), entity.ProcessedData); executionContext.ApplicationContext.System.Processing.SendCommand(command); PublicApiEnrichmentCommand This command allows you to enrich a record through CluedIn and get a callback with the result when it is finished. var command = new PublicApiEnrichmentCommand(executionContext, Guid.NewGuid(), \"&lt;HttpRequest&gt;\", \"Your Callback Address\", EnrichmentWebhookCallbackFormat.Json, \"&lt;EnrichmentWebhookCallbackSchema&gt;\"); executionContext.ApplicationContext.System.Processing.SendCommand(command);"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-40-datastores-adding-20a-20new-20datastore-20provider-html": {
						"id": "docs-91-developer-portal-40-datastores-adding-20a-20new-20datastore-20provider-html",
						"title": "Adding a new Datastore Provider",
						"category": "",
						"url": " /docs/91-Developer-Portal/40-Datastores/Adding%20a%20new%20Datastore%20Provider.html",
						"content": "There are many types of database families today. Leveraging these different datastores provides flexiblity when it comes to using this data. Due to this, it might be that you are interested in storing your data in another datastore type that is not currently shipped with CluedIn. For example, if we wanted to provide support for your data in a Time Series database, this is where you would need to implement a brand new datastore. To implement a new datastore, you will need to do two main steps. 1: Implementing the IDataStore interface. 2: Injecting your new implementation into the Container of CluedIn. You will then want to inherit from the CluedIn.Core.ExecutionContext of the CluedIn.Core library and inject your new Store as a Static instance of your new type of Datastore. This means that your new store is globally available when an ExecutionContext is available. As a simple example, let’s introduce a new Store that allows you to store data in-memory. Start by creating a new C# class library project and add references to CluedIn.Core and CluedIn.DataStore. using System; using System.Collections.Generic; using System.Data; using System.Data.Entity; using System.Data.Entity.Core; using System.Data.Entity.Infrastructure; using System.Data.SqlClient; using System.Linq; using System.Linq.Expressions; using System.Security; using System.Threading.Tasks; using CluedIn.Core; using CluedIn.Core.Accounts; using CluedIn.Core.Data; using CluedIn.Core.DataStore; using CluedIn.Core.Processing; using CluedIn.DataStore.Exceptions; namespace CluedIn.DataStore.Memory.InMemoryDataStore { public class InMemoryDataStore&lt;T&gt; : EntityDataStore&lt;T&gt; where T : Entity { public InMemoryDataStore([NotNull] ApplicationContext context) : base(context, DataShardType.Data) { } public override T GetByEntityCode(ExecutionContext context, IEntityCode entityCode) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.Code == entityCode.Key).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result; } } public override bool EntityWithEntityCodeExists(ExecutionContext context, IEntityCode entityCode) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return dataContext.Set&lt;SimpleEntityCode&gt;().Any(c =&gt; c.Code == entityCode.Key); } } public override IDictionary&lt;IEntityCode, bool&gt; EntitiesWithEntityCodesExists(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; entityCodes) { var codes = entityCodes.Select(c =&gt; c.Key).ToList(); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var existingCodes = dataContext.Set&lt;SimpleEntityCode&gt;() .Where(c =&gt; codes.Contains(c.Code)) .Select(c =&gt; c.Code) .ToHashSet(); return entityCodes.ToDictionary(c =&gt; c, c =&gt; existingCodes.Contains(c.Key)); } } public override bool EntityWithIdExists(ExecutionContext context, Guid id) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return dataContext.Set&lt;SimpleEntityCode&gt;().Any(c =&gt; c.EntityId == id); } } &lt;summary&gt;Creates the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public override void CreateDataStore(ExecutionContext context) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) dataContext.Database.CreateIfNotExists(); } &lt;summary&gt;Deletes the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public override void DeleteDataStore(ExecutionContext context) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { try { dataContext.Database.Delete(); } catch (Exception ex) { throw new UnableToDeleteDataStoreException(ex); } } } &lt;summary&gt;Gets the entity by identifier.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; &lt;returns&gt;The entity with the specified id.&lt; returns&gt; public override T GetById(ExecutionContext context, Guid id) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).FirstOrDefault(e =&gt; e.Id == id); return simple != null ? (T)simple.ToEntity(this.Context, context) : null; } } &lt;summary&gt;Queries the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public override IEnumerable&lt;T&gt; Select(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { throw new NotSupportedException(); } &lt;summary&gt;Queries the data store asynchronously.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public override async Task&lt;IEnumerable&lt;T&gt;&gt; SelectAsync(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { return await Task.FromResult(default(IEnumerable&lt;T&gt;)); } &lt;inheritdoc &gt; public override bool Any(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { throw new NotImplementedException(); } &lt;summary&gt;Inserts the specified context.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The entity.&lt; param&gt; public override void Insert(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); foreach (var code in entry.EntityCodes) { dataContext.Set&lt;SimpleEntityCode&gt;().Attach(code); dataContext.Entry(code).State = EntityState.Added; } dataContext.Entry(entry).State = EntityState.Added; dataContext.Set&lt;SimpleEntity&gt;().Add(entry); foreach (var edge in entity.IncomingEdges) { InsertRelationshipInternal(entity, edge, dataContext); } foreach (var edge in entity.OutgoingEdges) { InsertRelationshipInternal(entity, edge, dataContext); } dataContext.SaveChanges(); } } &lt;summary&gt;Inserts the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void Insert(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot insert data for a different security context.\"); foreach (var o in objects) { this.Insert(context, o); } } &lt;summary&gt;Inserts or updates the specified objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The object.&lt; param&gt; public override void InsertOrUpdate(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { Action action = () =&gt; { var entry = SimpleEntity.FromEntity(entity); var existing = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Any(e =&gt; e.Id == entity.Id); if (existing) { dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.EntityId == entity.Id).Delete(); entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Modified; } else { entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Added; dataContext.Set&lt;SimpleEntity&gt;().Add(entry); } foreach (var code in entry.EntityCodes) { dataContext.Set&lt;SimpleEntityCode&gt;().Attach(code); dataContext.Entry(code).State = EntityState.Added; } dataContext.SaveChanges(); }; action.ExecuteWithRetry( isTransient: ex =&gt; { if (ex is UpdateException) return SqlExceptionHelper.IsDuplicateViolation((UpdateException)ex); if (ex is DbUpdateException) return SqlExceptionHelper.IsDuplicateViolation((DbUpdateException)ex); if (ex is SqlException) return SqlExceptionHelper.IsDuplicateViolation((SqlException)ex); if (ex is ConstraintException) return true; return false; }); } } public override void Save() { throw new NotImplementedException(); } public override Task SaveAsync() { throw new NotImplementedException(); } &lt;summary&gt;Inserts or updates the specified objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void InsertOrUpdate(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot insert or update data for a different security context.\"); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { foreach (var o in objects) { dataContext.Set&lt;T&gt;().Add(o); } dataContext.SaveChanges(); } } &lt;summary&gt;Updates the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The object.&lt; param&gt; public override void Update(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Modified; dataContext.SaveChanges(); } } &lt;summary&gt;Deletes the specified context.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The entity.&lt; param&gt; public override void Delete(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Set&lt;SimpleEntity&gt;().Remove(entry); dataContext.SaveChanges(); } } &lt;summary&gt;Deletes the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void Delete(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot delete data for a different security context.\"); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { foreach(var obj in objects) this.Delete(context, obj); dataContext.SaveChanges(); } } &lt;summary&gt;Queries the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public virtual decimal? GetUsage(ExecutionContext context, Expression&lt;Func&lt;T, decimal?&gt;&gt; predicate) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) return this.FilterEntitySet(dataContext.Set&lt;T&gt;(), context).Sum(predicate); } &lt;summary&gt;Deletes the object with the specified id.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; public override void DeleteById(ExecutionContext context, Guid id) { this.DeleteById(context, id, null); } &lt;summary&gt;Deletes the by identifier.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; &lt;param name=\"version\"&gt;The version.&lt; param&gt; public override void DeleteById(ExecutionContext context, Guid id, int? version) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context).Where(e =&gt; e.FromEntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context).Where(e =&gt; e.ToEntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntityCode&gt;(), context).Where(e =&gt; e.EntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.Id == id).Delete(); dataContext.SaveChanges(); } } &lt;summary&gt;Deletes all objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"canBeSystemContext\"&gt;if set to &lt;c&gt;true&lt; c&gt; [can be system context].&lt; param&gt; public override void DeleteAll(ExecutionContext context, bool canBeSystemContext = false) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.IncomingEdges).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.OutgoingEdges).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.EntityCodes).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Delete(); } } &lt;summary&gt; Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources. &lt; summary&gt; public override void Dispose() { } protected virtual bool VerifyEntityContext&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { if (context.Organization == context.ApplicationContext.System.Organization) return true; if (!this.VerifyContextOrganization(entity, context)) return false; if (!this.VerifyContextUser(entity, context)) return false; return true; } protected virtual IQueryable&lt;TEntity&gt; FilterEntitySet&lt;TEntity&gt;(DbSet&lt;TEntity&gt; set, ExecutionContext context) where TEntity : class { if (context.Organization == context.ApplicationContext.System.Organization) return set; IQueryable&lt;TEntity&gt; filter = set; filter = this.FilterByContextOrganization(filter, context); filter = this.FilterByContextUser(filter, context); return filter; } protected virtual IQueryable&lt;SimpleEntity&gt; FilterEntitySet(DbSet&lt;SimpleEntity&gt; set, ExecutionContext context) { if (context.Organization == context.ApplicationContext.System.Organization) return set; IQueryable&lt;SimpleEntity&gt; filter = set; filter = this.FilterByContextOrganization(filter, context); filter = this.FilterByContextUser(filter, context); return filter; } ********************************************************************************************************** * ********************************************************************************************************** protected virtual bool VerifyContextOrganization&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { var f = entity as IOrganizationContextFilteredEntity; if (f == null) return true; if (f.OrganizationId != context.Organization.Id) { throw new SecurityException(\"Cannot access data from a different security context\"); } return true; } protected virtual bool VerifyContextUser&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { if (context.Identity == null) return true; var f = entity as IUserContextFilteredEntity; if (f == null) return true; if (f.UserId != context.Identity.UserId) { throw new SecurityException(\"Cannot access data from a different security context\"); } return true; } protected virtual IQueryable&lt;TEntity&gt; FilterByContextOrganization&lt;TEntity&gt;(IQueryable&lt;TEntity&gt; set, ExecutionContext context) { return set; } protected virtual IQueryable&lt;SimpleEntity&gt; FilterByContextOrganization(IQueryable&lt;SimpleEntity&gt; set, ExecutionContext context) { return set.Where(e =&gt; e.OrganizationId == context.Organization.Id); } protected virtual IQueryable&lt;TEntity&gt; FilterByContextUser&lt;TEntity&gt;(IQueryable&lt;TEntity&gt; set, ExecutionContext context) { return set; } public T GetByEntityCode(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes, bool? ignoreDuplicates) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var keys = codes.Select(c =&gt; c.Key); var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; keys.Any(i =&gt; i == c.Code)).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result; } } public IEnumerable&lt;T&gt; GetEntitiesByEntityCodes(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var keys = codes.Select(c =&gt; c.Key); var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; keys.Any(i =&gt; i == c.Code)).Select(c =&gt; c.Entity); var result = simple.Select(e =&gt; (T)e.ToEntity(this.Context, context)).ToList(); foreach (var r in result) { if (!this.VerifyContextOrganization(r, context)) throw new UnauthorizedAccessException(); } return result; } } public Guid? GetEntityIdByEntityCode(ExecutionContext context, IEntityCode code) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.Code == code.Key).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result.Id; } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Skip((page - 1) * take).Take(take); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, EntityType entityType) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.EntityType == entityType.Code); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, EntityType entityType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.EntityType == entityType.Code).Skip((page - 1) * take).Take(take); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;DuplicateEntityGrouping&gt; GetDuplicateEntities(ExecutionContext context) { return new DuplicateEntityGrouping[0]; } public IEnumerable&lt;DuplicateEntityGrouping&gt; GetDuplicateEntities(ExecutionContext context, Entity entity) { return new DuplicateEntityGrouping[0]; } public IEnumerable&lt;Entity&gt; FindByNames(ExecutionContext context, IEnumerable&lt;EntityType&gt; entityTypes, IEnumerable&lt;string&gt; names, int maxResults) { return new Entity[0]; } public IEnumerable&lt;EntityEdge&gt; GetIncomingRelationships(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetIncomingRelationships(ExecutionContext context, Guid id, int page, int take) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetOutgoingRelationships(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetOutgoingRelationships(ExecutionContext context, Guid id, int page, int take) { throw new NotImplementedException(); } public IEnumerable&lt;Entity&gt; GetIncomingRelationshipEndpoints(ExecutionContext context, Guid id, string edgeType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.IncomingEdges) .Where(e =&gt; e.EdgeType == edgeType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public IEnumerable&lt;Entity&gt; GetIncomingRelationshipEndpoints( ExecutionContext context, Guid id, string edgeType, string entityType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.IncomingEdges) .Where(e =&gt; e.EdgeType == edgeType &amp;&amp; e.FromEntity.EntityType == entityType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public IEnumerable&lt;Entity&gt; GetOutgoingRelationshipEndpoints(ExecutionContext context, Guid id, string edgeType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.OutgoingEdges) .Where(e =&gt; e.EdgeType == edgeType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public bool HasRelationship(ExecutionContext context, T first, T second, string relationshipType) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context) .Any(e =&gt; e.FromEntityId == first.Id &amp;&amp; e.ToEntityId == second.Id &amp;&amp; e.EdgeType == relationshipType); } } public void InsertRelationship(ExecutionContext context, T contextEntity, EntityEdge edge) { this.InsertRelationship(context, contextEntity, edge, null); } public void InsertRelationship(ExecutionContext context, T contextEntity, EntityEdge edge, bool? ignoreDuplicates = null) { this.VerifyEntityContext(contextEntity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { InsertRelationshipInternal(contextEntity, edge, dataContext); dataContext.SaveChanges(); } } private static void InsertRelationshipInternal(T contextEntity, EntityEdge edge, DbContext dataContext) { if (edge.FromReference.IsReferenceTo(contextEntity)) { var ids = dataContext.Set&lt;SimpleEntity&gt;().Where(e =&gt; e.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Select(e =&gt; e.Id); foreach (var id in ids) { var entry = new SimpleEntityEdge() { FromEntityId = contextEntity.Id, ToEntityId = id, EdgeType = edge.EdgeType, EdgeData = null }; dataContext.Set&lt;SimpleEntityEdge&gt;().Add(entry); } } else if (edge.ToReference.IsReferenceTo(contextEntity)) { var ids = dataContext.Set&lt;SimpleEntity&gt;().Where(e =&gt; e.EntityCodes.Any(c =&gt; c.Code == edge.FromReference.Code.Key)).Select(e =&gt; e.Id); foreach (var id in ids) { var entry = new SimpleEntityEdge() { ToEntityId = contextEntity.Id, FromEntityId = id, EdgeType = edge.EdgeType, EdgeData = null }; dataContext.Set&lt;SimpleEntityEdge&gt;().Add(entry); } } else if ((contextEntity.IsDeleted != null &amp;&amp; contextEntity.IsDeleted.Value) || contextEntity.ProcessedData.OriginEntityCode.Type.Is(\" Deleted\")) { return; } else { throw new ArgumentException(\"The edge to be inserted doesn't reference the context entity. ContextEntity: {0} Edge: {1}\".FormatWith(contextEntity.ProcessedData.OriginEntityCode, edge), nameof(edge)); } } public void DeleteRelationship(ExecutionContext context, T entity, EntityEdge edge) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { if (edge.FromReference.IsReferenceTo(entity)) { dataContext.Set&lt;SimpleEntityEdge&gt;().Where(e =&gt; e.FromEntityId == entity.Id &amp;&amp; e.EdgeType == edge.EdgeType &amp;&amp; e.ToEntity.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Delete(); dataContext.SaveChanges(); } else if (edge.ToReference.IsReferenceTo(entity)) { dataContext.Set&lt;SimpleEntityEdge&gt;().Where(e =&gt; e.ToEntityId == entity.Id &amp;&amp; e.EdgeType == edge.EdgeType &amp;&amp; e.FromEntity.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Delete(); dataContext.SaveChanges(); } else if ((entity.IsDeleted != null &amp;&amp; entity.IsDeleted.Value) || entity.ProcessedData.OriginEntityCode.Type.Is(\" Deleted\")) { return; Ignore } else throw new ArgumentException(\"The edge to be inserted doesn't reference the context entity. ContextEntity: {0} Edge: {1}\".FormatWith(entity.ProcessedData.OriginEntityCode, edge), nameof(edge)); } } public EntityAggregatedParents GetCachedParents(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityAggregatedParents&gt; GetCachedParents(ExecutionContext context, IEnumerable&lt;Guid&gt; ids) { throw new NotImplementedException(); } public void Update(ExecutionContext context, EntityAggregatedParents entityParents) { throw new NotImplementedException(); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-40-datastores-datastores-html": {
						"id": "docs-91-developer-portal-40-datastores-datastores-html",
						"title": "Datastores",
						"category": "",
						"url": " /docs/91-Developer-Portal/40-Datastores/Datastores.html",
						"content": "The concept of a Datastore in CluedIn is a place to persist a permutation of the data that is crawled into a purpose-fit format. CluedIn has 5 different datastores that it uses by default to persist any data that flows through the processing engine. A developer doesn’t have a choice over which datastores data will persist to, CluedIn will decide this based off that it deems most suitable. One can add new datastores and even new datastore types, but by default CluedIn ships with support for a: Native Graph Store Search Store Relational Store Distributed Cache Store Blob Store Developer may also change the implementation of these stores if desired, but CluedIn will natively ship with support for: Neo4j Community Edition as the Graph Store ElasticSearch Enterprise Edition as the Search Store Redis as the Distributed Cache Store SQL Server Community Edition as the Relational Store SQL Server Community Edition as the Blob Store CluedIn also provides a messaging queue backbone for delivering discrete messages in-between the different micro-services of CluedIn, but also uses this queue as a robust delivery mechanism for delivering Clue objects to the processing server from Crawlers. Although we offer our REST API as the delivery endpoint for posting Clue objects, we do provide integration templates via C#, .net templates, that will allow you to directly talk to the messaging queue. CluedIn ships with RabbitMQ as the default implementation of this messaging queue. During processing, your Datastores will constantly be queried and written to. Because of this we have different queues that are responsible for different interactions with the Datastores. To access your Data Stores via our REST API, you are best to use the GraphQL api. To access it from C#, you can access all of these Data Stores from your ExecutionContext object. ExecutionContext.Organisation.DataStores.Graph will give you access to be able to directly run queries against the Graph."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-40-datastores-switching-20a-20datastore-20provider-html": {
						"id": "docs-91-developer-portal-40-datastores-switching-20a-20datastore-20provider-html",
						"title": "Switching a Datastore Provider",
						"category": "",
						"url": " /docs/91-Developer-Portal/40-Datastores/Switching%20a%20Datastore%20Provider.html",
						"content": "CluedIn ships with many different datastore implementations. This gives you a choice on what system will host your data. It may be that you would rather use PostGres or MySQL than SQL Server for the Relational Store. It maybe that you would like to use a cloud SQL provider via PAAS. The Datastore abstraction layer allows you to support this change. To change your implementation of a particular store requires two pieces: 1: Implementing the IDataStore interface and the specific provider interface you would like to implement e.g. IRelationalDataStore. 2: Injecting your new implementation into the Container of CluedIn. using System; using System.Collections.Generic; using System.Data; using System.Data.Entity; using System.Data.Entity.Core; using System.Data.Entity.Infrastructure; using System.Data.SqlClient; using System.Linq; using System.Linq.Expressions; using System.Security; using System.Threading.Tasks; using CluedIn.Core; using CluedIn.Core.Accounts; using CluedIn.Core.Data; using CluedIn.Core.DataStore; using CluedIn.Core.Processing; using CluedIn.DataStore.Exceptions; using EntityFramework.Extensions; namespace CluedIn.DataStore.Relational.PrimaryDataStore { public class PrimaryRelationalDataStore&lt;T&gt; : EntityDataStore&lt;T&gt;, IPrimaryEntityDataStore&lt;T&gt; where T : Entity { public PrimaryRelationalDataStore([NotNull] ApplicationContext context) : base(context, DataShardType.Data) { } &lt;summary&gt;Creates the connection manager.&lt; summary&gt; &lt;typeparam name=\"TConnection\"&gt;The type of the connection.&lt; typeparam&gt; &lt;param name=\"organization\"&gt;The organization.&lt; param&gt; &lt;returns&gt;The connection manager.&lt; returns&gt; public override IDataStoreConnectionManager&lt;TConnection&gt; CreateConnectionManager&lt;TConnection&gt;(IOrganization organization) { return (IDataStoreConnectionManager&lt;TConnection&gt;)new RelationalConnectionManager&lt;PrimaryDataStoreModel&gt;(organization); } public override IProcessedEntity GetProcessedEntityById(ExecutionContext context, Guid id) { return this.GetById(context, id); } public override TQuery CreateQuery&lt;TQuery&gt;(ExecutionContext context) { throw new NotSupportedException(); } public override T GetByEntityCode(ExecutionContext context, IEntityCode entityCode) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.Code == entityCode.Key).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result; } } public override bool EntityWithEntityCodeExists(ExecutionContext context, IEntityCode entityCode) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return dataContext.Set&lt;SimpleEntityCode&gt;().Any(c =&gt; c.Code == entityCode.Key); } } public override IDictionary&lt;IEntityCode, bool&gt; EntitiesWithEntityCodesExists(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; entityCodes) { var codes = entityCodes.Select(c =&gt; c.Key).ToList(); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var existingCodes = dataContext.Set&lt;SimpleEntityCode&gt;() .Where(c =&gt; codes.Contains(c.Code)) .Select(c =&gt; c.Code) .ToHashSet(); return entityCodes.ToDictionary(c =&gt; c, c =&gt; existingCodes.Contains(c.Key)); } } public override bool EntityWithIdExists(ExecutionContext context, Guid id) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return dataContext.Set&lt;SimpleEntityCode&gt;().Any(c =&gt; c.EntityId == id); } } &lt;summary&gt;Creates the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public override void CreateDataStore(ExecutionContext context) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) dataContext.Database.CreateIfNotExists(); } &lt;summary&gt;Deletes the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; public override void DeleteDataStore(ExecutionContext context) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { try { dataContext.Database.Delete(); } catch (Exception ex) { throw new UnableToDeleteDataStoreException(ex); } } } &lt;summary&gt;Gets the entity by identifier.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; &lt;returns&gt;The entity with the specified id.&lt; returns&gt; public override T GetById(ExecutionContext context, Guid id) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).FirstOrDefault(e =&gt; e.Id == id); return simple != null ? (T)simple.ToEntity(this.Context, context) : null; } } &lt;summary&gt;Queries the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public override IEnumerable&lt;T&gt; Select(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { throw new NotSupportedException(); } &lt;summary&gt;Queries the data store asynchronously.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public override async Task&lt;IEnumerable&lt;T&gt;&gt; SelectAsync(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { return await Task.FromResult(default(IEnumerable&lt;T&gt;)); } &lt;inheritdoc &gt; public override bool Any(ExecutionContext context, Expression&lt;Func&lt;T, bool&gt;&gt; predicate) { throw new NotImplementedException(); } &lt;summary&gt;Inserts the specified context.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The entity.&lt; param&gt; public override void Insert(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); foreach (var code in entry.EntityCodes) { dataContext.Set&lt;SimpleEntityCode&gt;().Attach(code); dataContext.Entry(code).State = EntityState.Added; } dataContext.Entry(entry).State = EntityState.Added; dataContext.Set&lt;SimpleEntity&gt;().Add(entry); foreach (var edge in entity.IncomingEdges) { InsertRelationshipInternal(entity, edge, dataContext); } foreach (var edge in entity.OutgoingEdges) { InsertRelationshipInternal(entity, edge, dataContext); } dataContext.SaveChanges(); } } &lt;summary&gt;Inserts the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void Insert(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot insert data for a different security context.\"); foreach (var o in objects) { this.Insert(context, o); } } &lt;summary&gt;Inserts or updates the specified objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The object.&lt; param&gt; public override void InsertOrUpdate(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { Action action = () =&gt; { var entry = SimpleEntity.FromEntity(entity); var existing = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Any(e =&gt; e.Id == entity.Id); if (existing) { dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.EntityId == entity.Id).Delete(); entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Modified; } else { entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Added; dataContext.Set&lt;SimpleEntity&gt;().Add(entry); } foreach (var code in entry.EntityCodes) { dataContext.Set&lt;SimpleEntityCode&gt;().Attach(code); dataContext.Entry(code).State = EntityState.Added; } dataContext.SaveChanges(); }; action.ExecuteWithRetry( isTransient: ex =&gt; { if (ex is UpdateException) return SqlExceptionHelper.IsDuplicateViolation((UpdateException)ex); if (ex is DbUpdateException) return SqlExceptionHelper.IsDuplicateViolation((DbUpdateException)ex); if (ex is SqlException) return SqlExceptionHelper.IsDuplicateViolation((SqlException)ex); if (ex is ConstraintException) return true; return false; }); } } public override void Save() { throw new NotImplementedException(); } public override Task SaveAsync() { throw new NotImplementedException(); } &lt;summary&gt;Inserts or updates the specified objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void InsertOrUpdate(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot insert or update data for a different security context.\"); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { foreach (var o in objects) { dataContext.Set&lt;T&gt;().Add(o); } dataContext.SaveChanges(); } } &lt;summary&gt;Updates the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The object.&lt; param&gt; public override void Update(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); entry = dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Entry(entry).State = EntityState.Modified; dataContext.SaveChanges(); } } &lt;summary&gt;Deletes the specified context.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"entity\"&gt;The entity.&lt; param&gt; public override void Delete(ExecutionContext context, T entity) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var entry = SimpleEntity.FromEntity(entity); dataContext.Set&lt;SimpleEntity&gt;().Attach(entry); dataContext.Set&lt;SimpleEntity&gt;().Remove(entry); dataContext.SaveChanges(); } } &lt;summary&gt;Deletes the specified object.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"objects\"&gt;The objects.&lt; param&gt; public override void Delete(ExecutionContext context, IEnumerable&lt;T&gt; objects) { if (!objects.All(o =&gt; this.VerifyEntityContext(o, context))) throw new SecurityException(\"Cannot delete data for a different security context.\"); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { foreach(var obj in objects) this.Delete(context, obj); dataContext.SaveChanges(); } } &lt;summary&gt;Queries the data store.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"predicate\"&gt;The predicate.&lt; param&gt; &lt;returns&gt;The results of the query.&lt; returns&gt; public virtual decimal? GetUsage(ExecutionContext context, Expression&lt;Func&lt;T, decimal?&gt;&gt; predicate) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) return this.FilterEntitySet(dataContext.Set&lt;T&gt;(), context).Sum(predicate); } &lt;summary&gt;Deletes the object with the specified id.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; public override void DeleteById(ExecutionContext context, Guid id) { this.DeleteById(context, id, null); } &lt;summary&gt;Deletes the by identifier.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"id\"&gt;The identifier.&lt; param&gt; &lt;param name=\"version\"&gt;The version.&lt; param&gt; public override void DeleteById(ExecutionContext context, Guid id, int? version) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context).Where(e =&gt; e.FromEntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context).Where(e =&gt; e.ToEntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntityCode&gt;(), context).Where(e =&gt; e.EntityId == id).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.Id == id).Delete(); dataContext.SaveChanges(); } } &lt;summary&gt;Deletes all objects.&lt; summary&gt; &lt;param name=\"context\"&gt;The context.&lt; param&gt; &lt;param name=\"canBeSystemContext\"&gt;if set to &lt;c&gt;true&lt; c&gt; [can be system context].&lt; param&gt; public override void DeleteAll(ExecutionContext context, bool canBeSystemContext = false) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.IncomingEdges).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.OutgoingEdges).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).SelectMany(e =&gt; e.EntityCodes).Delete(); this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Delete(); } } &lt;summary&gt; Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources. &lt; summary&gt; public override void Dispose() { } protected virtual bool VerifyEntityContext&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { if (context.Organization == context.ApplicationContext.System.Organization) return true; if (!this.VerifyContextOrganization(entity, context)) return false; if (!this.VerifyContextUser(entity, context)) return false; return true; } protected virtual IQueryable&lt;TEntity&gt; FilterEntitySet&lt;TEntity&gt;(DbSet&lt;TEntity&gt; set, ExecutionContext context) where TEntity : class { if (context.Organization == context.ApplicationContext.System.Organization) return set; IQueryable&lt;TEntity&gt; filter = set; filter = this.FilterByContextOrganization(filter, context); filter = this.FilterByContextUser(filter, context); return filter; } protected virtual IQueryable&lt;SimpleEntity&gt; FilterEntitySet(DbSet&lt;SimpleEntity&gt; set, ExecutionContext context) { if (context.Organization == context.ApplicationContext.System.Organization) return set; IQueryable&lt;SimpleEntity&gt; filter = set; filter = this.FilterByContextOrganization(filter, context); filter = this.FilterByContextUser(filter, context); return filter; } ********************************************************************************************************** * ********************************************************************************************************** protected virtual bool VerifyContextOrganization&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { var f = entity as IOrganizationContextFilteredEntity; if (f == null) return true; if (f.OrganizationId != context.Organization.Id) { throw new SecurityException(\"Cannot access data from a different security context\"); } return true; } protected virtual bool VerifyContextUser&lt;TEntity&gt;(TEntity entity, ExecutionContext context) { if (context.Identity == null) return true; var f = entity as IUserContextFilteredEntity; if (f == null) return true; if (f.UserId != context.Identity.UserId) { throw new SecurityException(\"Cannot access data from a different security context\"); } return true; } protected virtual IQueryable&lt;TEntity&gt; FilterByContextOrganization&lt;TEntity&gt;(IQueryable&lt;TEntity&gt; set, ExecutionContext context) { return set; } protected virtual IQueryable&lt;SimpleEntity&gt; FilterByContextOrganization(IQueryable&lt;SimpleEntity&gt; set, ExecutionContext context) { return set.Where(e =&gt; e.OrganizationId == context.Organization.Id); } protected virtual IQueryable&lt;TEntity&gt; FilterByContextUser&lt;TEntity&gt;(IQueryable&lt;TEntity&gt; set, ExecutionContext context) { return set; } public T GetByEntityCode(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes, bool? ignoreDuplicates) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var keys = codes.Select(c =&gt; c.Key); var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; keys.Any(i =&gt; i == c.Code)).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result; } } public IEnumerable&lt;T&gt; GetEntitiesByEntityCodes(ExecutionContext context, IEnumerable&lt;IEntityCode&gt; codes) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var keys = codes.Select(c =&gt; c.Key); var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; keys.Any(i =&gt; i == c.Code)).Select(c =&gt; c.Entity); var result = simple.Select(e =&gt; (T)e.ToEntity(this.Context, context)).ToList(); foreach (var r in result) { if (!this.VerifyContextOrganization(r, context)) throw new UnauthorizedAccessException(); } return result; } } public Guid? GetEntityIdByEntityCode(ExecutionContext context, IEntityCode code) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var simple = dataContext.Set&lt;SimpleEntityCode&gt;().Where(c =&gt; c.Code == code.Key).Select(c =&gt; c.Entity).FirstOrDefault(); var result = simple != null ? (T)simple.ToEntity(this.Context, context) : null; if (result == null) return null; if (!this.VerifyContextOrganization(result, context)) throw new UnauthorizedAccessException(); return result.Id; } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Skip((page - 1) * take).Take(take); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, EntityType entityType) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.EntityType == entityType.Code); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;T&gt; GetEntities(ExecutionContext context, EntityType entityType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context).Where(e =&gt; e.EntityType == entityType.Code).Skip((page - 1) * take).Take(take); foreach (var entry in result) yield return (T)entry.ToEntity(this.Context, context); } } public IEnumerable&lt;DuplicateEntityGrouping&gt; GetDuplicateEntities(ExecutionContext context) { return new DuplicateEntityGrouping[0]; } public IEnumerable&lt;DuplicateEntityGrouping&gt; GetDuplicateEntities(ExecutionContext context, Entity entity) { return new DuplicateEntityGrouping[0]; } public IEnumerable&lt;Entity&gt; FindByNames(ExecutionContext context, IEnumerable&lt;EntityType&gt; entityTypes, IEnumerable&lt;string&gt; names, int maxResults) { return new Entity[0]; } public IEnumerable&lt;EntityEdge&gt; GetIncomingRelationships(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetIncomingRelationships(ExecutionContext context, Guid id, int page, int take) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetOutgoingRelationships(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityEdge&gt; GetOutgoingRelationships(ExecutionContext context, Guid id, int page, int take) { throw new NotImplementedException(); } public IEnumerable&lt;Entity&gt; GetIncomingRelationshipEndpoints(ExecutionContext context, Guid id, string edgeType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.IncomingEdges) .Where(e =&gt; e.EdgeType == edgeType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public IEnumerable&lt;Entity&gt; GetIncomingRelationshipEndpoints( ExecutionContext context, Guid id, string edgeType, string entityType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.IncomingEdges) .Where(e =&gt; e.EdgeType == edgeType &amp;&amp; e.FromEntity.EntityType == entityType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public IEnumerable&lt;Entity&gt; GetOutgoingRelationshipEndpoints(ExecutionContext context, Guid id, string edgeType, int page, int take) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { var result = this.FilterEntitySet(dataContext.Set&lt;SimpleEntity&gt;(), context) .Where(e =&gt; e.Id == id) .SelectMany(e =&gt; e.OutgoingEdges) .Where(e =&gt; e.EdgeType == edgeType) .Skip(page * 20) .Take(take); foreach (var entry in result) yield return (T)entry.FromEntity.ToEntity(this.Context, context); } } public bool HasRelationship(ExecutionContext context, T first, T second, string relationshipType) { using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { return this.FilterEntitySet(dataContext.Set&lt;SimpleEntityEdge&gt;(), context) .Any(e =&gt; e.FromEntityId == first.Id &amp;&amp; e.ToEntityId == second.Id &amp;&amp; e.EdgeType == relationshipType); } } public void InsertRelationship(ExecutionContext context, T contextEntity, EntityEdge edge) { this.InsertRelationship(context, contextEntity, edge, null); } public void InsertRelationship(ExecutionContext context, T contextEntity, EntityEdge edge, bool? ignoreDuplicates = null) { this.VerifyEntityContext(contextEntity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { InsertRelationshipInternal(contextEntity, edge, dataContext); dataContext.SaveChanges(); } } private static void InsertRelationshipInternal(T contextEntity, EntityEdge edge, DbContext dataContext) { if (edge.FromReference.IsReferenceTo(contextEntity)) { var ids = dataContext.Set&lt;SimpleEntity&gt;().Where(e =&gt; e.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Select(e =&gt; e.Id); foreach (var id in ids) { var entry = new SimpleEntityEdge() { FromEntityId = contextEntity.Id, ToEntityId = id, EdgeType = edge.EdgeType, EdgeData = null }; dataContext.Set&lt;SimpleEntityEdge&gt;().Add(entry); } } else if (edge.ToReference.IsReferenceTo(contextEntity)) { var ids = dataContext.Set&lt;SimpleEntity&gt;().Where(e =&gt; e.EntityCodes.Any(c =&gt; c.Code == edge.FromReference.Code.Key)).Select(e =&gt; e.Id); foreach (var id in ids) { var entry = new SimpleEntityEdge() { ToEntityId = contextEntity.Id, FromEntityId = id, EdgeType = edge.EdgeType, EdgeData = null }; dataContext.Set&lt;SimpleEntityEdge&gt;().Add(entry); } } else if ((contextEntity.IsDeleted != null &amp;&amp; contextEntity.IsDeleted.Value) || contextEntity.ProcessedData.OriginEntityCode.Type.Is(\" Deleted\")) { return; } else { throw new ArgumentException(\"The edge to be inserted doesn't reference the context entity. ContextEntity: {0} Edge: {1}\".FormatWith(contextEntity.ProcessedData.OriginEntityCode, edge), nameof(edge)); } } public void DeleteRelationship(ExecutionContext context, T entity, EntityEdge edge) { this.VerifyEntityContext(entity, context); using (var dataContext = context.CreateDbContext&lt;PrimaryDataStoreModel&gt;()) { if (edge.FromReference.IsReferenceTo(entity)) { dataContext.Set&lt;SimpleEntityEdge&gt;().Where(e =&gt; e.FromEntityId == entity.Id &amp;&amp; e.EdgeType == edge.EdgeType &amp;&amp; e.ToEntity.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Delete(); dataContext.SaveChanges(); } else if (edge.ToReference.IsReferenceTo(entity)) { dataContext.Set&lt;SimpleEntityEdge&gt;().Where(e =&gt; e.ToEntityId == entity.Id &amp;&amp; e.EdgeType == edge.EdgeType &amp;&amp; e.FromEntity.EntityCodes.Any(c =&gt; c.Code == edge.ToReference.Code.Key)).Delete(); dataContext.SaveChanges(); } else if ((entity.IsDeleted != null &amp;&amp; entity.IsDeleted.Value) || entity.ProcessedData.OriginEntityCode.Type.Is(\" Deleted\")) { return; Ignore } else throw new ArgumentException(\"The edge to be inserted doesn't reference the context entity. ContextEntity: {0} Edge: {1}\".FormatWith(entity.ProcessedData.OriginEntityCode, edge), nameof(edge)); } } public EntityAggregatedParents GetCachedParents(ExecutionContext context, Guid id) { throw new NotImplementedException(); } public IEnumerable&lt;EntityAggregatedParents&gt; GetCachedParents(ExecutionContext context, IEnumerable&lt;Guid&gt; ids) { throw new NotImplementedException(); } public void Update(ExecutionContext context, EntityAggregatedParents entityParents) { throw new NotImplementedException(); } } } You will then need to update your container.config to include the switch to tell the Entity Framework to use your other database. &lt;connectionStrings&gt; &lt;add name=\"MyContext\" providerName=\"MySql.Data.MySqlClient\" connectionString=\"server=localhost;port=3306;database=mycontext;uid=root;password=********\" &gt; &lt; connectionStrings&gt; &lt;entityFramework&gt; &lt;defaultConnectionFactory type=\"System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\" &gt; &lt;providers&gt; &lt;provider invariantName=\"MySql.Data.MySqlClient\" type=\"MySql.Data.MySqlClient.MySqlProviderServices, MySql.Data.Entity.EF6\" &gt; &lt;provider invariantName=\"System.Data.SqlClient\" type=\"System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\" &gt; &lt; providers&gt; &lt; entityFramework&gt;"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-50-events-events-html": {
						"id": "docs-91-developer-portal-50-events-events-html",
						"title": "Events",
						"category": "",
						"url": " /docs/91-Developer-Portal/50-Events/Events.html",
						"content": "There are many events that occur within CluedIn while data is being ingested and processed. These Events are made available to you as a developer to interact with. Here are some of the events that you can hook into: AgentJobAbandonEvent AgentJobCrawlingFinishedEvent AgentJobEnqueuedEvent AgentJobFinishedEvent AgentJobQueueEvent AgentJobSentToAgentEvent AgentJobStateChangedEvent AgentJobStatusUpdatedEvent AgentPingEvent AgentRegistrationEvent AgentRegistrationEventType ProcessingJobFinishedEvent ProcessingJobStartedEvent ProcessingJobStatusUpdatedEvent WorkflowEvent WorkflowFailedEvent WorkflowFinishedEvent WorkflowStartedEvent In your code, you can subscribe to one of these events."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-55-rules-developing-20with-20rules-html": {
						"id": "docs-91-developer-portal-55-rules-developing-20with-20rules-html",
						"title": "Developing with Rules",
						"category": "",
						"url": " /docs/91-Developer-Portal/55-Rules/Developing%20with%20Rules.html",
						"content": "Rules in CluedIn Using the rules engine, CluedIn users can configure areas of the application to operate on specific entities. The entities can then be modified, or simply returned to provide a filter on querying. Creating a RuleAction The following guidance is for version 3.2.4+ RuleActions are simple classes that can be included into any extension and deployed into CluedIn. RuleActions are automatically discovered by CluedIn at start up and do not need to be directly registered. To create a RuleAction: Create a class that extends BaseRuleAction The base class provides the simplest foundation to build upon Override the Name and SupportsPreview properties Name should be user friendly text as it will be displayed in the UI. Set SupportsPreview to true only if you want to support calling the action in preview mode Optionally add RuleProperty members Each property decorated with the [RuleProperty] attribute will be displayed in the UI with a field for users to enter data. Implement the Run method This is the method called when the action is actually invoked. Here you can apply changes to the entity or use the context to access other areas of CluedIn. Return a RuleActionResult that contains details of the change that occurred. NOTE: Your class must have a parameter-less constructor, or no constructors. using CluedIn.Core.Data.Parts; using CluedIn.Core.Processing; using CluedIn.Core.Rules; using CluedIn.Core.Rules.Models; namespace CluedIn.Rules.Actions { public class AddTag : BaseRuleAction { public override string Name =&gt; \"Add Tag\"; public override bool SupportsPreview =&gt; false; [RuleProperty] public string Value { get; set; } public override RuleActionResult Run(ProcessingContext context, IEntityMetadataPart entityMetadataPart, bool isPreview) { var parsedValue = ParseTokens(Value, context, entityMetadataPart); entityMetadataPart.Tags.Add(new Core.Data.Tag(parsedValue)); return new RuleActionResult { IsSuccess = true, Messages = new string[] { $\"Added Tag {parsedValue}\" } }; } } } CluedIn will then be able to list the RuleAction: Parsing Tokens The following guidance is for version 3.2.4+ Users may supply tokens to fields in a RuleAction to set dynamic values that will be resolved when the action runs. To ensure rule tokens are resolved, a call should be made to ParseTokens within the Run method: var parsedValue = ParseTokens(Value, context, entityMetadataPart); entityMetadataPart.Tags.Add(new Core.Data.Tag(parsedValue)); Here, the RuleProperty called Value may contain tokens - the value is passed to ParseTokens with the variable parsedValue containing the resulting value. The parsedValue is then adding to the entity as a tag. Supporting Preview To support preview, further changes are required: Set SupportsPreview to true Implement IRuleActionPreview on the class e.g. public class AddTag : BaseRuleAction, IRuleActionPreview Implement the BuildPreviewData method BuildPreviewData will be invoked before the Run method - it is used to prepare a dummy entity with data that can be modified. When implementing the method, modify the property so that the Run method should succeed. For example, in the DeleteValue RuleAction the BuildPreviewData method adds a field, so that when Run is invoked, the field can be removed public void BuildPreviewData(IEntity result) { result.Properties.Add(FieldName, \"Old Value\"); } Creating a RuleToken The following guidance is for version 3.2.4+ RuleTokens allow a user to provide a value to a rule action property that will be resolved when the action is invoked. Examples include: Getting a data Getting the value of vocabulary As with RuleActions they can be implemented with simple classes that can be included in an extension and are automatically resolved when CluedIn starts up. To create a RuleToken: Create a class that implements IRuleActionToken Set the Key property This is the text that the user will use in the UI Implement the Resolve member The input passed to this member will be an optional value that the user specifies using CluedIn.Core.Data.Parts; using CluedIn.Core.Processing; namespace CluedIn.Rules.Tokens.Implementations { public class VocabularyToken : IRuleActionToken { public string Key =&gt; \"Vocabulary\"; public string Resolve(ProcessingContext context, IEntityMetadataPart entityMetadataPart, string input) { if (entityMetadataPart.Properties == null) return input; if (entityMetadataPart.Properties.TryGetValue(input, out var value)) return value; return input; } } } Users can then specify the token in a field when configuring a RuleAction: In this example, the user is setting the user.fullName property to the combined value of “user.firstName user.lastName”"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-60-external-search-deploying-20a-20new-20external-20search-20provider-html": {
						"id": "docs-91-developer-portal-60-external-search-deploying-20a-20new-20external-20search-20provider-html",
						"title": "Deploying a New External Search Provider",
						"category": "",
						"url": " /docs/91-Developer-Portal/60-External_Search/Deploying%20a%20New%20External%20Search%20Provider.html",
						"content": "An External Search Provider will generate new binaries that will need to be hosted in CluedIn in run. This means that we need to be able to move *.dll files into the ServerComponentHost of CluedIn. It is simply a matter of copying the *.dll files from your External Search Provider into CluedIn. External Searches are only availble Globally or otherwise they are disabled. External Searches are not enabled through the CluedIn User Interface, but rather through configuration toggles. You will need to place respective API Tokens into the CluedIn Configuration files if your external search provider requires one. CluedIn does not ship with any API Tokens out of the box."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-60-external-search-external-20search-20providers-html": {
						"id": "docs-91-developer-portal-60-external-search-external-20search-20providers-html",
						"title": "External Search Providers",
						"category": "",
						"url": " /docs/91-Developer-Portal/60-External_Search/External%20Search%20Providers.html",
						"content": "In CluedIn, an External Search Provider allows you to take input from data flowing through the processing pipeline and then lookup services (typically external API’s) as to enrich just a single particular entity. Imagine you have an API that allows you to lookup a company by an Identifier, Name or Website and it would bring back more, enriched data so that you could extend what data you had internally on that entity. A good example would be Crunchbase, Duns and Bradstreet or Open Corporates. One can register for these API’s and lookup individual records via certain values and it may return zero or many results. For example, imagine the company Oracle. Here is an example of a “before” enrichment. Here is the same record after 5 of the 35 prebuilt enrichers has been enabled. This is the un-enriched record from another view. Here is thaat view after enrichment. At the same time, you could imagine that if you looked up via something like the name of a company, you might actually receive multiple results back. The External Search framework of CluedIn can help solve this particular situation where we lookup via “fuzzy” reference as well as key identifier lookups as well. To create a new provider, you will want to start by creating a new C# project and using Nuget to pull in the references to CluedIn.ExternalSearch as well as CluedIn.Core. From here you will want to create a new class and inherit from BaseExternalSearchProvider. Our External Search Providers are open source, so you might find it easier to simply learn from providers that have already been written. Before we start coding, it is worth planning about what we will do to attack the project. An external search provider handles a few core pieces. Firstly, you will need to tell your new class what types of data you can lookup. This will typically be bound to a list of EntityTypes e.g. Organisation, Person. You can choose to have one External Search Provider per type or you can include many in the same provider - the choice is yours. The next piece you will have to determine is what properties, core vocabularies and metadata you will need from the entities being passed to your new provider to properly lookup the external system. You might simply want to use the name of the entity, the display name or potentially some of the core or provider specific vocabularies. You might want to use the edges or aliases, the choice is really up to what your external provider can lookup values by. The next planning step is to make the call to your provider using as filtered a value as possible. Obviously if we are looking up external services via a name, you might find that you receive hundreds of results. This is not a problem, it just means that more processing will happen on the CluedIn side to determine exactly which records to connect to your input from your external search queries. If you have the possibility and you have the data, it is best to add as many other filters onto your query instead of just the name of the record. Finally you will need to map the results from your lookups back into Clues so that CluedIn can process this. The most critical piece to understand in this step is that your clues will need to have a perfect Entity Code match or you will need to map to core vocabularies that may or may not result in an Entity Code creation to an entity that already exists in your CluedIn account. All results from your external lookups will be natively stored in a local cache database to help in two main situations: 1: If we need to lookup that record again, we can look locally in this database first instead of constantly asking an external source for the same lookups and wasting API tokens. By default, this data clears its cache every 2 weeks. You can change this in your container.config file. 2: If a new Entity Code is created anywhere in CluedIn, then this local table may also contain Entity Codes that can now cause records to merge or link. You can explore this cache table by looking into the ExternalSearch table in your SQL datastore for CluedIn. At this stage it is worth remembering that two clues or two entities will only merge if they have a perfect Entity Code match. This means that the Origin, Type and Id are exactly the same. Considering that CluedIn comes out of the box with many different external search providers, you might find that by adding new external search providers, it will allow some of the other existing providers to have more possibilities of finding the right lookup records. This is due to the recursive nature of the External Search Provider framework. If you find that you are looking up a service via an identifier like a domain or business Id, then chances are those records will bring back with it not only Entity Codes but also a value that will allow other External Search providers to lookup their own services. For example, imagine that we only had the Name, Domain and Twitter tag in CluedIn and then enabled the External Search providers. This might allow us to lookup Open Corporates via the company Name, Twitter using the Twitter tag and nothing using the Domain. We then might find that by looking up Twitter, the result will bring back a company logo, a company name and the phone number. This might mean that we can then lookup the White Pages or another business register via Phone Number and in-turn, that result might bring back the Facebook and LinkedIn Url and in-turn allow us to recursively lookup those respective External Search providers as well. Many of these external search providers will require you to purchase API tokens as to use their API. This obviously means that we need to be aware of how many calls we use as to not run the bill up or process through this allowed token amount very quickly. It was mentioned above already that the cache table will help with this if you are looking up the exact same values and raising this cache limit to 4, 6 weeks or a larger capacity would help with this type of situation. It is also worth mentioning that we will only trigger another lookup to a service if a change is made to that record in CluedIn and the cache expiry on that individual record falls below the cache limit that you have set. At the same time, the more up-to-date you need your records, you might find that you lower this default 2 week cache to 2 days, or 2 hours. Very similar to your integrations, you will need to construct a clue and use Vocabularies to be able to map back to core CluedIn vocabularies. To enable a particular external search provider, you will need to change the respective “Enabled” flag in your container.config configuration file. By default, all are turned off in a new installation of CluedIn. This configuration file is also where we suggest that you place your appropriate API keys to talk to external providers that require it. To be able to understand what lookup values an External Search provider supports and what types of Vocabularies it will enrich, you can call “api integration enricher {id} details” where id is the Id of the External Search Provider you are wanting to inspect. Here is an example of what it will bring back for “Company House”: [Id(\"2A9E52AE-425B-4351-8AF5-6D374E8CC1A5\")] [Name(\"Company House Enricher\")] [EnrichSource(\"www.companyhouse.com\")] [LookupEntityTypes(\" Organization\")] [ReturnedEntityTypes(\" Organization\", \" Person\")] [LookupVocabularies(\"CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInOrganization\")] [ReturnedVocabularies(\"CompanyHouseVocabulary.Organization\", \"CompanyHouseVocabulary.Person\")] [LookupVocabulariesKeys( \"CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInOrganization.CodesCompanyHouse\", \"CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInOrganization.AddressCountryCode\", \"CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInOrganization.OrganizationName\" ) ] [ReturnedVocabulariesKeys( \"CompanyHouseVocabulary.Organization.CompanyNumber\", \"CompanyHouseVocabulary.Organization.Charges\", \"CompanyHouseVocabulary.Organization.CompanyStatus\", \"CompanyHouseVocabulary.Organization.Type\", \"CompanyHouseVocabulary.Organization.Jurisdiction\", \"CompanyHouseVocabulary.Organization.Has_been_liquidated\", \"CompanyHouseVocabulary.Organization.Has_insolvency_history\", \"CompanyHouseVocabulary.Organization.Registered_office_is_in_dispute\", \"CompanyHouseOrgAddressVocabulary.AddressLine1\", \"CompanyHouseOrgAddressVocabulary.AddressLine2\", \"CompanyHouseOrgAddressVocabulary.Locality\", \"CompanyHouseOrgAddressVocabulary.PostCode\", \"CompanyHouseVocabulary.Person.Name\", \"CompanyHouseVocabulary.Person.Officer_role\", \"CompanyHouseVocabulary.Person.Appointed_on\", \"CompanyHouseVocabulary.Person.Date_of_birth\", \"CompanyHouseVocabulary.Person.Country_of_residence\", \"CompanyHouseVocabulary.Person.Occupation\", \"CompanyHouseVocabulary.Person.Nationality\", \"CompanyHouseVocabulary.Person.Resigned_on\", \"CompanyHousePersonAddressVocabulary.CareOf\", \"CompanyHousePersonAddressVocabulary.Region\", \"CompanyHousePersonAddressVocabulary.Postal_code\", \"CompanyHousePersonAddressVocabulary.Premises\", \"CompanyHousePersonAddressVocabulary.Country\", \"CompanyHousePersonAddressVocabulary.Locality\", \"CompanyHousePersonAddressVocabulary.AddressLine1\", \"CompanyHousePersonAddressVocabulary.AddressLine2\" ) ]"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-70-integration-pattern-eventual-20connectivity-html": {
						"id": "docs-91-developer-portal-70-integration-pattern-eventual-20connectivity-html",
						"title": "Eventual Connectivity",
						"category": "",
						"url": " /docs/91-Developer-Portal/70-Integration_Pattern/Eventual%20Connectivity.html",
						"content": "The CluedIn Eventual Connectivity pattern is at the heart of CluedIn. This pattern of integration makes it simple to blend data from many different systems into a unified and connected dataset. You will need to understand some of the core concepts behind the pattern: Entity Codes Aliases Vocabularies Edges GraphStore Shadow Entities The aim of CluedIn is to build a unified and connected dataset where the model of that data is generated from the data itself. This means that you don’t have control over the modelling within CluedIn up-front (but you will have control later in the process). As you work more with CluedIn, you will realise that if we would like to do purpose fit modelling, then we will stream data out of CluedIn and make it available in other systems where purpose-fit modelling can be applied. The Eventual Connectivity pattern is here to simplify the integration of data and to forgoe long and tedious architecture meetings to discuss how different systems will connect with each other. The Eventual Connectivity pattern will do the blending of data for you, as long as you can instruct it and guide it using the concepts mentioned above. The major project advantage of this pattern is that we can take one system, actually you can even take one object in a system at a time, mark up that object and let the pattern take care of finding how it will blend. With this in mind, we start by taking one system, going through an object at a time and determining what (if any) Entity Codes, Aliases and Edges exist on that object type. It is also important to remember that with Edges, we do not want, or expect, you to know where this reference is pointing to - rather we would ask you to instruct us that a particular field or column should point to an Entity Code of another record. This record could be in the current system, or it could be in another system all together. This becomes slightly more complex when you start working with systems that have custom business rules that dictate Entity Codes. The good thing, is that because you take this process one object or system at a time - you can talk with the product owner and domain expert of their system and ask them to help describe that the different properties entail. For example, imagine you had an identifier which wsa actually a concatenated value of two different identifers from different systems. The question needs to be asked if the concatenated version of that identifier is the Entity Code or if there are 3 Entity Codes e.g. One for each piece of the Identifier and then one to act as a unique reference in the current system. If in doubt of uniqueness, you can always mark these records as Aliases or that you can apply Entity Codes in a post processor."
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-80-integrity-checks-validate-20server-20heatlh-html": {
						"id": "docs-91-developer-portal-80-integrity-checks-validate-20server-20heatlh-html",
						"title": "Validate Server Heatlh",
						"category": "",
						"url": " /docs/91-Developer-Portal/80-Integrity_Checks/Validate%20Server%20Heatlh.html",
						"content": "There might be times when you would like to Validate that CluedIn can operate propely once it has been booted. An example of this would be to validate that all Vocabularies are created properly. Essentially, the application will not boot if it does not pass all the validation and integrity checks. Other examples could include: Not booting if integrations do not pass the Validation Framework checks. Not booting if all databases don’t connect properly. Not booting if disk space is not available. Here is an example of adding your own types of Integrity checks when CluedIn boots: using System; using System.Collections.Generic; using System.Linq; using Castle.Windsor; using CluedIn.Core.Data.Vocabularies; using CluedIn.Core.Logging; namespace Custom.Core.IntegrityChecks { &lt;summary&gt;The vocabulary checks&lt; summary&gt; public class VocabularyChecks : IIntegrityCheck { public void Verify(IWindsorContainer container, ILogger log) { var allVocabularies = (container.ResolveAll&lt;IVocabulary&gt;() ?? new IVocabulary[0]).ToList(); var vocabularies = allVocabularies.Where(d =&gt; d.Keys.Any()).ToList(); var invalidPropertiesVocabularies = this.GetInvalidVocabularyProperties(allVocabularies).ToList(); this.VerifyDuplicateKeysDoesNotExists(allVocabularies, log); if (invalidPropertiesVocabularies.Any()) { this.VerifyNullPropertiesDoesNotExists(invalidPropertiesVocabularies, log); this.VerifyKeyPropertiesWithNoVocabularyAssignedDoesNotExist(invalidPropertiesVocabularies, log); this.VerifyKeyPropertiesThatDoesNotMatchContainingVocabularyDoesNotExist(invalidPropertiesVocabularies, log); } this.VerifyEmptyVocabulariesDoesNotExist(allVocabularies, log); this.VerifyEmptyVocabularyGroupsDoesNotExist(allVocabularies, log); this.VerifyVocabulariesWithNoGroupingsDoesNotExists(allVocabularies, log); this.VerifyKeyPrefixIsPopulated(allVocabularies, log); log.Info(() =&gt; \"Loaded {0} vocabularies with total {1} keys\".FormatWith(vocabularies.Count(), vocabularies.Sum(v =&gt; v.Keys.Count()))); } public bool VerifyDuplicateKeysDoesNotExists(IEnumerable&lt;IVocabulary&gt; allVocabularies, ILogger log, bool debugBreak = true) { var vocabularies = allVocabularies.Where(d =&gt; d.Keys.Any()).ToList(); var duplicateKeys = vocabularies.SelectMany(v =&gt; v.Keys).GroupBy(k =&gt; k.ToString()).Where(g =&gt; g.Count() &gt; 1).ToList(); Validate that no duplicate vocabulary keys exists if (duplicateKeys.Any()) { duplicateKeys.ForEach(g =&gt; log.Error(() =&gt; \"Multiple vocabulary keys exists of type: \\\"{0}\\\"; Count: {1}; Vocabularies: {2}\".FormatWith(g.Key, g.Count(), string.Join(\", \", g.Select(k =&gt; k.Vocabulary.VocabularyName + \" (\" + k.Vocabulary.GetType().Name + \")\").Distinct())))); if (debugBreak) System.Diagnostics.Debugger.Break(); else return true; throw new ApplicationException(\"Multiple vocabulary keys exists\"); } return false; } public bool VerifyEmptyVocabulariesDoesNotExist(IEnumerable&lt;IVocabulary&gt; allVocabularies, ILogger log, bool debugBreak = true) { var emptyVocabularies = allVocabularies.Where(d =&gt; !d.Keys.Any() &amp;&amp; !(d is DynamicVocabulary)).ToList(); if (emptyVocabularies.Any()) { log.Warn(() =&gt; \"Found {0} vocabularies with no keys; Vocabularies: {1}\".FormatWith(emptyVocabularies.Count, string.Join(\", \", emptyVocabularies.Select(v =&gt; v.VocabularyName)))); return true; } return false; } public bool VerifyEmptyVocabularyGroupsDoesNotExist(IEnumerable&lt;IVocabulary&gt; allVocabularies, ILogger log, bool debugBreak = true) { var emptyVocabularyGroups = allVocabularies.OfType&lt;SimpleVocabulary&gt;().SelectMany(v =&gt; v.Groups).Where(g =&gt; !g.Keys.Any()).ToList(); if (emptyVocabularyGroups.Any()) { log.Warn(() =&gt; \"Found {0} vocabulary key groups with no keys; Vocabularies: {1}\".FormatWith(emptyVocabularyGroups.Count, string.Join(\", \", emptyVocabularyGroups.Select(v =&gt; v.Name + \" (\" + v.Vocabulary.VocabularyName + \")\")))); return true; } return false; } public bool VerifyVocabulariesWithNoGroupingsDoesNotExists(IEnumerable&lt;IVocabulary&gt; allVocabularies, ILogger log, bool debugBreak = true) { var noGroupingVocabularies = allVocabularies.Where(d =&gt; d.Grouping == null).ToList(); if (noGroupingVocabularies.Any()) { log.Warn(() =&gt; \"Found {0} vocabularies with no grouping; Vocabularies: {1}\".FormatWith(noGroupingVocabularies.Count, string.Join(\", \", noGroupingVocabularies.Select(v =&gt; v.VocabularyName)))); return true; } return false; } public bool VerifyKeyPrefixIsPopulated(IEnumerable&lt;IVocabulary&gt; allVocabularies, ILogger log, bool debugBreak = true) { var noKeyPrefixVocabularies = allVocabularies.Where(d =&gt; d.KeyPrefix == null).ToList(); if (noKeyPrefixVocabularies.Any()) { log.Warn(() =&gt; \"Found {0} vocabularies with no key prefix; Vocabularies: {1}\".FormatWith(noKeyPrefixVocabularies.Count, string.Join(\", \", noKeyPrefixVocabularies.Select(v =&gt; v.VocabularyName ?? v.GetType().Name)))); return true; } return false; } public bool VerifyNullPropertiesDoesNotExists(IEnumerable&lt;(IVocabulary vocabulary, string propertyName, VocabularyKey vocabularyKey)&gt; invalidPropertiesVocabularies, ILogger log, bool debugBreak = true) { var nullProperties = invalidPropertiesVocabularies.Where(t =&gt; t.vocabularyKey == null).ToList(); if (nullProperties.Any()) { return this.DoFail( log, () =&gt; \"Found {0} vocabulary properties with null value; Vocabulary Properties: {1}\".FormatWith(nullProperties.Count, string.Join(\", \", nullProperties.Select(v =&gt; v.propertyName + \" (\" + v.vocabulary.VocabularyName + \")\"))), \"Vocabulary properties with null value exists\", debugBreak ); } return false; } public bool VerifyKeyPropertiesWithNoVocabularyAssignedDoesNotExist(IEnumerable&lt;(IVocabulary vocabulary, string propertyName, VocabularyKey vocabularyKey)&gt; invalidPropertiesVocabularies, ILogger log, bool debugBreak = true) { var noVocabularyProperties = invalidPropertiesVocabularies.Where(t =&gt; t.vocabularyKey != null &amp;&amp; t.vocabularyKey.Vocabulary == null).ToList(); if (noVocabularyProperties.Any()) { return this.DoFail( log, () =&gt; \"Found {0} vocabulary keys with no associated vocabulary; Vocabulary Properties: {1}\".FormatWith(noVocabularyProperties.Count, string.Join(\", \", noVocabularyProperties.Select(v =&gt; v.propertyName + \" (\" + (v.vocabulary.VocabularyName ?? v.vocabulary.GetType().Name) + \")\"))), \"Vocabulary keys with no associated vocabulary exists\", debugBreak ); } return false; } public bool VerifyKeyPropertiesThatDoesNotMatchContainingVocabularyDoesNotExist(IEnumerable&lt;(IVocabulary vocabulary, string propertyName, VocabularyKey vocabularyKey)&gt; invalidPropertiesVocabularies, ILogger log, bool debugBreak = true) { var vocabulariesDoesNotMatchProperties = invalidPropertiesVocabularies.Where(t =&gt; t.vocabularyKey != null &amp;&amp; t.vocabularyKey.Vocabulary != null &amp;&amp; t.vocabularyKey.Vocabulary != t.vocabulary).ToList(); if (vocabulariesDoesNotMatchProperties.Any()) { return this.DoFail( log, () =&gt; \"Found {0} vocabulary keys where the vocabulary doesn't match; Vocabulary Properties: {1}\".FormatWith(vocabulariesDoesNotMatchProperties.Count, string.Join(\", \", vocabulariesDoesNotMatchProperties.Select(v =&gt; v.propertyName + \" (\" + v.vocabulary.VocabularyName + \")\"))), \"Vocabulary keys where the vocabulary doesn't match exists\", debugBreak ); } return false; } private bool DoFail(ILogger log, Func&lt;string&gt; logMessage, string exceptionMessage, bool debugBreak) { log.Warn(logMessage); if (debugBreak) System.Diagnostics.Debugger.Break(); else return true; throw new ApplicationException(exceptionMessage); } public IEnumerable&lt;(IVocabulary vocabulary, string propertyName, VocabularyKey vocabularyKey)&gt; GetInvalidVocabularyProperties(IEnumerable&lt;IVocabulary&gt; allVocabularies) { return GetVocabularyProperties(allVocabularies) .OrderBy(t =&gt; t.vocabulary.VocabularyName) .ThenBy(t =&gt; t.propertyName) .Where(t =&gt; t.vocabularyKey == null || t.vocabularyKey.Vocabulary == null || t.vocabularyKey.Vocabulary != t.vocabulary); } private static IEnumerable&lt;(IVocabulary vocabulary, string propertyName, VocabularyKey vocabularyKey)&gt; GetVocabularyProperties(IEnumerable&lt;IVocabulary&gt; vocabularies) { foreach (var vocabulary in vocabularies) { var properties = vocabulary.GetType().GetProperties(); foreach (var property in properties) { if (!property.CanRead || !typeof(VocabularyKey).IsAssignableFrom(property.PropertyType)) continue; if (property.Name == \"CompositeKey\") continue; var propertyValue = property.GetValue(vocabulary) as VocabularyKey; yield return (vocabulary, property.Name, propertyValue); } } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-90-jobs-one-20time-20jobs-html": {
						"id": "docs-91-developer-portal-90-jobs-one-20time-20jobs-html",
						"title": "One Time Jobs",
						"category": "",
						"url": " /docs/91-Developer-Portal/90-Jobs/One%20Time%20Jobs.html",
						"content": "CluedIn has a generic Job system that allows you to run simple background jobs. Here is an example of how you could add your own custom jobs that run once and only once. var jobServerClient = this.ApplicationContext.Container.TryResolve&lt;IJobServerClient&gt;(); if (jobServerClient == null) { context.Log.Error(() =&gt; \"Could not find IJobServerClient in container\"); return this.Request.CreateErrorResponse(HttpStatusCode.InternalServerError, \"Our job server is down and not accepting new providers for now. Please try again later.\"); } jobServerClient.Run(this.ApplicationContext.Container.Resolve&lt;IInstantDetailedCrawlJob&gt;(), new JobArgs() { UserId = context.Principal.Identity.UserId.ToString(), Message = providerDefinition.ProviderId.ToString(), ConfigurationId = providerDefinition.Id.ToString(), OrganizationId = context.Organization.Id.ToString(), Schedule = jobDataCheck.Schedule(DateTimeOffset.Now, providerDefinition.WebHooks != null ? providerDefinition.WebHooks.Value : false) });"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-90-jobs-recurring-20jobs-html": {
						"id": "docs-91-developer-portal-90-jobs-recurring-20jobs-html",
						"title": "Recurring Jobs",
						"category": "",
						"url": " /docs/91-Developer-Portal/90-Jobs/Recurring%20Jobs.html",
						"content": "CluedIn has a generic Job system that allows you to run simple background jobs. Here is an example of how you could add your own custom jobs that run on a recurring schedule. var jobServerClient = this.ApplicationContext.Container.TryResolve&lt;IJobServerClient&gt;(); if (jobServerClient == null) { context.Log.Error(() =&gt; \"Could not find IJobServerClient in container\"); return this.Request.CreateErrorResponse(HttpStatusCode.InternalServerError, \"Our job server is down and not accepting new providers for now. Please try again later.\"); } Schedule Weekly Full Recrawl Recurring Job jobServerClient.Run(this.ApplicationContext.Container.Resolve&lt;IFullCrawlJob&gt;(), new JobArgs() { UserId = context.Principal.Identity.UserId.ToString(), Message = providerDefinition.ProviderId.ToString(), Schedule = jobDataCheck.Schedule(DateTimeOffset.Now, providerDefinition.WebHooks != null ? providerDefinition.WebHooks.Value : false), ConfigurationId = providerDefinition.Id.ToString(), OrganizationId = context.Organization.Id.ToString() });"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-90-jobs-scheduled-20jobs-html": {
						"id": "docs-91-developer-portal-90-jobs-scheduled-20jobs-html",
						"title": "Scheduled Jobs",
						"category": "",
						"url": " /docs/91-Developer-Portal/90-Jobs/Scheduled%20Jobs.html",
						"content": "CluedIn has a generic Job system that allows you to run simple background jobs. Here is an example of how you could add your own custom jobs that run once at a later point in time. var jobServerClient = this.ApplicationContext.Container.TryResolve&lt;IJobServerClient&gt;(); if (jobServerClient == null) { context.Log.Error(() =&gt; \"Could not find IJobServerClient in container\"); return this.Request.CreateErrorResponse(HttpStatusCode.InternalServerError, \"Our job server is down and not accepting new providers for now. Please try again later.\"); } Schedule Recurring Job jobServerClient.Run(this.ApplicationContext.Container.Resolve&lt;IJob&gt;(), new JobArgs() { UserId = context.Principal.Identity.UserId.ToString(), Message = providerDefinition.ProviderId.ToString(), Schedule = jobDataCheck.Schedule(DateTimeOffset.Now, providerDefinition.WebHooks != null ? providerDefinition.WebHooks.Value : false), ConfigurationId = providerDefinition.Id.ToString(), OrganizationId = context.Organization.Id.ToString() });"
					}

					
				
			
		
			
				
					,
					

					"docs-91-developer-portal-developerportal-html": {
						"id": "docs-91-developer-portal-developerportal-html",
						"title": "Developer Portal",
						"category": "",
						"url": " /docs/91-Developer-Portal/DeveloperPortal.html",
						"content": "Introduction The first thing you need to do when CluedIn is running, is to feed it with Data. You need to choose which data you want to add to CluedIn. Data is pushed into CluedIn via integrations. You have two options Installing an existing integration Building a custom integration There are two main types of integrations: Providers These integrations allow you to add data into CluedIn. They can connect to cloud tools, databases, file systems, etc. extract the data you want to send to CluedIn and assemble it in a format CluedIn will understand. There are many providers available in our GitHub, but alternatively you can also build your own to cater for your specific requirements. In order to do this though, you will require some C# coding experience. Enrichers Their mission is to add extra information to improve data that is already in CluedIn. Data in CluedIn is structured in entities; these are similar to records. They can contain information about a person, a company, a task, etc. An enricher will use the existing information CluedIn to then query other external systems to try to find out more information about that entity, i.e. enrich it. We have a list of available enrichers in our GitHub, but you can also build your own, as long as you have some C# coding experience."
					}

					
				
			
		
			
				
					,
					

					"docs-98-upgrade-path-upgrade-23-to-24-html": {
						"id": "docs-98-upgrade-path-upgrade-23-to-24-html",
						"title": "From V2.3.x to V.2.4.x",
						"category": "",
						"url": " /docs/98-upgrade-path/upgrade-23-to-24.html",
						"content": "Instruction In progress."
					}

					
				
			
		
			
				
					,
					

					"docs-99-releasenotes-0-index-html": {
						"id": "docs-99-releasenotes-0-index-html",
						"title": "Handy Links",
						"category": "",
						"url": " /docs/99-releaseNotes/0-index.html",
						"content": "CluedIn Releases CluedIn Release Notes Home Repo Releases CluedIn Home is the starting point for getting a brand new instance of CluedIn up and running locally. Github CluedIn-io Home Release Notes Charts Repo Releases CluedIn Charts are the installation scripts to install CluedIn in Kubernetes Github CluedIn-io Charts Release Notes Documentation Website If you are reading this, it means you have find your way to the new CluedIn Documentation. The CluedIn documentation is an open-source repository, open for collaboration. Feel free to create an issue and or pull-request with a change if you see any error."
					}

					
				
			
		
			
				
					,
					

					"docs-crawler-agents-using-20agents-html": {
						"id": "docs-crawler-agents-using-20agents-html",
						"title": "Using Agents",
						"category": "",
						"url": " /docs/Crawler/Agents/Using%20Agents.html",
						"content": "Agents are the orchestrators of running integrations. Agents allow you to run crawlers in remote environments, typically on different machines, even in different physical environments. Agents are typically used for running hybrid environments of CluedIn where you may host CluedIn itself in the cloud, but need to run crawls on systems that live within an internal network of a business. The Agents are responsible for running scheduled crawls and the robustness of making sure that the crawlers can survive times where they crash. For running an Agent, you will need to register an Agent API key within the CluedIn datastore and then the Agents will need matching API keys in their configuration files on the remote machines. CluedIn will use Websockets to communicate between the Agents and the CluedIn Server. When deploying your Agents, they will need to have the Agent API key match one of the API Keys that are registered in the Agents Database within CluedIn. The API key must be associated with the Organization ID of the account that is running the Agent. The simplest way to setup an agent is to remove the ServerComponent folder from CluedIn, leaving only the Agent folder. In container.config, you have to make sure that the URLs are correct (e.g. AgentServerUrl should have the value of the CluedIn’s API endpoint). For communication, Agents cannot receive incoming messages but rather uses a polling mechanism to talk with the CluedIn Server. In this way, other systems cannot instruct the Agents with a Job to run. The Agents will post data, logs and health statistics back to the CluedIn server so that CluedIn has knowledge of what is running within the Agents and any possible issues that could be happening. Executes Agent Jobs from the CluedIn System against a 3rd party provider api Job results (clues) is sent back to CluedIn as payloads Agents can be deployed: Within the CluedIn cluster (cloud) As a separate isolated component (onprem) Cloud (within the CluedIn cluster) Directly connected to the backend Communicates with Agent Controller via direct reference from the container Onprem (outside of our control) Deployed as VM’s within customers own environment Enables access to customers environments that is not accessible from the CluedIn Cluster Directly Communicates with Agent Controller over HTTP, TLS No access to CluedIn databases, Message Bus etc. Deployed with ComponentHost + individual components Ie. Smaller deployment package than the full CluedIn Processing, WebApi, DataStores is not available Agent API key is used for “Authentication” Payload Binary Format Multiple Records Compressed Types of Payloads Clue Payloads Clues produced from Crawlers Agent Job Log Payloads Logs produced from the job crawler execution (Log shipping from the Agent back to the CluedIn cluster) (CompressedRecord Payload) Job Types Normal Execute job, finishes when crawling is done Continuous Does not finish Used to monitor as system and produce clues when changes happen Ie. File system monitoring, Kafka queue,…. Jobs have statistics of Start stop dates Current number of tasks Number of completed tasks Number of failed tasks Number of clues produced Number of payloads submitted Jobs can be restricted to only run on A specific agent A specific group of agents Any agent with type Cloud SharedProcessor (shared between multiple tenants) Onprem (A single tenant) Orchestration Server Agents automatically download updates from the server (Zip file deployed centrally) Enables updates of Agent deployed in scenarios where we do not have access to the machines they are running on. Deploying an agent Agents can be deployed by downloading the Agent binaries and decompressing it onto an operating system of your choice. The folder ships with binaries for many different operating systems. We will use the example below of installing on a Windows machine. Please ask your partner or direct CluedIn contact to give you access to these binaries. Configuring the agent The Agent can be configured by visiting the container.config that is inside your &lt;agent-root&gt; Agent folder. You should see a file that looks like this configuration file: &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt; &lt;configuration xmlns:urn=\"urn:schemas-microsoft-com:asm.v1\" xmlns:xdt=\"http: schemas.microsoft.com XML-Document-Transform\"&gt; &lt;xdt:Import assembly=\"ComponentHost\" namespace=\"ComponentHost.Transforms\" &gt; &lt;startup xdt:Transform=\"InsertIfMissing\"&gt; &lt;supportedRuntime version=\"v4.0\" sku=\".NETFramework,Version=v4.8\" &gt; &lt; startup&gt; &lt;appSettings xdt:Transform=\"InsertIfMissing\"&gt; &lt;add key=\"ServerUrl\" value=\"https: localhost:9000 \" xdt:Locator=\"Condition(@key='ServerUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"AgentServerUrl\" value=\"https: localhost:9000 \" xdt:Locator=\"Condition(@key='AgentServerUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"ServerBlobUrl\" value=\"https: localhost:9000 \" xdt:Locator=\"Condition(@key='ServerBlobUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"WebhookUrl\" value=\"https: localhost:9006 \" xdt:Locator=\"Condition(@key='WebhookUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"WebhookServerUrl\" value=\"https: localhost:9006 \" xdt:Locator=\"Condition(@key='WebhookServerUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"ServerStatusUrl\" value=\"http: localhost:9004 \" xdt:Locator=\"Condition(@key='ServerStatusUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"ServerLoggingUrl\" value=\"http: localhost:9005 \" xdt:Locator=\"Condition(@key='ServerLoggingUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"ServerStatusDefaultRedirect\" value=\"https: www.cluedin.net \" xdt:Locator=\"Condition(@key='ServerStatusDefaultRedirect')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"AuthServerUrl\" value=\"https: localhost:9001 \" xdt:Locator=\"Condition(@key='AuthServerUrl')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Domain\" value=\"localhost\" xdt:Locator=\"Condition(@key='Domain')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailServer\" value=\"\" xdt:Locator=\"Condition(@key='EmailServer')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailPort\" value=\"\" xdt:Locator=\"Condition(@key='EmailPort')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailDisplayName\" value=\"\" xdt:Locator=\"Condition(@key='EmailDisplayName')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailSender\" value=\"\" xdt:Locator=\"Condition(@key='EmailSender')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailUserName\" value=\"\" xdt:Locator=\"Condition(@key='EmailUserName')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"EmailPassword\" value=\"\" xdt:Locator=\"Condition(@key='EmailPassword')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Logging.Targets.Exceptions\" value=\"false\" xdt:Locator=\"Condition(@key='Logging.Targets.Exceptions')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Health.TemporaryDirectory.Enabled\" value=\"true\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.Enabled')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Health.TemporaryDirectory.FreeSpacePctThreshold\" value=\"0.0\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.FreeSpacePctThreshold')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Health.TemporaryDirectory.FreeSpaceBytesThreshold\" value=\"209715200\" xdt:Locator=\"Condition(@key='Health.TemporaryDirectory.FreeSpaceBytesThreshold')\" xdt:Transform=\"Replace\" &gt; &lt;!-- Agent --&gt; &lt;add key=\"Agent.Enabled\" value=\"true\" xdt:Locator=\"Condition(@key='Agent.Enabled')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Agent.ErrorLogging.Project\" value=\"6\" xdt:Locator=\"Condition(@key='Agent.ErrorLogging.Project')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Agent.APIKey\" value=\"!2qwaszx12346\" xdt:Locator=\"Condition(@key='Agent.APIKey')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Agent.TaskScheduler\" value=\"Default\" xdt:Locator=\"Condition(@key='Agent.TaskScheduler')\" xdt:Transform=\"Replace\" &gt; &lt;add key=\"Agent.Queue.MaximumJobsToQueuePerCpu\" value=\"1.5\" xdt:Locator=\"Condition(@key='Agent.Queue.MaximumJobsToQueuePerCpu')\" xdt:Transform=\"Replace\" &gt; &lt; appSettings&gt; &lt; configuration&gt; Connecting to Kubernetes master server You will need to fill out the following configuration to connect to your Kubernetes cluster that is running the CluedIn Server, API and JobServer: ServerUrl should contain a value that is the route to your CluedIn WebAPI you have installed. Usually it is something like https: app.&lt;hostname&gt; api AgentServerUrl should contain a value that is the route to your CluedIn WebAPI you have installed. Usually it is something like https: app.&lt;hostname&gt; api ServerBlobUrl should contain a value to your blob url. By default, it should be the same as your WebApi URL. WebhookUrl should contain your Webhook API Url. By default, it is https: app.&lt;hostname&gt; webhooks ServerStatusUrl should be https: app.&lt;hostname&gt; api status ServerLoggingUrl should point to your WebApi. By default, it should be https: app.&lt;hostname&gt; api AuthServerUrl should point to your Authentication API. By default, you can find the Authentication API at ttps: app.&lt;hostname&gt; auth Agent Authentication _BY DEFAULT, THE AGENT IS ALREADY SETUP TO RUN ON A DEFAULT ON-PREMISES CRAWLER. CHANGING A TOKEN IS RECOMMENDED BUT NOT NEEDED TO GET THE AGENT COMMUNICATING WITH THE SERVER_ The Agent that you have downloaded has to authenticate against a registered agent within the CluedIn Server. You can do this by port-forwarding to the CluedIn SQL Server pod within your Kubernetes Cluster (kubectl port-forward -l app=sqlserver 1433 --address 0.0.0.0) and adding an agent registration to DataStore.Db.OpenCommunication &gt; dbo.Agent`, or just fill out the SQL query below and execute against the DataStore.Db.OpenCommunication database: DECLARE @AgentId varchar(60); SET @AgentId = '6CF17140-0FB0-47C5-AAAA-9A40A0ECF8BA'; DECLARE @AgentGroup varchar(60); SET @AgentGroup = '612ed11a-b1b3-463f-b4a5-7c1bb7bd55a0'; DECLARE @AgentToken varchar(60); SET @AgentToken = '--- INSERT YOUR API TOKEN HERE ---'; DECLARE @OrganizationId varchar(60); SET @OrganizationId = (SELECT Id AS OrganizationId FROM dbo.OrganizationProfile WHERE OrganizationName = '--- INSERT YOUR CLIENT ID HERE ---'); DECLARE @DateTimeMin varchar(60); SET @DateTimeMin = (select cast(-53690 as datetime)); UPDATE dbo.Agent SET AccountId = @OrganizationId, ApiKey = @AgentToken, LastPing = @DateTimeMin WHERE Id = @AgentId; After you have done this, copy the API Token you entered above, then set the ApiKey value in the container.config of the file you downloaded above. Deploying Crawlers In order to deploy a crawler into an agent you will need to have the crawler assembly files (Dll’s) either from a Nuget Package or by compiling and building a crawler locally on your developer machine. Assemblies required: CluedIn.Crawling.CrawlerName CluedIn.Crawling.CrawlerName.Core CluedIn.Crawling.CrawlerName.Infrastructure !!! Note, that Provider project is not required in the agent because the Provider is registereted in the cloud WebApi instance. Everything that is contained in the Provider project will be executed from the cluster itself !! Ensure that all of the dependencies needed by your crawler are also deployed along with the crawler assemblies e.g. Nuget Dependencies. Crawler assemblies needs to be moved into &lt;agent-root&gt; Agent folder and will be picked up once the Agent is started. On the cluster, you need to deploy all your crawlers packages above plus the Provider project’s NuGet package parts responsible for your crawler that will be executed by the agent. Running the agent Running the agent is as simple as starting boot.sh file. For this, you may need to install something like Cygwin to be able to run .sh files on Windows. You will see an output where your agent is trying to load the assemblies and connect to the cluster. Make sure there is nothing blocking the call getting to the API server networking-wise. You can now login to CluedIn and add your integration and the actual crawling of data will be then done through the Agent instead of the CluedIn server in the Kubernetes cluster. You can also register this as a Windows Service so that it can be automatically restarted if the Windows VM is to restart. You can use this guide here on how to setup a Windows Service: Here ###Enable Verbose Logging By default the Agent will be running with low logging verbosity. To increase this you can set the $env:ASPNETCORE_ENVIRONMENT = “verbose”. You can also make sure this persists on the machine by setting it as a System Variable. You will need to close and restart your Agent and the session of the bash command prompt you are using to invoke the boot.sh file to see the changes take effect."
					}

					
				
			
		
			
				
					,
					

					"docs-crawler-crawler-20validation-20framework-html": {
						"id": "docs-crawler-crawler-20validation-20framework-html",
						"title": "Crawler Validation Framework",
						"category": "",
						"url": " /docs/Crawler/Crawler%20Validation%20Framework.html",
						"content": "Crawler Validation Framework While building new integrations for CluedIn, you will want to make sure that you are building your integrations in the recommended way. The crawler validation framework will help guide a developer to produce a Clue that is of the highest readiness for processing. The validation framework will only run during Debug Developer mode and will not run once deployed to production. The validation framework acts as a guide to warn a developer if they may have forgotten to do something important such as setting a Uri, setting a Name. It might be the case that you don’t haave these properties (which is fine and normal) but it is also one of those pieces that is easy to forget - hence the framework. A developer can supress the validators at a Clue Producer level in the cases where you can confirm that you won’t be able to produce what is expected for a Clue."
					}

					
				
			
		
			
				
					,
					

					"docs-crawler-deploying-20a-20new-20crawler-html": {
						"id": "docs-crawler-deploying-20a-20new-20crawler-html",
						"title": "Deploying a New Crawler",
						"category": "",
						"url": " /docs/Crawler/Deploying%20a%20New%20Crawler.html",
						"content": "You can deploy new crawlers in two main ways: 1: Moving files into the ServerComponent directory and running the respective commands to inject the required data into the Relational Store. 2: Use Docker to layer your crawler ontop of the base CluedIn instance. For the first option, there are 3 main components that will need to be deployed: 1: *.dll files for each of the 4 projects of a Crawler. (You may choose to also take the *.pdb files so that you can debug) 2: You will need to run the provider.sql file in your Provider project as to instruct CluedIn that there is a new provider that is available. (Make sure the “Active” flag is set to true so that your integration is now available to add). 3: You will need to deploy your additional files for the web application e.g. Logo and some user interface mappings. Note: You will need to reboot the CluedIn host to properly pick up these changes. The second (and recommended) approach is to create a Docker container out of your crawler and then to change your Docker Compose file to reference your new Crawler. This will handle the entire deployment process for you and will even reboot the CluedIn host for you. This approach is recommeneded as it will also help to roll this new Crawler out to test and production environments with full capabilities of roll back and uninterrupted deployments."
					}

					
				
			
		
			
				
					,
					

					"docs-crawler-mapping-20encryption-20levels-20and-20integration-20location-html": {
						"id": "docs-crawler-mapping-20encryption-20levels-20and-20integration-20location-html",
						"title": "Mapping Encryption Levels and Integration Locations",
						"category": "",
						"url": " /docs/Crawler/Mapping%20Encryption%20Levels%20and%20Integration%20Location.html",
						"content": "Mapping Encryption Levels and Integration Locations"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-consume-html": {
						"id": "docs-h-consume-consume-html",
						"title": "Consume",
						"category": "",
						"url": " /docs/H-Consume/Consume.html",
						"content": "###Consume"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-data-marts-data-20marts-html": {
						"id": "docs-h-consume-data-marts-data-20marts-html",
						"title": "Data Marts",
						"category": "",
						"url": " /docs/H-Consume/Data-Marts/Data%20Marts.html",
						"content": "Data Marts allow you to prepare certain data sets for future consumption. Unlike a Stream, a Data Mart is not necessarily yet data that is being made available to any consumers, but at the same time you might find that a Data Mart is consumed multiple times. The idea behind a Data Mart in CluedIn is to logically group a certain collection of data in which you can apply operations directly to just that data and nothing else. Examples include: Being able to have Data Quality Metrics on a particular subset of data. Being able to Clean a paritcular subset of data. Being able to expose what the data in a Data Mart is being used for (Showcasing) Data Marts can be created in a way that they can be static snaphots of data, or live and dynamic sets of data. There are times when you might want to setup temporal based Data Marts e.g. 2019 Customers. There are other times when you will want the Data Marts to be more “live” and when underlying data changes or is added, you want the Data Mart to relfect this e.g. Customers."
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-export-targets-export-20targets-html": {
						"id": "docs-h-consume-export-targets-export-20targets-html",
						"title": "Export Targets",
						"category": "",
						"url": " /docs/H-Consume/Export-Targets/Export%20Targets.html",
						"content": "Export Targets are used to authenticate with an integration point with the intention that CluedIn will make data available to this consumer via a push mechanish that CluedIn calls Outgoing Streams. Good examples include: Business Intelligence Tools Data Warehouse Dimension Tables Machine Learning Platforms Custom Applications Databases Here are two examples of how to build a new Export Target: SQL Server Snowflake"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-graphql-add-20graphql-20entity-20type-20resolvers-html": {
						"id": "docs-h-consume-graphql-add-20graphql-20entity-20type-20resolvers-html",
						"title": "Add GraphQL Entity Type Resolvers",
						"category": "",
						"url": " /docs/H-Consume/Graphql/Add%20GraphQL%20Entity%20Type%20Resolvers.html",
						"content": "You can add your own specific resolvers to fetch data given filters such as what entity type a record is. Here is an example of how to return Calendar Events in a different way through the GraphQL endpoints. using System.Linq; using System.Threading.Tasks; using CluedIn.Core.Data; using CluedIn.Core.GraphQL.Builders; using GraphQL.Types; namespace Custom.GraphQL.Types.Specific { &lt;summary&gt;The entity graph type.&lt; summary&gt; &lt;seealso cref=\"Entity\" &gt; &lt;seealso cref=\"IEntity\" &gt; public class CalendarEventEntityGraphType : ObjectGraphType&lt;CalendarEventEntity&gt;, IComplexGraphType&lt;CalendarEventEntity&gt; { public CalendarEventEntityGraphType(ICluedInData data) { this.Name = \"Calendar_Event_Entity\"; this.Field&lt;ListGraphType&lt;EntityInterface&gt;&gt;() .Name(\"attendees\") .Resolve(ctx =&gt; ctx.GetDataLoader(async ids =&gt; { var ast = ctx.FieldAst; ast.SelectionSet.Selections; var authors = data.GetEdgesOfType(ids, EntityEdgeType.Attended); var lookup = authors.SelectMany(f =&gt; f.Endpoints.Select(ff =&gt; new { Key = f.ContextEntityId, Endpoint = ff })) .ToLookup(x =&gt; x.Key, x =&gt; TypedEntityConverter.CreateSpecificType(x.Endpoint)); return await Task.FromResult(lookup); }).LoadAsync(ctx.Source.Id)); EntityInterface.ConfigureInterface(this, data); this.ConfigureFields(CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInEvent); this.Interface&lt;EntityInterface&gt;(); } } } Here is a more complex example: using System.Linq; using CluedIn.Core.Data; using CluedIn.Core.Data.Parts; using CluedIn.Core.GraphQL.Builders; using CluedIn.Core.GraphQL.Types.Specific; using GraphQL.Types; namespace Custom.GraphQL.Types.Specific { &lt;summary&gt;The entity graph type.&lt; summary&gt; &lt;seealso cref=\"Entity\" &gt; &lt;seealso cref=\"IEntity\" &gt; public class FilesFileEntityGraphType : ObjectGraphType&lt;FilesFileEntity&gt;, IComplexGraphType&lt;FilesFileEntity&gt; { public FilesFileEntityGraphType(ICluedInData data) { this.Name = \"Files_File_Entity\"; this.Field&lt;ListGraphType&lt;EntityInterface&gt;&gt;() .Name(\"attendees\") .Resolve(ctx =&gt; ctx.GetDataLoader(async ids =&gt; { var ast = ctx.FieldAst; ast.SelectionSet.Selections; var authors = data.GetEdgesOfType(ids, EntityEdgeType.Attended); var lookup = authors.SelectMany(f =&gt; f.Endpoints.Select(ff =&gt; new { Key = f.ContextEntityId, Endpoint = ff })) .ToLookup(x =&gt; x.Key, x =&gt; x.Endpoint); return lookup; }).LoadAsync(ctx.Source.Id)); EntityInterface.ConfigureInterface(this, data); this.ConfigureFields(CluedIn.Core.Data.Vocabularies.Vocabularies.CluedInFile); this.Interface&lt;EntityInterface&gt;(); } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-graphql-graphql-20actions-html": {
						"id": "docs-h-consume-graphql-graphql-20actions-html",
						"title": "GraphQL Actions",
						"category": "",
						"url": " /docs/H-Consume/Graphql/GraphQL%20Actions.html",
						"content": "CluedIn supports GraphQL actions so that you can run commands in bulk from our GraphQL endpoint. You will need to be in the Admin role to even see these commands as they allow you to run operations in bulk. Split Entities in Bulk. { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { splitEntities } } } } Delete Entities in Bulk. { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { deleteEntity } } } } Run Post Processing { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { postProcess } } } } Run Entity Metrics Processing { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { processEntityMetrics } } } } Run Edge Processing { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { processEdges } } } } Run Enrichment { search(query: \"user.firstName:Tim\", pageSize: 4) { entries { name actions { enrich } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-graphql-graphql-html": {
						"id": "docs-h-consume-graphql-graphql-html",
						"title": "GraphQL",
						"category": "",
						"url": " /docs/H-Consume/Graphql/GraphQL.html",
						"content": "CluedIn provides GraphQL as its way to pull and query data from it. The CluedIn GraphQL endpoint uses a combination of the different datastores to service the result of the query in question. You might find that a particular GraphQL query uses the Search, Graph and Blob Datastore to render the results. This is due to the query optimiser of CluedIn that determines the right datastore to serve the different parts of your query. This also allows immense flexibility with querying the data. An example would be that if we wanted to find all entities that are of a specific Entity Type and have a particular value for a property then you will find that the Search Store will service both these parts of the query and hence CluedIn will only ask it to service the query. If you then ask it to run this query, but return the full history of the records then CluedIn will run the search against the Search Store, but then using the results from the Search it will then ask the Blob Store to fetch the full object history out if it. Likewise, if you asked it to also return the records that are connected to these results of type Person, then it will most likely ask the Graph Store to fulfil that part of the query. The GraphQL endpoint has many different operations, including the ability to: Lookup entities by Id Lookup entities using a Full Text search Lookup entities using property value matches. Lookup Metrics of Data All value lookups are case sensitive by default and hence if you were to use our GraphQL search point to lookup organisation.industry:Banking then if you had a value in an entity that was “banking” (note the lower case “b”) then you would not match this data. Although this behaviour can be changed, there is a better way to handle this. One of the main ideas behind CluedIn is that we are going to give downstream consumers a standard representation of the data and hence “banking” and “Banking” are different variations of essentially the same value. We would rather that you use CluedIn Clean to normalise these values and standardise as a business on the way that you will represent values for downstream consumers. It is perfectly fine to not propagate these changes back to the sources using the Mesh API, but downstream consumers should receive a standardised representation of values. If you decide that you would like to enabled case incentive values, you will need to extend the inbuilt ElasticEntity model within CluedIn and add in your own properties with their respective analysers to achieve that. To help you get upskilled on our GraphQL implementation, here are some examples for you to play with. Get an entity by Id { entity(id:\"e58f33ea-a916-500a-8c1b-4053bd042ace\") { name } } Get entities by search { search(query:\"Test\") { entries { name } } } Get entities by particular vocabulary keys { search(query:\"user.firstName:John\") { entries { name } } } Get entities by a combination of vocabulary keys { search(query:\"+user.firstName:John +user.lastName:Nesbitt\") { entries { name } } } Get all entities that have a value for a certain property { search(query:\"user.firstName:*\") { entries { name } } } Get entities, 4 records at a time { search(query:\"user.firstName:*\", pageSize:4) { entries { name } } } Change what properties come back in the results { search(query:\"user.firstName:*\", pageSize:4) { entries { name createdDate displayName properties } } } Change what metadata comes out of the properties { search(query:\"user.firstName:*\", pageSize:4) { entries { name properties(propertyNames:[\"user.firstName\"]) } } } Explore Edges { search(query: \"user.firstName:*\", pageSize: 4) { entries { name edges { outgoingOfType(entityType: \" Organization\", edgeType: \" PartOf\") { entries { name } } } } } }"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-outgoing-streams-outgoing-20streams-html": {
						"id": "docs-h-consume-outgoing-streams-outgoing-20streams-html",
						"title": "Streams",
						"category": "",
						"url": " /docs/H-Consume/Outgoing-Streams/Outgoing%20Streams.html",
						"content": "It is often the case the introducing new platforms like CluedIn can be thought of as quite “disruptive” i.e. our teams need to learn a new system and query language to be able to interact with the data. Although we offer our GraphQL api to “pull” data from CluedIn, we often recommend and prefer that you “push” the data from CluedIn to a target consumer. We call these out “Outgoing Streams” in which a CluedIn user will specify the filter of data that a consumer needs and a target and target model and CluedIn will open up a “long-running” stream of data that matches that filter in CluedIn today and in the future until you turn the stream off. You may want to be able to push data to a SQL Database. This screenshot shows that you can setup a Filter. The Filter is your predicate or query for what data you are wanting to push out on this stream. Because CluedIn persists data, this will fetch historical data as well as new records that match this predicate. The Rules part of the user interface allows you to apply transformations to the data on the way out of the stream. Imagine a situation where you would like to Mask the data as it goes out on the stream - this will allow you to do that. The Export Target Configuration is your ability to control the projection that happens on the stream i.e. what properties from a record you push to the target. The Data tab shows you a history of the records that have been sent out on this stream so that you can trace the full lineage of the platform."
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-rest-api-rest-20api-html": {
						"id": "docs-h-consume-rest-api-rest-20api-html",
						"title": "Rest API",
						"category": "",
						"url": " /docs/H-Consume/Rest-API/Rest%20API.html",
						"content": "The CluedIn Web User Interface is driven off the Rest API. This means that whatever you can do the User Interface can also be automated and programmatically called from systems. Everything from creating new accounts, to adding integrations, to running searches can all be run through the Rest API. This can be very handy if you are wanting to completely automate the deployment process of your CluedIn account. For example, if you had a task to add 300 SharePoint instances to CluedIn then you can use the Rest API with your scripting language of choice to automate this. You can view the full REST API Here: Postman Collection Post a Clue to CluedIn Our Rest API allows you to post data to it in the form of a Clue. To do this, you can implement a HTTP POST to the API endpoint like below. curl --location --request POST 'api v1 clue?save=true' \\ --header 'Content-Type: application json' \\ --header 'Authorization: Bearer ' \\ --data-raw '{ \"clue\": { \"attribute-organization\": \"9eaf94b3-46c6-431f-8df5-3682590048c9\", \"attribute-origin\": \" Person#HubSpot:27493554\", \"attribute-appVersion\": \"1.8.0.0\", \"clueDetails\": { \"data\": { \"attribute-origin\": \" Person#HubSpot:27493554\", \"attribute-appVersion\": \"1.8.0.0\", \"attribute-inputSource\": \"cluedin\", \"entityData\": { \"attribute-origin\": \" Person#HubSpot:27493554\", \"attribute-appVersion\": \"1.8.0.0\", \"attribute-source\": \"rest\", \"entityType\": \" Person\", \"propuser.jobTitle\": \"CEO\", \"codes\": [\" Person#HubSpot:27493554\"], \"properties\": { \"attribute-type\": \" Metadata KeyValue\", \"user.jobTitle\": \"CEO\" } } } } } } ' Posting Raw Data to CluedIn where annotations already exist in the platform. There are times when you would like to send or post data to CluedIn but those records are not Clues or in the Clue structure. For these cases, you can post your raw data to CluedIn in which if you already have an annotation in CluedIn to map that data, CluedIn can convert them into Clues for you. Here is a example of how you could post JSON to CluedIn and it turns it into Clues. curl --location --request POST ' api admin entity ProcessClue' \\ --header 'Authorization: Bearer ' \\ --header 'Content-Type: application json' \\ --data-raw ' { \"occurred_at\": 1382568826000, \"highlight\": \"accepted\", \"primary_resources\": [ { \"story_type\": \"feature\", \"name\": \"Reactor leak reported in Detention Block AA-23\", \"url\": \"http: story show 563\", \"id\": 563, \"kind\": \"story\" } ], \"changes\": [ { \"story_type\": \"feature\", \"name\": \"Reactor leak reported in Detention Block AA-23\", \"new_values\": { \"accepted_at\": 1382568827000, \"before_id\": 6024, \"after_id\": 559, \"current_state\": \"accepted\", \"updated_at\": 1382568826000 }, \"original_values\": { \"accepted_at\": null, \"before_id\": 6643, \"after_id\": 565, \"current_state\": \"delivered\", \"updated_at\": 1382568825000 }, \"id\": 563, \"change_type\": \"update\", \"kind\": \"story\" } ], \"message\": \"Darth Vader accepted this feature\", \"project_version\": 1037, \"performed_by\": { \"name\": \"Darth Vader\", \"initials\": \"DV\", \"id\": 101, \"kind\": \"person\" }, \"guid\": \"99_1037\", \"project\": { \"name\": \"Death Star\", \"id\": 99, \"kind\": \"project\" }, \"kind\": \"story_update_activity\" }'"
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-search-configuration-20search-20relevance-html": {
						"id": "docs-h-consume-search-configuration-20search-20relevance-html",
						"title": "Configuration Search Relevance",
						"category": "",
						"url": " /docs/H-Consume/Search/Configuration%20Search%20Relevance.html",
						"content": "The full text search component of CluedIn can be tweaked for relevance and ordering. This can be done by utilising the CluedIn Rest API and calling the Search Settings Endpoint with the appropriate scores and values. The range of scoring can be made between the value 0 and 10 with 0 being a low boost and 10 being the highest boost. For example, if you wanted to naturally bost records that were more connected in the graph, then you could put a stronger weight on that component."
					}

					
				
			
		
			
				
					,
					

					"docs-h-consume-search-running-20searches-html": {
						"id": "docs-h-consume-search-running-20searches-html",
						"title": "Running Searches",
						"category": "",
						"url": " /docs/H-Consume/Search/Running%20Searches.html",
						"content": "CluedIn allows you to run searches on the data that exists within your account that (by default) is not only from External Data Sources or that is not a Shadow Entity. The search supports Full Text Querying with a Fuzzy Search and an Exact Match. To run a Fuzzy Search, simply type the word or words in that you want to search for. For exact match searches, you will need to surround your search with “” e.g. “CluedIn Sales”. The results that are returned are sorted by Relevance by default and one can change the sort order to either New or Old. The New or Old sort order is based off a rule that is as follows: If the record does not have a created or modified date, then it will use the date that CluedIn first saw the record. If the record has a Modified date, created date and discovery date, then the one that is the highest of those three will be the one that CluedIn will user to sort by. You have many ways to display your results, in the form of different “views”. The inbuilt filters can be used to filter your data even more. You can apply multiple filters. You can also run advanced searches will the ability to filter on very specific Vocaularies."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-entity-page-all-20properties-html": {
						"id": "docs-u-user-interface-entity-page-all-20properties-html",
						"title": "All Properties",
						"category": "",
						"url": " /docs/U-User-Interface/Entity-Page/All%20Properties.html",
						"content": "This view will show you all meta data properties from all the source systems that have composed this record (entity). You can filter this list by the source system, the vocabulary groupings or the name of the property itself. By clicking on the “Clock” you can view the list of historical values for a particular property. For example, you could imagine that you might have the website for a company from multiple sources. Clicking on the clock icon will give you the list of all of those permutations of that value that have been mapped into the website Vocabulary. You can also see your data quality metrics that run at a property level in this view. This allows you to see the overall quality level of an individual property (such as the website) and you can even see the individual quality scores of each value. If the Pencil icon is enabled, this means that you can edit this value from CluedIn and once submitted, this will generate the subsequent mesh api commands that will need to run against the source systems to proliferate this value into all systems where this value was retrieved in the first place. The Mask icon will allow you to anonymise the values in a particular property and will also generate the underlying Mesh commands that will be needed to anonymise these values in the source system as well."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-entity-page-entity-20page-html": {
						"id": "docs-u-user-interface-entity-page-entity-20page-html",
						"title": "Entity Page",
						"category": "",
						"url": " /docs/U-User-Interface/Entity-Page/Entity%20Page.html",
						"content": "The entity page will show you a single view of the record, all metadata associated to this record and all records that have a connection to this record. The entity page is composed of many “tabs”. The Overview tab is a view of the consolidated view of the record where it shows the most statistically accurate values of properties from across the different systems that ingested this data. The “All Properties” tab will show you a meta data view of the record where you can filter and view the history of records by a property level. The “History” tab will show you a list of all the Clue objects that composed this record. This could be a combination of internal data sources as well as external data sources that have been enabled. The “Pending Changes” window will show Mesh Commands that are ready to run for this particular entity. The entity page allows you to show the “Suggested Searches” feature of CluedIn so that you can show widgets on the page that show related data. This data may be direction relations or very sophisticated queries. You will also note that your “entity-level” data quality scores are shown. There are many actions that you can run from the entity page. These actions are located under the “More” dropdown. These actions include: Select records to merge Mark this record as sensitive Minimize this record to only have the metadata that you have consent for Anonymise this entire record and all places where it is referenced. Remove this record from any automated processing in CluedIn or other third party systems. Manually reprocess this record through all the external data providers enabled in your account (e.g. Google Places) Manually reprocess this record through the processing pipeline of CluedIn. You can also see all of the “child” entities of this record in the “Parent Aggregation” component. On the Entity Page you will be able to see your visual view of a Record. CluedIn will automatically choose which properties from which systems are the most accuract and hence are the ones that we will natively display in the user interface as the “Golden Value”. CluedIn has 3 possible mechanisms to choose the “Golden Value”. 1: Latest Modified Date 2: Highest mean value across metrics 3: Highest mean value across metrics combined with the trust level set at the integration level."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-entity-page-history-html": {
						"id": "docs-u-user-interface-entity-page-history-html",
						"title": "History",
						"category": "",
						"url": " /docs/U-User-Interface/Entity-Page/History.html",
						"content": "For data lineage reasons, it is important to be able to show and describe where your data has come from. In CluedIn, you can see this at an entity level, but also at a property level. This is important for being able to have full trust and audit trail of where a particular value for a particular record has come from. This helps us answer, common lineage questions such as: Why are we showing “Brisbane” as the city for CluedIn Headquarters in our BI tool? Where did we get the information on the annual revenue of Lego from? How many systems do we have that are all reporting that CluedIn has 40 employees? Your history view can be your guide to show the full audit trail of these questions. You will be able to see where the data came from, what the data was and when it was received (updated, created)."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-entity-page-pending-20changes-html": {
						"id": "docs-u-user-interface-entity-page-pending-20changes-html",
						"title": "Pending Changes",
						"category": "",
						"url": " /docs/U-User-Interface/Entity-Page/Pending%20Changes.html",
						"content": "This view will list all Mesh Commands that are waiting for an “approve” or “reject” action to be taken for this record only. If you would like to see a list of all Mesh Commands then you can head to the Mesh Center to be able to do this."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-followed-entities-followed-20entities-html": {
						"id": "docs-u-user-interface-followed-entities-followed-20entities-html",
						"title": "Followed Entities",
						"category": "",
						"url": " /docs/U-User-Interface/Followed-Entities/Followed%20Entities.html",
						"content": "There are some records that might be regularly accessed and hence, the followed entities allows you to save links to these records so you can easliy recall them. To follow a record, navigate to the record you want, for example by searching and then open the record and click on the “Follow” button. This will populate your Followed Entities list."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-keep-in-the-loop-keep-20in-20the-20loop-html": {
						"id": "docs-u-user-interface-keep-in-the-loop-keep-20in-20the-20loop-html",
						"title": "Keep in the loop",
						"category": "",
						"url": " /docs/U-User-Interface/Keep-in-the-loop/Keep%20in%20the%20loop.html",
						"content": "You may have frequent searches that you run in your CluedIn account. For this, you can use the “Keep in the loop” feature to run a search and then click the “Keep Me in the Loop” button. There are some restricted searches that cannot be subscribed too. This includes empty searches or the “*” search."
					}

					
				
			
		
			
				
					,
					

					"docs-u-user-interface-user-20interface-html": {
						"id": "docs-u-user-interface-user-20interface-html",
						"title": "User Interface",
						"category": "",
						"url": " /docs/U-User-Interface/User%20Interface.html",
						"content": ""
					}

					
				
			
		
	};
</script>
<script src="http://documentation.cluedin.net/beta-docs/versions/3.2.3/scripts/lunr.min.js"></script>
<script src="http://documentation.cluedin.net/beta-docs/versions/3.2.3/scripts/search.js"></script>

			</article>
				<aside id="sticky-menu">
					<ul>
							<li>
									For version 3.2
							</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="/tree/master/search.html"
								>
									<i class="fab fa-github"></i>
									Edit this page
								</a>
						</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="https://twitter.com/CluedInHQ"
								>
									<i class="fab fa-twitter"></i>
									Follow us
								</a>
						</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="https://www.facebook.com/cluedinhq/"
								>
									<i class="fab fa-facebook"></i>
									Like us
								</a>
						</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="https://www.linkedin.com/company/cluedin-aps/"
								>
									<i class="fab fa-linkedin-in"></i>
									LinkedIn
								</a>
						</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="https://vimeo.com/cluedin"
								>
									<i class="fab fa-vimeo"></i>
									Vimeo
								</a>
						</li>
						<li>
								<a
									target="__blank"
									rel="noopener"
									href="https://www.youtube.com/channel/UC3jeVGsDC97C81ISJLUhszQ"
								>
									<i class="fab fa-youtube"></i>
									Subscribe
								</a>
						</li>
					</ul>
				</aside>
			</main>
		</section>

		<script>
			document.getElementById("open-nav").addEventListener("click", function () {
				document.body.classList.toggle("nav-open");
			});
		</script>

    <script async src="http://documentation.cluedin.net/beta-docs/versions/3.2.3/scripts/prism.js"></script>
	</body>
</html>
